{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Using multilayer perceptrons to approximate the solution of partial differential equations\n",
    "\n",
    "## Import libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from pdeUtils import TrialFunctionDataBasedLearning, TrialFunctionResidualBasedLearning, PureNetworkLearning, CustomFunctionLearning, SimpleMLP\n",
    "from visuals import plot_field_and_error\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib import cm\n",
    "import autograd.numpy as np\n",
    "import pandas as pd\n",
    "from mpl_toolkits.mplot3d import Axes3D\n",
    "import time\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style>\n",
       "    .dataframe thead tr:only-child th {\n",
       "        text-align: right;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: left;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>x</th>\n",
       "      <th>y</th>\n",
       "      <th>A</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>9995</th>\n",
       "      <td>0.955</td>\n",
       "      <td>0.995</td>\n",
       "      <td>0.001700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9996</th>\n",
       "      <td>0.965</td>\n",
       "      <td>0.995</td>\n",
       "      <td>0.001311</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9997</th>\n",
       "      <td>0.975</td>\n",
       "      <td>0.995</td>\n",
       "      <td>0.000930</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9998</th>\n",
       "      <td>0.985</td>\n",
       "      <td>0.995</td>\n",
       "      <td>0.000556</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9999</th>\n",
       "      <td>0.995</td>\n",
       "      <td>0.995</td>\n",
       "      <td>0.000185</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "          x      y         A\n",
       "9995  0.955  0.995  0.001700\n",
       "9996  0.965  0.995  0.001311\n",
       "9997  0.975  0.995  0.000930\n",
       "9998  0.985  0.995  0.000556\n",
       "9999  0.995  0.995  0.000185"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# define data path\n",
    "fullDataPath = \"./fullRawData/\"\n",
    "internalDataPath = \"./internalRawData/\"\n",
    "fieldName25 = \"A25.dat\"\n",
    "fieldName50 = \"A50.dat\"\n",
    "fieldName100 = \"A100.dat\"\n",
    "\n",
    "# skip column 2 - z is constant (2D)\n",
    "colNames = [\"x\", \"y\", \"z\", \"A\"]\n",
    "columns = [0, 1, 3]\n",
    "\n",
    "# read full numerical solution (interal field + boundaries)\n",
    "field25 = pd.read_csv(fullDataPath + fieldName25, sep='\\t', names=colNames, header=0, usecols=columns)\n",
    "field50 = pd.read_csv(fullDataPath + fieldName50, sep='\\t', names=colNames, header=0, usecols=columns)\n",
    "field100 = pd.read_csv(fullDataPath + fieldName100, sep='\\t', names=colNames, header=0, usecols=columns)\n",
    "\n",
    "# read numerical solution only for internal points\n",
    "intField50 = pd.read_csv(internalDataPath + fieldName50, sep='\\t', names=colNames, header=0, usecols=columns)\n",
    "intField100 = pd.read_csv(internalDataPath + fieldName100, sep='\\t', names=colNames, header=0, usecols=columns)\n",
    "intField100.tail()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Visualize numerical/training data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAArMAAAD3CAYAAAAOsdxIAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\n2SKLWRkZGRkZGRkZmWbL/wMR2cCKkcnogwAAAABJRU5ErkJggg==\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x7f0eecf3c5c0>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "fig = plt.figure(figsize=(12, 4))\n",
    "ax1 = fig.add_subplot(1, 2, 1, projection='3d')\n",
    "ax1.plot_trisurf(field25.y, field25.x, field25.A, cmap=cm.coolwarm, linewidth=0.0, antialiased=False)\n",
    "ax1.scatter(field25.y, field25.x, field25.A, s=2, c='k', depthshade=False)\n",
    "ax1.set_title(r'25x25 grid solution')\n",
    "ax2 = fig.add_subplot(1, 2, 2, projection='3d')\n",
    "ax2.plot_trisurf(field50.y, field50.x, field50.A, cmap=cm.coolwarm, linewidth=0.0, antialiased=False)\n",
    "ax2.scatter(field50.y, field50.x, field50.A, s=2, c='k', depthshade=False)\n",
    "ax2.set_title(r'50x50 grid solution')\n",
    "\n",
    "x_domain = [0.0, 1.0]\n",
    "y_domain = [0.0, 1.0]\n",
    "\n",
    "for ax in [ax1, ax2]:\n",
    "    ax.set_xlim(y_domain[0], y_domain[1])\n",
    "    ax.set_ylim(x_domain[0], x_domain[1])\n",
    "    ax.set_zlim(0, 1)\n",
    "    ax.view_init(30, 120)\n",
    "    ax.set_xlabel(r\"$y$\")\n",
    "    ax.set_ylabel(r\"$x$\")\n",
    "    ax.set_zlabel(r\"$c_{num}$\")\n",
    "fig.subplots_adjust(wspace=0.05, hspace=0)\n",
    "fig.savefig('mesh_resolution.pdf', bbox_inches='tight')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "## Visualize the trail function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Initialized weights and biases for MLP with 20 parameters.\n",
      "\n",
      "MLP structure:\n",
      "--------------\n",
      "Input features:    2\n",
      "Hidden layers:     1\n",
      "Neurons per layer: 5\n",
      "Number of weights: 20\n"
     ]
    }
   ],
   "source": [
    "baselineMLP = SimpleMLP(name='baselineMLP', number_of_inputs=2, neurons_per_layer=5, hidden_layers=1)\n",
    "dataBasedModel = TrialFunctionDataBasedLearning(name=\"trialFunctionDataBasedLearning\", mlp=baselineMLP, training_data=intField50.A, beta=25.0, d_v=0.025)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAuoAAAD3CAYAAABLsHHKAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz\nAAALEgAACxIB0t1+/AAAADl0RVh0U29mdHdhcmUAbWF0cGxvdGxpYiB2ZXJzaW9uIDIuMS4wLCBo\ndHRwOi8vbWF0cGxvdGxpYi5vcmcvpW3flQAAIABJREFUeJzsvXuQXcl9Hvb1ed3HPDGDxy6AXWAG\nwArAckUu9kGClClbBBNG5RITBzSjlFSRU9ayYpVLTjnBhuWKUmVFpnfLxZLtuFy7dFJUKCUhF5Es\nFSsyuVBoSuFzd2EtXyK3iJnBDN6YJ2bmvs6j88c53be7T59zH3PuzGC2v6qpufc8+vR53D5ff/31\n70copTAwMDAwMDAwMDAw2F2wdroCBgYGBgYGBgYGBgZpGKJuYGBgYGBgYGBgsAthiLqBgYGBgYGB\ngYHBLoQh6gYGBgYGBgYGBga7EIaoGxgYGBgYGBgYGOxCGKJuYGBgYGBgsCMghJwjhFwihJzb6brs\nRgzi+phr/nDBEPWHCObHtTXsxutHCLmY1Gl8p+tSBHbjNTYwEEEImSaEvEQIocmz+kLy/yVCyMUC\nyn+LEHKhl/psZf9ekZznC4SQS4M6Rhd14OdMKb0K4ASAZ3eqPoPEVu/nIK5Pv2Vu97NqEMMQ9YJA\nCLmQPLSs4b9GCHldeAlcUx9yYd+uHvZuf1zb1RDvth+t7vi7/YVAKb0M4FMAJna6Lnko8hnN+h0Y\nGGwHKKUzAD6TfH6ZUvpq8v9FAM8V0G5+glJ6pZsNkw6t2qntev9eQQh5AcASgC8BuDqIY3RRB905\nr+5EXbYJRdzPa4XUZAtlbvezatCGs9MV2EOYBvARSukqABBCPgpghlL6avL9CoAs1fQTycujG+T+\nuJSGeGCENPnRTgMQ693LeQwC0vEz6rgbXwi7sU4qerm3meeTcU8MDHYFKKUvEkJWCCGX+23Lut0v\nGUX7NIAv9rN/nxgHsJq8p7adYGWd817GDr8TC8EOPasGCYyiXhyWGUnXIVEatappwQ87b4gHqMqw\nH62Enf7RKiRdW0eD/lDEvTX3xOAhwRUAFwE+UvpCYlF7KVl2MSHzryXfx5MR05cS69dbiWACoQxW\njqjWP4u4vf5osm46Y/9zyXpWxrRQ7jVh3UVCyCtZJ5V0kj8qHE/c/6JwPt0e7wIh5JWk3rrz0yF1\nzsK68azz0N0HzfllXg9xnXIPz/VzbhnPRep65tzPS0Idx4X9u72O6rmPK+d9SThW6l7mXTvd9cE2\nP6sGCiil5m8AfwBeA/CSsuwCYkX8BcQvgtcQDyW9BeAFZbsLyXaXlDIuidsq684BeD0p9wXxeMn6\niwBWAJzT1OdCsv4VTZmXhPXjyefXAbyS7DudcR7nhLJfADDd7XGFMlidXxKO8XpSD7bugnp8XR2T\n5S8p56M9bt49y7tHfV7Ta6x+PV63C8n5TWc9L31cR1bORSTPb8a9Zdu9kJR5Qbj22muc8dyMK9dK\nW3/zZ/6K/EueO5qx7hXEbeg0gNeF5fz3xZ57Yd1F4fNLym/lGoBxoewLyrEuKsfn+6t1SJa9pWz7\nirhObEs053ZJqdsrwu/zXJ/HE899pYtrn3XO2vPIuw+asvPKUe/L60jehb2cW4fnQrqemvs5rlzP\nS8K+ec9J5ntfOIZ4Lhe7uJfqs9Dp+mzrs2r+2n/G+rKNoJReIbEF5hlK6acIITOU0qvJMhGvJNtc\nSXr1F2gX6nhS1uuIFXVmubksrL9MCPmUUp/LybHY9p8mhExTSmeSnv7nKKXPJOvYD/tlQsgnEP8o\nefnieSQ96ZcopR8Vlr0lnFfmcZVzukwIeQ7AteT8PgPgkzQevbhMCOEjB+Lxk2Ok6pjgBKX05bzj\nKuVI9yxZpb1HvV7TZP1LwuderxtroC8n31cAvKw5j47XEbEd5RV2bELIBCHkUnK/1efvNUrpvmS7\nawA+ozyjumucuidJ/b9I4xEnkAIm8xkYbBETAN5ATHZWSXtuxjLbgFL6avLcv5j8ZkW/95JS3jMA\nLhBCWNmd5miI+19E2ks+o7wT3hLWLSPbYqnDKhI7ZdIuXOrjeEWNpGadR+Z96LGcfuqgO7e8+kjX\nM1km3s+/DeBN9oW1kQl6fU5EvALg9eT99HrSZndzL7eK7XxW37UwRH370emHDGztB9sPsn48eY2K\nDoP60TKF69XkOx9OUxoc9TpmodfGQnfPOt2jrq6pUD5DP9et2xdl7nVMGvasFxC/tklnQ305TSjn\n0cu9lV4wXZ6LgcGgcAHxZNMLiC2NWaTmSjLsv6wRAwDw38qfIZnjkXSWtSCEnBPalzyMo1iC04n4\nFn08jh7OOe8+7ATy6tPpeqbQy3OSU58TiVXlUyS2Mb2h2a6Qe7mDz+q7EsajvjPI/CELP9irSeO/\n6ydrkO5D8fX1o2Vqd6JcrQL4YqK8dl1WD3XMAr9nBdyjXiO8FNLYdXkdl9nIAKX0MlPulXJWERP6\ncWFZz89pck+WKaUnALwI4ETygjEw2BEkndVXExJyGcqEfKETC8RD+S8ifzI4I/vs98H9yMl31skF\n9O3CZaQjbUxgcBNBt+N4nc5ZV6e8+9AtliC3df0GW9hKfb6E+JlQ9+30nHQCG7W8Sillo+a93stO\n12e3PavvGhiivvuw1R+siq00TlmNCrD9P9ovIrZbXEnK/jQ6R0vp9YXQLbZyj76EtPou3p9BN3Z5\n17GXF9AXAfzthOh/pIfjq/dE94IxMBgYko7qp5PPUhx1xLbBFwHe+XyRCBP/RBU1WX9FXJZ0Pj8J\n4BPJca4g7tReTH5LryEejWN4BcAzye9oRt0/OcZLSR3YJMFPUEpXk20vsG2T8p9FrKimOvZK2ReE\n/T/JhIytHC8ZXRgnnSdC6s45s9xO90E5v7zr8SpiMeBCcuwriNuf8V7OLas+uuupuZ+ryeeXhHq8\nmfecaJ4pHZYQj/BeTMp8pYt7qZaZeX1y7ttAnlUDBb2a2s1f/h/aEwWvIZmAh/ZkFjYp7zW0J5qI\nE/vY5LrXEFsgxAmDF9RtM47N1l9Ilo0L+7MJrK8ly9VjX0A8qfAVtCe1nEN7suBFYfl0sh2btJKq\nW7LPpWSbS5rrkHlczbmNQ5689VrOuU93U8cuj6u7Z13fo4xryq6LOJn0Nc36nq4b4meNImdCZhfX\nUapbzrV9RVj2GtrPW6c6qvfkEtqTVy+ycsyf+TN/5s/8mT/zR0EopTAwMDDoFokaMg3gSzRWS6YR\nd+Y+Q7vzLRoYGBgYGBh0AWN9MTAw6BUfRTzkvwrw4fIvYvCTng0MDAwMDN5V6FVRN/K7gYEBXn75\nZYyPj2NiYgLLy/E82xdeeKHDXgYFgRRQhmnLDQwMDHYWXbXlhqgbGBgYPFwwRN3AwMDg4UdXbbmx\nvhgYGBgYGBgYGBjsQhiibmBgYGBgYGBgYLALYYi6gYGBgYGBgYGBwS6EIeoGBgYGBgYGBgYGuxCG\nqBsYGBgYGBgYGBjsQhiibmBgYGBgYGBgYLALYYi6gYGBgYGBgYGBwS6EIeoGBgYGBgYGBgYGuxCG\nqBsYGBgYGBgYGBjsQhiibmBgYGBgYGBgYLALYYi6gYGBgYGBgYGBwS6Es9MVMHh3IQxDhGEI27Zh\nWRYIITtdJQMDAwODHkEpRRAEAADLskx7bmAwIBiibjBwUEoRRRFarRbCMESr1YJlxYM59+/fxyOP\nPALbtvkfa/BNo29gYGCwu0ApRavVQhRFaDaboJSCEIJarQZKKcbHx6W23BB4A4OtwRB1g4GBUsoV\n9LW1NVy/fh1nz54FIQS2bYNSivn5eRw8eBBhGKb2tywrReBNo29gYGCw/WAKehAE+P73v4+pqSl4\nngdCCCzLwubmJhqNBoaHh+H7PifwAPg2RpAxMOgdhqgbFA5G0IMg4I21ZVlSww2AN9JMXVfLYC8G\n3/el5WKDzz6bRt/AwMCgWIjtMBNTstpstkxdRykFAERRZAQZA4M+YIi6QWEQFRdAbtAZURfx5xPP\nxh+W30yVlUW6WRmsI6DCNPoGBgYGW4M4GhpFEQC5TRaFF9Ymi59FiKq67jhGkDEwyIch6gZbQpbi\nojakYiP+nWMflNZxwq7BhxUS302j7/s+Wq1WSr03jb6BgYFBNnSjoVntOSPw4jIdUc+DEWQMDDrD\nEHWDvtBJcVFhWRaiKMI3pp6HXbER1kPYlVhtZ98Zwnr7BZBF4lUCn3f8vEb/zp07OHLkSIrAm0bf\nwMDg3QJRbBHtilkQSXknRb0f9CPI1Go1hGGIffv2GUHGYE/BEHWDntCt4qJCbMQZKbcrtnZbRuBV\ndEPggd5U+Js3b+LRRx+VXlAMjLA7jsM/m0bfwMBgr4D5xnV2xTzoSHmRRL3TsXXtb61WQ71ex+jo\nqFaF142oGkHG4GGAIeoGXaFXxUUFIQStX/t7cEZtOKMxQQ8etFV0UVUXCbyotGcR+Hi7/lX4rMms\nQPbQq27Y1TT6BgYGux2iIt3NaKgObIRUxHYR9TzkteesU2IEGYOHDYaoG+SiX8VFxZvnfg6WQxAF\ncUNuOQTehMO/A+AEHmiT+E4EPkuVb2/XhQqfQeDF/yI6NfpZBN40+gYGBjsFdTQU6J2gM+ykot4P\n+vXCG0HGYDfAEHWDFBgRXV5ehm3bqFQqWyKa33zP85ykW06Gh10g8YBM2kVCLxJ4HVS/u6jCZ3nh\ne7HRAJ0bfTWCAdsnj8QbGBgYDAJsNPT27ds4cOBA13bFPDxsRD0LRpAxeBhgiLoBh6q43Lt3D0ND\nQxgaGtpSuaUxDzSMtOuCRiip7CJZ15F6kcCr2zNkqfDp7YqfzCr+F5HV6NfrdURRJGXzY6TeNPoG\nBgb9Qh0NvXbtGg4dOlRI2YyUq5G1Hjainod+BJnFxUUcOHAArusaQcagMBiibqD1n4vZQ7eCtz70\nc/wzsdvqNiPuTlkm0sS2QMMIQSOdGIOhE4FXv4teeBHdTGYVSXy/k1mB7EZ/c3MT9XrdZPMzMDDY\nMorwn3eD3epR3w7kCTILCwuYnJxEq9XKVeGNIGPQCwxRfxejk/9c1xj3gu995OfhciJuwxfINyPk\n4ncRTtlObcMgkvg8FZ4tF60zWQS+HTIyXi8q7O1tOttogN5UeEDv+zfZ/AwMDLpFr+Fyt4q9Yn0p\nGpRSTsLV5UBbhTeCjEEvMET9XYZeFJetEnUAsFwbkR8TTbesKtjxdz9HPWcQSbuqwjPoVPgsTzyA\nzMmseT74QYWUPNZHYqe8bH6qh9I0+gYGew/9hsvdKgxRz4bu2me150aQMegGhqi/S9CP4kII0TYe\n3eAHH/sFWG5MahlZZ98ZcWcojXjKcpnAq2o7r1+yXCTwOhVeVeBF6HzuMYFvE/J+Vfg8f7xI4IHB\nJ3YCTKNvYLBXwDrqW43G1S8MUS8GRpAx6AaGqO9xbEVxsSyrr4b3xx//KJySTFLTtBGcuGfBLduw\nXC+TwDMwcq4SerY8S4EH9CRenNzKoLPOiN9VAi9CR+S9CVeJTtObF76fyaxsJEWdABWGIXzfx9jY\nmMnmZ2CwSyESNyag9PobVX3T/cKyLD6nKQxDbvcwRL049CvIbG5uYmRkBJ7nGUFmj8AQ9T2KIuKf\n92N9+fHHPwpiWaBMtU+O6ZTa25CKy9cDQNCUibdI4BlJF9X4tIUG0HngdbC9hDS34m2dsg3bs/l3\nXqcOdhyV1HsTDgBHG4WGH7vLOPBFZ2YF8hv99fV13LlzB+VyOV0Xk83PwGBHUZT/nAkvRfx2KaVY\nWlrCwsICJ+iu66LRaOD27duoVqsYGhqC4xiKUTQ6CTLXr1/H1NSUdrKvrj03gszuh/kV7SEUobiI\n6Ieoe0MltDabnKADAI0i6bsKpr7rCLyquoued/YdkAm8qsKHfvocGDlnxD1VJw2BV8l7Xlx4Xs6o\nrbXXOKN2bjQa3WRWb8LN7AgED9rKSq8qPJsAZdvytTDZ/AwMdg5F+89Ze74Vi0yr1cL169dx8+ZN\njIyM4JlnnuHtx4MHD/DOO+/wmO21Wg1BEMB1XU7c2X/P80w7MQAQQhBFEQ8PydBtYicjyOxOGKK+\nB8Aa9JWVFURRhNHR0UIIU69Efea//JsAALfi8mWiug6k1XO2jQ6qfYajZEvliP539h3obK2xXStF\n4rOIO9CexKqq8kDniaxqVtYooFobTV5GVtWSI5J2lcSzY3RD4AHgyBtfTC3rd+jVZPMzMOgfTGyZ\nn5/H4cOHC/OfbyU4QK1Ww+zsLNbW1vD444/jiSeeQKPRgOM48H0fhBBUKhW4rovHHntM2rfVaqFW\nq2Fzc5Or8M1mE5ZlpQh8uVzeVq/9XoSuM7aVxE5GkNl5GKL+EENVXNbX19FqtTA+Pl5I+b007Iyk\nd62eW3r7CyP2rAyqDt9prDQQCH1ro5Wpwusms9puXJ5bdaXljMCLqnq3JD6PwHfKzMrIe/xd8fkL\nCryq5ut89QDgjOp/4iKBB4Cbz30SNzXbFZXYCTDZ/AwM8qDaFefn53H06NHCymdqay9YW1vD7Ows\nWq0Wjh8/jrNnz4IQgrt37yKKoq4SHnmeB8/zUu+lMAw5gWf2u0ajAQAol8sSga9Wq6kRPwM9eh01\nMYLM7och6g8hsmb827a95XCKInoh6m0Sm1abRXSnnrc/+/V84q8q9t6wp92u5evrk6W6ixNQxc9Z\n3vU8C424zVZUeFWBZ2gtB7kWHL31xtEuVwl8PzHhe83mJyqGjuNI6o1p9A32MljHNgiClP+cEeui\nFOZu23NKKRYXFzE3NwfHcXD8+HHs27dP2qaIqC+2bWNkZAQjIyPS8iiK0Gg0sLm5iVqthuXlZdRq\nNURRBM/zUgTe8/Rt/rsVRT0zWxVkKKXwPM8IMgXAEPWHCLoJouKDv1NE/fqvfRx2QnjDhBB7Qx7/\nLBJvkajqJp2KpNtybJRGbERBmtDqCLxK2sXlIoEXvfCihYYp78QmqbLZ9iLxdqtCOQLpVv3vWZ2W\nbuPBqyTcLlkIm0LnZMLJXMdUeJX0q2DLshR4QI5a8xeHngP10y/lPAIv/hfBGv2ZmRmMjo5icnKy\nXS+Tzc9gD0IdDQX07XkYhttG1KMowu3btzE/P4+RkRGcPXsWQ0ND2m1FUi7+touI+sIsMdVqVVpO\nKUWr1eIE/u7du6jVavB9H47j8LZheHgY1WoV5XL5XdtGbEcc/U6CzJtvvolnnnlG2idvVNUgG4ao\n73LkKS4qWMisotANUb/x63+Lk3QG9bu43HZtgcADqgJvOZ2HN9MEXlC8m2n7jdoZEJHywSffdV56\nBm05CukWv2dNWtURedtLT2JlEWz8TR/Eju+7SsjtUrpOdskCRvXnIBJ4HXRqO/PK83UVVlbScXQJ\n/uLQcwAgkfgs8s7PT1APWUgxwGTzM9h7ECf7d5ogul3CSxAEuHHjBm7evIkDBw7g3LlzKJVKmhLk\nstjvUyTsgwzPSAhBqVRCqVTCxMSEtC4IAiwsLGBjYwMrKyu4efMmGo0GCCGc9DMFvlqtGh/8gMCe\nYzHWO0O3tkgjyKRhiPouRT8z/rdbUb/z9z8Jp+yCRu3G2fbiRypstS0UInFnJF1V4LOsI1EgK+yd\n4A15GgXe7op4i2q8q0SgYZ0BtRxVxVfLIpaF0mhbzdfZaPK877ZnIwrj6+sOudI6b4ggCimChi5K\nfT6ybDTBgzA16ZVBN4k1DiWZbkYCP03eWRkfuvld7bFZPGaGLBVezebXqdE3qo3BTqOfcLmDFl6a\nzSauX7+O+/fv48iRI3j/+9/fdThFnd99J+OoO47DCfixY8f48iiKUKvVuBf+/v37qNVqoJSiXC5z\nAs9IvAknWQzUthzo3hZpBJk0zFO5y6BTXLrt/e+Eos5IOrGItMz2HE7a88AUdpW48zo4FiwnbXEJ\nWwEsp62qiyReXR4FoSaCTDbxjs8hrcLTKMqIRCN3BHQTanUKfyfrSxaBt2xlwmjZUb6zcgKuvDPF\nPUuFZ8t1BN5yCFrLMcGIApqK825XrFSmVWfUgV2xJGWdWWe+ceR5voy4Fj449+14vaZx16GTjUbM\n5sd+Q0zhMY2+wXZBTDLWT/zzQQkvm5ubmJ2dxfr6Oh5//HGcPHmyZ4U5y6O+28CsMMPDw9JySika\njQYn8Ldu3cLm5ibCMITrupL6bsJJ9o5u23LACDLdwBD1XYIiEhQxT2NRyCPq9//hr8D2XIQtX7se\niMm7qLYDseKuLrc9R1LggXwVPgop7wSonQGxnCwFni1PE3jZQtMJunLETkJcTpjppc+KbOOUbcn/\nzsvqkIQJaJP4KKScwDtCHiOnDK0Cn0Xo7ZIFGlLJBy/CX0/XiRF5yyEIBaIu+t+DBwHsio0ooPjm\n8Q+09/3J1zueYx46qTbsdyYuz1Jt9mqjbzBYFJmgqMj2vNVq4Z133oFt2zh+/DiefPLJvp/vnVTP\niwALJ1mpVKQ5MUA6nOT8/DxarZYUTpIReTGc5MN8PYpGL0Q9C0aQacMQ9R3EVhUXFdtlfbn/D3+F\n+6Ztz02tB8AJvEjKRdVdXZ5HvFXPu2UTbgdh34FsAq/rCABpUt2ZeNvw636qA6CWIyKLwGfZcRh5\nj6+PUlY5ozOgix6jXCO2LC5H/7MPmy3pu+p7JzYBVcp0R+I6eeMOXyeSd6a2E5dIy9yR2AMbBZSr\n9XbFwl++728AAN7/V9/U1rFfdNPo+74P3/exsbGB1dVVHD16FIQQXL58GRcuXJCG1A0MVBSdoKgI\n4YVSivv372Nubg7NZhNHjx7F1NTUlsoEis1yutvQTTjJBw8e4Pbt2zycJCP9vu9jfX39XR9Osgii\nnodeBJnZ2VkcPnwYpVIJrVYLn//853Hp0qWB1W0QMER9B8Aa9MXFRYyNjQHYGkFn2C7rCyPp6jIa\nCraKhMCry1UFXiTvuuVZ9pmo7ksWkCikqe9Am7CyclQCGwWhRLx1VhoR3lB7kpXY2Wgp5TDkqfpC\nV4J/ihV4wrfpVB/LsSXFXERrM0jZZKR9lWtRGmt76Usj7WRQogKvknU2udWyCULmpR+x+XIACOtR\nyi4jwptwUv73/3D+w7Bsgvf+f1tT2LuB+ttjoU+ZUvaVr3wFH/rQhwZeD4OHE8xO0mq1UK1W+xoN\n1WErCYqiKMKtW7cwPz+PsbExPPnkk7hz5w4qlcqW6wX0F5P9YUencJIbGxu4c+cOFhYWeDjJUqkk\nWWiGhobgunpxqwjslnsShuGO+P11gsz6+jpc14VlWVheXsbXv/51Q9QNsqH6z//qr/4K58+fL0yV\nKNr6ohveXP2tvyup6FnWF0bmRZJObCtTgQ8arUzSLpWbbCNmPwXaVJcp5ypBFVV3cVlppCwt002G\nZYSZWFZmHUUCj5Jg8Wimy8lT5J2SLVtlkrLEcrqB5djw9JHVEDTCVEeGgSWAEqFT4C2boLXp888i\nRJJu2QTVwyXJ6hS2xAnCyn1y5M7W2z/389tC1kX4vg/Xdfnvcm1tLRVH2uDdDXHoPQxDLC8vY2Nj\nAydPnizsGP20577v48aNG7h16xYOHjyIZ555hkdw2QrxV/GwW1+KBLPElEolLCws4OzZswDyw0na\nti1ZaIoKJ8msHzuNQSvqvSAIAh6+c319/aFsyw1R3wYU4T/vBkU2xEDaJrD6W383Xi4oyR2tL7Zq\nn5AVdrbMHSqnlrNyaERTBFnnf7ccW2tDsZFN4HWwbAKr4nYk8CrUCbUA4JTkn1gcw51y4q2SdtH6\nIpbllBRvv0Dgdcq7TonnnYGynfLNdxM5hpF4VgdvyJXq5JSB5oOWdI11900X+UY3sTb0ox0h66xh\nZ1hdXS0s26/Bw40s/7njOIWKJEBvRL3RaOD69etYXFzE0aNHtRFcDFEfLFQrUF44Sd/3eTQaMZyk\nZVmoVCqSAl+pVLrmC0UmyNoKgiDYNUQdaHOZh7UtN0R9QFAVFyA9xM4azqIe6EH6BRlJ1x5XQ1BV\nAs8InajA6yw0KpxKKaXK0zCSyukmbKNbLWl95JJynmGdEb9nWXFUAq92JNg1Egk8I+0AgJLTs2rO\ny7EIaCTXK68kXWdAp5o7ZaC53pJUdt18AxFiKEogO9kTEEe1YcfVTnAVOgbbSdaDIJCSrfi+3zGu\ntMHeRif/edGjmUB3xHpjYwOzs7PY2NjAsWPHcOrUqUyiViRRZ2Xdvn0bi4uLnFRGUbRnveud0Mt5\nu66LsbExbn1l6BROUs3KqnbGiuQTW8FuUtRFrKysGEXdoLcZ/6xx340PNEMURfjhD3+Ix1xHq1ZH\nvp6gqsSUIU+BZyRcJPCMpKuk3vbceHul/MgPurKYtMtx4OomgyrEO8siwnzctufAdtu+7vZ5tctR\nRwZY3Vn4SSfn56gj8VmTc+O6acoqOWhtNnuyGJVG9Om5Jd+6LrqPa4HYRCL56rXRdRSCRgBvSP+M\n/Pg/+ShO/+nrHeu+VaiKusG7F0xs6TQa6jgO36Yo5JH/lZUVzM7OIgxDTE1NYXJysiNJJIQU0pkI\nwxC3bt3C0tISqtUqHnnkEbRaLTx48ADNZhNvvhknOFO92b0oww8jirCddBtO8ubNm6jVaqlwkrZt\nc5FwJztLYRjC8/Tvju1Eyrq7umqI+rsZ/cz4Z437bnigVaytrWF2dhb1eh0/+7Xf18YmBwCrCwKv\nI4Yqkbc9V0v48sI/6mC5scJsuYrSoHQo8iaQRkHIlXO17iLx5iRdJe3C9ywFPmj4qfjwlmNJCZ7E\nZaqFhpfTVMNaZr8obNeSvfRKOWpHwrIJYBPpvNhnp+ykr02HEYE80g4AbtWFW3W1ZVmujcgP8ZO/\n+R/hZ7781dzjbBUsJTlgQq69W8GyQWeNhqoYhKJu2zZ8v93+UUpx7949zM3NoVwu4+TJkxgdzUg9\nrIGYTbQfBEGA+fl53Lp1CwcOHMDY2BhOnTqFZrPJfy9LS0t47rnnEEUR6vU6Njc3sbm5iXv37qFe\nrwMAV4ZFAr+bxapuMUhy3G04yeXlZayvr+ONN96AZVkpBb5SqWwLgd8tAqRaj7W1Nezfv38Ha9Qf\nDFHfIrpVXHQYROO+FVBKsbS0hNnZWdi2jampKTzx//4eUCkj8vVhCYHOkVJEqIQZyLZRAJ098Fnl\nqKSfEXe7JGdSZQq8CFWRF+OIDlAaAAAgAElEQVSZtwm8BdtrL29ttlIkWSS2qtruDek7Z62gldQh\nTeJ5nQO5HLVzEyOIOwo5dRJRGvZS4Rzl7LLpctTtbaEzkRWukm3jVtuWIr/mc4IubsfIupWE57Rc\nGzSKBk7WRUW9Xq8XFinDYHeDpTgPgqDncLmD8KizKF5MwV5YWMD4+DieeuopyZrVS3n9WF/EDKZH\njx7F+fPnEYYh3n777cxrw0ji0JA8o51SKhH4paUlHiGlG2vHbkYURTuiYovhJEdGRmDbNk6fPt0x\nnKQ44lF0OMndQtRZYACGtbU1nDp1agdr1B8enl/BLkOviosOu4WoU0px584dzM3NYXh4GGfOnOFD\nb7VkG4s97BYBIsq/6wi8Dp22YQQ+K+66uCzOfOom5coqdCcCn6qXq/8JZBFNdRImg0i8WUIjAAj9\nNIG3bJKqEzu/0kgpRbwDqKMB8TmnM7aKk3wd2EpfIJ1UKrtDadkE4DHphX08eWKuzg5ELAuRMqFU\n7ByoE3pVks7LKzmpkQS/7oNGEeZ/5eN4/Pf/OLP+W0EQBLxxf1iHSg26RxHxz23bLtz6woSTO3fu\n4JFHHsGzzz67pdHXXol6rVbD3NwcVldXcezYMSmDKevU9ApCCCeIBw4c4MvzrB2e50kKfLVaHWiI\nw36x03YTQJ5M2imcJOssLS8v885S42N/L7d8lg/Dcgg+dPO7mdvtFqKuCwzwMLbnhqj3gK0oLjoM\nwtfI4tt2o+qHYYibN29iYWEBk5OTePrpp1Eut4Ny1z7737Y3ZsRSJJEWaRN4BTqPuIhUsiGBMIvr\nRAIv768/v14UeLFcdeKnU9FbRGRLjyWp7ToSLxJ4RnjzIsfokIr4AgAl2fbSKR593LlhmUrlslTi\nnRcZR/W/28ju1KT3A6+LCCcjnCWrv7i9W3FzOxhFQIxY8LA27AadoYbL3Uo0riInatbrdVy/fh33\n7t1DuVzG+fPnCyE93dZxfX2d2x6PHz+OM2fOpN5xRUd9ybJ2qCEO79y5g83NTd6ZFhX4ot+lvWK3\nEfUssHCS/+HUX+fLwnrCZ4TEdFTIKs3yYBDXQliP2/tvHHkediV+Lj/w029JxzBEvVgYot4Fis44\nxzAoX2OnH6vv+5ifn8ft27fx6KOP4vnnn08pFI1/eUki4RHzSqqEMFHY1WXOUCW9XEFWZlBpG0Xx\nZmQ48sWoL7LCrPN5A50JvEratT52jQKvs/OkzkPjf1fPX0fgdR0JVlbWj9ev+5mjEey/WJ6UxVVT\nF523XFymhmMMmqEQe76dvElF2EonZHKEsJMsVr5Y39JICUEjvmeDUtXFSWEmhvrewyDC5RZB0BhB\nrtVqOH78OB555BHcuHGjMMLTiaivrq5iZmYGURRhamoKExMTmee1XeEZ80Icit7s+/fvY2VlBb7v\nS1Fo2J/neQMn0buZqF99/q/xDNAsqRxxLVA/TdAZ2DK7YvPt2PewHsJyCMJ6CG/CxRtPfQjOsA1v\nyMWZr76+a4n6w9qeG6KeA9F//r3vfQ/vfe97C521PqiQXllZwRqNBubm5rC0tITHHnssV6khzJ/N\nCWqGci4Q5hSJF5cz8quUI1poMs+Je8UFclnylO8xmVMJfLosmczrFPg8hVg3cTbLPkM191a3P4NI\n4MVzCxqtruxFbF9XsVQTi8Cv+/xzvF1+5yaOhmOllPc8NZuV7ZTiutqJjYcdW0VpJB69oRryUB4r\np0I7snKcssvJ+qDxsMbdNZDBImH4vo+FhQV4noeDBw/uOLGilPIILpRSiSBvbGwUmhdDR9QppVhc\nXMTs7Cxc18WJEydSIQN1EIm6Sti3i7CK3mwAuHv3LhqNBo4cOSLZOhYWFtBsNiXfPFPhi0gyxLDb\niPrV5/8aooBKajn1KU8qx5Rx4lqwXTnZnL8ek3qmmDNST/0o3r5io/Koh2BDbqODRoC3P/TX8eCf\n/jbeeecdDA8P7+icg72SE8MQdQ10ikutViv8R7hdIb3EWLvHjx/HE088kdvhaP7rT/PPxLFBgzBF\n3PnxKmXt8sj3ZeLOyLrOQgPAKnnSMh2Bz1OJxf0shXvHPnp9hBWVqBKLSBFfRM83t4hoEgzFZcjL\naEbHJcsWlEXGnXLalxq2ugtDGZP+KJXJFUh739v7y4mN2DVWI9gQi6QIczqBE+HH1k96ze682J4t\nkfXSSNuO5A2X0dpoFK6qqy/bhzXurkEMXbhcZqfYSVJFKcXdu3cxNzeHarWKU6dOpSK4MNGlKIhE\nXZyXNDIygrNnz6ZCAuZBTewjft5pwuo4jjZGuTi5cm1tDbdu3ZKSDIkkvp9Qkjt93m//3M8DAPxa\niGWHcIJuOQTEjb+LyrnjOtI2ItwRh9tdAMBfD0EUkSZsRnCG4/Y+2AgRbIRwhm1YNsH4P/otnPzK\n/4N6vY5arZYZTpL9H9SIh+/70ryOzc3Nnp7z3QJD1BOIiksR/vNuYNs2ms1moWWKjTEbyuwl1q7/\nv/5PsDwPoG3lRafpkAxSyYi9ToHXquY6MmuR2P+dUU7YbPUUCzzbRy9fe9EaooNIVJ1yO3pM2Ao6\nevLbx7DgDlW4iszU+N7j0esTHUkJnLLuUfICEr3h1lCbLOtCNRLLArHanRGunpddqUx2fPUasmvH\nJ9m25HJEaxArM94vJuvlsXiYoLXZvmfecLkrf3wvUIdsV1dXcejQoUKPYTB46Pzn7M9xnMLbXfG4\neW2sOC9oYmIC733vezOjChU94sqI/8LCAhYWFrBv377UvKReMWjvepHIm1zJCPzGxgbu3r3LQ0mq\nWUKr1Womgd9uov7DCx9B2AqF91CbdKukXPSbi3BH5HcEI+4iSVe388bid09rI0DYjGCXLE7YAfB5\nTj/9j38xlaSOcSw254AldGq1WgMJJ6kmrwMGmxhyUHjXE/VeEhQVjUFZXxYXF/GjH/2op6FMICbp\nANokncQ/Vsna0SE+uQ6M1FvKMlGFTynwOeXY8KRlrJxOBF71ZosTRu1KuxxVxddlGBURR1hJE2Zx\nsmlWtBiGbuLR59VBrIv4n9WFRpFUB95R0JD5ePKqXF/dxFU7CZsoRX5JEj8ByLS8AEB5rKJN4lQe\nq0hkvBOqk8NYffHXMP7S57veJw/qUOmDBw9w5syZQso2GDy68Z87joPNzc3Cj83ac93wfqvVwsLC\nAp8X9Nxzz3WM4MLmGxWBIAhw69YtLC4uYnh4eMsRZLKwm4l6FrKSDKnRURYXF1Gv16VQkiKBHzRR\nf+eXPoagEQjEnNlSbdAwglNuW0SJMPeHhhRwCGzEyriqnotQCXoevOH4OW9tBLBL7f2GDsTEmFgE\n7/zSx/DEn/w7vo4Qwi1L6khlEAQ8K+va2hpu376Ner3OJxqLBL7bcJJ7JXndw38GfSJPcdGhl2gq\n3aJI60sURbhz5w7u378P3/fx5JNPpmLYdg0inCON0t/ZNjRqk3hRhXcdhPWmpLqL9pn04QjsUvLS\nsCwgeTlFmv0YmdaV5QxVUhYcICbeanhHfmztBNYu/Pi68xDsQVmJjnQTRtV49GxZFIT8+uoi4Yjn\noyuDwa2WUsuJRTKjz6geeaCtvltC3HZJvddEihEVe0bmiWXx5eLEUQYaUXhDJYmsDx0Y6TlSTr/Y\nK57GdxPYaGi34XIHYTkE9ES9Xq9jbm4OKysrHecFqSjC+tJqtXgEmYMHD2J8fBwnT57cUpl5eBiJ\nehZYdJSsUJJMFV5ZWeGqMJuILqrw/RLFa//5L0phbVk2aD7ammTrZtm7gfTIq2UThIKQ4o3LdaFh\nhtI+5ErZp4HY3uKNsZHR9oirN+wg9CMMHxpCadhD40G77S6PljD3y7+E4//nn3Q8X8dxMDo6mrKA\nseRZbNRDjL1fKpUkAj80NCQFxhDbczGR3cOGh7PWW0C/M/4HkUW0CEU9CALcuHEDN2/exP79+3Ho\n0CEcOnSoZ5Ie/sE/gVUuSRMgiW2DhiEoI4YqaQfS3xPYanhD18lX39k9EBQkq+QBrqIodSpHW5dy\nmsBbJLMcVe3nu+jsPJr7l+fr5/53x5bOlXnOebk5SaTi7Kvp6y6eT54NR/Sci6q/6MNPJYzSTMzl\n6j2yLS+sHlbS0uhUdKfkIGgGErFnZL08VpXqVh6roLEWD0sPPzKOoB6/FIpS1dXG/GEN5/VuQL+j\noa7rDpSoA/FIjBji8PTp0z2rrVvJJCp2EB5//HGcP38eQRDg7bff7qu8PIh1fBhtBb1CDCUpYmlp\nCffv38fExAQ2Nzdx+/Zt1Go1KZSkqMCrXGLul39Jm/kaACI/5OFzWbvslu14G6Xjxwg2E09sz0LY\nirjKzpazRHKM6AeN9nsqVugdiayPHK2iud5qbyOQ9bGjIwj9CM0NeT0Qt+/dknUdxEnAaoep2Wxy\nAn/37l0eutNxHFSrVayvr2NjYwPNZhNhGG5ZdLl8+TLGx8dx9epVXLp0KbX+6tWrmJmZAQBcvHhx\nS8cS8a4g6r0qLjrsNqLOlJK7d+/iyJEjeP/73w/HcfDTn/605zLDP/gnANpRSojywyeuw0m7CKoS\n3URll74DfJlVTsg7K0f1Z0dRm7AzCAo7+2y5jrxcIO8iyVaJsqTMR1E6WkuHcnRl2Rk2lCArYVIG\ngVYVeJE0q9BFwQFiAq/LvMr2EcMlZllx0hNGrcwY9Op+YucgaKRtSJZjwXO8pEz53Eoj5dSy4Uf2\ncSIuojxWgTcSE3inUtJu0y/2SjivvYythssdRHIiVu7S0hJ+9KMfgRCCqakp7Nu3b1vJqxg4YGpq\nSuogFBnrPQt7SVHvB67rYmJiQhtKkinw9+7dQ+V/jkmeSMbFkdGgGUjzkxipBkLAJiA2SanhTFxx\nysl7VVhfGvF4FugsOGUbQSPkFpp4WUzWK+Pxu7syUUHQCHg5TtlBeUwW5cqjJa6qNzdaKA17qcR1\nRYAQgnK5jHK5nLrevu/z0Y4HDx7gC1/4Ar785S9jfX0dv/qrv4rTp0/jYx/7GJ555pmuj3f16lUA\nwIULFzAzM4OrV6/i3Llz0jaf+cxn8Nprr+Hll1/Wru8Xe5qoF+k/H8RwaT9pp9Wh1A9+8IPSiEA/\nvkZSLrfJcy/7JQQeQsMcNVvyRjTd4+ewbSAMu4tN3oXHm2+nqPDdXA1GXLP8+JEf5Np31LpKCZOS\nckLdOYn3SjzHKOqJvIsEnEb6RFJOpZRhn1ESN4n3i9JUCMuw1bYS5UWqsVwHofo85JxL3vmx9Vz9\nL3sIG/qytwIxKylgFPXdBDFcLtB//POiFfUoinD37l0sLi6i1WrhZ37mZ1ITFgeNtbU1zMzMIAiC\nzMABgyDqvu/j7t273HYApMM1vluQ5VG/9xv/BUI/aZODCEMAqGXF1sZE3ZYJu97KEvmh5D0H4nwW\noR+lRBFG0tVwuiwLtFN2uDpObIur6uXxUsryooJ1BABg+OCQdpRUV/8bv/a3cPTzf5hbdlFwXRdj\nY2NwHAenTp3Cb//2b+PjH/84/uAP/gC/+Zu/iR//+MdotXp7f3zxi1/ERz/6UQDA9PQ0rly5IhHx\ny5cv47nnngMArdq+FexJoj6IBEWDCqXYbZlqMoysodRefY3R//1Z/plorB3EdUE13mxO5lijzFSb\nUvaIg+TxVsl7QtoZsmKTx+VorlnywiaEQH1NZHUEdBMsxe/EInKiI91EWlXxB/h3VYm3M84pReAF\nqCMZed533tnoMpGU2LgH9WZqJCULTqWkVe2lYxE27Oom9WNx3NvX1x0qwyl7CFs+wqbPzy9sBSiN\nxS/90tgQwlbAVfMoiDD0qKyelPaNoLmyXoj9RVXUa7VaKmqAwfaCZYPudzRURVGKehiGuHHjBm7c\nuIHJyUluO9wukk4pxfLyMmZmZmDbNqanp3OH9otUu1utFhqNBr773e9icnIS9XodCwsLWF1dxfe+\n9z2MjIxse8KhnQalFKP//H/EPTGJXULQoyDibSZ7z7DQuTSinFBHIQWx7FQbLm5DLBIT+6SZcqtE\nIvoAJLKdSiqXEG1vKJkMuhlIy9l/ViabHBr53fMKUVWPz7+7qGiDAHvuVldXsX//fjz11FN46qmn\nei5ndXVVUu6Xlpak9W+88QaAWHm/cuVKoWR9TxF1NsmDoYiMcwzbFfNcRF4yjH7LFMFJehjKxJnt\nnyxjR1OJPG21OCHjIERS2MVl3OPtee1tXLdN4BlZZ3VRz0OnwCf2mkjoHTNyy4inatkhtg1btfKw\njKeJApwVWUWKUZ4RqUWX6EiFSOKl5ouFOWy24zx3Oh8gIc/iyEaHhlG1wDjKnAJiWQgaLel+sslL\naqhGd4glLqLpTlSyvztU4fVSoSr2dkn5LnROhh6Z0HZsGFn/0Y9+JE0s6jW8VxAEqXB1e51g7Eaw\nMG5BELR/BwVF49qK9xuISer8/Dzu3LmDw4cP88zO165d25Y09pRS3Lt3D7Ozs6hWqzhz5kxXsaGL\nuHaNRgOzs7NYWVmBZVn44Ac/iHq9DsdxQAjBD37wAxw7dgyUUmxsbGBpaYknHLJte0cyhg4KK//9\nfwVAmFQfRaAQiDTk0cr2O0XPSUSrSuz/VkY6LVl9103gZ2Bkm0fg0lheWPnekKN9V7jIzn3hlB2U\nR0vadQzekIfWZvw+HT2yDxt3H2yrqq5iO0ZHJycnce7cOVy5cgWXL18uzKe+J4g6a9TDMMR3vvMd\nPP/884Wnrx0EUc8aimQN8dzcHMrlsjYZRl6ZfofoJAzEcWMi5VDQINlHJe06MAKf4ddPEXhFdU/V\nWewA6DKV6tR3BmbBUOrCOhXiaEA3irFV8iTlyc4YURAbNpGsxyS2rXywhk4XwUYFEaKt2KmRCReR\n4hUXz4fVl730nGp2I6obOSCWmiSJZiRayn+2RKWeJ4gSOlZZ0Wlsz+Vls+P2GunFG6nisccew+bm\nJtbX13Hnzh3ece82HvJeCef1MCMMQ/i+j+XlZdy9exdnzpzZFWSuVqthbm4Oq6urePzxx7W2w6LD\n7QJtS0UURbh16xbm5+cxPj6eG4O9aNRqNczOzuLBgwd8RPdb3/pW6jfExLGhoaHUO4uF33uYCPz6\nb70AoD1qyIgzjShXw8WRVyDbxgKwtrbtPSdWe74Rm3Av7yNH4GputHIJOt/Lld9BzPKi1k/cXrdc\nhOXa8aRW18bQ/qFU+1wZr6C+WpeWHTzzCP88cjgmyZv/+L/B0G/9647nsFVEUSQ9Q1sl6uPj41he\nXuZlTU5OSusnJycxPT3Nt33jjTcMURfBrC6EEO4/LJqoDyJSgNoQqQ3xU0891fOwe7cvC/rH/1Ii\nziJpl7YL/LTSDaRUd3GZjsCnyK6qvLO6UIF4W1ZKVU81JR0ynhLXBVGynrK6pJTqDJVNHUmwNZ0A\nABJBB2Q1wi55oG66fNYhyQtdycqyEuXZsstStBlRfef7ZQxzO5VSKlIN7WCTaa+zYHntEJiREoYy\nO8kRTS2L/eZqEic3N9kUU/1t19Gq6tXDB2H/u1cw8on/TlqeF96rXC5LBL7ZbHKPuu/7kl+9H3SK\nEsDWz8zM4IUXXtjSsfYKoigCpRSe50mK+k5hbW0Ns7OzaDabmJqayuw4DCovhu/7uHXrFm7evImD\nBw8OLAa6DhsbG5iZmUGtVsP09DTOnj0rnbvqy867V1nh93YDga//07+PKCGeURBKhDw5MUR+ILVP\nbGRRtB3SKILtxmJNPCHUgmXHCfxUK0tcBslUrVVFHYA2wzQA2F5a2MjO+RF/Zmo3X5eQe0bYy6Ml\nWI6NzcV2zgFG0llZeWLK+OOyRdEbraL1oJa5fdHQBQZ47LHH+i7vk5/8JN58800AwMzMDC5cuACg\nHcL34sWLuHz5Ml/G/OpFYE8QdXFo1HVd+L6PUil/WKZXOI6DWm0wD1kQBFhYWMCtW7e23BB3M2FI\nJel50BF4Tt7zoBB4nf89Rd4pza4Xt71oQiTqJizmJT4SyhA/04y04myZFIZMOX+VvKuZPQG5UWad\nBMt1+SgCg5q4iU/czLjmovpu2TYojaSJluJ+jKSLyyibX6BOnuUTRvVDtW61BMt14o6yMrGTh6FM\njpPKvOrYIAB/OWrPK3kR2J6D0sQYwnpDWu+NDaO1tiGVGa5vqMV0DO/FojHcunULy8vL2NjYwNe+\n9jW88847CIIAX//613HmzBkcOHCgJ5LQKUrA1atXMT09zYdKi4wSsBfA2vJBoFNeDEoplpaWMDs7\nC9u2eQSXPDiOI1kvtwrf99FsNvGd73wHR48e5ZG9tgMPHjzAzMwMWq0WpqentZNTs34LvdqKiiLw\neQj+xSU+sZ1GEbcPiiRZJcXCCSWjkGlSLU/8bI9UBs0gpXyzkde8d4M62imuIxaBbaX96IxgW7ab\n2k9MOqfOb/KGPDTXGynyb7uWtEynnouo7KuivtLmRs2NFg6ePQxAb9MMGi2s/9YLGPnHr2aWWQR0\nRH0r4RnPnTuHN998E1euXMH4+Dhvqz/ykY/grbfe4nNELl++jKWlJeNRz8OgGnfHcQovt9lsotFo\n4Dvf+Y4UYnEr6ErV8TzAsgEhOgh055aV/Mlx5W1Yw+y4bQuNvnKSQi6RdyUeey/lSBNYhXVZCYpU\nks0aE9E+w848pZorPnHJfiJ0SMQrl1VGVr1UvziQTWgZ4VZVcltjWwnqzUyyryI9YdRPRd4Rib1d\n9mC7TuJTb49YqNuqhN3yHH5uzOsuhlvMmjyrTsytHj7Y1XmJEMN7sWHMWq2Gn/3Zn8XU1BS++tWv\n4gtf+AL+6I/+CL/zO7+DX/zFX8Q/+Af/oOvyO0UJAIAXX3wRr7/+uqTQGMQYJFHPCrfLEsddv34d\nIyMjXfu/geImqTYaDczNzWFpaQm2bePpp5/uP3ldj1hdXcW1a9dAKcWJEydyOyess6MuK2rCai8E\n/j3f+L+wH3E75SMh4IpNBRA6EZYltakiQaZRKH0GITy6GX9NERK398nET/EYAOAI1IpGEWwrVtVR\nciRlXfalR1BmLEnfg4b8W1AjukhCUNIRYIQ7y9YytH9YO28oC+Wx2GrFOgBi+9zabKIyXsHY4/s7\nlrkdE0t1yeu26lHXjXq+9dZbqfVFxlAH9hhRp5QOlKgXNazJ/H5ra2uwLAsf+MAHCrPqdJyg+qev\nxCRdhGUDnkZZagnxqRkhVywqKjiJFwiztKVIsjVec6aE83KETgEF5H3F81QnoNp2TEqVF0kq9rvu\nHAQyzjsTFkG3WqoaqUXyzwtRYkIN2Y8XKPHoERNacbkFJ0Xes0i4rgNAbBuIaCp8YlYDalfKctx6\noSEmhPD9iEX4qIdOoWITcKWyXQf+ZluJdColwLIQbMZ+x9LEWHzY8VH4qw+kfb2xYZQfPRjPiygA\nYRjCtm0cOnQIp0+fxvve9z787u/+bl9ldYoScO7cOUxPT2Pfvn343Oc+t6V67yUwlbufULPdQiXq\nauK4p59+OjWpuBO2an3Z3NzE7Ows1tfXcfz4cTzxxBP43ve+13d53YIFLbh27Rocx8HJkycxNjbW\ncT8dKd/qRF3tcf733447+Ikn3AIwTCNUWRsUUVAAlEYpxVtXZwApGwub4CnmmmBxzSXSyUZXw7SN\nRQedQs6LylTI7SSxkTiR1BJsg3I5eWp3O+Fc/F9U2MVtxHN0yixSV7tc23MybTd+rcVV9bHH92fW\npbmyjtK+kW2zvwyCqG8nCCEO4geT7AmiLkYDGFR66CIUdTacyLyOZ8+exXe/+91CG7Y86wv9yr9J\nk/Q8lMp6Qs4IvE5x15BFSYEHOPnmqrnoMVfBPOisHLEstZwuJsISTYhE4jq5BJ4kdhL+nVipUI3a\nfZJ5E/F5pDOv2tWK9F0bdlIYaSAWARUD1Qhqhm23s8oy8q2q/dIycdKqcI3FUYg8/6RTKfNrootn\nHltbErVfmIDKfPZqB6r66H74D2TbijNUgTvMQoMlIcSGhxBsbPLzHDp+NHVs+7V/hlDxqXcL0XPL\nvIeDAiv/05/+NH7913+dE3eDwYO9J5rNJubn53Hv3j0cPnx4S6Oa/b57RJvJ1NQUnnzySf4MDrKz\nQinF4uIiZmZmUC6Xcfr06Z5CS+qIereKuv3aPwP1g7gNiSgXIMT5MqwcNmmTbyO2SZJ1RVyentAp\ner7ZCB9vF5O2TlSIGXlV59PE5FtQzEN2LCGSl5eeMM8n1ydhGUXE3nYl0WBS3yyiz8pKxVAPsp8X\nRrb9ui8lIWJknZH0+BycVMZsBjFDNMMj7z2W6jSUJ0bRWJbFFWB77C9FW1+2C4QQQuOH/1kAnwDw\nzT1B1EUMUlHvpxFWY92qXkem1BflPcxVdRxXUaKTH2XgpxXzPHgZ/v88u0pcufg/axxT9RFIt0Ae\npc/sO2LCTMNQr77zciw546n60rOseGgzo8q8IScWKI1ikp4Bq1ySlJZUB0DJvBq/HIT9M8i/qP6o\nar0uZKNTraQItnTW6uiFkCTJcl2p0xTWm6mJrkwxZ9fEqZSTeurvv+WlI9aIIwtOErrRrpTbPnRd\nllrEHQBG1nVx6btKSNUlBh0l4NVXX8WnP/1pjI+PY3p6GpcvXy48UcbDCNX7nJVIZiuglOKnP/0p\nGo0Gjh07hvPnz285lG8vijpTsWdmZkAI4SMrKnrNi9EJhBCEYcgJ+sjICN7znvf0Za0R1XP3T/4X\nAMCZMAC+37adMH83gFj5Thq8CFDa/TRBjzdMk04pyorVVrjleT3sP00mfZJ0GF2aJvl54kS8XZv0\ni9/lCiZzfhSlms/bUerilK10FCzo1W8GrpTbBLBtHq89Vt7bz7GqijOUxyqSr1wsU4fqZGz/8mtN\nXi4j6zp7oi5ssYrKwX24d+8eD6NbVChtBjUYwNraWlcjRTsN2n4wawD+FYATe4aos56867qFTuhh\n6JWoU0px9+5dzM3NoVqtZqoVzNdY1OTXzJCPf/Z72WqzhjBz0p1neVFDMOqU82ajY3IjcXkmYW61\nJAKpEuYUgVXrAsX3noMXHyAAACAASURBVNUoCHUjriMp8AR660yeb13aToj5zom3Y0thG6mmcbMV\nv7iahEqMXJMFcRIusUjK8sLqIV5jy7FBhuTwb7r6qceINNlm47CN6WsuThCN9ykr3/VwhofgHTqA\nqMDfukoIt0rUO0UJEHHx4kW8+upgJ1c9jGDtWVHWwNXVVW47PHLkCJ5++unCOgHdEHVKKe7fv4/Z\n2VmUy+WOWUzVMumf/V78IbP9Ssjgh385tYoljvr2t7+Nffv24X3vex8qlQro174A2mzIQotAtLmS\nzeqRqNvnAGDma7DDsG1v1BF0cV9hf0BQwRnxlbYT3mOCoqESQDEkLmufiJsIK3x3CksgsDQI43V2\nrG4TWz/xsX0Mwkl/u0rKe1YXklgpQ6w/I8Zi2y4S3LTVxM0l7yqckgNvyAONqHa/yr6qtLw0Uo6j\nvNxvK+C66FwimGe9MjnKt+8UVleM/nLoK5/D95/+OOr1WJ0vl8vSJOFKpdL3b1/NiUEpLTwa4IAx\nDmCNUvr6niTqg1DUu1VLwjDEzZs3sbCwgImJiY6xbosO6aUrj/77388m6XnkXQdfk+RIB7ZNKcPn\n2U28dgbbBsnqyOSQVJVAW0ldRBsLJ7yqdz4j8RIj7mxyEUPUaOSSZbFjYZVL0ggBUS086KwM80mr\nQgeFamwrageAOLFvPx2nHQhzkiSxc4vPO1HFWgE/LymBUzIcK6lJyfFUe487Pgrq+7yzYbkOIj+A\nMzaCqNYeWrVcB1GzFce5d2y4k3LoL+J5W/ap64ZKjx071nd5naIEXLp0CS+//DKmp6exvLxswjNq\nwNrzrbxgmcVjdnYWrutiamoKKysrKJVKhSr1efOYxEmqo6OjXYfeZdYXTtDbBcb/lbksCIOYJL/+\nvylEl4IA+Dkgbs9WAcz+udQWSuKJ2DnI2CZFVFMxw9uqM5sTQ2mUiAHxd2IJhF2Ie00plUZBeVsu\nxBvn+7HPosqep4rznBfMi57U17bjzrqV2GlFS4llyap/RMGsLmpbJxFv18mdB6QjwTo1mi1zK22y\n7g212/DQb0+AFS0tDOJ+nZYPHRiNk91p6lsaG0JzLbEeJhG5Ro4ekAIAqGDJ6HTw1x7gPe95D4D4\nntfrdWxubnYMo1utVju6ENT2vOi5E4OCYH15AsD/QAhZNUS9h/Lz4Ps+5ufncfv2bTz66KN47rnn\nugqxuC1E3SuDiBFeRBIuqig5BJWvtyt6NTzw9dYZXaZS2waqQ+1y1E6BjsQLVgmp+Erb500cB1BG\nPTqpzUSxcYj7aZFRlqWZeMbjtRMrN1wkr4vjwiqX5RdCcn/UIVndORE3HYdcJMa5yZZsG85wVc5u\n2mzpr10yqdVm2UZVdd7SD/0CcvQXFq1HHSFgcMbHEKyuZdY5C+H6Rl8+9UF4GjtFCTBWlzTEtpa1\n571O7ARicnz79m1cv34dY2NjePLJJ7nFY319fSCZptUyReFm//79OHfuXE+jpyfnvwnMa1ZQmYRL\nUAg631YkhTSSVXNpuUB+uyHp6vGkagptalIWb2tFwm6Dt600DNsjq1HEl/O2LGl/UqEVlVwa7Dgq\nmVfbSB42UWexEq2KbNSRfc9QyEUveow2D8gKAWmXXITN/MR6DKURlgm6XVZlX/xci5FhWFhdnvSo\n4sIdKksT94GYrGcJNOV9I2itty0ykR9IZH3k6AHtfiKyuNPmrUUMHd4vbVetVlGtVjPD6G5ubuLm\nzZuo1WoIwxCe50kKfLVa5XYXsT3PC8XaDTrlw3jxxRfx0ksv4dVXX92y4CJYX75JKf03hBB7zxB1\nhkGG9NKh0Wjg+vXrWFxcxNGjR3H+/Pme1J+iJ7+qk3mib17mn6kwkZQT94yJnikCr4OoQjuufjhW\njBzTzXUpldMdAdX7nveDU3rZhH3PuMZZnnMiEoOMBEedIuEQNyHe4gvO13jwkU2iRQsPm4SUNfGV\nv4BE9Z7ZR5QY6bxzk3NPxLCXuZYXjTrPYLsuLM/ryaLi7Z/gfneRrLNz8Q4d4h0Ye2IS4XIcTcUe\nG0e4ttq3T/1hjxKw19Cv8CLmpThw4ACeeeaZFDl2XbfwvBiiZ1sVbp5//vmekmfRf//7rND4v05B\n70TQUxUU2hxixfEkokhuv4gVC8XK3BwAcfvN80AwshzJwg4hXFRRxQ7W1kqEXW3r1RECfjrx6KCk\nuLM2iUZxhk8i22dUIo2ItutqEXlej+vKIW6jCGDEW2d3VCyCDFmTL9k+YgZoEeo+7lBZ2s7fbChK\nfCTtx0ZWxcmg6nFY+FsdWWcdhSgZVXXKHlfVK/vHUF+UBZPSWOc5DZISL6jqw6emAADN23fjun/h\nd0B/9R9llqMLowvEz0Cr1eJ5MO7cuYPNzU0EQcAt0K7r4tatWwDQ00RpEZ3yYQDxfKPLly/jlVde\n6esYGfhZQohHKf3LPUfUBxX1hYH1ujc2NjA3N4f19XUcO3YMp06d6qvHVrSiLvZew+/8MWC7IKEv\nkXQd2HqRwKeU+DzyrlhBOHQTT3WTTjXhFfl3TchHqRymtrN+Z5jcf9tpf3Zkrzl/AQp2CVVVT3nP\nXZcP4YqIWulhv0w7jm6UpcPzqhJPNXINcZ3Y9pEz0VWEar3Rqdm8bNHykiBqtlIJrHQRZuRjliWy\n7u2fQCAkJyKuC0QRnLF2zOSo1YLlebCHhhBuxgqOd+hQvH2nmP19QJ18ZIj6zkCnqHcDUTTplJei\nqJjnKqIowk9+8pO+hRtAIOlie8raOGIJn9kOUXubdkWEEq32dqKaHELuCIiEndWZRiCR4Mu2Iml/\naVKleJ6Cb5zXxWZqedKuqgRYR9KTc1WzoUr7CO2eOqk0XZ4cXlEr1PARWptVu12XEhQLjBq9pTsr\ni7gOSKvyKhjJ5lXsEGbYLnmwS4CfhLhV9xfJurqOobyvTWxFsh75ASzXwfDRg3yOUeXQJOp3Y8Gk\nPDmKxlI60ktp3wgn6QBQevQQiOch7GPUFIjbiVKphFKpJIXBBYBWq4W3334bjuPgT//0T/Haa6/h\n+vXr+PCHP4wzZ87gk5/8JH7hF36hq+N0kw/jc5/7XOGx0wGcAfBhQsjemUwqxt4tOo2zeIyVlRVc\nv34dQRDg+PHjUiitfjDI+gIACAG1NRMrbReW30gReGrZMjkXwbzUIoF33O4mnorrGfF2PXkbXcbT\nrImnbH8VgUDSVdiSuTH+rxBnwurBNuswQRQALNYZERQryW/OQjVyn6XyQnIcKXQYcRzQRjraShaI\nbafPw7LSGVuzOg/iRFN0mFRl23FYyQSRxhcudnAs1+EvU0bW7aEh0CCEMzKMyPcRNdIdHct124mT\nXAd2TlQKpqRH9Rr/bB07gV5/VTrriyHqOwMx3G4not6PaOK6bqFEneXGqNVqGBkZ6Vu4oV//P9rE\nU32t6NpVkdyKNkPRKkgIpOR2ebHGs8oGpKhXNEP57gWq4NAmz0qELyVmObssluukosVIbWZy/rZt\nS4nY4mNk3xuxjHiCfXYcdPUy2CWXd1BEhV4KpZv8DzN84CrS0WPclIoPtN8z4vwjN7En+pv1VCI9\nlaCr9hv2PWz6sEuuZHkZPpqdYM5fT49UDT+RhJ3ViHn2+BjCDqp6r/A8D4QQHD16FL/xG7+BD33o\nQ/j85z+Pz372s/jxj3/ck7reKR8GEAcLYNmlC7Qzfh7ADUqpv2eIOkPRYbyA9mQklpDixIkThcXj\nHNQIQPjdP0n7xRXSG7n6njSJQq0Cn6nK6yaeimqnzrfeqRyWOZWVk0oKlBGf3HFk+4vGt54J1ngL\n9SA6K1BGuEipqKzJuNLh0hEEuButnN5WnCypdhzESaWsXKvkSS9vKXyiZsIVixcvxQj2gw6dFK9d\nngK7WklZXpzxMUlJE6PRWB2sAfaBg3EUodQKuX72WH+/TZWoP3jwIJUV0WB74bouWhmThFdWVjA7\nO4swDHsWTYpqd9fX1zEzM4NGo4GpqSmsrq7i8OHDfZUVfeNL8W+tk3Aj2Eu4ws5GDgmJl0UKwRQJ\nki184COY6SRriFgbCyBqK+EA4s9im6PkZ8ga7VKjc2mhtiXJNVGzbsr7WLLIIFj7KKVAFGrbFyJY\nYCykI5yEmk6N6FVXSbRK0tXM0eJ3lj1al4cidXrJcWwvfQ5OUk6QU447VMn0x4uwSy5oGKE0rs/G\nm2d56UZV305QSnlnmc03Ghsbw/vf//7Cj8XI+euvv44rV64UlWV6BYBFCNm3Z4j6IAi6mk56bGwM\np0+fLjSds23bmS+iflCr1fBcdQMRLcHyhcRErKFWXwQay0kksESmxpNQbnhV5V21zlAvKUNp/Imv\nnKs62ZR1BkTF3ivL33UvAd1k07wh5yBIDZumoHrwU2V0MSTPrCPCoth6kwzBtlpJVATFzmI7oGHA\nP4NGIJ6XEtlU2won/2JHhr0kXDfVScr0u2ssL2x7cZmoPuniqKuWlyzYw7HCQXVEHAlJz9s/UdIB\nAEengLB3W4yaUl5s6A22F2JwANFLLoY3LJVKOHHiRF+xkbeawI7FQKeU8hjohBD89Kc/7WviWvSN\nL7W/iL85nYUlXtG2G4aB3I5ljYiqkPzuQlvYp1pOiAUKhbhz33jSlllWpkWHuG72hFXbBtGNVioT\nPFmZncLVask+sYCMcTinUpLtK8JogFWK803ww4ehlCVa/a6CEXaV1It7ZJF5Uexxyh5X7oOaPErJ\nYp2LZN4dTkZGo4jHRwcgkXRRZWcRuSqHYp94J/sNw/DpU/z+Z8E5cACDnFm4FRtjN/kwJiYmcPHi\nRUxOTmJmZmbL9SWE7APwWQCLAL61Z4i6CJYkot+QXmEY4saNG7hx44aUTvoHP/jBQCIFFGF92djY\nwMzMDOr1Os6VABCCyNVHF7DUqCpZoRIFEq+zz6jKe5YSH69MJh9p7Cqk1YSUMTUKkZtBVSTNIvHU\nxWzPQqmkmbQq3NtuwlnqSL7flCctifux44nx4EtlEClcpPx85c73tuW488R1AWKBqp55NZqP9HIU\nM+wpqpmS1INYFkgyrKqbXMrjqPu+rJSXy3FoxXKJH1M8T2ffPv7dGhlFtP5AKs/af7B9b8b2AWsr\ncTlK58keGwc9eIR/d7/7R/Cf/89S9cyC7/tdhcwzGDzUKF5RFOHWrVuYn5/H+Ph41+ENs9CPoi6G\nefQ8DydPnkx1EliIxl6IevStPwQsJyWGKAfXW18IkdtDJawhJ91Su5XYSYgdc3/aDjUYb2+392GK\nsu3EbZuCdgZmUb1XfeOB3JakxJn0tRJJenvfpBPgedqkb0A8wicp9iyWuuvEyds6RN9SFX/VLpLa\nnlluBbuJ+uawgXSIRttOqex5sMse3GQ7FgqRj5xqEg451VKKrAOQJom2K2PBrRaTx6U0MYbmcuw5\nL0+OonpiuvMIyjZgK0S9Uz6MZ599lmeUvnbtGj71qU9tub6U0hUAf4cQ8iiAv7MniXq/sXdbrRbm\n5+dx584dHD58ODVTv2hfI7D1Idi1tTXMzMwgCAJMT09jdOGt7I2Thj7yFF9FQsKtMMNmooL1/BV/\nRpb6ngtigaYmnCaxynXXhVliNOVoY7Yz37s6SZV9FpfrXq5ZSZuyUCqnPaCqbSbPHykmWHJlCwpo\nlN8BYDHNvVLK2pLbWDL1XD3/PK+6Y2fGLrcqlZTdyB4fAxWUdeI6+snHQBxyEwCt17XrMTIKrKeH\nVaNH45jnpIN6kwUWLQDYejgvg62BEXVCCFZWVvDNb34Thw4dwrPPPttV2NtO6DWL6J07dzA3N4eR\nkREpzGNWud1GeIm+9YeghLRFDqJYUGwXiIQQq+x3pW4XV1QoODtjJsIQqUmkEvnPUNbFDgEr37Ik\nkk4cJ+WH53NWqKx0x0nslN9Y0p7pkqPlQes5t4gkkNsC6VYTyIkIa3UtoRdtLln1S4VwTL7r8lZA\nnUOUAZaBmdWZdR6IY6c6ACKchHyroopT9jLnP3njo7BKHpqLK9r1TE1nKB2YQPP+MvwHG3BH20q8\nv17D2Pvek9rf2n8Q0eI9fX3/7T9H8J/+Zub59AK1s7y6uooTJ070VVanfBjnzp3jqvqJEydSE037\nASHkAICfB/AmgH+1Z4j6VmLv1ut1zM3NYXl5GY8//njmTP1B+Mn7VdTZ0CsA7plv/vDPETntBsEK\nWrISA7QbZFWdISTeV0OeLECvuuv8lLYNmuGzJKGvt5uILx3L4S8mKkVscNovKUbWSQbpFctz3DS5\nVy0reSSckOwOAIPYqOuGT9kLrlRu10skuKy+qu1HrEaplLo3kg+UXQeW8VQYhgba5Jfvqw77UxZ2\nTRiyFmw7tNWSI+ewDIgJaUoRds3cAFIuS2QdhMBKCI+osrOILqRSARmf1ITn7G+krBNEj/ra2prx\np+8gwjDEO++8g8XFRVBKcf78+Y4JTnpBN1bJKIpw8+ZNzM/PY3Jyko+s5qGX9jz89h8BhICwsIhA\nu33QkGWSRGOhSd1J1nEobf9GCGn713klk+vYaqbb8Dz7C2unoqhNsLlIoFP79VYSFpaRjYgR2263\nZZr6cNuME3fuqe/rlXwdss5PsOhI6rxF4nwSGVFaaJSd2IeXoWSbztrGRkaHs9mKPeyiUKOE8mWK\nPOsAMMJu67I5l7zU+bgjVTRXhSREgnU0arZQ2r+Pk3W7FN+nyuE44pZoZQw20hNHSxNjKE9PSe2/\naOUUQUbHQR+sai7C1lB0ToxO+TAGkKxuCMCjAF4AcHrPEHUGSmnXhHp9fZ3P1D9+/DhOnz6d24Db\ntl14jPZelZ2lpSXMzMzA8zycOnUqTSYEkiqSdgCxZ10l7RnnK3rQI9vlqjsvS1XNu1CcqVdOKT0k\nEAielfE4JsuptFogjaLKJO2XUSfV8w7EZFCNWsPKJEiTZ9YBEF9oPOJMxjwAqQ6aRlp9tjoQUm79\ncGLvOVX8/6loM+JLX1SR1OdP05EiLKur2tgmzxtX1zUTecVOghifngyNAI3sWNZkfDK9sFEHyhWu\nqtPaJkh1iKvpW4HYuJvQjDsL27YxPj6OkydP4o033iiUpHcCi8N+8+ZNPPLII10nrwO6F3PCN78c\n/26jCJyCiRYUyCOThFJO0Pnmtp1N1rMib4nrWLuk2vloJNj0lH11yesYWGddFErYRFAxYowyAZWK\n0b6U0UGp2lZC8FmSOlexQPIwuu3ROj5vRg3hqE7G72SJSUi3VfJ4NC0pf4QwKJyKtqUph33WdQjc\nkarkh2fl5RF/N4nElZonlHSoxGM5SSSY0viIpMirk23dkSH465uZx2SedXWZ99hjmftkQX13bRV7\nICfGCoB/SyldAES285Cjl9i74kSgqakpTExMdKWwDCKZUjcNO6UU9+7dw+zsLIaGhnD27FkMD8uz\nsps//PPsApJGSvKsK0OnVpD9Q9GFbIwYce9E4IFcEk9ZZ0Lo0RMgVtVF4q5+Z8sAre8dAIiketv5\n3ndOvIUXkcaTycvSliHUz/Xi69vppS0qWKyhtISOhPi8qUPUyqQzorsOHUItSnVIkB+e0Ul5zBms\n4eG07ccr5YdyGx4FNmIbixquTUJG9B7iuAiOxEOalnC/WuOH4K3ezS5PA1VRf8ga9j0Fy7Jw8OBB\nKYnQICBmo2y1Wrh+/Tru3buHI0eO4AMf+EDPHYRuhJfwzS/HH0RlGpDVaUpjKyFL8CO0v0RMaGe7\nctvMkgmFYbutEEcg+XZUzjPRSZnmUWaENkhV/yWvvPCbd1zJjkaEa0r9Vud5PwJSUWNEOw8gTLAV\nJtC7bptcV+XM2lGj0TVJZ6SUhadNdTjYMUueRNxRyifvDDqvOQA4w0P8egb1Jve0iz53XoYwT0h3\nHoys04h2PG8gJusA4Izoo8AUiiPHQN/5JsgTH9xyUXuAqDcAUEJIGcCRPUPUgTZZ1xFqNVqAVo3u\nAMdxUM/yzfaJvIadUiqlwH7ve9+LimJhAIDaj78N2MJklrCVtpiojaoIYqXUdwCgtsMbWJGs6zKc\nsvU6Ai+RdzU6ixpe0bJALQ+IlEdTqIsWmnOlqUgtrkzeeR0yiLfYsbGTjKnq/qpXlFtQLCCMUplS\nAQB+S39vdPVIJohK6DbRj+2AiH0zID0yoGQihOOkfKQ6As2WpQi7ksYbjtPOyyKoJmQoP44tOXRE\nH4pRxMgo/IOPZ8f9B7C4uIhqtYpKpdKxMx5FEbe8PYQNu0GPsCwLURSh1Wphbm4OKysr3PrY7/yE\nTkTd/8uvAsxiKPzcpdHOLLsi2iRdVdfjhe02njqW/LvIevZdT+7Mi5NJRS+77lhZpFqdwCp23sXO\nAZAdxlaI/c4TuqnrGbqwZVrV9HsTgDTPRkxGR4TXIV8vtos6VV4TelK3zC63G2VmHVFJelaSJKdS\n4sdlkWbUyahWyYNV8hDU0lyFqem8LiUPYbMFN0k0Fwr7iKMHUhlCtmig7VMHAO/kqdx2O1q6n+tT\nd9fuoQhz8cNK1AkhhMbKxN8A8F8DmAdwY08RdQaRqBcZLWC7POpinScnJ3Hu3LlUCuw8BI4SSYRY\nIDSKCXwnKMSTClFH2GdGmHVRXnQkPlIjxtgACGmr+IqyRAlJhSFkx5eGezuRd6VeJAo5eadWO2kT\nUWO+Z6nWgD5Mo9+UX1p5L3nb0b/gssi3qvJnjU4EvpABMCPZk0WAUKPgMWg6FYQ9dxlkWPW+S1Bt\nTkzxHxqJOysMlWRiXn0z7hg8elSplyYkp2XD338EnfDozF/gauUk6vU6CCGoVqsYGhrifyKBF5Vb\nNqO/X1y+fBnj4+OZCTCuXr3K55gMIKPdQw+1UyUq30Xi+9//PhqNRlfWx27Q8zuCTSRl7RKxQCL9\n/sRv6Qk6kEucKSFxhCjx964KJKwMBrcE2OKk0Chb0JDsf0rSOpWki1B97kCbgLP2hoX9ZSSaxPHS\npTCO4vtKF+o4o81kkWWsSr5Czo4tXfmIapO98dPQkGce8tYibRvKcDW3HNWOwjI4A/HkWBuyf12E\nNzaSKjtr2zx4hw7Aqg4hWFwEAG2COiAh6Rp0ip3PfOpk6R7oZH4Y3m6hZpneqkd9u0DbL6F3APwK\nYq/6+T1F1MWQXhsbG5idncXNmzdx8OBBPPPMMz2RXR0GQdTFoV0xLOTBgwe7inCw+ZPvxo0XjUBF\nH16yTERop8vi5F0k6CpZJwL5pmFMmIkNQoXh2DDIzHKqxljn0WccL1VP5ssUCTnNamjV5cyP58cj\nClnKf6ocJ61a50auUT3oYhQdNmIgdiB0L6hUdlJXKdcFWokywToPefMASor/Py9Dq3hM8XPKfy68\nmiwbYNF5VNKsSwrVC9h5V4aAQxN9xUAXQZVzfuqppwDEHeBarYbNzU2sr6/jzp07EoFvtVq4f/8+\nHjx4gOXl5b4VmKtXrwIALly4gJmZGVy9ejUVCeAzn/kMXnvtNbz88sva9QZtMDGjKJ/66uoqn5t0\n9OhRHDlypLBOQJ6i7v/lV9tf2IRQ1i7lWT501hLht04dV2txo7Ybzx9RRzQ1ZaSOBciWGV2AAE60\nhWRzujCQbB2D48gddbYtr5diUYzCbPVeiewiqeIqQYxo2w+vKOI6hVyqni0nd6JR0E72JiCs13PL\n1UaTYR0Bx5aItIXOlhli22Cv50hKJOfwshlZt8plSTFnKB3Yn9lZKD12BLTVQlT7/9l7vxg7jvtc\n8Kuq7j7nzJl/nBlSJCVR5FB/LEuKZMm0zNwbIIHpBFjswwViQw9+SoBrb4ws7uYlRoK7QPKQB/lh\nHy7uw1pAgCBGsJCtLHaB7OI64ibYe52NI4t0bNmWbZkz/DfkkMPhzHDmzJzT3VW1D91VXVVd/WeG\nQ1ka6QcMZk6f7uo+Z8751ddffb/vZ2vVFauuztM5PFM+eDfNBo24cOGCRab0+33dabRtuIy62yPj\ngxgkf4E5WH8SwOMAviul/L8PHFAfDodYXl7GysoK5ufn96QzrIoH1UVUSomFhQXcvHkTx44dw8sv\nv9zqmu9d+iEQ2G4gLmteNQEogKzAuwuYWTq0AHrja8jBUQnAl6QWxLoudU59nQb7viu7R2MCElHH\n0nECgKz5t0nKyvszxXAR/ZyVJtTE5XRI1asBLCiD5Cr23y2kUtu6zsqPDwhXTS7u50f9H9R1q0nf\n9HanBhBIkmqQX9V0ytze6WaTrXqslkMbJC/WJU/Ngm4Y7ZonpoDNbMk1PnrKLkT2hKtTp5RifHy8\nVN8hhMDW1hbu3buHzc1N/Nmf/Rl++MMfglKKf/qnf8InP/lJfOUrX8Hc3Fyr63799dfx+c9/HgAw\nPz+P8+fPW0D8jTfewJkzZwBgP9tNH6jw1RzdTx5XhfiLi4sIggCnTp1CGIaYmJjYV6a+CqjH7/wj\nwEIrnwDOCqSWGQYZq+7Wjij5YQ6wFVFh5S53RRB5LiMUMnDysZk6TLKkqbCvZKmYy1lMTbwzpr0/\nA0zHkw5FWX6DLAfVgfSK8AFuBa6V1Mb0aae9MUjOS/0n3GL8xvOGAYJwopI9rmKidZGo45JDwgDM\nAPYaiPuIHhTFndSRKppgnY31LLAe7EL+G8zNaVa99NzpJzMXoZZBZ+Yg7tpjKWnk888/j8FggMFg\ngNXVVVy9ehVxHIMx1hrAp2m6bz0xmlZHVXz961+/r3wu7WKchwG8AOB/JISsHiigPhqNcPHiRRw5\ncgRzc3M4derUvo6/30BdFS8NBgMwxvDZz352902anCRdy5qjDNxdwKy28aBsQ6a05j4AXwXqzQJW\nEXRAIDXAclcBqsJttiRZ6GfeFUtlToS5hSPxfNJlajJapHycM5YIu85EF1o3EZXL0uo6zcdRfnOg\ngKxPZ+lOuqbLi4q0Rv8PFGOak2vJHcdhzgFgzLDqjOPytVVpS9XKgPk4TTLg3uuXmbQ01Uyc7Nd0\nmDTOFx+1v9cKwAynjqK7sQwACDfvIpnwsDyeoJSi2+2i1+thfn4e3/zmN/Hnf/7n+M3f/E2cOnUK\nP/3pT1v7YgMZBvAmqAAAIABJREFUYzszU5x7dXXVev773/8+gIx5P3/+/MdgvSFUF1FffU5TSClx\n69YtXL58Gf1+H08//bS+UVteXn4gfTFGowKwpGmKq1ev4mFPbshyjvFY3RgTCikq5gEWgqbF+CWQ\nrn5LWa8jt67BAfbqhshpwiQjJ/+pXKMICJMF94F0l5DwSQQJgbbBCUMANd+7/OZ/t4AaQNZrAjao\nJ6V+HgU5o0C8lq+YdrIVRfAuo08NbTqJ/DKdOpmIOl4Bfuva8/zEwrBsvYsMkCuwHoyPlWwVFZgP\np6eQ5Prz8Mhh73Woa1CsenD6ycr99hLdH/wdgjP/rtRMLE3T1gDevLGXUu65IL3N6igAnD9/Hm++\n+eaeczkhpAPgswD+RUo5BPBPAL4lpdwkhBw6UEC92+3i5ZdfRpIk+MlPfrLv4+8XUB+NRlhcXNS+\n7f1+H489tjt7uXuXflg8sOwEy4wKZ1FpO+NxJWgHIRkz7nF6MbXdQOh1eXFZdRfEK6cX8yoVeFfM\nfkkuo45VchoDvIsgS2BqArMAc81EJU2bRnU9ZntqVbhVoc2UhBSuNY5ciHBuSXMqZTxN4NsMdzlY\nHed2ZwVssOwyXeb+QeD1wtf7SQmM9e2xzWOB8vKmAudqCdvHwPf6wE6xnMoPHQHNJ0PzRkz0J0EH\nRXOj4fzzoGL/m027jO36+joOHz6MZ555Bs8888y+n0/Vnpw/fx5vvPHGxzr1ijC7k+4mzDqfQ4cO\neQvxH0QDO8Womw4yn5plEKwDyguA7b2pbyIt1Gqkyn1S6hwqHRKBNDDkpfOr/CWFdtfSZIgv/5nS\nm6gDklYV5KPIiyoHtC3Ureoroep2DBKI+PB8UgOuK0B9FVAmUQe0QWvdNrS9pPM+mDKXEvg3rlcD\nfo/ePJPCMMvrnOaWuCazHoyPgU1OQnikMHXBpqfAjULSYLpMrogjD4PeXtrVuGTqEOSGv8mSPlcQ\nYGpqqhWAv3fvHtbW1vCzn/0MV65cQRAEuHnzJo4ePbqrFbSm1dF9jNMAvgTgB8gcXyiQ1dVKKdcO\nFFAnhIAQ8kBsFIHCJWCvsbOzg8XFRWxsbOCxxx7Dk08+CUoprl27tqtOiHcX3wWozZwHwrNcWfOB\n1My7B8C7UQVUgaJQ1LVwZCmvlc5IEBADqkvDdcYC8BXacuux8swNyoyI90bCuQmwxgqL90xdh9kh\nVd8oaPaq/D+TLCytAtRFyf7SZat9k4PPco2yDACbY3Fua3YoBUBtW7bA40Dji07XX9FvahFVcygX\nnPecTo5t9exBWALrDyLMrqTA/dkzTk9P4+7dzAVhfX0ds7O2H/zs7KxuOT09PY3vf//7ewLqhJAp\nABMAVqSU7dedPwShcjmwO0CdpimuX7+ua5Pq6nwYY/sO1IUQuHPnDlZWVnDixAl8ajYvTCQEgnpy\nQr6tlHfNJm11K3UV9T9mEhVRVxeoSseRmRhgnzhuVHV5H3CkNz4NO2MZoFcnCEJ7Ra3TK15jiCK3\nVHWfrroe37kBIOyAdO3nSItiylITJKe4VG/zMPAALAlIaayalTnly+7Tket9PMf7dOwKnJuAvSmU\nVj6cngKbm4NUKxZOB2oTrNPHTpdXSc2YPQKsehxeDKyzdfIFSEIxfusXra/VDB+Af+edd3Dy5El0\nOh3cvHkTW1tb+P3f/33cvHkTv/d7v4f/8B/adUBtWh0FMtb93LlzePXVV/d0/Xk8DuCPAGwDgJTy\np4SQ3yaEnJdSuh54H+5QiZ0xdl+Aumn83YYqbB0MBjh16hSefvppayzFwuzVEkwSitQA7qbe3Avg\nASOhm3IPZktnjImFVbCYVck8jfolNlxLZ3LkqMC6eqyAuyAMNGfk06iv/wYylqd0TrdRkeMzDAAI\nbfac+sapCHOiy647sMB7KTwTjQXcCQVJR9ays8+rXvoYcDWEef661+GC/iorQ82imzr1Ufl4BcTd\nc4adyrFlDtJJmkCOTYBs5x3x8u18vEbyoi67P4l46mj2Nw1BRYKd8YfQ27K90uOJw4g2V4qXMNlO\nV76fdl6vvPIK3n77bQDAwsICzp07p8ecnp7GF77wBbzxxht6m9Kr7yEeBvAyAEoIuQrgvwJ4FsB1\nKeXuTOQ/wNGGeInjGFevXsXy8jIefvjhVnU++0noKBJmdXUVURThzJkzoJRi+2fVzKKujTFqhSwm\n3PO91iy3asSmn3BccjShUDGnENu+UVJWqknRq5xRV4NdyUKQdFRf2F73XBj58xWh1cBXhQLAKr+b\nAL2qyZz5PGCTDWN9TRjod8+Ro/gsGM2/fQw77fet7XIwqAXoVgQBmONZLoZDL0DX58sBvq95EnU6\n6dIogkwTsFybTsd6JVad1dTiqK7RbHoqs9BtGYQxkJk5iJXl4lpm5rB18gX9eOuhJ/cM1t1I0xTd\nbhfPP/88giDA1atX8dd//dcAsO/YUJEy9xnHAYxLKc2q3VjmlkYHCqh/0OLevXtYWFhAHMeYn5/H\n7OysF+wHQQDOeSsd7Orln8N1eSkVZBqR0jKb5IJ3k/l2ZSsquMkGKRbIAe+uy4sbmXTGthtkqU0E\nioYCVhPwSmOmounumhO5lpF+5j1fAhap9RgAEHo65yVxMWG6YN1ZAZA5+++uHtTePNCgaPIUBLDM\nfoGyR7xbMEpIMRmzwO+pboQG2L4iIY/ESvTGQXe27PO7Fo1OE6l0ak6Dj3T8EIKtbPnTXZHYmT0B\n1lA8al27Km6mDMu313DsSD3o9gH1vdp5vfjii3j77bdx/vx5TE9P66XSz33uc7hw4QLm5+cxPT2N\nN954A6urq3vWNeasCwfw7wG8gsx7dxvAX+1pwA9YmC5epu7bjOFwiMuXL2N1dRUnTpzAr//6r7cm\nPIIgwHAXjKMvtre3sbCwgM3NTZw6dQonTpzAL3/5S1BKce/SD0FYVC7wr/iO6zyuOjFLoUkKRV7I\nPIeqwlMRdDInLj2G4bnuu3H2FKhmf4hSp1MZRHk/CBsAS7ObjzQ6q0YMNDZAn+6NYeTCluSIDlP+\nooC8mVNUmlCsru8mwQXtY/3yPiq0A4vjpDMaNmrgdW8Jh3mnfft8hFCI0dC+uai4qSRBCNY35mfz\nRsI5hvZ6Jd272lYncVHXJwbVXUh3G3L2CMjqbWDDD2I351/yfj43jj+LqRs/RvD9/wPpmX+35/O7\nXabNXL4bQrRpdVSx6fsQPwDwPxFC3kRmzcgB6LusAwXU3y/v3aZx19fXcenSJQCZrqmJmWu7BLt8\n7TLAOhbrEoikXUGmAep94B0AmOPf69oymtuI5DZ419dTwapWJGjukasAAE25F7QL65qK98Ere3H9\niKvAsxRFwydDa266MLQJEdn+9ZUNlnzBGATr28u06ubB1L8bYL3YkWZSHH2ZYbXbjtGNlAjnc1PV\neEsVXinAroveCrCuusOK3rhdgNYZA8ltJmUQWqx6OjWXX1OhSU/HJhFs2zKXndkT3uvyhXA+Zzen\nP2lJrKrCBeptb5yr4stf/nJp24ULF0rP71HyQpGB8veklD8nhPx5XnQ0AeCTAPzWDB/SUHa7ZgwG\nAywuLmJzcxMnT57EU089tetcr4pU9xJbW1u4dOkShsMh5ufn8cwzz2jXMc65VUOknbUCaq0OqmCp\nspM18oyZ23x5sCJv6sMpgwgLNlWQjpUTaBo3kAKuswuz/y5ZNWa5VedR42bCF9YNu24S1wLQmyDd\nlNeFEeDIXOB2PW1TbFpVJG+GWlU0ZICupryKeSf5a6V6ZTK3KlY3jMphTF2HYz9J85sM4TrUKN27\nIVOhnS4gJEgUwfxvkv4RyMFmsSEvOqb9PsjMnK4dIp2ulr+U4vhJSKCdI5sTm/MvlbaZxKPsjIGM\ntkv77DZUPrgfGWPT6ujCwgIWFhZw9+5d3L17d89Wu1LKfyGE/BsA/wnZymgI4F0A/wU4YEDdDKUn\n37WLSkNUefpKKXH37l0sLCwgCAI8/vjjpaKHpjF3G4IwpNR+rCaCkKvCymYQLygDFRw8nygE7YDm\nCZOJNGNs3IJQD/MuCUMS9ovttCh0tFilGqkKkE0yqeE6IyqLVste8dmScNY0RHgANnU9ems6trp+\n3AC8k4/XOUedIyjvR5ORvXzrC7NQ1Xf+/HVWhcVIs7AM7t39KSuaiyRx4fxghOiN13YBVSsCCnjL\nTrU9lhyrsGmkrATWzc+1G8P+HLqDOxboUPKXm9OfrDy/G0mS7Jud1/sQhwCcA7CJrHPdYULIpJRy\nCcC//EqvbB+DEAIhhCVR2djYwOLiIuI4xqlTpzRA3kvsxRxgY2MDCwsLSNMU8/PzmJmZsc5fjBm0\n80gHkIQ9a3XS/LwLoGgMB2SAviIXEynLtTzeDqayWFF0yYuQlCWLqFjJkp4mSCqHGB2tVVhSm7wQ\nNFsdsHN7oz2k27ehKoeGUcG0h04uVdsV2G8C6J1umZnvdDNAbt0cjNq5z7CCJAIKiUqp07Pa18EH\nNOqUwLoKV1Pu29ameBNA0YzIrPM4frLxOMnCYr3b0KlvnfnvQFAvPbk3O4/J1YXGc7SNtbW1PQP1\nptVRRbS89tprWF9fv6/rlFL+L4SQ/xXAGQBLUspfqucOFFD3ee/uN1BXidi0/llZWcHi4iJ6vZ5l\n/9U22jDqy9cuA8jAoGJYzGVR8zEAjIKxErhR7LsClIp9VKDcZSMB5ODd/pjUMe/udvWldG0j2xat\nqm1VX22WjlrdjGRjBXCrMiSh5WtxGz4ZBVsKvAvDe11rOWsYKvMaRdS1bk58NyGAUaxlJGkRdQA4\nPvFp4c5gOuKY1pOmP71ZROuT24hOLz+vwYxXFA2r5+qYFRl1LVYdyGQu7jl8MZguupRujc1hfNvx\n3q347F0+9m/A0P7m1/1Of8BjHsB/llIuEUKYlHKBEPJvCSGJlNLfm/tDGGbN0dbWFt5++20QQlqt\nUraJ3QD1tbU13Un29OnTlbIoxhiePDIBDoA1fI6EXp0U1TejBqjmtNrZJUichnNS1NrFlhxVPN7n\n6rsuPISBDAuNO1XsuAHSGwtR9b6JtY0Irlfn3CBJbN8YtJnbu4bTjwl2zXOYf1fdJFRp3pvC0MQT\nQssNoYhZNFxjmwuDZVdP5yudUpZnRzoxWdLbAwAx+liQsT6kamQkBcg+dQVVoeUvedw72Z5pvv3Q\nc5ht3s1/Xud7t76+juPHj+9xtObVUbWPb7/dBCGESim3Afy/7nMHCqiboYB6t1v2A7/fcdM0hZQS\ny8vLuHz5MiYnJ/Hcc8/tmZFTGvWquHp9GSBdUGK4qojUWhJ1QbsvUhr6J4QWONcE9Vwz1dUAvhK8\n5yczgbt5TVSkzVptNRZlSKOxEqtOeVo0DYEtXSltNwq5rNeSDnfR2dTHfHsAqK+AlzL7JoSF3hbi\nVd1ZAVhskOSu5aahmQcASq19VPdCX1Q1nLJvXEK9r7kf709bTKAC/3S0o0G6ey4ab0NE2XcoHZvE\nYPK494YOgN9BI48b47v39TWB+mAwQL9fo2P91ccMsm/tkpT6y5wgs/U6MCGlxO3bt7G4uIjhcIhP\nf/rTmJho3zCrKZqAurlKGoYhnnjiCUw2NIhZv/wugDwPe/IKh5+kUOHL376c467epWEBSCWhmnyx\ngmV5zXaB8qwu1oSPFBFhpwDrFddrjVHj4FVXVG/B1rpmSr6eFFX7AjUAHpkEyJR/NI2rLCMDowao\n082uXb13VXISZMy3zufm/9jsDOqsdhJKqyUq5n6HZm3wHkYgHsgiD82BrN3R16pYdQDgj8xrG103\nxOxR0NVl73Nrn/qdEsHXFFtLlzD+8OldHQNkskWToN3Y2MAnP9l+dfVXFdJ3x5XHgQTqe/XebROM\nMdy4cQN37tzBoUOH8KlPfeq+bwZ2K30RklngWkgGSgupit5uLqEaE4AL1nneDcjaTjNQ7rLsSibj\nbrMj0Ak94DEIhAboADTLLj13CCnr6NeigorUSuBNE4FgQf5aio83dSQrdbpzSVmp4VPb8hOX1ZZ5\nN0LAXsKuWz0oOTjUMNbuxGZKViQCi433BmPQLQorilF5Z8zL+qfRJJhRPKpAv+iWQS6RMrOo67Ro\nWkMZBuMPlTb34nteVl3JXwBgaezJXTHpKtzio/1gbB9g/CuAPySEfArAdwHcBXAUwDu/0qt6ALG+\nvo7nnnsO77zzzr6CdKAaqN/vKmkVWaIZ9IrcY35qabpTdnYBKvOgD+CWzk+Z1YzUJSiIFKA8rhy3\nbuVShB1Iyqw8IZgtWRTONZJ42BrUK6a9ylq36rWrlTwrXLDt09wrsCy4V5NeebyvKJQyuyi3Y8wt\nCvQSastvHMxGDs1m+nz3GvJaAlNPTtT4UeRl1X0hD82BDI2C04lpYLOQcpDJafCZck4GgPTQQwjW\nPjgmU2690f1o1D8ocaCAuuu9u59AnXOOpaUl3Lp1CzMzM7X+vLuNOkb96vXsDpUSngF0I9zHAAy2\nO3+MoPFO1jexcBKAOxptJu1xfFIZwE7oSWDLNEI+tAB67fKsPk/RKU9FqVBU7VsxSaZBp7gJYdmN\nBEtjS+ZSN/EpBldbqgGgfFTShXpvJpTUyGHefTrOqsgAvz1RUu5IXlTBqHKrkQLS8EjXr6eCdZJR\nN5tsTSY8v+FQv82JlwgOEXVBfZNh/noloWBJ8XwyNpOxesbr2ktBki+WxvbeIc9sePRBB+pSyluE\nkO8C+I/I7BlHAFaQFx4dlGCM6V4TD0KO5PbFUF1MFxcXMTExsetV0syRqwGke4pE3RVOSSjisO9n\nxQEQKipZeS/jrVZDeQLOIm++0nJIFlmrjtn11Vv0l6RzTq5old+iAryS4aB0jXWP3WuxiIvIT6J5\nAbyKKr26Cdg7zrguy+zWQlVF1MnAvZR+UF1lj+u5afAWf0ZRRsAoyYsHvMtD7SxsVYymj6GzfrO4\nxK1qrbucPYL148/tanx9nmAMuxMRZ7GfVrvvZxBCugCElLL0QThQQN2M+6noNyNNU1y7dg03btzA\n0aNH8eijj2J8fHzfQDqQTUhV18rBICQFzbkQxRb6QLoZ6nlKeAm8Z+PWA3ifREYx74kJelEG8EA1\nAE+Y6URAQVUxDS9cXqpWAsxt0mOx5HasVOO44F1NZjyISo2XKvXinqQrmMdpxnu0Oq+arJUnsbOE\nqYB7Q3dWxbgLp2C0Sr+qxrEmcRaWC3FVPYAHlKtQLJm66ZAsyDT36vmoB5rYkwXP3Sd4lPumB10L\nrKedCQSjTb0KsTH5CIKK/2VVDPu7m2zcMJP7xsbGnq0Z36+QUn4HwHcIIaezh3L/qq8+IqFIHSEE\nbt68iStXrtzXKmlakmOF1udYg+Gaz3JTvU3We6HMhgOACJgmMDgrS2ya2Ws/HFAOM5IGVo7lzutg\no0HJ9ra2bicnBgCb4OBjkzbYdrqstmHhK+Uz6jweAO8tYvU5hblN5SgrA3cTMKu5ymXuFZgkFJBc\nW0Nm+zRo8Ttdr56euNcBwNfBVEdFPYAMAhCDVU8eyjqn02SIcLuwXDSJFrpZBuyrj3yqXNMGWllQ\nuhHOYSq5P9OqJEksx64POlAnhBCZMRGfAXAP2YqpFQcOqJveu3HLZR9fJEmCK1eu4NatW1YDjatX\nrz6QttM+P9/F6ysQ0k7ciQxBiedDLqtZd1PbrrYB8KNKY+gqwGydllCkys/bGM/0ajcb+2RjlU9s\nAngA4AgtJwQf4HbH9umWaZXloGq6RKjlXuOCX7/bTHZOtxjXvQZpOt7U6D+zolvp7c5qNzgxbn7U\n+2n5IBtfZxfcG42VsoHz2xMpK5s/pd2Mz6CO9znv9J1C0wCEpxBRJmtRwBuwbbfMGPYOobtTJHZ1\nzMbkI6V93djuzWBs5y66wzUMu1kCvhmdAiRAyN6YVyml9tf9oCd2M6SUl37V1/Cg4kHb7QohEMcx\n/vmf/xlzc3N46aWX0Ok0NN2piFtXF0r5kkqOlGZgPWZdL6EBAJGSudSESXz4gL4gzFpldJvXuTfl\nQAHkiWsNCZRYdfd5r2Wk2eMiiLxFjr4C9tr6H8pAuEE6VADLqiLQtsx76fn8PBq8m+O4BbiEoNIB\ntj9RgHOTxa9rxtWmuBXIbhhcuWKnmwF/s2bg0BzItmFvGkXa5lJMH84aWFXFxDTiuUd2tepJN1Yh\npmax8uina8mVuuimAwyDvdUJ7WdPjPc5ngbwM8AC7wAOIFBXEYYhtrd378U5Go1w+fJl3LlzB48+\n+ijOnj1rGeQHQVDZfGOv4XN98YH0ukhkqFl3PS4qALoTJpjfi3TGbDyk2GmfV3tVd1PrWpSFoRTa\np10QqscNeKz1ztZxRiI1gbO5hCyQ6cVdpsll3mkFcFevs7YYzJyAZIXjTVq2uzSvxbwGyULwwPZn\np+movKzqSnjMiV9w1HUxFCyEDKKCCSfEkum4xWLquuqSN8+L21iS6R7jsQz4mk2L4u4kouE9sHQI\nHnRxd+bxUmLf7M5hYljPsFwLH0dQo0uXIPjJksAzD1d/n0wA+GEC6h+VqLLF3UtwznHt2jUsLS1B\nSokzZ87c9wpp1eqdIAwxY5Ve/oIwDMNxf27M85+y2q063gxfXsnsG8ugmbMIMnR04/l1Mu1kZcoN\nk1qzAhlEVv4SYde/emDlshYMuZFrKv8OIyvnSfjlLa7G37aMjEp5zR0XFeOWmPdON5OtmERR1LUA\ndDaYk5PCyD5Guda4gF1JYkwveZNNDzvWucTEIZvxTtP6m4WGcOUvbtw5/vyexzbjwpUAU8nP0O/3\n9U+Tk58L1Hd2dj4s1rshMkYd0tH6HTig7vPebRPD4RCLi4tYW1vDY489hieeeMLbwSoIAgz2sYOX\nGtPVqA9FxuywnD2nEF7gXgfmeQ7cE2Gz8CaArwPvgC2dkdIAeCREIGss+RxJCQADeBvJWySgUpRY\ndlMWY0bCOqVx3RsJX8ErULBSvmXjKubdpy1NHeAsaODVzHt93ilDGvXtyVFw6xVVrR7o8znNTqr0\n+vqcrsTGXRbOQbkC59TTAVSBdYv1Nxxj0u6ktSyuGDke9hB3JyuvUYH1vYSgIa4HpyqfXxnN4HCn\nub2zlNLSQK+vr+Phh9u3x/44Hky4drtxHN8XUE/TFFeuXMHNmzf1KumFCxd21anQF9eu3wBImP0A\nCGRSCdotEqDuO24A3iSX2Jl5zxyrIC/KoF2tFgoaOt/PCvZaSWeCCMz4yiZhD2Fczse+VdeqVdji\nebNRXWTXxNTkILdbsftcFUMuuna+hQ/Ae8Y2t1msvyGbkYSUa3R8GnfdaK6rr0H2+sVK82hYAH1f\ncati2X0FrSZYd19Dz2Cl1d87AyCKICYzA0QZdLysugwCJNNHveNWhZjd3f4AcLf3MGZ2lrzPHemu\noT8xg62tLSwtLWEwGEAIgW63q4H7+Pg4xsbG9PfYBepm7eJu44033sD09DQuXrzo7SB9/vx5AMCb\nb76JV199dU/nMGIMwJbviQMJ1IH2xaRmh7tTp07hE5/4RO0/dS9NMprCdX1592oZuCSy+PJzA1zb\noJtagLwKxCsAb46zG+mMKgZNiaczqUz0hOKCdW/RKg0tLtRllnwymfIY/sZGvmJXX3MiQRhiw3de\noBp4u8f6fN7VfibzXscaWaw9ZRCCV2rlreuhASQh1rK5mowLeY4qLC28eiXLXwdPbGvJnJVPcy25\nq/lPu5OWthzIJrC0kxUquWAAyIB45evOJ/m4O4lB55C1zYxB5xD6ozXjOIrt3gyWcGJPDi+l63Aa\no21sbODZZ5+973E/jv0LZYu7l4jjGFeuXMHt27fxyCOP4OzZs/r/7fbF2G1cuX4LFIAAA80/i6kB\n2gFbjhXx+ptwwJ8n6/JoyiKEBtBKDSIiaNmvwhdc3cDn30kedsGSIZKob99wqDqjHFy7dUUlwO7k\nCRFEWjKotruF9yyvi1Hn8NnC+gpJvTckJtDOfyuJC+FJ7Q2BL3SNTs6q65sMt2A/l5tIluvclTYd\nADrZtVpsvVvsH0aZpaLpzgIAYQTRnwQdOgRiaBM6+v3r+SUlYmoWdGNVP945fApBsuPdFwDiycOI\n7q0AsJ1flo++YElfVdyJjmMuvlE5nm/VhxCJmZkZzMzM6G1SSgyHQwwGAwwGA9y9exfb29sQQqDX\n6yFJEkxOTmJlZQVTU1N7LkS/ePEiAODcuXNYWFgodR09f/48vv3tb+Mb3/gGXn311T13JTWiA8DL\nAh84oK6iCahvbm5iYWGh1AK6KR4EUDcZdR9IN4Pn4Fsx7dyRu7Tolq4BvAnqExmCGB+HAE4BiCRe\nO0U3YtLVE5aKNmBKJXNJ/f+DUIwsiY17HGAzOYnBOnPql+/UMlqm9/ougLcJmkuSF8/yth7HSSYa\nvBufSZYMbU949/NKqFXgWvJjd256eFgsB+oJ0tDY+4C3aVvJ0mFpUrM0+sa1Vq06AMC93pGWsqji\nPV/Cicb920aapqXio71qGpsYGBVf//rXa5//OIrYq93uaDTC4uIiVldX8dhjj5VkjMD93QBUhQna\n3YhZVxeYxkZvjJSGXolLmzohICvwF4Q59rzUAu0i6FrPKzBl5jLze+pjxIUB0tXzzHC14tSuLVIz\njI8hz87XDIoFDSF605n0JogAB8T7ZqU6bTpQMOSqOL5S+14zppV/zfdQXV+oOlGbLlcNMp8c9JOd\nDK/5rkt1dSaujMYJ0Z8EGfnlv8nELJgL7PMgwx3IbgsbXXU9xhywfPSF1sftNQgh6PV66PV6mJsr\nTASklNjZ2cF7772HJEnw7W9/G3/5l3+J1dVVfOlLX8Kzzz6Lc+fO4cyZM63O8/rrr+Pzn/88AGB+\nfh7nz5+3gPi5c+dw7tw5AMDCwsKeQbohcwnxUQHqTYz6xsYGLl26BCGE7nC3m2WRB8WoqzETEVrs\nCWNZguMNenX1vAlmBQioGksCAfGA1YpxU/OjIe2PiQLxZvW2yG8YzAlKuDcRZhA/2+KVixCKhNrs\ngFuYVSWvHwY2AAAgAElEQVQXaboBqNLfm2Op/4e3MZLwsDmeok9JGKxeRM6EpsawwhhHEqrdU/TT\nVUu9ajKyPteh3c20dEwZlJvbfQ43POgiCXsQNEAnLx4Fci0rDbHTnfYyK2bc62Xd8NwJ3roG5/96\nXTwKtoei0YWFBYyPj6Pf76PX62nQZlozAnvXqDcxMCrOnz+PN99882Og3hB7tdvd2dnB4uIi1tfX\ncfLkSW3x6Iv7yeeXrq0iIEWeMwG6+luAARJgRu6Nc/mDAulCMjCDGBnRMeuxilBUNJopAWpq/e2T\nEAJFHZErFQycPFEl41FR9d01t7sMOY0TL0g381BbEG9OVYKGoJKXnKfa+MwDBTAWHimNkrhYY1Wt\nThj6dBF2AUJAzGvK/0eV3aGnMt904r4OVuRxGXZAkpHlutUUIojAexnQT8ZnEG5l0kDpyCl3DtuS\nwjY2utef+BxoZQ/x9rFX5xdCCMbGxhCGIR599FF89atfxRe+8AX8wR/8Af70T/8UP/7xj7G5udk8\nUB7r6+sWk7+6uurd7+tf/zq+8Y1v7Pp6PRGiomndgQXqrveu6jBHKcXp06cxNTW1p/EfpPTlR5fL\ny0xDXtZkAwBoCkZECcBb4NyJ1AHcLniXkoAQqbXoroOGktakno8NawvQ80gQ2Z1WPQyU1xO4Aowr\nlofJtFHjbYYpm6kqnPXp7fVysMUgZ0leg1OPbhQoCr44K988aNbZkbRIEEjjdUpDwuJaMwJleY4p\n23GbP4lcwy5yf3l3rCTqI/QsgSZGN8Qk7Ol9FEgHMjBggvVhdwrd4QYAYK17DCHaOTO58pe9xMTE\nBAaDAW7fvo2dnexax8bGtD3qYDBAt9vdc4OMJgbm49h7tAHqg8EACwsLGAwGOHXqFJ5++ulGEqbO\nGrdNjGTHyV3Zd8lHigB27Y/5GMi+KxxBpXVdQivmAmQ5J+tW7b8hqdzugHRBGVJ0Kp8P+Kgyt/IW\nwFrtx7vTlTfmVSDevm5bKmNuA3Jw7I7rgF7XSrJquwLTFiD2adMbQoZdkGQI0Zso/Otju6eE7xgA\nFmB3XdT0dXf7mfxFCohxe0WwjaSHjLazG4S2r4cGlvzlQURPDgDUdwN2w21eNzs7i2eeeQbPPPPM\nA7hC4I//+I/xxS9+EZ/+9Kfv112GS+lvWnDggLoZqsPcwsICut0unnrqqfvubvegGXUC6ZV4uJGI\nEKmTtANqLHtWjKG2KzDfBN4BP+vuyme07t0A6T7wXQXiU9i2htrlBQmIFI2FWSrcyUwv7+ZMfVWT\nEc28s/JrNXWeday9AscpjawJjokURJadXnx+soIGEMxuEsVqWOmi26H9/66zxVKdW6v287FPSTRW\nW7QqaGAB96p92oQgDJvhBCYSuxB00DmE5dTfHa8qVuNpzEaZF/Dhw4dx+PDh4jxCYGdnBzdv3sRg\nMMB3v/td/Mmf/AkGgwG+9rWv4YUXXsCLL76I3/qt32p1rjYMzMWLF3Hu3Ln9KDz6SISy2w2CwGth\nC5RljHNzc61XScMw3FVXaBWXrq2WZYdGxDKy2EXzcUgSC6AzpODOVCxAMwOBXNjhMpUcDIRIzZan\nNPI6bQFAJy3LH3wdp82/q5ot+VYVgbKsT4F2k1VvA+Q5DYHIr6GmsZ9pb8W8EwYY4J2NBpBBBJLG\nRW+IquJRI1RhvgvctTxytF32TVeN6IIQMghLOnrOQhv0e+QxvD8NSAnqSFl4PwOxSmPvdoXmY1Ng\n2xv6cTIxl7mG6ecnwbZtue1g+pFWUkQV8eRhrEydbr2/L9Z6x3Fox69fv3AlwEuPtcdcJlBfW1vb\ns4PX9PQ07t7N5iAF+M1QK6gvvvgi5ufn8dprr+15lZQQQgH8Q9XzBw6oq8S+vLyM7e1tLC8v49ln\nn0W/vzdPTjf2u0uearbRPXYGiZGLQ9rg/52z32akovzvjGie3FqAfw4KBqHBO5fZYwBghIOSsvOM\nW8BqbmsC72p5WGnf9WszGOwUoWUt6C4JNzHnVZOXcnlpw7ynnskp9DBLPtkOkLH2ktQXeJnXYnnO\nUwbA0JkitHzffSEJBTcLTI39JaG2PMph6fV+qkiWmBN4oV9Ngl6lTeVONGnpbYfhOLpJUcw+7E7h\nXpglvU0xiQnazvVlKT4OVvG9aIqZcAOAnbAppdo5IIoivPTSS/id3/kd/MZv/Ab+6I/+CD/5yU/w\n05/+tDVQbxMq8X8c7cLsi+EuW29sbGBhYQFpmuL06dO7ljECe2uMJ6W0choHc/IatYC1cFTUKQIt\nHxzJDhgp8nbdChOBrLw5qJK4CEKxE47rlcKURpaEpqroHvCDed+KI6cBdjpT+jlfbU4TSG8D4kfd\nabvBUjJsZt59lrs0BDoZHpB5IaswO7GmI7/Pu+GeZdk8shDKsYV3irofsyOz9DjBmDcGJuh3jQSs\nhnKdsRJYd0NJW1jSzPgLGgA5WJdBB4Npu4/F5vhRTGwt145xc+oTCFBj1WvUiK12jmF2VG3raJ0b\nU5jAxq57Y3DOdbH4/dQbvfLKK3j77bcBZLJJpUdXY5orpuvr6621776QWdOBjw5QB4B3330XlFKM\nj4/j6aef3hfv3f0OIQRu3LiBK1euoHvsTImhToSdJH3AXQFiX3Goen7IO94GSQFNQSHBjUlEgXLu\nKc/hktkuMcjAu3u+qscm827ZPCKw5TeVKwFU768iIZEG9BGq5S7CAq3ZdYyCMWti01rz/CahiXmP\nnQZNgbUEW57gTPBtFXjlE6w6f9G8yc9wZS4vprexv5sp9DgBBAI9rsnOq31j2rXeAzMS1kWYFrIX\nQYPSpEpFmp2HBhgF2US1HU1iLM4AeMBjC6yvhZkunYGjS4vJpE6nvhQftx5zSSydeioZAsIhJcFK\nPI3DnYxFb9OLwHX9oJTixRdf3LVspQ0Do5L9x9EuTKCuVh3X1tZw6dIlUEoxPz9/X8vNu+mLoVZo\nV3bGYOahMF/JdAG5+9iMFAG4LPurJ8ZNuflchJEF0hV4MQG6C9ar5C6q3kcQBlpx49sRBRg0849b\nmO9z3DKD0wBxWCbJGI/1993NJ24eaAPiLSeaZFjpNuOGbxuP+lYhPq3pm1EKw0+dh10gIlYhLRHc\nltaoG0ulOw8i8PyGgMU7UE4xel8pITpjYMMBRFRIlGQYeTurUp5oVj2ZqO7ezMfayUuGk0fRvVcA\n95tTn6jd3135VnN/tppcT7ocTm/suemRumHfq4wRyJjyt99+G+fPn8f09LSeDz73uc/hwoUL+PKX\nv4xvfetbeO211wAAX/jCF/Z0njbxwUOw9xmEEDz99NNgjOHixYulQrFfdQghsLS0hKtXr+Lw4cM4\nc+YMfnitOqEroJFU1GhEHhDeBpzEPCoD+BZ2wgrEMwgN3C27SK299A9mgnQFihWDbzFRBNbSrwqX\nrdKvh9jAmVVoRKvC26DJKFhtZu49k4lx6U3dWdVkpLuZNnijV0l1mpYsk6Bn7WdOgu4Eab0nUupJ\nRU3WcTiGKCkm9O1oyjq+xKSH49ggM61cgHq8YE+vD49WFo/e2ZnAXK+5QKiu6VGapntqGe9GEwOz\nsLCAhYUF3L17F3fv3t0PO6+PTARBgK2tLbz11luIoghPPvkkJid3p12tGrepL4aUErdv38bCwgIm\nJiZAJk5az488uVStZLpBiCwRHiqqCBcuGYbo2dudr4PJxDdZ2rZZRdwJJyyARWguRcxv9NsAdPNv\nzbbnf1dJaMrjVIN0VVzvgkHXPjKMBxYgV8XuZridWFUIFnprpZRNrVf37TQ/UraIvNMHjavtDt3g\nUa9VAawKGUZWjw1lpwlkEpjSZaYja//NieOalPGRJru1rbyfWA8PYzq5f+37/Tav+/KXv1zaduHC\nBQAZMeN7/kHEBwfB7mO4TgG9XnurobbjCyF21SjDBOhHjhzR3fD++b0i+QZUlFhyfXwFex4bchch\nAr09cPzU1XZz3NJ2380AzUF5A/OutsUy9BY8hUhBifB2MfXpMLNOqwbj7bL3FXcVanzuau9RgHd3\nItOSm/xzowA1JwE4scdR4L1J8w6U9fKZXKbmpszowOpOhFRwSEJqJ1lOA+v9tSUvdnFq1QSobxiM\ncZKgpxN4HBTfJf8yuOH00MIDH7DlLwM6ib7I/t4MZ7C0c8S7IrSbMHXqvjAZ9dFotOc28k0MjGJc\nXnvtNayvV1/Px1GEAsmLi4sYDoc4c+YMxsfH9238upojKSVu3bqFxcVFTE5O4oUXXsAvb0mrrqSJ\nFEk8NUAAEJK0yH8e+aDOAx5Q79s/kVFxnJN+A5JogLyXJkxmDhuxMTBafr8CERcgvGUtClDIZFym\nPg7HNIsfGDK6JrtK77ZdaNply2vnQTfTtqs6pNQjM1HadBaC5wX/ShcPADzKVh9V92YZlG9e0qgP\nIoXeR0UykdXCmEA+jfoIRt5+ORiNzyGIM1IlHjuEaNvoS1FjnQsA22NzGNsuu7AsjT9V7n6eE2kr\n8iEcJrcqxwSA1d4jmN25DqB6JX234cqSNzY2cOpUdVO8D0scSKAOQBcg3U9Ff1Wo5N6m7bQQAtev\nX8e1a9dw5MgRfOYzn9GezSZIB4BU0HxJv0j+AW0GKb7JInW21f2j6yabhIfgZldTyr3SmKaIZdke\nMDDAtwLlVQDcnLASFDcDISkkKyokCCRIuQBLBkhQLA9zZMWqph+ulp54bh4EKKSR5JusHc0JToLo\nDoNmMJFYS9ZVDjPD0GaJ1LnV/u7k6OrULW9jYzKThFTqTrMiWJHZYwY9631yWfXtyO+ipFj1gMdY\nDdp3rROEYWnnSGn7ne0+5sZ23xl4NZ7Gj65O4JmHy+y76xJwP1KKOgbG3Of9YmI+7EEIwcbGBp57\n7jn86Ec/2leQDviBuqpxWlxcxPT0ND71qU8ZKy4Z2Gmq1RnyvLN0Li3huZRRPa4C8ADQJSMr31U5\nvZjXoaWFkpVIjVSGpe2uFr4Ny65IC06Ckj1uSiNwEqCDav20D8CrbXVymtTJm4HjN9/UBVY3bqKB\nBrZBXlCZGoxyGLdfhXWdYXjQ1fnRbBLkY6EVQNeP8yJ8FyybPSt42CuBdaBc+J/0pkDTWO/Lwy7i\nsWZGeXPieOVz7v9jOHkUy939B79rvewafnDrEQgBPHc0xAQ2Go6yw5Ux3m8+f7+CEDIFYALAis/5\n5cABddd7d78dWoB2QJ1zjqWlJVy7dg0PPfSQBdD1PrkMhBFp/W0+B2GZb+u/IlZm3X2h9ql6F3zS\nGfM4l4mXosKPnMZeAF9b/GqxRcwC7kB1UZY5ccUysh67k5R7rHtDkCAqMVCqYNUnvTHDTGCJ7GjL\nNSbSkntPVsRZZt4Vg53m+5pWhnWTp1vgWufywmngnRDd11EUnIV6u7l/QjteL+f1bgbAVUERz5u3\nqBuTYTiOTTFZ67E7kOPoky10sYMB3ZukYWU4icNdf2HqD69MoKrOcD+B+sexv0EpxRNPPLGr1cvd\nhAnUpZS4efMmLl++jEOHDuHFF1+0JFEXFmIAASIPo2yGypUuSK/b18yz27znzWORUbBXVcTvCy5Z\nKfeaWnhB7DyrQLy2oCXNMEGBdyWXc8G8Ky30Me9tmHjXPtIMF8AXx5THTXKwbIFcy22mo4vlTTBc\nbelY3ASkYc/yYQ/SYaZN9zTRU+YD0szR6RAw7XXz1Vse9sDibQin46ikzAL1rme9G4pZV7E++agu\n/r/XO4zJnbLkxGTVlzqPaybdd2PYFFUFpRdu2U3s3lk+Ai4ewonZ7dbOLy5Qvx+N+vscDwN4GQAl\nhFwF8F8BPAvgupTy1oED6mbspZtdm6hbLuWcawb92LFjXoAOAP/4LgFAEDJRgHLA+ls9dvW5XBLs\npGHJLz1kzdIZd5spnYGkkJLowlXfOD7ALSXBNu+UrjNwJjT3GtxJKpWstE9EbLCsQLnPISZ7b2zm\nnULoROICbx97DWR+6gKFDMWsaHdfv/AsTXMaWFr8QNY38jGdYmLWs15nIOLSsqApYRGEZUlcsV0s\nQCCSyknPBOWKATKPFYRZNwucBqBS6OLZmPUQ8R1rrLoQhGLAd8+CLg3mWq0mAcDqzjhme/4lXyAD\n6XWRJIn+jroWix/HrzZ26+Ky21CrrktLS7hy5QpmZmbw0ksv1cqfVM705TMrd3mO5YJpAF9Fhlj7\nG0Aols484rDwvmPrwkdCCFALxIekvpDSBfHmYx/zDmTyGbOINRSjxlxSVVxvkgku8x6l2xY49jUW\nqmqEJKijTVe1Q2l14XHVWGnQrT3ODR50LaBuRjx2CEFS7/pi7d/JSA8mYgx7h9DdWdPjmPIX73XU\n1Qc4AH0lnsXhyN8QqE1cvPVo7fNtnV98jPqHAahLKX9KCOEA/j2AVwD8FrLlu78CDiCjDthOAW0r\n+ncTPqDOOce1a9dw/fp1HDt2DJ/97Gcbi1iFIEgqGNuQeWz+ZP2klXCaNzwqxvSNU+eLTiANx5ki\nqflcZ2TD9fjsIn3g3zy/O/GNPNNdSArZB1DNvKttXDILeANG0WuDlRpge7znJ64tiLQLZilSYhRs\nEub3l69gz00miiNEIGK7a6rl8pBtHwWh3u4riOUkQEI73q6hTRMmkULfTAD2ezNCFx1/Y7Xs/J6b\nRfO5ATJAv7xdTqy+G9Y28a+Xx+CxxrfH3ic7r4/jwYeUct/AuxACt27dwvr6OqampmoB+vcvpVCt\nCtoAbDPXDHlkgRpfI7lW11vx/VEsvH5OhohIQQ5ohxgj19WtFJqhQLubJ5Pcblax70ymjcx71fPm\nSt2IjenVwRCjWvvI2nPRAKPQJgjCmtykQunXiVXfw3RdAnc6eCIdwV2qK7HuhFiFnanWpqtOp0ad\nGbPfI9MxRp8yHLPyvtL5Mx7rmxFVwGqGb8VhfbIeIJuxPTaH2+zh1vu3ib9f/yya3Hl/tjSGTzzc\n7ubEJF2AD34+z/3TfwvAe1LKnxNC/lxKuUkImQDwSQB3gI8AUN/aqmba9homUDcB+vHjx1sB9P/n\nJxSUOslaEGtbYvSbVynXB7r18RXFGOY4XAZgeTIPmagEzUCZbTbtIk3QFFLuFTSYNxWmnIfzqBFw\nmdfkY/ATGTQy7+6EZL4W9zVztF++08c6bzcDh5BMy1+kPq95E8Cs53SQsj+899yEams1FdQB/a5D\ngdrfx2wpxkTp1/WytSN5GZIxdGWRKJVufUeO6RoBIHsPFFhXY68lhyodMHxxfWsGAa3/fNzdHsPM\n2Lb+jLkrDjOdTayMpvHOlXZF5Cb4+7AwMB/FUHnXt0K5m3Cdt8bGxvDUU0+1O7bGgtaNRLAS88gl\nQ1yTaiKWWvv6zu9blTSD5k5aAUkxFJ2SpBAAQs93sgq8V9lOUoiCfW+4d3JBesmphXZKZEVCO5Zz\nliu7q9O8Z+ew7SoT1kXIh/pvFZEYeAtMVYM6SQgk/A32eNDR+Tzg9TcCSlqjC4XDrpahuABdvwYD\ncJs3CVU9O8xQbHpV3Jr9JEJZvhHY6s5ifFhmx5foYwhbzFMqVnEYsyhkNIfS21gLsrqj1c4x/OBm\ntS7ejZ8tjYHf+gf0ej30+32Mj4+j3++j1+tZN+4uo765ubkv7lAPMA4BOAdgE8BVAIcJIZNSyiUA\n/6J2OvBA/UFJX+I4xuLiIpaWlvDwww+3AugA8J138hbTCkgaAF39LXIduAvmTdCtwsuYg1iyGO5J\n5tlYZebdBOhlWUk5G1sAHig1pDH191WRebQTazpwZTMqqpj32GG9pQy0vzGBrJ1QS/IepRfPnRna\nMF4cmWyHGF+pAGllJ9by8bZjS6ETtW9azCJZSYg1AVKPrlGPX8N2cRrqcxMps3FzsJ6QrEX6kIyB\ngSN06lx2RA89ahc5KbC+ltQD3i3exzgrCkOvb/klJyVwUoEKDnUGWBlO4khvAz+63KvUpLthJvqP\ngfoHK8z/jcrnewXqbmG/ct66c6fsaGHGd38hka0uut9lhg4rgzeCsgWjelwqFHc+2zFXspqoKDwF\nK+ni2+QkF6SrXgNA1tnajVA3x7O7ogLN9TpVzPuI1K+yAdWrib4aGf1cEBj5btjabWaYs+zmuDud\nKdvOMR2Vukj7ry97D5UMJ2VdyMCQPGJU6qptEUY0wLB3SBsYVAH9pDOOaFimnSWhlrSHs6iyCR2n\nUUn+UhVt3bruJy7cOA5ak58phWUhDwAvvfQSdnZ2MBgMsLm5ieXlZezs7FiN60ajEcIw1OSLlHLP\n9S1vvPEGpqencfHiRW/HUeWffunSpfvpND0P4D9LKZcIIUxKuUAI+beEkERKeVvtdCCBuooH4fqS\npinW19exvr6OkydPtgboZnABWFRB/oGs0+S64F2BegXeXUa+w7gXoJeuRVIwIvQ46rGKiPFaoO1K\nEtzCKZX+qjpKqrGpu82RzfiOr7OazCQ89hjmpFWl5bdsCUVYYvO9shVVaOvc2KTO10sda+7n098D\nQIyOZbVW5wuvdPLmtTOjAZQ7wWqgb1wHR1Bi9RU4NyMh2WSZ5HrZgKQWWGfg4GC4lRzWFqH3kj4m\nwwKQb6V9jAe2c8vS1qFdd6Azw9Sp/8O7R0CpZf1eGa6d171793D69P21w/449j/uh3hRdUPXr1+v\nLOxvHENQME9+HvF68M4IL4H2qtxjPu+GWUtUV8yasen2+czHJlh3t/nAO5AB+CapTJVLlsm6uyC+\nSf8OlMG62mZGxo7X3wyYx5ie7k0RB12ry3J2nOnJnksoqafeKBgrao0M+0ZfcSuQAX3AXoVVsT02\n55Uqupp4ziJQyTHoziChHYyP/KB8rXfMu12Fy6pfxXxj/4s7yQzmwubOyxdutGfSzTAB+ZEjhSMY\n5xyDwQCDwQB37txBkiT4u7/7O3zrW9/C5uYmXnvtNTz77LN49tlnW7PrFy9eBACcO3cOCwsLpZ4X\n58+fx7lz5zA/P48vfvGL+vEeYgYZ/FmSUt8tJnA+0A/+1ulXGPvJqKdpioWFBXzve98DYwyPPvoo\nTp06tSuQ/p13ghyk+2OUUsQpQyooUpH9HafMAulCEP0YQOmxHoszpIJYP2ZwSS0g7z5WEXMGLqj1\nUxzjR0FcklKBbCoC/TPiYWkf8zgL+OfbuGDWT8Irqu9rlqZTyayf2JmYTIcaKUmJueWSgoOBgyGR\nIWIRtVoKF3mRbioD/ZPIUINdANolxvwxI5UhOAJwBEgQZZ7qkmmQ7ib3EbpIEJUmWAmqx9Fj56sR\nvOG+XXVG3BE93aRKn0+28x73vT9LW+0Z7EPd6gl5dWccP768Ow/0NE21Ph344GsaP2rhunjtJp9z\nznH58mV873vfQ5qmePnll/H444+XQLrqi+GL7/5C6pynflflXHfbThLYxe2eFVHreEn9BaUO+RGL\nALEIMOQRhjyyHpugPCC8BNrd8IF6d5sL4BMRIhGhzi0+aUxTozouGRJE3h9BWKWspQpcJ6yLhHUx\nCsb03+qx292ybhwASIIOkqCDYd5RNWEdJKyTjeXo3gVlXpAuCLNkiGnQRRz2S7p5NwRllUWcidMN\nOw562OlM6SZ2KgbdYmVyaHSF5bm0cdhrn2+jZBtLonBjub3jt+E1Q38u8s+yIAx3guzG4MKN4421\nbVXxdz/yv3eMMUxOTuLYsWOYmprC6dOn8Yd/+Id4/fXX0ev1kKYpvvnNb2oGvE28/vrreh6Yn5/H\n+fPnrecXFhb0tvn5eSwsLOzpNQH4VwD/PSHkfyCEPEsIOQ7gKByjvgPJqKuljv2wZ0zTFFeuXMHN\nmzfxyCOP4OzZs7h79y7W1uqXj3xhgnQuoIvc1Haz6M3cluYTROCAMXdiaJLOpIIgFoG1PaCyBNAV\nq662M6eAiQuK2GDeOcrMe5XkRcgcEHsKTW0tZ/OXOeaBw8Iyb9Frce5yEViiALExTEhMnaj9XqhQ\nycYFrO74pWvwFLpmpy/kNm6RrAnaBaiWwKjQtojO11mC6G0mW64ANwcr3xAgQISRbqDCYRe/bvJx\nr+Y1K/zNQPJWWtaGb6U9jAdlH+AtXkwkbj3CylYHh8f9xeBKp27GO5ejxsJRN1zN88fSlw9u1Llt\nmcE5x9WrV7G0tITjx4/j5ZdfriVUgiAA59y7RM5Fuw+UmYtHPCNXAlqsVKaCIqACsWbgCzDcDfzs\nuukQY/4NVEtfTEeaLotL2xUb7wPwdcx7IsLSth3eLeUCU/vuk8uYNy6mft/8O0FkgTmGdrKWUrfn\n/D2mEKXaHqCsedfH5TJBtxGUJBRUcm076zLtar+mUIDb7fxpFs2ajeNMQwFTZ6/CtepVEYoREtrB\nMAS6SbGCuR5lbLQ5J9wLZjCZ3kUv2cJOfjOx1Z3FjdFRhEb+P9St7mHhA+CmTv07yy/sOj/vNsx8\nHkURjhw5gq9+9au7Hsd1/1pdtTX7Zh+Mixcv4pVXXtnT9UopbxFCvgvgPyKzZxwBWAHwX8z9DiRQ\nV0EpLS1tt40kSXD16lUsLy9rgK6Yt7YThhn/58XyXXLCgZA51os1jHsqqH7et6Qf1RWbVvifm0y7\n+qLVFa36wDuXFENus/yUSsPmMQPoTaG06iajXiRygpLHPMrgLvF4Fgcot+Wu03iOhN0S3L1R0WNA\nyXaMFQBQcGF3JI1IeenYZJhMsBzLyGKeNOivWfyKczabkUJaYwaXrJi0PO3J3ULae3wSPVpMBryl\nzh7IZC6+81S939fvTdd+3trGO5fbtSR348Nq5/VRjCZGPU1TXLt2bdd1Q0oi6TLtf/9jhk4gEJtM\nuPF3j6ZWXjVJEn1NBtBXYN19fpB0KnOM+6lu2zuDEuG1kYxFgKG081tE0z0x776o0r5TiEa7SBVc\nMrt5FBnTNTsj0dXPhSQGlcIC6EUzuxaA2QHvPmlJHfA2AfswKBrShXyk2XRVNKt6U9jFrcXxbjNA\nABgFuTuMI/1JWNdqPmfGdjSJyKN1H4Z9dJOBBulu9KQNwN1C3Lpo83+9cP2h0jYhUdKpX7kh8dhx\ne/dPH4sAACAASURBVOPt1RRHZttBVTOfvx8e6koSY8pidhtSyu8A+A4h5HT2UJbo+QMJ1O/HvitJ\nEly5cgW3bt3Co48+irNnz5aYlt0C9f/97QiMmjru3JOVSiRcgWNZC9KrnjN1uDGnsGRkvBm8u6w7\nAMRpVmiqxk1ALTClmfQmdxYfaKbqWOJt9OSGtXScX0Ib1hww/dkpYE5wkmr9dNWxZgx5AbxT+Atd\n67q1mv7HLgNlsudAWeMZy2KqZoSXWAsumdVuXI1X2fjJmLTVsS5YZxDYEV0LrAPANs+Ycp/OFShA\nelUoVr3HhthK+1i6N1FbUNQ23lkMWxeO/s3/N4Ev/XrRnfRD3CDjIxOmOcDOTnlVJk1TXL16FTdu\n3CiRKm1CMeq+2I5zcoaV89PmMLS2d4zkG1BhgXRfND3PJbUcYoiTT02HGKBe9ldnjWqGqYUPGG+U\n0pi5oOrvTCZTfEFN60ignbtNIqNy91eRN4bK/wUhib0A3S2O9TVe4wggWXGNnXTb6eDs/19xEoA7\nN4MJ6+j9faw7YLP/CesgEIkG8/Z+DIKxynFUV9aURQgqCklTGiEQsSWDUbGJqcrunzdG9Z2k2zY7\nOn/taey1X5mJm5ZuNbN9bvO6veby6elp3L17V48zOzvr3e/8+fP3U0hqhZTyUtVzBxKou9HGezdJ\nEly+fBm3b9+uBOgq9sKoAwVANx+rD6IC7OY2ABgmxWOTfTcBuq9oTuYSk9iji4xMMJiD9UyXXX3t\ntl1knoR8jjMVchwVFsOU/zbBuy/UdgVI624AmkIXmjqTg3t8nWexzx++qljWHcec6ITMilV9HVm9\n55XMuEli2hrR17jJ5zKhJS/Gseb+zCkoMzX0g7RbKmLzgXkz0vyGqMtiDLnRCVFSLN2rb0Kk4u52\nBzNj1fKXd6/eXwpzgfrW1ta+t6n/OO4vTKB+717hfqFWPW/evKlz9m4Augqf6cDf/5iV8rWKlPu3\nj3hWSDqIA/Sj1NoOQBeZmjnQNy25q5aaEJF2nlIOMWZELK0tVPVFLAKvLHAr7dk5sGbIOkY+dRhy\nk7gQoqOfq1oTq2oE5UYsO5p5BzIJjfnYF1V1OYrNViElQShjy1ayyklLOiBcAX5XsgIUhakx64FA\nevdR4yimfhj00eFlX/HtaDIfq+tl1TfIDCLn/eiSnVJnbgC4tn2skhC7vTOFIz0/uHcLSn9wbc67\n315iYrz5M71fXaZfeeUVvP322wAyPboqFDXHfO2117QbzH0Uk7aKA11MCmSFBnWgOo5jvPfee3jr\nrbfQ7XZx9uxZnDhxotbSZzfa99e/10GSksqkr4ILex/3MZCBefPHB6ylLEC6GcIAwTGn1s8wYbUg\nHTCKOo1xEk6tnzh13A0qXrNbjJUKimEaIOEMieGi4J7PfU3Z66I5eLZ/1HPufr5j1TU0jVEXXDCr\nYDYRYVFMU3Fe9XpUAVciAoxEMV0pa8hqPWpkTVjm+5PJiBhGMsJIRiWmaUd0S50OB6k9OTUth5tx\nd2QD76pr3kp7+PmqnbzvbNZ0wBPFeKvbtvb9x4u7B2VuuJKH+7Hz+jgeTLh2u0mS4Je//CXeeust\nhGGIs2fP4rHHHtsTSAf8xMtO7P8MVIF0FYM4QMAkRpxhxBl2kgLMjTjD5ijSz42sXEet3+72qjBz\nFZCBd/O7t51EFqBv02yuSf9uFrDGIsA2tyUkvrxRdV6bIe8glqH+cY+jROi8psIF7AkijGQXQ9nT\njxNEGMqelgjqYx2QLqxxi+fUKmZCIiQkwpD2MaT1q4dAzraTQLPnqrhVEApOQw3SFeCXILoIFijL\nbhKj86pbINs2NpLC9cR9/feC+o7MblFzU/zg2v53eP6f/6r++yeEsJrX7ZVRV1KW8+fPY3p6Wj/+\n3Oc+p7d/7Wtfw+nTp9+XFdgDyai38d6N4xiXL1/GysoKHnvssVoG3Y0m8O+LJM0lDo4uaySJfhwG\nuYShBaivC1f3LmQZtFNHauJj3juBaFXUCRhsuJo0DCImgPDqN33SGykztwSXaQodv+ImAK0mL+l0\nV23riw4AI8dZJjAmMl/TEVP6IyWxwLG7TFi+kaLGikHZVtJ1oDElMDwH+hLELoQ17sPdZWkVsQyt\nY0YiQofG1nHqfdhJO+gFI2uMHdHF+mgMjEjEPCgtx7uv8ea9cqHp1Fi7ZlNmvLPArM9IGytGFRsb\nG+j3+xqgRdHe9O1uvE++ux/pWFtbw1tvvYUTJ07sKmfXhQvU//b7EQImEacGuZESREE1m+ECeJ+d\nrhtCkFxmqKL4OwrKBeylc1ZIZxQwdwG8G03SGV+YK2PmcQrExyakqBiuDWjnkmHAx+ycKUlJNqOf\nUxLChtegwLq7omgG9xgEAAWwVdIZZVULAKEcQRBDhqjAtycpVbm6mJGwbqnYFCgAuqtp36JTiFDd\nhX2DtAPN94IZLG/7ZR67iQcB0ncb9ytjNAtGVVy4cAFAZtu4F0ORvcaBBOoqFAtjJmHVqOjOnTu7\nBugq2mrg/7d/7hhtp7Pf7mMzkpRosG4+VnIYE6ArOYxv2zChhnwmdxNhNmBXYF1tc4G7kAQ7CStt\nD5isLOysihFnoLwYx5XMuLIbn5QjKfkVN/jOGwlbAWhXMuMD3r7jNOtuXnM+lloeJERa4Jk4FpMU\nZYcYRrl1bt/rBoDY8XNnWntfZtPVNp8neSpZyT0CyFhuU7M/EpHW4Q+SnqXJH/EQHVZMIOsjm4W/\nN+pgslM9YTSFlAR3tkLMjRc3C6b8ZXW7h3cv1/j6CzQ6CywvL2Nrawucc3DOMT4+jqWlpZJV427i\nffTd/ciFlBLvvfceVlYy94j9AugqmqSMKScauEdBGcCriAKJgPkJiaZwC01NAG8+F0R+7bu5TxWA\n9xWz6vMZQN53s+1bFWzSvO+kHSvXdFlcYsjbylpUgam7CggAPcLtbtZ5LlTkiBrP1KYrWZ9rGwkZ\nIjCIC5XTTfa5dAwK0K5ki1WSG0EoBKEIchBeJZ9JESKlYam3RYpQu3wNgz66aVEEGssOIpLlyW06\ngTGR1eKs8aI7dC8YYiOZxFRYSMhMnfr1weHWMlIAuDOawlzHlsHcSXYH0hevC5x6ZP9XMdfX11t3\nHP6gx4Fc4/V5745GI/z85z/H22+/jfHxcZw9exaPPPLIA1vm/uv/1kGSAqMk+1EhZL0DSpISjBKi\nGfgkJRjGBMOYIkmz7Qq0NzHrQAHkU17+8YWQpMTAm5FygpQTDBOKYUL1dXrP7fFKB2zJzHYcIE6p\n1sAXzEjxu+p6zHFGaaBlIk2sipLepJLqnxEPLQadEFt2YgJ5c/xEMCSCIeaBvhEgeZGs+frN1Ke2\nKU/4EQ9LNxHma7FdbUIMeceSyPgkQQkPK73m2yxhMk/Blboud6WhLraSwvv3xkaZTVffhTr5ixk/\n2bNdLXD9ZjZ5PvXUU3jppZdw5swZTE1NYXJyEpcvX8arr76KxcVFfPrTn8aXvvQlvPnmm63Hfh99\ndz9ywRjD9PS01qDvd842gfrffr/4Xqlcp4ILYkli3Bwcp2V/9Tr5X1W4chYztuN6bq2pQNWMmAfY\nTiJsJ1Fpu/oZpuVeEWZUPecSAlwwDJKeJZnZSTs6l/hkLW1jK+2XJDNNobze3dwpQXSPi6HolXpO\nVAUHsxrcJYgQo6Ntbt0YkR5GpJcBctRI/zz9Lsz9h0EfW7TZ27xNXB8c9m6/vV2W2awM7cZBh6IC\n+P/w6v5cj4qTD7fjk4UQFom6sbFxYHpiHGhGHchA++XLlzEajXDy5Ek88cQT75sGlXNAEXSJh7AJ\nnXffBfBVTi9Dj36yrWymGJtgmJj72syQL1zArNjfYUI1w658yUNnDAFi2xh63F6GKbMeB56cbV6D\nOr9Ktr6GIiZ771qpudsI8ReqRiwtTUhWV09jhcE9vqrIVAU33HJ8jL/L6pvuOmpSSwXVLJgEsYB4\nwkOEOQNubncnUlX4qWIkolpAPuIhNkZd/f6a78FW3MF4NNLXvJV0cXtzd42I3PinH0e7cg5YujHC\nw8frz6m0z7Ozs/jd3/1dvPDCC/iLv/gL/M3f/A1+8YtfoNvt1h5vxvvlu/tRjcOHDz+wvK2A+nA4\nxCjpoN8VFkAPmCzl1ao8q3IqFxkL38nz4GBEDYcYore3sW90I05pad9UUIxFNasCoqjF8THvdeeO\njLRkMu/doMwa+0iAqm2K/HDzjMvo+zq7AmULWH2NlkVk7qlt+LuXvN2dXhHmOX0kiCmbaWUDmYP1\nkMSVshoFvgMkpRsDjsB7HQAw4OOWd70Z23QCa8kkQsoRi9B6D7Z4H+NsoM+1if0B1/sB0ussGk+c\n6APY8h7nGgOsra0dGAevAwvUR6MRFhcXcevWLczOzuLFF1+8L9tGNyilEEJ4J4+//m8dKLcvBdbV\nY3NlXYF3zoGugSmkBLgHtDNagHd3eT9JCYaC6O0JiAXefVKZKl3vMCYGKKrXZlZFkpa7jHYCuwlS\nk3zGYrNkZhep7CZVJ08V2U0OKdk8JpyWriMKeKvlaHVtrr7TnNB83VXNc7kOMT5feJ+1JZAVb+kW\n1MY5zf3VZKpAvg/KJDxEamjg9XbBrPMqsC5BsBV3Kv3NhaTYjKt13e4N5431rgYp6jN3d5NhZsJ/\nEzM9Zk887y5Uf/5u3tzBsWNlpl5FU5G02SBDaRqjKMKzzz5bf+AeYz98dz9qsZ952xdCCKysrOBf\nbj8HRoHBkKITlu106x67+VV93gej5puLkbkqGbRwtnDAu3q8HQdWnoiC7O97w8gC8W1Y9yZ9OwDs\nJBE6QTau6r5sxm6LD/W4acfQphd1Lz65jXsO8xps//jQ8mfv0NjLpgPNvuBKNmNKZPQ5PbIYNSbP\nC1x9+ngli0wR1Orn9f4oZDGJCDVY3xZ9jNFMDrOWTFYe78b1rRkEHqvmpjgUbWn5y79emSyRKT6v\n9LZxbVni0aPtD06SxALq9+7d+xiof9AjTVO9rD0ajfY92StLr06nzNoNRxJh0EKWYoD3JC0edwwM\npCQqPnZZj+PBU1wAsCaAxsuB6qKtvmzqceyRtyjw7gPZPracS4JR6kwwICV/YhfouuP7il6rNHVV\nNwCj1H4zXBuqppsJ6VwDo6Kk22fEvpEojm/2hdfFqTABfyG5Uft72bYc3PustVwNfCJY1sEwMItH\nKXacpfBhGqEbxGCUgwuGuzvdxiZFW3H2vVCSFqXzrYvVrQCz4/bk97NFAfoAgdqH2Xf3oxpt7Hbb\nhCJzVlZWsoJiAx+NEmLVO3SjZhDjAvg6h5hRWshqqr4Xg5h5mXc9fgPgNpl3pXl3mXe1XYH6Ov27\n7pBt5NtRGlh2khFrLgxX4FoB6SaG3IydNMsrZp2MG77xXPBtSgcBoEd5LUBXtT+B0VQukaEG1XE+\nngLMLug3Abw6ztfVWoJkTe9MD3kRgoPqov+IZPl6PZ22jAB2E1wwbCFj1a9vNWvK72yPYc7oBO3e\nlP3jL44+8K6jACC4xMrKCsbHx9Htdq08sJ9dppuMAQCU6pAeZBxYoD4xMYF+v487d+5ga8u/VHI/\noZZLXaD+2vkoB9xmUWjxPGMEZm8NxbCboD3lZQbe1ZWnvB68u6G05L5iO3OioLQA6HWRgXfjS+IB\n3YANcF0QDmSTGTecbwCABby124wQBKnpboKMRa0Cku65gIKNNpkx1uDtbo0pKLikVndBxgrmvCnU\n+SVIJWhXzxNIa3/djVQSpzkUKwC9ZwwzTNbf18ypLu7thJjs5dIa57Uur0eN4NwXqqD0vSvZ9Qsp\nQQmBENhT44zrN0Z4pEIGY66KHSTf3YMUPhev+3HqieMYCwsLuHv3Lk6ePIkTJ07gby/aE7pLfgyG\nOflgfP4IgWbedwPS3efNm1jFrqttozSTyWzHFGNR+yK/OmDvK1SNU4oYVAP2plArVa4zTewU/UeM\nYzsJC3DPWS2Y9/qj86AoCHXkMqq3QxV730bzzgXDlug73VqznNZUNLsjMnmcqulRfSiaGgENxJjO\nwz4nG995ExkgJKnVBM8X26KPQdpOtrfFC/15Kkgjq26+zyvDSRzu3sM7V6pXNB9E/Kf/6wi++OIP\nMRwOwRhDv9/H+Pg4OOeWwmF9fR1TU7uX4jQZAwBZ/v7KV76CS5cqexTtaxzIYlLA9t7dS3OipvA5\nBRQgHfA1uuMcGGxLJGnxwx1Qbj6uG4d7ikNTnk0wapIxJ5uq7WaMEuIF6e42Iext6rEqvkp5VvQ6\nTOo/XqrY0gTOXGRadbNQ1BemF7spY1F/ux7v3FoSda7DU5jLBW28huw1lEE6lxTbSYgRZ6XCMF1I\nKokj3SkkLLpA1QDQdcu07mSkNfuCYYeHJX94Nb4K8/q24q6efJUHdEAFhmk2OdzdKU8Ag1FZy768\n3gyk1raql61/vpBCuPovTxw50l5H7guTnT1IvrsHNXzNidpGHMfaUGBychJnz57F8ePHLRbOzJNK\naujLl+reYTAkGAwzAwDzxwy1bTCkpQJVN3wFrNt5TdIoJdZP5RjG91kx4a4e3d1PRZxSxCktFa26\n+6q8525zj9lOwtK2mDP908YS0sxv7v7ayz21vd0lSGuQ7otYhPrH9YWXILr3BVBml10rXaWJV79V\nwavK0+5j87Vq4K/Z+Oz/4q4IqK7RKkzdv6/uSsWNzXbyGPc1qfjHn/kLUKvi8lKGma7eaL4hdHXq\nZvzar/0aPvOZz+D555/HsWPHQAjB2toaVlZW8M1vfhO//du/je3tbbzxxhv48f/P3ptH2XGV16O7\nqu7UfbtbrVke5EGSZcWTZHnAhJWVtWwnDMkvCxJY5heT90ge8Y+YR4JtpjAaeAYHPMXYBhwIg0PA\nYAdiTAiJIITBEKslYxBGsrql1titbvVwh7q35np/1P2qzjl1TtXtlmzkdn9r9ep7azh16nbfc3bt\ns7/97do1pzEjzxgAQOzg9XzFgmXUKYiBOdkhA+qKKtTK7QDgepFMxrJDVMrZLExWOxS2k9adl3IS\n4WliMfSE6fEDSTuFeTDvrsa1w2rn2ZBp74n9J6DMrgbInvyzdOeWa3AVX1VtxG2xdpGhlgLrsnOz\nfI+5iU5SBVV5XqjH9yVbJYgnXaSXpSlEFt3vnGMIyWjiZCy2NVbvk6+aKKRXeUuhxMqxn/VUs4C9\n+72umPO5eKd3E7VaDZs2bZr3+aeS7+5CjPkSL6wlr8xQgKRPZitEtZcFyem22P/rrBVO29W6ki2y\nkZfAmvQhWvkTwXq0PUlUBebmAsMeL8plaCwoFYKU5l3VRjdhdfTvFcNDyyvF1Vu7dX/J8pJnk1JJ\nZ0+VW7P08yKbzVVrRjGqUipIb9g6FUACqL2gEOcBlRRJnwBfz4J1rvEDI/M8YtnZEIvPUdSdKgZK\niZ1jtyBdFbtG1cn6Bw87OOtMNVmz9jQDrOqALBrPPl3DgaPqeVmsTlooFLBkyRKOOb/qqqvwspe9\nDG94wxtw8OBBfPvb38YZZ5zRteQwzxjgNxELFqiL9ownO0Sgfte/FmCIIMYFSsV88O37YUenHsbb\nZEmnAJTadwLxBFwIBGka4Ehuv6ecoW2XRBimPYPFIGkCVUdVti1hg1gAKPZB1icv0Hhm3NdQkoLI\nLOaJTazVUDQCTv4iMiVA8jDAeaqHGlzoMZBWJTDGbL/I0HSkOmySqOyhgx4WssApTVxF3c+VvPiB\nDkNPNKJNu4C+cmcpubNK0HYL6Cl6mGllM+SmXUS1HP2jjc0kx07XgWXCfJCV4Ll3fz4IGx9vYc2a\nxL+dEkrHxyysOS1h2POcX8Rk8BMtkLEYz03I7Ha7CbGoncrx67PfL8PvrN5YNj8GE3liMwYn5ZzF\nIpX1LSAH8J4PLKmK301J7o+QtMoSD+zxNlMJm44pF3hgX5DcgypJlXWJ8Zy0fIaVy3SbqFrQg5il\nDQINraAIXQ/jaq1BoKGnmD0WyPpb0IN43I4kM9HKJTlaOX5Bmr+jCjHPKOzIN8n6NkAiv4ned+aH\nIKk2SuOuExSVMho3KMJFkZMe0sOE6NhSd6tSjX7L78Gs3TsnH/SsmDR7sLLajt+zOvVf7C9JiZj5\nyhNPRriui97eXui6jnPPPRfFYhHvete7fjOdOcmx4IH6iSyVZkVWkQwa9A0j+ZI7LjMQKwC8qEuX\nadktO0xZPrLgnSsc1AHMqUzsADDb/PZSMQ2IibGUASsC7eIXUwTwmqYG/zw7RdrM6D0B96xzqX3q\nhxMPqp2/vZ6e/ERW3Q8S6Q0BYZf53ItGZJHYjUtMinWXVGSN+9uZTOJqpIKPfDrBlewZ+XZEi7bo\ntwYLhpT1N1KuCZGbTtNRL7uwZdCzwrSLODZrKBlEWULpTNPA0r7oXvfud04oSXD1mnwZzOExB3/x\ngeP4xw+vkNp5LRTf3YUa3QB113UxOjqKiYmJroraZa1UWnYkTyx0YQ4AZIN0dj8L0oHIaQbgx1NK\nYLWc5NoVAWDnFcGjaDk6N/axiaq1dgHVcjIusImjcb8VADyABsszUpVYy4YP0ykk2nvfiNly7nxF\nBdcQWgzaKej8plNGpZD9P5CVG0SSwpLucfJC2gbwrHpenhFblbWQkxDLSlJkYJsA/lxyhVyF5WPd\n7sFAOQLaovzlcL0/JpYm6yWsHJAXaJLFL/afnErOgLqmDGvRSP+3xyY9rF6pnovY8bzRaKCvr29e\nferWGOD5jAUL1Cmea+9dgNj0jmREAOltK+De0372a1Eqarm6dFkQ60NMPBsy5j1LquK4iUSGZeBV\n7NFcHGKANGNOAF312nbTSZ9RldZ8SQWFJ5mwxAqv8f10uheGfPtRlVfmPjrAOAg06HrYSeJkJksm\nWVasyArwnwPrw04RPzTEzgjZEh1K3hIryALp5CDX1+GEOnQ95CZNFqSLrDoA1CxKkEraalgF9Fc8\n9JR8tB0DfgAcr6cR+kBVw3Q9TLHqYuzd3/1EkReqhzsA2P3UYWidD1kE6ouM+qkbbM6RCqizAP2s\ns87qqoLpp/8jPcBljb+eFwISva7XGYNZ6Uy8ryOLEUG8CNrF8bllEYiV98W0dM6NZq4sPOs8Q2x7\nuRB2DdIpZGC7HWRDC1mbIhlCK35xf30DfqBD10JOyiIG59ClzO3R0PaLgpWujoABvT0Fl29LAOye\nIJEJQ43710iK90XHUZ9p1ZSqPIsWvtG982OpExRh6H7semP7kQSHdewSq0QDgOmWUS0mlaLrThXH\nmunj4nsKNMy2y1hRlVeXPt5Sn/ubDnY8P5GxvBtjgOc7FnQy6XMZhUIBjuPg9q/rcN0Qvh/GIBwA\n9z5yeuH3s+G4IRw3jEG9KrImEHGf64Xw/OjHccOu9OSWHf2w0bYi4O640fIvm0hKOnUxsVQWjqtx\nyabdyFvEoMqslqPFr1XXFPtF29qODtfX4h+g+0JTQGQPST+Wa3CWk4akIqkY3GcgOY7tS1xVNtDg\nBVpqtSTgEmSjCq7iwwmdy38OEVtFPxT0EGF5Biwv2k4gnaJpRdsrRf5DOjYzP89kANj+y5Of7C3G\n4aM2dj91mNu2CNRfeCED6q7rYnh4GE8++SQqlQpe+tKXYu3atV2TNGYrgGWHsOwQZosdw5Nx1WPI\nEE8gRtj3tpP+MVuhEqQD+WMqG1GVag0tK0r8J7adZd1l78VQJbWatt5V0qoX6AigrsQqY0ppvDGd\nAjfumE6Ba0dW3McP9Di3Jr5Wp8YF/bTcIkyhvoPTGcdIBsiOuQZTi8ILeNAdhDraXpFrP+6L4NXO\nBiXqW34pVe1aFrZflPrU234xToylIJDOBiX5s5H1YHWk1pu76pMVz4zOf5yfTxwa5/+Rjk1m5Eac\nJKvdPGMAILJvHBoawiOPPDKva8w1FjyjTnGyvHcBwPd9TE1N4dixYwDOBwC4Lumao+sUO9IWxwkY\nfSWjd/NDKQtPEpmUfIadmxjpDKtnzyquRGE7IadxrpTTTi9ZOjPXiyqqikmrqoRV1YTjMhMA3VpS\noEl+jqi79AON19/7ck/irIcUVzJZFXR5P2jyIYBMLDZNeG5nO5ssK1ZkBRKtI7td1BWK7FgYavCY\nCYKTvAj3oEsYeFbfHobJioDlGqnPjCQqE/USSkV+HwvQiVVXgXTZ6sdMQ8fS/qSN4f1y5uZkxZGj\nNoZ3HZXuEwtkzNfOazGe+9A0DUEQcEDd8zwcOHAA4+PjWLt2bVcMOhv3fKsIX/iS+37IkRVsgr8I\n0FXbZPttYcGIViptJ8oXolCRDuLKJRssWGcBP20X5TLdWEcWjDDXnYaVy0R9y2+Xf52MG0QMVApp\nFMkCdF0LUxWygWT1NKrXUYhfG1oYg3UZ2hFBbcBIDP0wYfTdwOAeIERpIrHtsW96mKyO2n5RadcY\n69CZ5FcC7pSzRGC95ZbQW0z+kTQtWtG1vBJqVhnlovyzE1l1MQarXlfyl+cbpFN867FDABB/V08/\nZwX+7/cewxdvW80dx47nJ8p+ZxkDAMBrX/tavPa1r513+3ONBQvUWVBOMhXWhms+EQQBDh8+jEOH\nDmHp0qV4cirxQQ4FIbfrhtJteaFi3cVjuK9UB6xmJa7aTtQuC67DMEmeYoPaUU0a4qQDyBNW88C7\n+NwUVVflwV25GKZ0mKolXkDO5ORVVhUfALwgYpISOUn0cCNOEEGYHBOESdKQK7BQYkVWWXTjKkMR\n9a8AXUsDfGK52O2WayAM0yy45XbYpoxiRI2Wjv7eRKsp+uAPHy125Wox09CwtD85t783wFO/clP/\nA6oHavJSp+g2ofTZXx6GriWfraazwItn1IMg4N4vxqkTbM6R4zgYGRmZN0CnyBtrNS0ZNz0vRLWX\nSTyu+9x7MWSse6GgMduT/8O2nRzTX1WPE3mrolksPEAsvoZqhVkBcLPHJVkQ4LaEcytFyYNMF6Cf\n66uXDCbRwwC/nx2DdS2UyhtVQYC9VPBhewXOz50tvNRVWxnVqmXBMuusXEVsU5TyEFhvdYrQQcng\nBAAAIABJREFUNZ0y+ko86K5Z6oR5CtMtY6rJP7G5vi51EhvsSU/mv9qXSEADP4RuaLky1ENHXKw9\nIw0CRo94OOeMAg4eDXDW6eoG/vWbh6TbD+4dl24/WYz6qRgvilmJWJj5AvUgCHDkyBEcPHgQq1ev\nxkte8hJYlgXnqeSfvFQixlJLAXQxZBNEsajlThx5+x03VDLvUd+i36wtniEZ5xzJA4XsIUBmj8fa\nLM4FvIv2jPSenUjCkOwd6Vi1hRkbjqfFDDKF+AAQXTOdbAooWHcGnMoYnrjNUEPLNTi9fYHRckY6\n9+T6ACUBM+yNEXATEvtQIqsWCMi3W66eAuuqqLfU6LtpGeirqNdPZ+ohlg7IP5P+3hAzDR37Rtsw\nukg2ODZmYvVpSVGObiRcFM/+8nDm/pPx8L4Yz194noexsTFMTEzgvPPOw1VXXQVDtmzYRdzzrSKA\n7PE0uS6x4mHqfbmkxb/ZfXMJ9py2hPiUAfi2nc3Ei6uirNyBxlTWUlIE7LIHdxFs03hJ45ElGe89\nX0NvWf2llQF4dptpJyt+nq/B0IFKkSozZ4+7XJuc7SSx2B3gbviZIF1MxPQCnSMsxDodxLYTqy66\ndZFcReY+M2P1olrMZrbbXjRm9RTcWO8OALV2CUt6HO5+KcZmo3+WPNKKjcEeB5NmD57ZJ1+dz4s8\nHCQGWTT+/En5uG0YOrcC9n9uq+Ez7+VXQOmBfaHJGBc8UM9LQMqKIAgwNjaG0dFRrFq1CldeeWU8\nsd/1r328ZMUJODkLkIB3liUUdex0vKuQvLhuyElmZOfKzpMlrcb9KibWhuJTsQow54H3bpxdonb4\n/SyBSQVGVBEPfB3SwSWHFKYNYiDEBCcx2IlJZISzwD9NTsnEF2nDS4WQs+LKYtDZCaao8Q8MsrGN\n08FL7ifSoRvShFLLLaCgh7HkxXJ1lAsBTCe66SLp0hkQX28lk2O5GKZYdSAC61M15S2mIgzDmFXf\nN9rOP+EE4sff665anOd5qFQi9n2uk8piPP+xY8cOrF69Gv39/Tj77LNPqK3YjtFKwFKlYsT7DEPj\nADTPhidB4N12wnh/oaDFwJ2NE5HONMwwdp4hdp5AfU8OoTofTbJp6XHV1W7Z8PR1O/JPhc7d8bQY\nOCbjav61LNfg5pVSwYfjGSgV0tWgZauAcUJpZ9x1wLP43JzhGzHrzPrJy9qN7ysj0ZWNtleUMvGm\nW+JAfNstcBaYlGTb9oqot4sod1lNNi+mm0Us6+Mn6Gf2nXi7Qdh9MqSKRZ9rzM7OYu3atSelrVMh\nFixQn6/3LhBN2gTQly9fjiuuuCJVsrob9ttx0l+gQkH+L9uN5MWyAg6cU5Kq6r3YPp0rgm4PvP1Y\nHmCmoHa6tS7j+hN7qifbqFssyUkDsiz5lAZqVxgPC8IygSzhiR2MWamKFWhxu8VCtLyaxbwTm55M\nRp0leoWUhO1/dD7//yAO/qyUh00MlRZ8ErXzTDIqe/zxRhE9CpZrpmnEha/Ye2hZOnoryTkT09ks\nS60RYkl/Omn457ssMEqUeFVmYtzEqjVVzDVYfD021sa+X8v16LIQ7bz6++WFQhbjNx+6ruPKK6+E\nrusYH5cvfc8lLMuPAa8sxHG0W0BNQQCe3S9KZVTAXfVQoAqVdCYLoOc50ABpz/eyRNoib1tLvWdZ\ncXodhBosV0OlGKBl6+gpzY15pzkgZsgpcTTQUqSFCGZlZAxdg50fwlBLNO5ACjGJkkV2JdTxCyk7\n3PhaTHVYAuuOf+IacLaeBcWhqYokD0ldiIucX369L+jaYS0vDh7xcNYZarj5i6Ejyn214zUsWZHO\nHapP1QFE20WiZXZ2FhdffPH8OnsKxoIF6mx066UehiGOHTuGffv2YdmyZbjssstQLqfpivd8zkOx\nyP+XZwFtFiR7Hv/FNQwjF6RbgsUjtcme143+XdW3cjn5Ntp2AF2nBxyeMWcBM/sF9ry0baKHTiGf\nIq9lyyqoRGHZIQxGK+IiDd65exCvzVpdMpVRS8ykzNorsm1y1oyOxvVDBb7ZZWdabvb8KLGLrk2T\nHNt30ec+DBEnjHpBxHarlnfzALs4UXlBxAC1HV15H5ab4TUdJGB9YrqzzVeDdVmuwIEDJ5dJF3Xq\nI786wmnQKRqzdfQPpv0hF7KmcSEGS76cSNz2VTkQJnadxmti2PNCpUcX94vSmbkw73n7RJad7UO5\nFOUVUQJrVkVVrk0BzIvyGM8HNz4CWiaYV7HlLTsad2jlUATV7HlZuugsKQy1XSgFqZynblcM/FCD\n7SYfVnZ11s44zrD1skRZAHFROTbcwEBR95U1LPxAh2mn/3BeoMVg3dADHJrKry0hi1/vS0/UY+Nt\nnLamZ17tqeLg0QC/3KEG6d2GWLxuoY3nCxqod+O9C0QAfXJyEiMjI1iyZAm2bt0aL4mL8Z7PRV8o\ny/IYeUq0j8B7N+w4HccuvbKhAvCsxIVsIdn3xaLGMe+sdEblNON5STssYE6Bf5kERwK8/YDxA++0\nwX76MilPdJ5iNSAIAVFDWUDqAUAm4WHDEcbVSok/Lu9hgneqkSdgqhxzutGAUsS2jH7ykCEeH/vW\nKyYnx9NTE5rBTKJtW0dPOUDbSZisRiufPjkwpnVdEp1YdQAYPdDmkkHnEhPHWli1ujf+LcbIr+Y3\n0LN5KwttYF9ocTLtdlVgV5QlimOzrJBdFpifi3SGPUcF4POuIb6ntij5n36LDxIki5cBeeV1O8dk\nAWdeXiiuFKrdxlipHz1IlDnph9CXLqpPE6FhOhHkofa6AekkpWFduvxQ44vE+REQz+oLJcrKKl43\nnSJKksROVcy2CrFsURamLc+9marpWL5Efh0/0DDdpPNOwMMRwBmnlXBkTK21Hz3iwdA1PL39cEwM\nzif+15/vwrc+f1Eq32hRo/4CjGKxiHY7zeaFYYjjx49jZGQE/f392LJlC3p68p8YRQBNANh1fbDP\nA67Lg3cRJANAEITSf1TTdCXJdsl7UfJC22TgngXc9JoF7xSiTFfUwbPtWJ191I5KLiNjqrn+gNep\nJ+dFrLoKvHtedAw7gfeUu5PsABEbnOc1HPUjPREl12AY+k43SXOZlfToB8Jg7VMxp7RWnaQh1Fcx\nGUh0QQASGY7Y90abt2Js2zqKhYhlrzW1eKJWTb6TM9kPoLV6gCUD/Im1RojDR/KZ9OUre+clf/np\n94c5Fj0MQimrLsb+/fvRarXgeR7CMDzhgf2RRx7B4OAgdu7ciXe+851z3r8Y3QdZNc7H6eVD/8S/\nJ9A6n0R+3w9h29EXnV2VpHaz3ouRJ52hbaqk1W7kMnn7bScC7bIid6qiTak2BLlMcr4mfZ3ZXzYB\n1tPj91FCafS511oGqpX0YCtbVWTDdCInLALw5ULAOc4A4HThbMjyjzxfQ4sqngrMeSxz8UmaCKnb\niuPrHFifaZdjFr7lFNBb8jh7StfXMsH6sVkjzt/KcvYSY++oB13iMrFq1dzZdJXzy655sui147MA\ngOqSpOronj17UCwWEQRBvEp6IsTLqTiWL9iCR4CaUSeA/uSTT2J8fByXXHIJLrzwwlyQ/q4H3Tmx\n5b4fdsB79GNZHizLg+uqRzvfDyTevh0vVzeA66a/4FnFlFTHuG6YKZfJ0rrL2rHsMH6taxHYY28j\nC8Sz4XnJj7JvQZgC6WEYwmyHcXEmNmmVfQBhi5iIRaI8P/lxPV77Hobpe5KF0ynI5Hgal0RF5xJI\nZ0F5ECYTnOfLE0rZ9rOSs8Sg/tJqgGySrDXl7ZmWfHutzn9wtYb6Qzlw0IxfB10mbIZhiInx5LxQ\n8bC2/5nu9ehiVKtVOI6D/fv3441vfCPe/va344knnsCDDz6In/3sZ7Dt7v3dd+7cCQC49tpr4wF8\nLvsXIz9kdrvzCdv24h8AMJvOCbttRe0G8Y9pevFr2icLAs55oN7zkkRV2wlhtgKYrYAD8/MF6Wzb\n8b0whZrYwk1zYdrF17arwXY1mJamHFfY80Swz4J0ICrMZJJkptN2qh2ZDl1SAA6IHgSyxnYdIVcs\nif4dqXAUO7Q5ngHHM+JCSrJgte0+p2uPtjdtNYc62+L31drRewL1XqDh2Kx6pWemkVz7eJ1va+/o\nc1987pdDajeu2vE5OBR0YvXq1fB9H7Zt47vf/S4uvfRS7N27F/fffz++8Y1vYHR0tOu2TtWxfMED\ndYBPJp2ensb27dtx9OhRXHTRRbj44ovR29tdWVzX9mC1XLi2F7/Ok6fQe/E4FsDTxCECdGU/OoA9\nAf5B6oeuJ6uYyrcVwrICWFYQD9q2nbxWtSO7XwpyQGB/uOMFwOt5yWTARttOgHfbDmOAzkYYhp2f\ndL9Y0C5rX34v/EOC6wGWA9hueqKie4jAt7y9tp2upJoVQZA+VjZBmpYOx9O4hwsg0XvOJ5psVUbh\nX3FyWsEuZUzeBNK7/b/OixWrejBxrIX9zxzNBenNmQb3vjFbBwDUp6OJYHBwEMViERdddBE+//nP\n401vehOuuuoqeJ6HL3zhCzh0qHv3gYcffjgurrFu3Tps27ZtTvsXY24xHxevMAzxns958Biiw2xG\nAwKNo5bFgxTTVBMzsu2eF8Q5SPSbBeum6XEgnsbZrGD17Ox7CtsJuR8C8Oz7ubrNyMZsgK+5wb72\nA34MUrbL7KcKq/RDIZIGsnZpOBElJATYoweC9DjoSVYs2YR7MRxPh+PpaNk6Wp0q1CTLUVV2jc/1\ntfh8Ol4MMRE1L1pOAS0nXwQxNtVdu+Lnund/FxNkJ4IuCUs2Dh7xMkH6fKNaraK/vx8rVqzAK1/5\nSjz55JPo7+/HZZddhl/84hd46KGHum7rVB3LXzTSF9M0sX37dhSLRVxwwQXo6+vLP5GJmz/ZlstK\nOswMp8EuF6QOLKKUhLYBpBWPRqAgBqM+SiWSzgSxFKYb4BNp6JMvbLGY/vJ2yxSJK8xsW93cIxAN\n/pYXcm31VNIshkx2QYOCkyotHSp92eNzBWlRxFgzKwIeUJQ4P8gSJYmN78ZTViV9cRjmxyhFbHog\nsOuxvt9LlpHZCqEiO846KtBvdpnTtDSUmG86O2lM1zXu825ZQC+TnmFaGo5P+9zftFo1UKv7WDIQ\nfRDV3mhfXzVp6OBBM+WxfyJB+vQDe8ZyjxVBuhgfvdHDrl27MDg4GC+X/sd//Acuv/xy3HjjjXPu\n2+zsLJYtWxa/n5qamtP+xeg+5mq3G4YhJiYmsG/fPnjuZfF2FrB7boBCZ0wjsE7bLMtDpVJAs+HE\nxwBApRJ9oTwvQKGgp0wCxJDtZ7eJ0hnT9LltWQ4x4jEpDTojmRHP72YbkIyZZovmqwSse14yBtD1\nACQynS6Y+EaLYZU7Y6RYFRnIX9Gksc3Qw7gd2tYrkchktxX9DkINAVP12nJ1TnKS1MGQSwadjmwn\nlkUyY3zTNlLnNC0jPtZ0DFRLPgp6AC/QUevUtyh3pD+ur2XWxpiuhVi2hP/bi3PT8XoB+w/KQXos\nQe3M62JCqR8AY+M2zjwjv+jSrp0RSJ9PvonM+aU5E5Euhw4dwsTEBDZu3Ajf9zE8PIwjR47g1a9+\nNV7zmtfM6Tqn6li+oIG6ruuo1WoYHh5Gs9nElVdeOW8LtjxXF4DxPrc9oJz90araCxjGOPBDOA47\nykWvCbzL25V/aVnJTKSdz37qTpJLQ7Ba7Gg7SXHYM3jwLvN4F6Nt8X0l33mgA2C7eJAQfdlJXxkI\nzDv7ni38FA1ayT7R2hHgWWMC6bRNHPTCMF3UiS0MxQ7SLJPEF1ACRJk1TTqyMS6p/scuFUfb6BqO\nF/WjzPSt0ercCzPBBEGIlqXFYP34dHLzqocwNmr1AAcONrsqZiSLyYkWVq6Sr3AN/XDkhJIKP/nu\nHhw4MIHh4TGUy2V87nOfw/bt2zE1NYULL7wQr3jFK1LVShfj1Ii52u2K+Udf/Z/NKHf+pz2JfJBC\n3Oe5AZpuGsQQgAfkIJxrI2c/wMtj6HgRvKvbzx8nzVaQAvVzsYsEkjGPxkB25VGlqTcFEE+RZa9L\nwZIargdUexTynYzqp+w2tr0gTJy4akLdiMieVt6nuD8MKRI7exXIZjG9KgDwvvEAYCtyiWTHUj9V\nUWsXsKTH65pNn2noWNof9Xd4v507rncTh47YWCsB7N99fH9XeUNzCbPWxD98bAX279+PiYkmAOCV\nr3wlBgYGcOTIEfzVX/0VJicnsWrVqpN63d9ULOgZKQxDHDlyBBs3bsSvfvWreYP0Gz8eLZsXS/zH\nlQXeiWkXwY3FvC8qwDwBVEroEN/z4B1gmXe+fwFXzYsFT64boNhhjGi7CsCrABpfsCnbdlKUhsjc\nUch3nt3Hst1Uulj8PNj97TafTFgqpkF6llTaY/rM6sWLBQ2GwYP2GHwH/Dbx4YGAexYbZDPnJAlA\nsv5FvwsG/wDQtqMJgz2n2QIqzLipacl1GqacZaNoWcD0rCd9cInbN4OYRa81Aizp13HgYDPeL6tc\nO584pCgZDQBmrYG+pWnrRTE+/7FV2L17N1auXImLLroIzWbUz2XLluGGG27AzMwMPvWpT8E0Tbz8\n5S+fU/8GBwcxPR15Vs7OzmL58uVz2r8Yc4s8oD41NYXh4WH09vbikksuiaSN/9nKbZcF6YWingno\nAaDZcOJjKQi8A0Cz6XDv4+t4PChna2uw+wi8i8fwkQ3MssC3aBdpGBoq5W6Zd9X10uYAIsNOoQLo\nqmPMtpZazSwXAbMNVHtEcKweeGg6aEtkgrabBshsxJKZTvvsGBdbQGYkbVIeUUVYLUg84fl+Gzri\n4nRsmJaeSqLde6QgNWagkH0mw/v5XJxuyJi8OHTUwdrTI8bsmafSUpf68RoGGHZ8dmIWg6sGu27/\n0fs3YP/+/Thy5AguvvhiLFmyBE888QR6e3vxspe9DBdddBF+9atf4W/+5m/wla98ZU59P1XH8gUN\n1DVNwwUXXDAvdwAKAukA4Hb8/SxGhgIkAF7l7KL65ycwD3TcTwRvdpFRJqAqOyYN3vOZ927lNAmj\n0p0uTWU7WSjoHKgVwbpMLuIKk4RM6ZLFvLOgmf07RACetoMD4SxrrsesT8hNGsVC2l6MDZYlcVxw\nlpWsq0Jc+ChgrxX99jz5sQA49pyOlwHjthWipyIfeFmg7XN/lxCzQsJo1gBOn9uuX9VOKnMyMW7i\n8Mix1Haz1kB1SfLQ3Zypo2/pAMxak3MDaM408PX71mF4uIaRkRFcdNFFqFar+Na3voWPfvSjeNvb\n3oYHHnjghK3/rrvuOgwNDQEA9u3bh2uvvRZANJAPDg4q9y/G3ILMAVTJpDMzMxgeHkapVIr/1myY\nTRsFwVu0TKy4hEnPirlKZ/KiG9adjmNBO+s6Y5pevM+2eUZe5gqTp0MHou89y7xbtkpGkwB0ei3+\niThduw9uXHLckKt0DXQH5M2OoRQREOWimojhii115IZinnoYJompkU98sk+UGFKwY3cQJJa5pWLI\nzRGc5NBOA21xCmZZ9dmmjgozFxSMBKy7GQ8lAG+Ty8bOXY5yTD82ZmL1aXMvQEdx6KiDPU/P19lF\nXuCodryGz922Ak8++STWrFmDyy+/HLVaDW9961tx9OhRfO1rX8O6devm3Wfg1B3LXxTJpCcSrAuL\nzJEFiAA8/cwl6ZLd7vshPNePf/Ii8MNcaYjV9tBuu3AcH47jo912uXsgtj1P8+55IVotD44TwHEC\ntNs+l2Qq3ofqXj0vgG37cTssg64CveJ2xw3jH7Plx8Wg2CCXkKy/hePyLi9iUmRWkqTvR8Dd85Mf\n6XGKe7KdqJpgWzAW4QZ1L+knAX02PA+wbP4YSpoF+EmRTaTVtIhNPxnh+yGaZtSxQwezdeH0PxaE\nIaYmoyTTyWMRq318gnd4mZyI2M+De/P16Fnx6Q8twdDQEAYHB7F161bMzMzg9a9/PR5//HFs27YN\nf/7nf35SxoitW7cCALZt2xZfCwCuueaazP2LMbdQSV9qtRp27NiB0dFRbNq0CZs3b+ZA+lvuSP43\nPWGZymza8Y9tebAtD2aT/2LG2xsOPDeYE4gHIuadTVil90mfMqQ4Odp2em+aXs4x+SBdDN+PkvSb\nZgDLDuMxRSZzidrMbZLL84nH8XaHbOq4kM02wlyQrnIGs93kp2VFQJ7dFhWiy//Osz7xFM22zoHt\nPBMrx9VgOboy8dS09Ng4wPWiBFjRMphl2Oum2ALipFkqQif7u56kXP44xsajp6PxMUu6f/dTh7Fb\nwqTPNVgHmH++ay0+9FcWDhw4gM2bN+Pss8/Go48+ile96lW49tpr8W//9m8nDNKBU3csX9CMuhhh\nGM5pYv4/H4s8O1n5iCri/Y4XM+yuwzPm3UpngjBhyAM/gC7R+2ax5dF5aTYe4Jn3ksQv1xdWC2T3\nHWnnk+2stlwVak2++hzaR4xFkrTDt+VIbCbFJcC8HIOQ6UckAVJr1dnl10SrzrfP6kHTCbPJ/6Hj\nJolaxdjZId1HmhwNgbEm0M0mzIruNpRQG1mu8VpVllXXdQ1BEGJ6Ru6jq4rdu2uSJOlg3jr1w8Pz\nA+jVJX0wa0185Z6zsHv3bkxNebjssstgGAY+/elP45/+6Z/wiU984jlhQW644YbUth07dmTuX4y5\nhWi322g0MDw8jCAIsGHDBixZkmbhKESArtpGYRMz7vkpFl55jRzpDL0n9l0mnQEg1b6rwHwWyJet\namYVaKJVs25WTrvRvLMRhmnyQwbGRdAusuzRtdLnEYsfzxGKP23bjupt0Gt2xTIMtVxtOrHt5WLI\nvQaS+UoE5WLSPwu+ZVp0FqxXSiFmm8nnyib7k6/9wTG+crcsRFZ9ZF97TmN8t/G97+yTbu+2voUY\nteM13PueHuzYsQPr1q3DqlWrcODAAdxyyy0488wz8YMf/OCkFzY6FcfyBQ3UWVBOg3tJhk4V4Vgu\nfM+HwQzUpUr0jRD130CiAbfaTmobkAD3tuvH26m9qM1EXhAw7bJgnV7LpC6VngIH0Fk9t/T+nEjb\nbrVFYCbXvKvbCWKwbrV9ri0ZiGcfmAjwE/gWgSjbhqr4kRh+EALMYC66IMTHsYWnmAROPwAgBf9s\ngmx2H4jZoGbiAlMZ90AyH/YziPzbmVWLThY+wOvpWXcb24mYMHYSCoIQbUuR1NuO/gY9PTqmZ6IP\njmRWjYaH/v5kmKjXPQwMJO8PH+KZdHFAnjluYumK7pdQDw+PdTWgy+Qv3/jM+Th48CCefvppnH/+\n+Vi6dCl+/vOf45ZbbsHVV1+NJ554oquCZotxaofneTh27BgajQY2bNiQOVG/8f3HYBQLqPTy474K\npGeBd8/zAYFEJOmM2XDi10D3oJ1es2DdsjxuG7nO9PWL9zA3qtT3w5h5V1VXzQPp3Wre2eqqtXrA\ngXjDgCAjzEooTeQjJCfMyq1hx2WR8fa8iPBhVzJtJxona83E/UpW8Ilrx4/ANx1vuxp6yvJkVv48\nedEhVV0MAJipd0A+k7ckOnNJz6sFWLpE/uC0fzRdgC6r2Nyx8RZWr8m2sD58xMbwrvnXtRCjdryG\nL/7dGuzevRv1uovLL78cAHDPPffgG9/4Bu6++278zu/8zkm73qkeCxqos1EoFOYE1P+v945zAJ3C\nseRJTD3Vcle2iaKsRWxPlWCaF4EfwGqnR7sswB34Ye45BOJpGwF/US9vtfPlOiygp+gGfLctPwXg\nCzLGWwL2/SCErmlxcSe2yJOMNc8KzwsjTWPnTyZje5LrRr9jq8XOdYOOKww9rBCbzk4wfNVZoV2/\nI+kJkqRTiiDgl4xpElIt0RaL0edCE+DYMUdqVdlseujrK6Qm8aOHGydNk358wsSREXXSaKveRO+A\n2lL1i584Ddu3b8fKlStx5ZVXot1u4z3veQ+eeuopfPazn8WFF154Uvq5GL+5CMMQu3btgmmaKJfL\nuOKKK3LPMTpfEqsVkSe+66G6JAEdVstJgXgKFQvPMuycTIYB8XMB7alrSKQzpHen/SJop32sHl6V\nMyWGZfmcXzsB9zTQTTaoKqGy26ieBpA4fPVU9BQol4H0yDYXHEjn2mVydJJqrfL7Y/XzMhMDU8Ct\n4opkbyUat9l/B3ax0POB2UaS6FoW2G1mYR0tS+MS/OcSpWKSc9Xq/K81mtEHM1vzMLiEVmLkf+ta\nI8TBQy0YXebskSVuN/HsLw9D19Lt1qdrGFgWrXQ1puroXx4l/lNCab0jb2ETSr9277nYv38/du3a\nhU2bNmHJkiXYvn073vnOd+IP//AP8cQTT8yJcF0I8aIA6rRcOtdqdr6CXaHtLJBvmza33ygYHFve\nje7c9wMuWZJNNi33ljiWnUK2jQ3H8VPyGRl4Z4F3dE7aaUYE52WjkKuTd5xAaDvdX3Hpl0LFJAOA\n1wG+HsJEOsI8BIgPADz4Jaca/phySeuqgmrstGIFHDMlns8x9Mx5NNhSv1Ur636Q9FH2UEGTGUWr\nHSIIQi6JzHaiAZ605Iahod0O0NPDf+azdfV3Q5yw63UPx8YifflcljSJXV+2PGFuJo81MbY/nTCq\n7kvSGbPWwCMPbMDw8DCXLPrv//7v+NCHPoQbb7wRd9111wklky/GqROGYeDMM89Ef39/VxUBiU2n\n8DuI0Ky1uO0E1pv1dgzaWZA+H/bdtrwUqC9XCl2DdBoTVUw86+suS2al6C6RVZBeCJIZ2l+tFpTn\ndAPiAd6Sl1YEq73JZ8QmxwL82EtTmPixk1yGjiHnGl5LL5HQdOHvDgB1M1kBZtl21fm2G11b5qAD\nRLlFLFi3hFwl2tdmHvy4ZN3OQ8r0bNQBGblCKxj9VQ0NMyJjjhxJOx9l1XahXK8Vq3q47RSrVpXx\no/8cPmlkzezELD572/I4WfSKK65As9nEzTffjOHhYTz00EPYuHHjSbnWCy0WNFCfq/cuxf9+R1KV\nsFgqxsBbBO7idpGBl0lnEoeYtOOKYehwmQqlrGzGbqW9fEX2nQA5gXeZtl0MktCw4D2N+hFZAAAg\nAElEQVTPHpK2iZMCAJSK6Xb4PvLSHABwLB/0MB5p+buT3fhBiDBgLCI7DwH0Vy6WdIQB4CP7YQKI\nBiGWSfEAqVwmK0THBNX59BlosYQl2i6mT7CfleuGjHwmOUYE6xSsDSWBdAoR9M/MJJN+ve5gYKCU\nuj6x6gBw9HA9pT+fnTIxuFy+dFqbamHJcjkzI4J0q9lCzwDfTqvehCwe/P+WYWhoCGeffTY2bdqE\n8fFxvPnNb0axWMR3v/tdrFmzRnreYrxwY3BwEJqmwc/Tngnh52QnEnhXMe9iZIF02X5RNkMgnpj3\n2kwb1b4Evc3VhYY9TkV8iA40cymiBAC2zd9TuSwfp2Xtikw2K9uLbSj9MLaDjbYJPu8ZdrVsWHao\nlDtmnSf6mVMfOQtGJyELiMkX/xXpAYHmEwL3LE9IYF0E6bQvLwiks1Fv+hjok/9NDh40M20Xl6/s\nzZS/iDE23sa+nOrQc4mv3HMW9u7di9HRUWzevBmVSgWPPfYYbr/9dtx888341Kc+dVIS/1+osaCB\nOhvdAvXrbjoAnbG/ch0XgRdw24oZ5TBVLHzSHp9gGrWXgHcKGYgX37OMO0VZWMaVMe7EsgMJmO/W\n3jGWvoQh4IcpwOa4PnyLTySsVPKZdzZcSV8AwOhMMt3IZXw/hN/mK2qSdVnekrDP3qMQpaLOHUPB\nHipr09DBPS4EITqfHy3bdsC7Rn1ND0r0ICCOV57HX9+2g5iRMs0O61JM7pdl1Wdmui8b3Wx6mBiX\ng2Y26tMtDCzr5ZxeAGBgaU/8fnayhbH949A0HVazhUpf/hIryV++eu852L17N44fP47LLrsMhUIB\nn/vc5/CP//iP+OhHP4pXvepVXd/TYrxwgibquUzYeQBd3E/vCbSz72l/FoAHFCBd8Z6Yd3oN8LKZ\nvMgD8Cq7yNqsjWpftnwgzxmG9O5iUmIPo3lvWz73XuWQJe87m0wbolrl56NuGXEAnH5eNzRO527Z\nafa7m35Sm1m1JqLj5Lr3LEButqOHjYG+7lcDxTmWdOoHD0rsYuYYok5dBtKDMJDKX/Lik+/r5ZJF\njxw5gltuuQXLli3D9773PaxYseKE+r4QYsEDddEpYC4RKJJ1XMacm30tMurdSmdcxikmPkYA7d3o\n330/yGXeReAuS1SVBSWe5l1fFuwA4nQkQMS8i4NLll+3ZXkc287aMHajxWSTr5xYZx7dr+h2EIYh\nNMlyqeNGUh4WLOQ5JQQB76fbTUEJYplky5qiXzpbsIScW2w7nQ8gBiWRisGy6j29Btqt6G92bCxK\nHI0YzbSrS32mhYGlvegfTCdsEqs+c9zE+IFsqUu7bqZYdQD42/+nBtc9jv/5nxEYhgHTNFGv13Hr\nrbfiqquuwk9+8pOUd/bJiEceeQSDg4PYuXMn3vnOd855/2I8//H6Ww7Ccz0UigVUeiOm2mrZ8WuA\nB+lGsZAJ2tn3onQGACq9JdSmGikQr5LQFAqGkpW3OetGnnln9xWKCfpTJafKgDzJZmw7AfEsaO/G\nupE9RpREti1eOtnuSGm6qbRKrLrM4Yut3sq2RYC+UtG5vrGsunhPrH7eMLSYBBEBtUqnT3p0x1Xn\nFLGmCWYr7T9vtqMVYXLdYrdT1JtBDNaJkZ+ajl6wK6PVXgNmy49Zdern07+opeqzUMjqsogh06mP\n7DqSKXdpzNbRP5gUoWN16my86eXPwnVdHDxYweTkJEqlEr72ta/h4Ycfxh133IGrr746s2/ziRfq\nWL7gxZtzkb5cd9OB+LUKpLP72WMCL4DVsuA6LlzHhdVKe4z6ns+Bd/Y1+bBbLZtj3a2WLQXA7DaZ\nFzr73rU9tBoW7JYD1/akTLyKebfaLqy2G7+nH9ZPOM+L3XF9tC0XbSZx1nF9mKYDx/XhuH4kUVF4\nsLN+6K7jw2p7sNoePC+If6y231VCK7VFIN9xAzhugLblx69paVNmcSluc90QlhXA88LcCY784l03\njH/oXIrIxoyRvHT2iR+vLEFU/NwmJxPKhq7FHlebdZQrHWYzcX8BgPEjdelxQPLAJFpUilGbauWC\ndFV86Y7Toes6TjvtNGzZsgW2bePee+/F9ddfj4mJCQwPD+Oxxx6bV9tZQVroa6+9Nh7A57J/MZ6b\n0DQNQZa3ayc8NxpTrVb0XaDXrEcz0D1oV4VZa3Xad+KfzH51IZ0RmXeWfQcipxk26Ji4DUkSqwjo\nCdTbtgfb9mA2nfg1ALTbyZhNY1zuOOfLxwPbDmKwzYJuVS0KAumh5M9MbZlmUtPDsgKYLZnvfFpu\nqboHVv5otkLYdvSTN7ZRiFWp+bZDqQd90wzifewDBD1ktKwQrY5jF4F0NsSigHnhd/G9kcWT/z2C\nkV3pIkbNmcT5qzGrnicovn7fOnzi7ToMw8AFF1yA9evX42c/+xne8pa34J577kGpVMLDDz+Mo0dP\nnrQGeGGP5S8KoE7V7PKAuus4sFsW7JYF13HiHyAB7ixAJzmMCOrpPYH2Vr3FgXhZiCCeQLtRMJKK\nqB0QnxRXCqTMex6IBxADdtf2YgDP3YMf5CaqBn4As2HDtX3uh64rXp+V74iMbNtyOeAdHZcAdGKt\n2ffiMdHnFnA/LPjvxh848MNOFdLox7aDropL0TVsO4DrRj+W5cdseuwJH4acpCYIw/gBQRaeT4Wd\nkgcJ+rE7xafoetQHADBbciBgWQEsK0Btlp/kydO5pzeRdRFYP3Y0f/DtH+RZl/p0OnGJQLrVzC/n\n3u5U9/jS7Wvwd7doGB4exoUXXogNGzZgx44d+OAHP4g/+ZM/wZEjR/DrX/8ad955JzZv3pzb7lzj\n4YcfxuBg5Eawbt06bNu2bU77F+PkhcxuVxbEpsvCc714H4F2FsgDajmMLHzXk+73XY8D7Sx4V4F4\nAuEqqYwM3FOBJtZ9xu7YO3JtdGEXKerbzWbUz3bbRbvtxu+zQhwnZdehcSUG202hWJMf5sobsx4Y\nCPCaLT91jOw8cV6I6k2IDxlhXPiJQpzK41IqLr9PxuazrDkbYp4Te35LYa/LRuCHqDcjZv3gAd46\nt5v5j4rNsTFxrIUDe+Ze2yKU/A3vvrmIJ598Erqu48orr0Rvby/uuusu/PznP8dXv/pVjI2N4Xvf\n+x7+4i/+IrMuwnzihTyWL3jpC0Ueo/7Hb9mbIXXpDFDMOFUslbjj9YIuPV+2TSadId27KmlVTEqV\nymoY+UyWXEYGnFnNu8wiUpTIZAWBdRdJH4tloysNPm2zxIGMkbzIpCZBGNkwimFZXsqOqiiRhKiA\neOCHcIQBp1TSuWXNrAFQdLnJWmpk3RZkx4lLlWl2v/OASIxMUUer5aG3N/p70udWr9twLC9OXgv8\nMOUO4QcBDF3H/uFpZeGi+oyJgaVVYVskf2Fj4uCE8p4pZDr1e99diZNFzz//fExOTuKv//qvYVkW\nHn/8cZxxxhnxsSejKp0sZmdnsWzZsvj91NTUnPYvxskPVspYLqe97rJAela0Gm1O+hf4AXr7ExkX\nC+arS9Ksuxi+63HsPJusSvp30eM9j2kXI08+43lRzY5KD+tA1r3HO8CPMyzDDgA9nXZrsxb6+5O/\nhYyBzioAFSeUupGTVqUnLdcgKUt30pwOcx9r0zv2kJJ2xWvkBYHpPHvfeiO6pszG1/VClEo6NF1D\nGIScWQD1Q8xTmpl1Y6kmgNQ5bIwfaZ6QG8vKVb1Jheg9YyecyPnZW5fh2WePx8miPT09+M53voOP\nfOQjeMtb3oK77747dueqVqt4yUteckLXk8ULeSx/UQH1LHvGLKmLyKADEXgvdrw87ZaFMlOBQAXa\ns9pUJa3KQDv7mgXvpHUnJj5pJ+00E7ehYNtlx6dcZhSshwx8u7aPIuMSkCXnCYMQPvhEVdfxUSwZ\nsNou3yeFbp6W91iQTuDTZcAzsdui601WsSipxWQhe3HK7+jak88sRKGovoaqDzIHHjFEZozcGspl\nA/V6F5YCnZgY55n0uVaXq0+3MHl4UrovK4H04fvO5ZJFi8UivvSlL+HTn/40PvShD+HVr351131Y\njIUT3bh4/cEbfynNb8gD6arcHQLnYvJ9Y7rBgXiKPPAuO06WtKrydmdDxryzdpDsfqvtcuM5Afcs\nkK4afzg9ejtaCS0U9BjE9/TwZgviNfIeDnw/lMoYPS9AtZpdgpPNQ6Ixmd1mS8ZuIJHXqHT0VCmV\nTf5n5z86T7T8pXNZsK6SqkxOuejvk0OymdmO/NQNOLBOblxsm+NH5m6dS8HmfQERSAfyq7o3Zxro\nW5oUoGN16g98oB9DQ0Nxsuj4+Dj+8i//EpVKBd/97nexevXqOfXxxRgLHqjTU1oWo/4Hb/wl976I\nkhRsi0DadZx4GwF3EbTLzhW17boE5AVeABdJf2WgXQxRPpP0M+00AwBGj3wyUDnNwPZisC5KZWQM\nvgj0XduXgniRbVf1yZckP4oOMTJrMhG0sxq9pEIq306lR/7VkPmxAwmbTVEu69yx7CBHtpKeF1k0\nyhJGXS9EIHgKs9Fu+Sn3l1JJR7szwbFgnZJtp45HlT10Q0OpUoDZtDlLOLb64fHxpJiR7AGPojHb\n4mQvQRCiPtOCpmmYODgBx7JQ7lVXAw0ZAepX7z4rriy6ceNGLFu2DHv27MFNN92EzZs348c//jH6\n+/uVbT0XMTg4iOnpaQAR47J8+fI57V+M5yYKhYKSeHEYOw2/M/apwLvosS8D7KpgGXYAXNJqsi19\nnipJlW/b4Y5nmXdZ0qoYBNJTY3gnSCojfq9Z5j1P7ierjOp5gaBrj0B8mSF55loAir2WyiLS90Ou\nxoKqf0kfQmnRPDFhVbRsVAVJDVWuYCJYB/gCgI2m7MEkaYMdz9lkfzGOHm6kSKOZySaWruSLxU1N\nNrF8ZR8CP8TMVAsrVjG1LSZaODwcAfQTqUHxqff1Y/fuI5id9XH55ZfDMAz8wz/8A77whS/g9ttv\nxyte8Yp5tz2feCGP5QseqFOoBnaZH6/VasNg0reLEm8lmU7d9qwYtNM2DoR3xt6ipKpWHgPvOi6K\npSLslp2yj8yzi5RVWPU9H2ajnTjPACnnGSAN2mkCYQd4x3K54k7SfmRMeKKundpWeQITcJSBfgrb\n8jpAM/GJz0qiEcGoWLFVxpirHGp8P0Cbke7QYKzSXbpemCp0wt2LANjpvej+MjNjd1XghI1Wy0Fv\nbymelJsNBzOT3TMyNDmyesTJQ3IWPSseuusMDA0NYfny5bjyyivhOA5uu+02fO9738N9990Xl5B+\nvuO6667D0NAQAGDfvn249tprAUQD+eDgoHL/Yjy30Y05gM8QFI7EC893PFQH1Q9+eRI/MVoNehBW\ng5s8/TuBctl+lnmncVhk3mVSGBXxEgYhvMCPXUE814dogVDpkY/rInOtAu3Jby9+bRh6PE7lFoAS\n2iXgT2HbfnyMYehdOcvEbXXAuiew5UnbSd4P264oj3HdMK4DIjLebDhumCJXHCeIiRpD19Boeigy\n1WEBSC10TdNFtVpEuWzErPrRw43UcWLMTJpYurKKQUVdCwAxSJeFWWuguqQ/c2549P4NGB0dxa5d\nu3D++edjcHAQu3btws0334yXvexleOKJJ9Db213F05MZL+SxfMEDddZ7l33iDoIAhw8fxg3vnY5B\nuaqIhsukgvstPz6+WE7r1IFuQLfwxXN48K463xYYnDhpFTkTVjf6dtl5QlIqhTjga7oG1/GkhZZs\nRtvOPgjICj6JITIkxJ5nFUSyJUWYgDRjTlIXXmsulwYZBj8RxZVQi3ypbdV9WO2koFOxZHCgVhzw\naOLSJYMg64+e9CUC65blpSYadhKcnW7HEzK7fC0m/Bwfr8NqWrmMHRssqz515HjX51nNFr7x4CYM\nDw/HyaLVahU//OEP8Z73vAfXX389fvzjH6NQ+M0NU1u3bsXQ0BC2bduGwcFBbN26FQBwzTXXYMeO\nHcr9i/HchMput9Vq4XU3jsTvWZAuCz9O0Ofrx1cyVn8o8ipEs/k8MukMMe95SaoU82XezVqL+x5n\nkSWqytnNusXZ+3muj95+Pi+gG9Ce6rflpcZblmSQMeTSfgvXYhlxICI3zKbHV9RWFGpSta1pWtxu\nb2/+ua2W3I7SsnwEITggXyxqmJl1U0QQ5Su1GQlQVjGrkb0zAOSkFZCw6gPLssf1w8Nj3HxEtStk\nYdaaqC5J9jVnGvj87auwfft2rF69GldccQUsy8IHPvAB/OxnP8MDDzyASy65JPP6z2W8kMdyrZsv\nAxNz8wE6BSIIAti2DV3X8ZOf/AS//du/jbGxMYyOjuLDn4r+aQ3DkIJ0llVn96vYdtd2pOw7K6MR\nZS6q7UAE3kUJjkySo5LOsNsrvRUpQJd5v4ug3igYUrYdiEA6O0nJEk5VDJOqTRpsYpvEDpiUgdek\nLfkAKj71Z33ehYIufXgQt6n6IQ629AAQ+AnjAiR6SEpqTSXG+kkyKAXZpQVBiKLwNyuW9LiwCbH8\nRWZybdSsTnvJNt3Q4HTOoc9odipyWmGBOu1rNdroH6zGr/sGqzDrbe4+p8fSyTei9EXTtdj15bMf\nW4l9+/bh7LPPxmmnnYbp6Wm8733vw9TUFO6//36cffbZqfYWA4CsHvrc4wU3lgOA4zjwfR+Tk5No\nNBo488wzMTIygkajgfffEz0Ey6RWBuXpCADeEOtXMPvZc9jjWDDPVoJmQ0xKFbdn1axgvd5VIfq4\nq4C9SvNutRyUO6ughaIRA3URlLPbWDDPHlfqYhWPBfBZRgJ0nULRUAJq1cMAuxIb9TF5rwLqKkad\nBekUJFlk5xQWdNMY7zE6dd7vPQLq4nm27SfF+DoHEMs+O2tzsqFCUY9Xew1DQ7VaxMHRWe4zaNba\ncZVoTdfQmI3GaZK/0H3OTrWwfGUfap39R0bG43PiPjdbMVDXNC1m1AGg1TBRXdIHs9bE1+9bh717\n98K2bWzatAk9PT3Ytm0bPvjBD+JNb3oT3vzmN3O4aTHi6GosX/CMOkUYhvB9Hz/96U+xbNkybN26\nFb4badN9YQnVKCbLfXmlql3biY8xDINj3ymKmLvUBYiSVPnk0tKck1Rpm8zXnZXMZCWtApHOXSzU\nxEbC1ubfl+wY0tETeA9DPllHZ/TSgER7KXzsInDv5vNmrSGBBLiLTFQQhArGO9GNE2MvK+hE7jSu\nE8RynKKkKIVKDuN6PgfWTdPlJpkI+Ed/q2bdTpKqXJ9j1dkguYtqOVOWOEdx/GgkdVFVpbNbbZR7\ne2B32Muv3b8+lSz61a9+Fffeey/e97734bWvfe2Lulz0YqiD/b+YmJjAxMQE1q9fj02bNgGIfI8d\ny0KpwucJqRh2FoTnsfAUVquNSm8PzNnGSZHOiKCdWPhKbzlVpCnut+AokxeyIk12p66FbbkxaFcx\n66rtAOBYXgrUZ4H3bpl92/ZT7ZbLBmqzlrSqalYuDRukcxeTU1UsfpZVJJuoH/U/7R0PpFeHSSLD\n9kUkemZn8xP/TTN7NV1mkUjyFzYIpHcTLFg3a0186tYBDA0N4dxzz8Xq1asxMTGBt7zlLXBdF9/+\n9rdx+umnd932YshjwTPqYRhibGwMw8PDaLVauPTSS9HX1xfr1f/gDT8HAOgFEdj53DYWvKukMux2\n1dMj7ROZdxFYy7bz/UsYc5nmfb7Mu6h3l9lFElAX2XdVBTQZuyQy7+Ix7PtypSj1i+f6pBig2cFb\nHAhln7m4TQSu9ADAAnVixMMgbZ9I55fLRnyczrE0wn3oaVAuXi+IWRcjrvQK8LaVhqGh2XF4Ye/b\n8wLG8SE69+joJHr7EwZdlL6IrDrLqE+N8VIXXdNjIANEoInim//wWzh8+DDGxsbiZNGRkRHcfPPN\n2LBhA26//faT7p27QONFy6hbloXh4WGMj4+jWCziiiuugO/78H0ff/Tnu2Jgwn5vaZsqqdkoFaQg\nnd2uYt5VTDv7HWBDZODZ9yLDLo6DVGk1uZ6ceRctIQGeXRf3cfflepkP5RQy5l3cLrqCVXrlenfD\n0KVtZDH6iXsNfw9scSQ2UVV0ySJChnWGieqtqB3FxP8tGQguFPSUsUDU9yC2nKTTrLbHs/sFPX4o\naNTsuM+u46OPkRslKxMaJsca3JxFn0sWq16fbmHpyipmOwXoNIFgERl1AOgd6IsZdYqv/P3Z2L17\nN3p6erBhwwYYhoEvfelL+MxnPoOPfOQj+KM/+qPU57AYqehqLH9RAPWjR4+iXC5jeHgYq1evxtKl\nS1EoFBCGIV75p9nVp0QADyQgngXvFKyMRgTrqu0UMtlM1Ie0dCYNsPPtvOb7MCAD7wAP1EMGcGYx\nG8Vy+jObi1wmulaYsorMsp1k97GTR6W3mJljEITpBCM2xAcTGrhpgGWBOru/WDKgaxoH0qOCTPzk\nwUbghzGzTvfQajroqcr/7s26Fd+3a3vccQTWPdfH9ES90+foXqi/LFhvNdqoLunlgDoATI1NpR5k\nnDaxgTxQf/+bW2g0GiiVSti7dy/a7TYOHjyIn/70p/jkJz+Jl770pdL7ONHIKwn94IMPAgBGRkbw\nd3/3d89JH56DeNEC9ajk+ME4Qe2CCy5AuVyGrutotVr4k7/cAwCxP7UY4v+rSioDqIG6TB7DBnsO\nMe8qQM+GahxkrSULEnAtAvYspl0G4uN+SxJZeR95J5bQsHIZMQpFIx6jxIcQv3OOUTRi4G61XJQz\nGHjZtWTzSyKZidoSXa10Q8u0bxSBulgRlaSKMqDOrsaWhbmJzRWq9BhKoE5tN2p23F8gAuqarqHa\nGcPpOpNj9fhzEMF6HlAHgGOHovoWVrOFnn5eh57MAwlQb9WjVdePvi3A1NQUXNdFX18fHn/8caxd\nuxYPP/wwrrrqKnzkIx9BX59c136isQDH80XpC9DRVZkmKpUK1qxZg7GxMYyMjCAMQ7iuizveuwzn\nnHMO+vv742XV379ue3w+gfJAlk2f4zoQH5cjn4mPkclmMhxn2CCrSAAdcF2Kt1OUC5U5+8VHbbip\n9wTeWbCe5Y2egEY3BdazlohlDLymaSl7SBeRDjNrWZUF6WEYwmoJfz/H5/zUo/6HKaaatodBejJU\nSUt8P2AkL9GSrqywFJBme6gtMZlIN3S0TR6ss31lH5istss5OFhtF7PHGykJT1Y2f29/DxqzJiYO\njKHUk7YgZYMYRUoWNU0DV111FYrFIvbu3YtvfvObaLUiG8ebbroJH/7wh/H7v//7mW3ONdiS0Pv2\n7cPOnTu5BKFt27bh2muvxbp16/C6170ufr8Yp260222EYQhd1zEwMIBnnnkGjuMgCKJx4TMfOxNn\nnHEGigyJ8vL/vYNrQwTwdqvN6447spnWbCMG8fOVzojJqnmRBdJl4bkemrWEaafjVcy7qoqq7L1R\nLMTyGN/1oBtGDNbr081c5j0LpAOAWUtW20jkIQPsWbIbdj+NvbbtxaDdtj2Uy5EVrUhqZNo3Emvd\nWeEMwpCzhiyXDeX5ZFwgziVAZCxQqhixzty2fYRhuuAckE76N00nBuuTY/nVorNi/MCxeRVE+sLH\n1+DZZ5/F2rVrsXbtWkxOTmJ8fBzbtm1DpVLB97//fYyOjuJf/uVfTrp88cU8ni94oA5ET1n/+Z//\niUqlgmXLluHZZ5/Frbfeipe+9KVwHAejo6MwTROFQgEDAwP44j1nYmBgAL29vVLwDvDSGBmIBxIg\nL2Pe42O60MCDlarZafAuY4VtRpPObmOZd/J+l3m553vIJyBXXJXJqooa3VNn8FckExXLRWmyVRag\n9/2gK5DO2gmGCFPJXaI7jErOowrP9eFbEdiVDdRsuLYXD5ayQbMtaM+p/VKlwNlHktNNuVKA74do\n1tP5CGEQcmB99jhv5dVqtGL5CwW5RvT298SvZydmpPfitG2Uenhm71O3DmD79u1xZdFarYa//du/\nxeHDh/HQQw/F1URbrVZmMbL5xsMPP4zf+73fA5CUhGYH9n379mHfvn244YYbsG7dOuzbt++k92Ex\nTm4cOnQIb33rWzE5OYk1a9bAdV2USiV89KMfxeDgIBqNBp5++mm4rove3l4MDAzgn+87BwMDA7ng\nnVh4yqXQdA2OZeVKZ4Du9e1ABN5Fpr7S24NWw+TYdhlIZ+UvIihXgXpW857qt5CEqgLtOrMKTCtq\nrOOMGGLyqi+AbfF90rYdA3kKAu/s+BW7cQntyEA9ecZTQn65XIgBPAXNCzKtuBg8e27E28RwHF9a\nLVsMTdNiMwCz4WSuLphmZJ8rzpmNGRP9QpXo2SkTg8ur3IPpzGRTWYQuK750+xrs3bsX+/fvx8UX\nX4ze3l7893//N9773vfiz/7sz/DFL34xVilMTEw8JzlGL+bx/EUB1O+44w6MjIzg+uuvR6lUwhve\n8AY8/vjjuPPOO9HT04PNmzdjy5YtuPTSS7F06VKYponJyck5g3cgDdr1gpFi3n3XlYJ30q9nSWe4\nhFUGwBfL8iJNYrBFmug9AC4Zs1zg2VJVm7E9ZJCw5K7tAhJ5Szd2jHEfbTclnykr9Jgs2FdVVGU/\nfRWLzQb7kOC5PlwnGehYv/kgBFh8TUw2gW4C/bHkpRxJXsTJxPdDQHB6Sbzl044EraYj1fTbloe2\nacNz/XiiFD9rq+2iPt1MHlAUibEUrMXb0ZGjqf222Ua5ygOYRz61AXv27MHExEScLProo4/ijjvu\nwLve9S786Z/+KTeQP1eeunkloW+44Yb49c6dO3Hdddc9J/1YjJMXF154Ib7//e/j7//+7/Hggw/i\n6quvRrFYxDve8Q4cP34c55xzDrZu3YotW7bgjDPOQKFQwPHjx7Fv3z54nodqtaoE76+4npdBdsO8\n2602ypgbgBdBuu94MJ3owdlCwsCXKsmYx/rAF4qFFCifL/PetyQBd3mgPVCQSqJ2HuBlMt2CdDHo\nOJanIrDeavAMuSiPkcllaN4jwM4Cd+5+RJKKAfEAv+pILLtKIum4PkrCg0dt1pI6BJ0AACAASURB\nVEpdE4hAOhDJFvsGojlYlfRP0W0CLQBMHJzIZdGpYnS7bsZFwj794UEuWXRqago33XQTarUavvnN\nb+Kss86Kz9c07TmrNPpiHs9fFEAdANasWYMvf/nLWL9+fbwtDEPMzs5i586dGBoawp133ok9e/ag\nWq1i8+bNuPTSS7Fly5YYvE9MTKDVaqFYLKK/v39OzLsI4KWJKAqNuox1F8G8aztSdxm+LxL/X4nc\nhWXj2SJNNNCx7cQDVot0dXrMmIusOAFmmU6dvzd+8PH9IOUhD0g0ll1Jb4QJjXFCYUPG6NMAzVZ6\nLZcLfAVSXYvPByIgnFw7e1AHsoo8RRIcmizE/gV+EC9Ti1EsF+DaHsIgRGPWVF6bWHXxf/PoyBFo\nQpKr07ZQ6qnEEphSTxmP3r8Bhw4d4iqLjo6O4pZbbsEZZ5yBH/zgB9xAe6oELaGeSr65i5EdL3/5\ny3HjjTdyQDsIAgwPD2P79u340Y9+hHvuuQfT09NYt24dtmzZgq1bt2Lt2rXQdV0O3j8ZSSCzmHcK\nlnlnATwnHbMSGQ1JZETgnsXEO5Yt1cizoJwFVGx0y7yrqqt2A9ID34+ZdhlYb9Za3HUBxO4yAGA2\n2igUC/E2Aua+66dYdTZaDZ4hl0WeXCY6hoB3Mm5GrlzJMdW+spQtZ//Oru3BRTT2lyuFuDYHnceC\n9TY57dj8HNJNTI3XM3O3iFVn50Fi1ScOTmS23W40Uzr1z9+2Ert378bMTIjLL78chUIB//zP/4z7\n7rsPH/jAB/DHf/zHp6Q710Icz180QL1arXIgHYie/pYuXYprrrkG11xzDYAEvO/YsQNDQ0P4xCc+\ngWeffRZ9fX0xeL/kkkuk4H0uzHsYBNB0Pf4NAJ7tgB16jN6elH97llSGmPYUIy/IZWTMu0r+wjLv\nsgTWLA911XvXdjmgGenLyymgnQe8aTIJgpB7CMiT3rBt6QLwju6LXxKVPVTR52cFaXCcJZcJw5Cr\nbMcy9xROZzAXfebbZrLsUSwX4s8w8AO0zeRBCRAYrc5nUZ9uJklCpoVKNVtnDgC147zUxWmnZTW2\n2cZXPnlOXFn0iiuuQBiGuOeee/Doo4/irrvuwu/+7u/mXutkR7clobdt2/ZCSTxajE5Edox86LqO\njRs3YuPGjbj++usBRGPh3r17MTQ0hP/6r//CnXfeiZmZGaxfvz4mYgi8T05OYmRkhAPvX7n/3K7A\nuyq3g6wiReZd5TTDhmo/seuy/bIKrGzkMe/NWvQgn8W8s1IalTxHFUQmsMfYlgsIJIO4fhqGYeZq\nsdm0uXGXBb9WO2o7cbuSzSvkyMVvq822OXCcJ2e0LS8lXXFtD67tcYmt3Dm2h7bpoMAYV7Cserfz\nmSr27To4p+O/evdZOHDgAH75y1/GlUWHh4dx0003YdOmTfjhD3/4G3HnejGP5wve9eVkRBiGmJmZ\nwY4dO7B9+3bs3LkTzz77LAYGBrBlyxZs2bIFmzdvxsqVK2GaJur1Ogfe6aenp0cJ3gmsh4Jdn8hk\nAmnmPc8ukqJYLsFqtZUFm2SVVcVtIpgvKPT3eYVAREaYZdnFhNM8O0Y2STSrXZn0hmQfsqJKvhfA\niC0wC1FxJ8lkQRO0uBIQBmHMDBmGzmn5CZjTQ4Ch8KFnwbqjYMzZsuE6o9ukeylVipidSJKPIu2t\nwwF1x0oeAnr7e2G37BRI13Q9BdQf+8cLO8miJjZt2oRqtYqhoSG84x3vwKte9Sq8+93vRrmcX8Dl\nuQhaKbvhhhvw8Y9/HNdeey22bt0al4wGovwVWjJ9ASUfvWhdX05G+L6PZ599Ftu3b8eOHTuwc+dO\n1Go1bNiwIZZA0oNAo9FAo9GA7/ux5n1gYEAK3lmnmTzXGdbnnfV9zwLwec4zYmEmVjqjCpbtzmLe\n2W3sOaVKiTtGBdJl18lql2XejaKhlMvohWS8EwkS2i5KQ0ps20zyPZAA9dhyNwxTK5dABPo1nTc1\niFeXO3pzEdTbgvsWBUvAFApGTBzRdXurJUyO1ZI+GjrnymUYOsxGIpnqX1pFu6PLF4vQsQ+TorML\nADx015nYs2cPVq1ahbPPPhue5+Huu+/Gd77zHdx777246qqrUv1/vmKBjueL9ozPZYRhiKmpqZh5\n37FjB4aHh7FkyZJ4sL/kkkuwYsUKKXivVquo1Wqo1+tYv349VqxYgZe/fkh6LZF5l4VMNtNNZVU2\nWP/3LM27CNY91+O84TMfBnJ8g2XbVVIZFsAnVUDVFf/E9nRdkxdVYu7ZUCQWseyHpmspiU231nB5\nxwARWFeBdABo1ltcopgI1ps1E07bQaW3El+DgDmBdRaoN6ZqcG0nlRwqFvP6wh2nYWRkBGeddRZO\nP/10NBoNfPjDH8aePXvwwAMP4Pzzz1f2+fmKBx98ME4sogH8sssuw44dO7Bt2za87nWvw7JlyzA9\nPY2vf/3rL4SBHVgE6ic9fN/H7t27MTQ0hKGhITz11FOo1+s477zzYuZ906ZNCMOQA+/VahX9/f3Q\ndR3j4+Po6+vD+vXrUS6Xpcy7VCLTiRPxewfSDjSidSQL3Fl5Y7mXX1mTAW4VoKag48WE9Kw2s9ol\nXEI1PKoKhxkVUO/GzpEF7PF1BWcwgF/1FPN52MRTTddikB5fgwHrjhM5vFBfqCiUCqjTtevTTa5e\nCQF1ADFYZyVM/UurOPTs4ehYjVZYE1//RK7aRqWvF1azhX/5zPnYu3cvLMvCpk2b0NvbiyeeeALv\nfve78brXvQ4333wz92D6m4oFOJ4vAvXnO8IwxPHjxznmfWRkBIODg9xg/6Mf/QjnnXceBgYGAACl\nUilmaWTMOwF1ANxrChWALwvSGTZkSatioaYsX3eAZ0ZEVj+rXDC1K/MNjq8hGViz2HJN03IfAFTt\nAkC5p8QBdAoZUPclhZAKDHMOJAM+AXj6ntHfVTZxxD7qEumMKM+hBwWr7cBzPSVQbzUitoQF6lF7\nCfCvVCsxUG9MRcwNC9Rd20GxXIqB+r88eD52796NQqGAjRs3olQq4bHHHsPHPvYxvO1tb8Mb3/jG\nU1K7uIBiEag/D+F5Xgq8N5vNGLxfeumlMAwDzzzzDC688EIUChHwItkMjee0HUjLZmTMu8wqMitk\nfu95hZrY+iAiUJcFC7RFoJ7l884y7nnt0nsRpAPpatiVng4p1GZXAitKbTo73qqAfdp9LLkX2Uql\na3vxXEhSG9E1DIjAemwsEJI5QCJR9FwfPYwshgXrpPNnP4M8oD5zbBpO24qT/MUidCxQf+xzF2J8\nfByjo6Nxsujs7Cze//73Y2xsDA888ADOPffc1D0txkmLRaB+KkQYhpicnMSTTz6JL37xi9i2bRvO\nP//82G1m69atuPjii7F06VI0m03U63W02+0YvNOAnyWbAXiwzoJ6lmn3bCd+LwL1LIDtuW5qoGTb\nVWnn89oVGXw2ZMuN4nZVFIqFOUtvZG0APFAnIM9OpLJ2SqUCJ69hv2P0N/Rcv2Ph2JlYJX2liUQE\n6RQ0cdBEZxh6/PASBCGaM5FDgF7QU0CdZdUBoDlT4yrUESgv9ZTj149/4WIcOnSIqyx6+PBhvP3t\nb8fg4CDuvPNOrFy5UtrXxTipsQjUf0PheR5+/etfY9u2bfjsZz+LqakprF+/HqeffnrsNrNx40YE\nQYBGo4F6vY4gCGLwTuN5FngH5EV1WNmMyvtdDJXHe1YhPwLu7bqpBPQqS0gViOfY8iDkXLzslh2/\nVzHv4vyjejiQseRAAoyzKqBGBY/IbtHnSSHmgUcG1KkdMa8IiMZv2s7OBSSb8VwfRkGP+05AvXa8\nwSXVsp+BwzygVJf0xkB95th0Z38aqAORDSi9/voD67F7925UKhWcd955KBQKeOSRR3DXXXfh3e9+\nN17/+tcvEi7PfSwC9VMp9uzZg/vvvx/vf//7sWLFCkxMTMRMzY4dOzA6Oorly5fHTM3FF1+MwcFB\nJXgfGBhApVJRMu+y6IZ5ZwG1J9hKysA6C/7FOJEqrUAGo58hnwGSokh8W8WuzwVUEpRivC8riVa8\nNgv2Ncaekb2GzBMYSAZ2kWW3OwM1/U1YsA5EXsfUD5pMVPKX5kwNnuNxnzcL1AHgoTvPwO7du7F8\n+XKcc845AIDPfOYz+PKXv4xPfOITz8kSY14VOoqPf/zjmfsXYCwC9d9w3Hbbbfit3/otvOY1r4Hn\neXjmmWfi8fzpp59Gq9XC+eefH8sgN27cCN/35w3e85h3NljQrpLLsACcHMnytrGhYuFVWnR2H3sf\n4iquyoaXnXvyHg7oGLYgkzi+ivaN8TirAOrU51KlGBkQ2LQykX4AEME6ES3FUkEJ1KN7TMA65RTZ\nlh3LifKAOoH0aL8aqH/9vnU4cOAAJiYm4mTR0dFR3HzzzVi7di0+/vGPY+nSpanP9ERjcTyXxiJQ\nfyFFGIY4duwYhoaGYtnM6OgoVq5ciUsvvRRbt27FRRddhCVLlsQayW7Auxhs0mqW5l20PgT4gYKW\nJeOqpMJ7NgrlkjLhFZgbiC+WS0qwzYLnPCaABe7iuYB6EmRDxtyHQcAVB/Fcj9M16sI9abrGWaGJ\n5wLpz9RqWfFDA+1ngToVJGE/A2LVAcRg3XVcNGciqYtHk0kHrBNQ/+D/a8G2I7YmDEO022309fXh\n1ltvxdVXX433ve996Pn/2zv34Kbq/O+/T5pLbwktBSq0UEhpuRTaUijewF2X6OqqODJ1cHd011WB\nZ3zWHVG0sqs4IypSeLzwAGrrLrIsuJW4z88rImGHVcHZSQgUCwulTaWFci+lTZrbSc7zR/r99pyT\nk6alLW3T72uGobmd823avs87n+/nktD1hMJrwW63w+FwoKSkBOXl5ZgzZ45iuy1S4b9nz54+X8Mg\nhhn1QY7f78fRo0dpwWpVVRXcbjemTp1KzXtOTg54nqd6HgwGkZycTI17dyPv4gJWMdFy3uVtg5Xa\nCIuH+slNOzHr3nZPt9NnOlMCu66Z0iXq4LrqkkbLE5QDN10VxIrvi4tTRUzF6cqoyz8gEWOti9co\nGnWCRqtW7CgmCAIt9hR/YCABnbYrTvqeiI166Dnh6S9XL13pSFUUFSh3FP3rkhIkRn3lE1fh8/mg\n1+tx6tQpTJw4Ef/+97/x6aef4q233sL8+fMV35/ewvQ8It3S8mHTnnGww3EcbrjhBtx777249957\nAYTE4+zZszRSs2PHDjQ0NCA9PZ3mvM+cORMGgwFtbW1oamqCx+OBTqeDXq/Htg0TujTv4haRkrWo\nVIqjpuPUcYp5g4FIk1n5QMhEdrSdjBR5p8+PMqUVCC9UBZQj76SXrdLxQ0OjwoszIxWtKnXBAUJR\nDfHFUU07t4T6Cytt4fp9fnAcRy8kSv2K5WaevI/knEJQkOSYi2k+30xzEZXeAyBk9IFQ79yw9XXk\nowPA1jfHoa6uDtnZ2TAYDNi/fz82bNiA48ePIzU1FSdPnsSnn36Khx56SHEtvSHaFDoGYzCj0Who\nR7AlS5YAAHw+H6qrq2Gz2fDPf/4Thw8fhs/nw9SpU6mejx49Gn6/H+fOncPJkycl5r3y3Wzo9XqJ\n/ilNWJXj83QWjpJou7fdHWbgI03YjoS33UOH95Hi1IDfj8QR+i5fJ9d5eVSd9/Pgr/Z8UnF3hkCJ\n0/2AUA69OL9brU+gXbTkaY3iFBmO42irSZLLLjfrbqcnLNovn+PBd8zx4P0BSZcxMeKp0e2t7Ug0\nJNLgUMuF5uhDjNrd+J+KqR3FoomYMWMGeJ7Hxx9/jHXr1uHSpUswGo2orKzEpEmTkJmZ2eXxrgWm\n572DGfVBDMdxGDduHBYuXIiFCxcCCIlEU1MTjbxv374dDQ0NGDt2LI3UFBQUICkpKcy8GwyGMPN+\n6dIl/OZ/10vOK4+2EyPvdbkl9wf8fPjQoQ5TqWTeea9PMfJOpKsrIy8Wd7mwiyGRYLF5784HgEAg\nIB2Bh/At1WiTX8VCrVXMyQ+KntuxdaogssFAINRhQDz0SWTWSUoRHbjh80OtUaO9tR0arfJ7KF43\nMei8j4daIafy401GHD9+nE4W1Wq1+Prrr7F69Wo8+eSTeOKJJyAIAmpra+F2u8Ne3xdEm0IHhKI0\nJpMp5nrmMmITrVYbNojF5/Phxx9/hNVqhdlsRlVVFXw+H6ZPn071fMyYMfD5fDh79ixqamoimne/\n3497f3sk7LySSZrtnX+vxMB3VbAqNu/iqLp82rb4PsnAPEjTZKJpcVc92MmOIOkQQyLsrZdbkWjo\njDp31SZSeq4AeL+0T3pbi0tyO9Qrvus1k2F88muhSsWFdbPxeXwIBAJISFLe5Wi7Ig2e6OJ18Hq8\nkrkXxKy3XGiWPJcMoRPzz805dKd+4sSJuOGGG9DW1obXX38dtbW1+OSTT5Cbm4vW1lZUVVXRBhd9\nDdPz3hETRp1MolKiu3lRQwWO45CRkYGMjAzcf//9AELm/cyZM7BarbDZbPjb3/6G06dPIyMjQ1Kw\nmpiYCKfTiaamJrhcLvA8D51Oh7/+n/FIS0uLmjYjhpj5SJF3JSKly5D7eFn7v+5E4YGQOVea1Aoo\np/CIjX5XU1/DT6ScDyk/JgDJJNUApJF4EpEJBoIIdpxKqbWYPKLjdkp3E8QX4JYLV2j7NXGFvyAI\noW3RjsdcV0OjyskHBLFZ/2pbARobG3H48GHk5OQgLS0NZ8+exfPPPw+NRoOvv/4aN9xwAz3/QLdf\nJMMvGLHDcNJyIGTeZ8+ejdmzZ9P7vF4vNe+VlZWoqqoCz/OYPn06jbwT897U1ITW1lb4fCENXP+n\nUcjMDA3dI1rxy18fVIy0E/3weTxh7SK7iryLTXqcRqNo2sWII++Acn5794y1NK3F19HxinwNhNJj\nIh2LzLkQG+9IQ/mA0ITUdlF/cm28FvJUYfHEVnHgSnxdVGpp6fP4aBoOWY+zxQm/19dlGpEuUQdv\nuxcXTp2L+ByC+b0cHDp0CPHx8XSy6GeffYY33ngDzzzzDN599116vTcYDP2W9tJdmJ5HZsgbdYvF\ngmXLlqGuri7sMbvdDgAwmUxwOBxdXgSGMhzHITMzE5mZmXjggQcAhMZpNzY20j7vW7ZsQVNTE9LS\n0uD3+zFixAi89tprSE1NRVtbG44fPy6JvP/9/2bBYDBAp9NJzLs42i7/muBzd5pU3usDp1JRAYsU\nbVciwAegFtUWESOv7iJaLjfMAT+vGIGX7wZEm/pK19BxPJWKDELSiAYeKUf6A4EAeD8PTsV1dmWR\n5cR72r3KnWj8oQsE2bJVOoe33QONTgOfx0sNuc/tlfRB93m8NB89TtYPl/fxqNxshNVqxciRI1Fc\nXAyO41BRUYEtW7bg9ddfx69+9auo701fEm0KHYm+MGIHpuUhdDod5syZgzlz5tD7PB4Pjhw5QlMg\njxw5Ap7nMWrUKDgcDjz33HO48847wfM8zp49S9Nm9Ho9KtaOpnnvYvMOyPqGK0TeiZEnj8tnK0SK\nrBONUXq8/WqbRIMCfn/Y+HoCMbndSWsBQtFmeT92cYEqMd7iXHWl7mABfwBBIYg4hAdftAlaxbRC\n3h+At0OndbJceN7PS84TCAQkaTjylBy5zgeDQXhcHkl3LpKmKP4Z+twefPHhTDpZNDc3F6mpqWhs\nbMSKFSswcuRI7N27F6NGjQr7nvsTpue9Y8gbdZPJBKPRqPjYcM6LUqlUyMrKQlZWFhYtWgQgNCzg\nnXfewd133424uDiUlpaiqakJ48ePR2FhIY2863Q6tLa24syZM90y73KUOs+QCIM40qAU7QakkXel\ndBne64NXlAoCKEfeu4qWx8XFda4pEKCDIdSiPPUw0x8gfXQ7zbSn3SNZR1xcHISgIDm3WEj9Xr9i\n8RcRcSWz7mxpC8tbJ7hb28MGUAFAUAjSD0yuq61hF0Z03P78wxmoq6tDTU0Npk+fjuTkZBw9ehTL\nly/HLbfcgv379yMpKSns+P3N4sWLYbOFBoA5HA4q4mQKncPhgMPhQHNzM5qbm2PauA0XmJZHJj4+\nHnPnzsXcuXMBhKKPixcvhkajwWOPPYb//Oc/+OCDDwAAeXl5krQZj8eDpqYmtLW1QRAE6PV6fFA2\nhhasdhV5l9/2ub2S1pFy4y6mO5F3cp+7zRkWQNDG6+Buc4aZeCWTLo5ayyPY3nZv2DWERLTl3bvk\nE1CVAiMkeq/RqSOmxXg9PolZJ0ZcrVHToBA5ttvppmvT6LQRi3NJoEVcRyRnx4YsWK1WjBkzBsXF\nxRAEAZs2bcJHH32E9evX4xe/+IXi6/obpue9Y8gb9a7oTl7UcGL+/Pn43e9+JxnpHgwGcerUKZo2\n8/777+P8+fMYP3487Qs8c+ZMaLVatLa24vTp0/B6vYiPj4der++WeZcXrZIovGR7UBZ5B5QHXkSK\nxovTZoiJV+oZT48dIfrNy4pMefgl5p2YdKVc+wAfQHubi5pqcZeD0Psg3V4mj4Vvp3Zuw/J+Hqq4\nOEmRKe/n4fN46YcLcfTc5+m8kLqdrrDvj67V78ff3sqE1WrFhAkTkJubC4/Hg5dffhkHDhzA5s2b\nUVBQEPH1/U1RURFsNhssFgtSUlKoaC9YsAAHDx5ESUkJgNCHz5aWlgFbJ+P6wLRcCplbkJ+fT+8j\nXZmqqqroLmp1dTU4jsOMGTMkaTNer5eadwBITk7GB2VjYDAYkJycHBZ5FyPWMfm0YvK43Eb2NF0G\n6Kyl8XmkxUPiCaviXcSwdSgMThJDzLM2XivRbSC8sYC8pSQA8E5e8nz5NYaYdfH0a/mHCNdVJz1v\nQBZ88rR7aIeu9o6URTnk/f9sSx5qa2vhcDhomuuhQ4ewYsUKmEwmHDhwAPHdGJ7VXzA97x0x0Z7x\njjvuUGzns2zZMixbtgxFRUWwWCzYs2cPK1ToBsFgEPX19bTbjN1ux4ULFzBhwgRq3mfMmAGtVkv7\nAhPzLu4LLBYGuXnvyYTVSJF3eS58pEl23enzruLCzx3JzEtSZmSpPIIgbc9IBF4pt52cOxgIQK0w\nnpn3++mxSC4kuU0uXipORfvdk/ePbl+73JLXEv7nL9Nx4sQJxMXF0cmie/fuxapVq/DYY4/hySef\n7LK/PWPAidn2jEzL+xZBENDe3o7Dhw/TmR1Hjx6FSqXCzJkzaeQ9KysLbrebtooEQAtWo5l3pR7v\n5H4geuSd0FW6jDzaTp4jvl9s2LsqKJX2ZO88V6Rdy/jE+LBdUqVZHUo7mwAkHcbIdYBTcfB0RNLF\n541Tx1HzTe5vvRRKGSFRdF7U8vGrbQU4f/486uvrabGoy+XCq6++iiNHjmDz5s2YPn264roYgwLW\nnjFaXhRDGZVKhezsbGRnZ2Px4sUAQubd4XDAarXihx9+wMaNG3Hp0iVMnDiRDmkaP348NBoNrl69\nisbGRol5375xIo28E3oSeZdE2jtud9VhRgyJtssjFuJi1aAQpGY9WocZshMQFILw+0A7rQiC9IOH\nIIukkNeJI/TERJMLhtywy9s1epztEIJC2GhwIBTV8Xv9aG/pjL6I37ddf58VVix64cIFvPDCC/D5\nfPjiiy+QkZERdlwGY6BhWn5tcByHpKQk3Hrrrbj11lsBhMy7y+Wi5r28vBxHjx5FXFwc8vPzaeQ9\nPT0dbrcbp0+fhtMZim5fa+RdnDIDRB5m151Ie6TniCPvXUXaCfKBfpFwXXWKpsJ2HlOcp05SWQBp\nNF7eBphXSAEVa7z8WuYSRdLlKS//rJgiKRbVaDTYtWsXXnnlFfzhD3/A22+/LUnTZAxdYtKok7yn\nSHlRjJ6jUqkwefJkTJ48Gb/+9a8BhMx7bW0tbDYbvvvuO7zzzju4fPkyJk2aRM37hAkToFarI5r3\n+Ph4NDQ0wOPxYMqUKVj0+H/pOcXGnZhtsXkXgkEE/J1CFC3nnXwtMesKW7fy6E3Y8QLSyaJ+n/QY\naoAW/Mij2WTt4teL8bRL2x2qNeqwY3AqrnPaoFYNtUYD3u+Hz+2VFPKSc3EqDh+/O1lSLKpSqbB1\n61a89957eOWVV2gHIQZjMMG0vO/hOA7JycmYN28e5s2bByBkOp1OJw4dOoSDBw/ivffew7Fjx6BW\nq1FQUEAj70rmneS8GwwGeDwe1NfXIz09Hf/rT80Rc96JTkmi8bICeDk9MfFxGg017gHRziRB6Tzy\noIhSRzOfx0vXS0yzPOXF7/VHnMnR+Ryp6fb7fJLWuu2tTgQDgbBrkd/rw1fbCtHQ0IAjR47QYtGz\nZ8/iueeeQ3x8PL755hukp6d3eX7G0GLIp76YzWYsWbIEFRUVNM9p9uzZOHgw9Am/vLwcRqMRDocD\nS5cu7Zfzd9UyjDzeX+cfbAQCAdTW1tKJfHa7Hc3NzcjOzqZin5eXh0AggKqqKiQmJkKj0Ui2WaNF\n3uX0RcpMV1NXQ8dSFl4lsx3kA5I1yXPVORUnEXdxEVQkAj6eRuzJxUR+TJ/bG2bqv9xWgLq6OrS1\ntWHq1KlITk7GiRMnsHz5chQWFmL16tXQ67seUMIYdMRk6gvT8sEFMe92u52mQB47dgxarRYFBQU0\n8p6ZmYmamhq0tbVBq9VKUiBJ5J1EdsWR92gpM0DIDBNTH8nEy1NmlFJolNJatAk6eJzt9DFtgg4q\nUZMBpTUprTdOo5a8pqupq3EaNf0AEeQDivM+xCmL8nz+f2wy4sSJExg9ejQmTpwIQRDwl7/8BVu3\nbsWaNWtw1113ha2PMajplpYPeaM+kEQbi0taipG8ypEjRw7LSuZAIICamhqa8753716cPXsWs2bN\nws9+9jMUFBQgLy8PHMfRnHefz4eEhASa7x7NvPck550IvjynXb7tGKkffFAUUZF0bhG9XmlgFNCZ\n2hKUpcmQ18rHdBMCPtIWsvN8qo4OM6R9GsepJEZ986spOH/+PCZMmICMjAx4vV6sX78e//rXv7Bx\n40ZJ+7e+JJrhIX83AKghY/SImDTqAwnT8u4hCAJaW1tx6NAh2Gw2/PDDEpZpKgAAE+RJREFUD/j+\n++8RHx8Pk8mE4uJizJo1C2PHjoXb7UZra6sk8h7NvAPRDXyktBlCpGJVuVmX1/10dWwlox4IBCQd\nvCLN5hCbdb/XRzVefL3Q6LRwtbR1mHO1ZH0AUL5mFK5cuQKe5zFt2jQkJibixx9/xDPPPIN58+bh\n5ZdfRmJi59CnvoTpeb/CctT7m+60DCstLcWePXuG9XZtXFwcpk2bhmnTpsHtduPixYv48ssv4XK5\naCV4WVkZWltbkZOTQyPvkyZNAhDa/m5oaJCY9x2bJsFgMEAr2i6MlPMuvg2ET1gl+e7RIM8hr5V3\nPJD075Wdm8D7/WFbrKHnd3Qo8PFhUXrez1OxDwYFataDgYAk1UVAaH0fbZqEkydP4ty5c9BoNHj+\n+edx/vx5nDlzBr/85S9hNpv7ZUw00L1+12vWrMHOnTtRVlbG2nAxBgVMy7sHx3EYMWIEfv7zn2P+\n/Pn47LPP8Prrr2PRokU4fPgwrFYr3nzzTZw4cQIJCQkoLCyk/9LT0+FyudDY2Ii2tjaoVCokJyfj\nL+vSo5p3oDPn3e/1he0mSiLTUVJklNIRCWJNF+suEArwdDVrI9JjpM6JHFs84VXpvOLUyM8/nImG\nhgbU19cjISEBp0+fxrJlyzBixAhcvHgRr776Kh544AFJEKsvYXo+OGBGvRdEaxlWVFQEo9GI1NRU\nVFRUXO/lDUqeeOIJybbx9OnT8dvf/hYAwPM8Tpw4AavVit27d2PNmjVwOp2YPHky7TZDzPuVK1dw\n6tSpHpt3JZS2OoHILSEjmXCf2ysR4NAwiqC0RaNA+qWLou+yjjPiIixe1OuddKkh+Y2kQIsc/+sd\nRWhsbERNTQ0tFr18+TIMBgMCgQAefvhhnD59Go8//jj++Mc/4p577on63vSUaIbHbDajuLgYAGJm\nuiRj6MO0vOfExcVh37591FzffvvtuP322wGEIu8tLS00bWb9+vWoqalBUlKSJG2GmPeGhgY4nU6o\nVCro9fqI5j3ShFVidMXaqRQd97a7w+6XB04ipSB6RfVDatmgIQCddUMALfYnNVA8IGnTKDZeHmc7\n/Zq04gWA//fBNFRXV0Or1eLmm2+GVqvF1atXkZCQgPnz5yMzMxPffPMNtm7dil27dimuubcwPR8c\nMKPej5BCqJUrV2LJkiVU7IczXVWhq9Vq5OXlIS8vD48++iiAkHn/73//C5vNhq+++gqvvfYaXC4X\ncnNzqdgbjUYIgoDm5mZq3hMTE6HX6yOa967SZLqKvCsRjNSn1+3p/N40mrCuMAAR784hTEBnNwL5\nhSDIB+gxSCSdPGfn+zlhxaI7duzAhg0b8NJLL6GkpCSsnVh/EM3wWK2hD052ux0Wi4WJO2NIwLRc\nmUh6znEcUlNTsWDBAixYsABAyLxfuXIFdrsdVqsV69atQ01NDZKTk8Mi706nM8y8/3X9DTAYDEhK\nSgqLvIsNeqTIOwB6vxja9rCLGiG5douvDfLpoIDUtNNjiAI8nd1fpNH/YCCAXdtnoaGhAVVVVZgy\nZQpSU1Nx/vx5vPDCC+B5Hp9//jnGjRsXca19CdPzwQEz6r0gWsuw8vJyrFy5EikpKTAajTCbzewX\nuYeo1WrMnDkTM2fOxO9//3sAIfN+7NgxWK1WfPHFF3j11VfhcrkwZcoUat6zs7MRCATQ3NyMn376\nCX6/H4mJiTAYDPhosxF6vT7MvCvluYvFVRx5J/eH5T0qbGsCoa1NQQiC41RQd0RbgnJx72JbtfO8\n0qFJpFhUPFm0trYWzzzzDHJzc/Hdd99hxIgRUY97PUlLS6O5vmazmeU1MgYcpuX9D8dxGDlyJEwm\nE00dIgGWgwcPwmazYe3atTh58iT0ej1NgSwoKMCYMWPgcrlw6tSpbpl3pci7EkJQCGsdCYgKUkXt\neSOlR/q9voi6LwSDkloncethpRSdynezYbVaMXr0aDqFdsuWLSgvL8fq1auxcOHCbn1f1xOm5/0P\nM+q9INpYXDGkSInRe9RqNfLz85Gfn4/HH38cAOD3+3H06FHYbDZ8+umneOWVV+B2uyXmfezYsQgE\nArh8+TLq6+upedfr9XhrVSJaW1uRm5tLL9IkbUZu3sUo5Tuq1HERU14EIQhe1MFFnvZCnqOKi5Ok\ntQQDAboOIQh8U1mMixcvwmq1Yvz48cjNzYXf70dZWRm++uorvPPOO7j55pt7/N72lmiGJy0tjUYi\nU1JSYLVambAzBhym5QMDx3FIS0vDnXfeiTvvvBNAyLxfvnyZmvcvv/wStbW1SElJkUTeR48erWje\n330tFRcvXoRer0dOTg40Gk1Y5F2pbaTYrItNdFf6D3TuqEZqCCDuIka+5r0+iWn/4m/5qK2tRV1d\nHWbMmIGkpCQcP34cy5cvR1FREfbv34/k5OSevbl9ANPzwQHr+tJLlFqGiVuKlZWVwWg0orm5eUBa\nig3nimy/34/q6mo6ke/w4cPwer2YOnUqjdacP38eLS0tdBQ3z/M08k7+aUStGZW6zQDhYi5OmVFq\n4ShHOfdS+QLx2daZOHHiBFQqFXJzc6HT6fDDDz+gtLQUJSUlePbZZyVrvp6QnNSlS5eirKwMJpMJ\nRUVF1PA4HA4ajSR/G735vXQ4HHA4HNizZw+Ki4uRkpKC999/Hzt37uzD72rQwbq+9AMDreUA0/NI\nCIKAS5cu4eDBg7BarbDb7airq0NqairV8uzsbOzevRs33XQTUlJS4PP5oFKpJJ3DlNJmxHAqLiyV\nUakeKU6jpvfLn6+U4kj0Xa7pu/8xBxcuXIDD4aCTRb1eL9atW4d9+/Zh06ZNA1qcyfS832HtGWOd\naC3FAODBBx+kFdnkj2w44/P5UF1djW+++QYVFRUQBAFjxoyB0WikkfcpU6bA7/ejtbUVbW1tkrSZ\n7ph3QGrg5UIvN+7dNem7/zEHjY2NaGpqosWiV65cwapVq3DmzBls3rx5UOTNRjM85eXlGDlyJKxW\na6/HwFssFphMJpjNZlRWVmLnzp0oLy+P9T7XzKjHIEzPe4YgCHRXcdu2bdi9ezemTZsGrVaLwsJC\nFBUVIT8/H2lpaXA6nWhtbYXL5aLmnfxLTEyUpc2Ea29Xeg6IZlt0YdLJ6wmffjgDx48fh1arRU5O\nDrRaLfbt24cXX3wRjzzyCJ566imo1QOf9MD0vF9hRj3WKS0txR133AGTyQSLxRIWhTGbzXA4HCyX\nUoEVK1bAZDLhrrvugtfrxY8//kgj71VVVeB5HtOnT6fRmpycHPh8Ptrnned5JCUlSaI11xJ57y6f\nfDAVx48fpzmyKpUKn3zyCdavX4/S0lL85je/uS7FooOV0tJSFBcXD5coIzPqMQjT82ujtrYWb7zx\nBtasWYNRo0bhwoULsNlsNPJeX1+PUaNG0WnZBQUFSE1NlZj3uLg4SZ/3pKQkqqdEy8U1TJHmZHQF\nef3XH81GQ0MDzp07h9zcXIwcORKXLl3Cn//8Z1y9ehUbN27EhAkT+vhdGloMIz1nfdRjHVaRfe2s\nX7+efq3T6TBnzhzJACCv14sjR47AarXio48+QlVVFQKBAPLy8mjknQwRunTpEhwOBwKBAI28/+Pd\nbOj1eol5V9pqjcauHUWoq6vD8ePHMW3aNCQnJ+Onn37Cs88+i4yMDOzbt0/yOzBcsVgsWLly5UAv\ng8G4ZpieXxuTJ0/GBx98QG+np6fjnnvuoe1nBUHAuXPn6MC9yspKnDp1CmPGjJFE3lNSUtDW1ob6\n+nqJef/wrYyI5r2n7CzPhc1mw6hRozB37lxwHIft27dj48aNWLVqFRYtWjSsAy4EpudSmFGPcVhF\n9rWh0+lQXFxMe8QKggCPx4MjR47AZrNh27ZtqK6uhiAIyMvLo5F3Yt4vXryIuro6iXn/aNOkMPMe\nSfDFxaKZmZnIyckBz/N4++238cknn+Ctt97Cbbfddl3ei8GKw+GQ5EqSoj/2e86IVZie9xyO4zB2\n7Fjcd999uO+++wCE9Pzs2bPUvO/YsQMNDQ1IT0+ngZjCwkIYDAa0tbXB4XCgvb39ms37l38vRF1d\nHWpra5GXl4ekpCTU1tZi+fLlmDp1Kr799ttB153resP0PDLMqA9hWEX29YPjOCQkJODGG2/EjTfe\nCCAk9m63m0beP/zwQ1RXV4PjOEXzfuHChTDz/o93s2EwGCS5iB6PB1VVVVCpVJg1axZ0Oh1sNhue\ne+453HPPPThw4EC/TaIbSpCc3qKiIqxduxZmsxnA8CqyY8QOTM+vHxzHYdy4cVi4cCFteSgIApqa\nmmjazPbt29HQ0IBx48ahoKAARUVFKCgoQHJyMpxOp8S8GwwGbH07E3q9XmLeBUGgqThZWVm0/mnt\n2rXYtWsXNmzYgJtuumkg34pBA9PzyDCjPoSJ1lKspKSE/rK3tLTQ6HBfE61TAaGsrCymtms5jkNi\nYiJuuukmKrbEvB8+fBg2mw1//etfUV1dDZVKhRkzZtBoTWZmJjweT5h5DwaDcDqdyM7OxtixY9Ha\n2oo//elPqKmpwbZt25Cbm9tv30+0nyN5XFxUNJAwAWfEEkzPBxaO45CRkYGMjAzcf//9AEJ6fvr0\naRp537p1K86cOYNx48bRQExhYSESExPhdDpx8eJFat4TE0Mtf+Pj41FYWIiEhAQcOHAAL7zwAh58\n8EHs37+/X7tzMT2PHZhRH8IUFRXBZrPBYrEgJSWFdgBYsGABDh48CKPRiJSUFJjNZly+fLlfRNVu\ntwMATCYTHA4H7Ha7YicCi8WCPXv2xJSwK0HM+y233IJbbrkFQEjs29vbcejQIRw8eBAVFRU4duwY\nVCoV8vPzqYjbbDY88sgjSEtLw+rVq/Htt9+ivb0dt99+O1atWoWxY8f227qj/RztdjuMRiPddo/0\nc2YwGNcG0/PBB8dxGD9+PMaPH48HHngAABAMBtHY2EhbRW7ZsgVnzpxBZmYmCgsLkZ+fD6vVipyc\nHMydOxctLS10F5bneTz99NO4++676SCl/oDpeWzBjPoQR+mTMGmbJH68vz6tVlZW4o477gAAGI1G\nWCwW9gcvg+M4JCUlYd68eZg3bx6AkHl3uVw4cOAA1qxZg5qaGkycOBHLly9HTk4O6urqcOutt2LJ\nkiW0V21jYyMefvjhflljd36OpaWl2LNnjyTax2Aw+g6m54MflUqFrKwsZGVlYdGiRQBC5r2hoQE7\nduzAihUrkJmZib179+Lzzz9HYmIidDodnn76aUycOBF2ux0vvfQStm7dioSEhH5ZI9Pz2IIZdUav\niNapAAh9ejeZTL3usRpLcByH5ORkBINBPPTQQ1iyZAk4joPT6cT333+P2tpaPPXUUwCA2267DY8+\n+mi/rifaz7GoqAhGoxGpqamoqKjo17UwGIyBgen5tUHM+5UrV7Bv3z7k5uYiGAzip59+wo4dO/Dm\nm28iKysLAKiB7k+YnscWzKgz+h1SIMUI56677pLc1uv1uPvuuwdoNZEhebIrV67EkiVLqNAzGIzh\nBdNzZTiOw7p16+htlUoFo9GIF198cQBXpQzT86EFM+qMXhGtUwGJvjAGN9F+juXl5Vi5ciUduETG\nRjMYjNiB6XlswPQ8tgifhctg9IDFixfD4XAACO9UQO4zm80oLy9Hc3MzLXJhDC6i/RzFlJSU0B63\nDAYjdmB6HhswPY8tmFFn9ApSoKLUqQAIiQApfFISCcbgINrP8fnnn0d5eTm9SA+Gdl4MBqNvYXoe\nGzA9jy04QRB68vwePZnBuJ5E6xtbXl4OAKirq2OFUIyhTF/MGGdazhjUMD1nDAO6peUsos6ICcR9\nY4m4i7FYLDCZTFi6dCkcDgcsFstALJPBYDAYUWB6zmB0wow6IyaorKykeXakb6wYsZgbjUaav8dg\nMBiMwQXTcwajE2bUhwlmsxmlpaU0r9But6O0tHSAV9V3ROsbu3TpUpqHZ7fbMWfOnOu6vv6mq6Iu\ns9kMi8WCsrKy67giBoPRXzA9j109Z1rOkMOM+jDAbDajpKQEdrudtmyqrKxEdnb2AK/s+kNGJcfS\ntD2LxYIHH3xQ8bFoW8gMBmNowfS8k1jTc6blDCWYUR8GlJSUoKWlBQ6Hgw41IDl+sUK0vrEEi8US\nc4VHJpMp4rCKaFvIDAZjaMH0vJNY03Om5QwlmFEfJnz88ce0rRYAicjHAt3pG1teXk67BwwXkevO\nSHAGgzG0YHo+/PScafnwhRn1YUJdXR2Ki4sBhLZOYyn6AkTvG2uxWFBaWors7Gykpqb261qi5RGy\nPEMGg9EbmJ4zPWcMH3raR50xROE4zghgGQBrx/87BUEoH9hVxR4cxxUBMAqCYOY4bikAmyAI9u4+\n3ovz7hEE4Q6F+9cC2CMIgoXjuJKOc7MrCoMxhGF6fn0YCD1nWs6QwyLqwwRBEByCIJQKgmAGMBLA\nxwO9phhlMQCyP+sAIA91RXu8T+A4jsyErgRA9sSNAGJ/j5jBiHGYnl83BlzPmZYzmFEfBnAcZ+Q4\nbmfH1yaEPvWz+c/9QwqAZtFteRVUtMd7TEd0ZU7H/4S9AECiOx0/95a+iN4zGIyBg+n5deW66jnT\ncoYS6oFeAOO60AygUrRdtmygF8ToOzqiambZfbNFX7MtcQYjdmB6HqMwLWcowYz6MKAj2mKO+kRG\nX9CC0FY0EIq2yEvzoz3OYDAYEWF6fl1hes4YcFjqC4PRtyjmEbI8QwaDwRhyMD1nDDjMqDMYfUgX\neYQsz5DBYDCGEEzPGYMB1p6RwWAwGAwGg8EYhLCIOoPBYDAYDAaDMQhhRp3BYDAYDAaDwRiEMKPO\nYDAYDAaDwWAMQv4/oYNU+aEN6L0AAAAASUVORK5CYII=\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x7f0eee322eb8>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "evaluation_points = np.column_stack((field50.x, field50.y))\n",
    "prediction = dataBasedModel.predict(evaluation_points)\n",
    "plot_field_and_error(field50.x, field50.y, prediction, field50.A, \"trial_function_random.pdf\",\n",
    "                     [\"Trial function with random weights\", \"Deviation from the numerical solution\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data driven learning using a trial function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "The initial loss is 375.9459495336366\n",
      "\n",
      "Wrote weights to file ./trialFunctionDataBasedLearning/baselineMLP/best_weights\n",
      "The loss after 1 iterations is E=373.07254500802543\n",
      "Wrote weights to file ./trialFunctionDataBasedLearning/baselineMLP/best_weights\n",
      "The loss after 2 iterations is E=370.22409110355176\n",
      "Wrote weights to file ./trialFunctionDataBasedLearning/baselineMLP/best_weights\n",
      "The loss after 3 iterations is E=367.4010908042502\n",
      "Wrote weights to file ./trialFunctionDataBasedLearning/baselineMLP/best_weights\n",
      "The loss after 4 iterations is E=364.60400355831655\n",
      "Wrote weights to file ./trialFunctionDataBasedLearning/baselineMLP/best_weights\n",
      "The loss after 5 iterations is E=361.8332000140046\n",
      "Wrote weights to file ./trialFunctionDataBasedLearning/baselineMLP/best_weights\n",
      "The loss after 6 iterations is E=359.0889531822155\n",
      "Wrote weights to file ./trialFunctionDataBasedLearning/baselineMLP/best_weights\n",
      "The loss after 7 iterations is E=356.3714325670884\n",
      "Wrote weights to file ./trialFunctionDataBasedLearning/baselineMLP/best_weights\n",
      "The loss after 8 iterations is E=353.68069854113907\n",
      "Wrote weights to file ./trialFunctionDataBasedLearning/baselineMLP/best_weights\n",
      "The loss after 9 iterations is E=351.01669656286606\n",
      "Wrote weights to file ./trialFunctionDataBasedLearning/baselineMLP/best_weights\n",
      "The loss after 10 iterations is E=348.37925122243865\n",
      "Wrote weights to file ./trialFunctionDataBasedLearning/baselineMLP/best_weights\n",
      "The loss after 11 iterations is E=345.7680602020219\n",
      "Wrote weights to file ./trialFunctionDataBasedLearning/baselineMLP/best_weights\n",
      "The loss after 12 iterations is E=343.18268828769\n",
      "Wrote weights to file ./trialFunctionDataBasedLearning/baselineMLP/best_weights\n",
      "The loss after 13 iterations is E=340.6225616197442\n",
      "Wrote weights to file ./trialFunctionDataBasedLearning/baselineMLP/best_weights\n",
      "The loss after 14 iterations is E=338.0869624312461\n",
      "Wrote weights to file ./trialFunctionDataBasedLearning/baselineMLP/best_weights\n",
      "The loss after 15 iterations is E=335.5750246054121\n",
      "Wrote weights to file ./trialFunctionDataBasedLearning/baselineMLP/best_weights\n",
      "The loss after 16 iterations is E=333.0857304807877\n",
      "Wrote weights to file ./trialFunctionDataBasedLearning/baselineMLP/best_weights\n",
      "The loss after 17 iterations is E=330.6179094436189\n",
      "Wrote weights to file ./trialFunctionDataBasedLearning/baselineMLP/best_weights\n",
      "The loss after 18 iterations is E=328.17023895596236\n",
      "Wrote weights to file ./trialFunctionDataBasedLearning/baselineMLP/best_weights\n",
      "The loss after 19 iterations is E=325.7412487500587\n",
      "Wrote weights to file ./trialFunctionDataBasedLearning/baselineMLP/best_weights\n",
      "The loss after 20 iterations is E=323.3293289346118\n",
      "Wrote weights to file ./trialFunctionDataBasedLearning/baselineMLP/best_weights\n",
      "The loss after 21 iterations is E=320.9327426542945\n",
      "Wrote weights to file ./trialFunctionDataBasedLearning/baselineMLP/best_weights\n",
      "The loss after 22 iterations is E=318.5496436706578\n",
      "Wrote weights to file ./trialFunctionDataBasedLearning/baselineMLP/best_weights\n",
      "The loss after 23 iterations is E=316.1780987722713\n",
      "Wrote weights to file ./trialFunctionDataBasedLearning/baselineMLP/best_weights\n",
      "The loss after 24 iterations is E=313.8161143220007\n",
      "Wrote weights to file ./trialFunctionDataBasedLearning/baselineMLP/best_weights\n",
      "The loss after 25 iterations is E=311.4616656364677\n",
      "Wrote weights to file ./trialFunctionDataBasedLearning/baselineMLP/best_weights\n",
      "The loss after 26 iterations is E=309.11272743711123\n",
      "Wrote weights to file ./trialFunctionDataBasedLearning/baselineMLP/best_weights\n",
      "The loss after 27 iterations is E=306.7673034507013\n",
      "Wrote weights to file ./trialFunctionDataBasedLearning/baselineMLP/best_weights\n",
      "The loss after 28 iterations is E=304.4234534045562\n",
      "Wrote weights to file ./trialFunctionDataBasedLearning/baselineMLP/best_weights\n",
      "The loss after 29 iterations is E=302.07931608482227\n",
      "Wrote weights to file ./trialFunctionDataBasedLearning/baselineMLP/best_weights\n",
      "The loss after 30 iterations is E=299.7331276750755\n",
      "Wrote weights to file ./trialFunctionDataBasedLearning/baselineMLP/best_weights\n",
      "The loss after 31 iterations is E=297.3832351436965\n",
      "Wrote weights to file ./trialFunctionDataBasedLearning/baselineMLP/best_weights\n",
      "The loss after 32 iterations is E=295.02810491721925\n",
      "Wrote weights to file ./trialFunctionDataBasedLearning/baselineMLP/best_weights\n",
      "The loss after 33 iterations is E=292.666327419119\n",
      "Wrote weights to file ./trialFunctionDataBasedLearning/baselineMLP/best_weights\n",
      "The loss after 34 iterations is E=290.29661825595275\n",
      "Wrote weights to file ./trialFunctionDataBasedLearning/baselineMLP/best_weights\n",
      "The loss after 35 iterations is E=287.9178169040867\n",
      "Wrote weights to file ./trialFunctionDataBasedLearning/baselineMLP/best_weights\n",
      "The loss after 36 iterations is E=285.5288837147992\n",
      "Wrote weights to file ./trialFunctionDataBasedLearning/baselineMLP/best_weights\n",
      "The loss after 37 iterations is E=283.1288959463495\n",
      "Wrote weights to file ./trialFunctionDataBasedLearning/baselineMLP/best_weights\n",
      "The loss after 38 iterations is E=280.71704338349787\n",
      "Wrote weights to file ./trialFunctionDataBasedLearning/baselineMLP/best_weights\n",
      "The loss after 39 iterations is E=278.2926239479073\n",
      "Wrote weights to file ./trialFunctionDataBasedLearning/baselineMLP/best_weights\n",
      "The loss after 40 iterations is E=275.8550395584391\n",
      "Wrote weights to file ./trialFunctionDataBasedLearning/baselineMLP/best_weights\n",
      "The loss after 41 iterations is E=273.4037923809218\n",
      "Wrote weights to file ./trialFunctionDataBasedLearning/baselineMLP/best_weights\n",
      "The loss after 42 iterations is E=270.9384815171163\n",
      "Wrote weights to file ./trialFunctionDataBasedLearning/baselineMLP/best_weights\n",
      "The loss after 43 iterations is E=268.45880012113236\n",
      "Wrote weights to file ./trialFunctionDataBasedLearning/baselineMLP/best_weights\n",
      "The loss after 44 iterations is E=265.96453289450454\n",
      "Wrote weights to file ./trialFunctionDataBasedLearning/baselineMLP/best_weights\n",
      "The loss after 45 iterations is E=263.45555389287534\n",
      "Wrote weights to file ./trialFunctionDataBasedLearning/baselineMLP/best_weights\n",
      "The loss after 46 iterations is E=260.93182457260593\n",
      "Wrote weights to file ./trialFunctionDataBasedLearning/baselineMLP/best_weights\n",
      "The loss after 47 iterations is E=258.3933920097657\n",
      "Wrote weights to file ./trialFunctionDataBasedLearning/baselineMLP/best_weights\n",
      "The loss after 48 iterations is E=255.84038723336587\n",
      "Wrote weights to file ./trialFunctionDataBasedLearning/baselineMLP/best_weights\n",
      "The loss after 49 iterations is E=253.27302362652054\n",
      "Wrote weights to file ./trialFunctionDataBasedLearning/baselineMLP/best_weights\n",
      "The loss after 50 iterations is E=250.6915953616556\n",
      "Wrote weights to file ./trialFunctionDataBasedLearning/baselineMLP/best_weights\n",
      "The loss after 51 iterations is E=248.09647584762325\n",
      "Wrote weights to file ./trialFunctionDataBasedLearning/baselineMLP/best_weights\n",
      "The loss after 52 iterations is E=245.48811617683708\n",
      "Wrote weights to file ./trialFunctionDataBasedLearning/baselineMLP/best_weights\n",
      "The loss after 53 iterations is E=242.86704356869606\n",
      "Wrote weights to file ./trialFunctionDataBasedLearning/baselineMLP/best_weights\n",
      "The loss after 54 iterations is E=240.23385981116272\n",
      "Wrote weights to file ./trialFunctionDataBasedLearning/baselineMLP/best_weights\n",
      "The loss after 55 iterations is E=237.5892397050991\n",
      "Wrote weights to file ./trialFunctionDataBasedLearning/baselineMLP/best_weights\n",
      "The loss after 56 iterations is E=234.93392951539175\n",
      "Wrote weights to file ./trialFunctionDataBasedLearning/baselineMLP/best_weights\n",
      "The loss after 57 iterations is E=232.26874542889516\n",
      "Wrote weights to file ./trialFunctionDataBasedLearning/baselineMLP/best_weights\n",
      "The loss after 58 iterations is E=229.5945720118515\n",
      "Wrote weights to file ./trialFunctionDataBasedLearning/baselineMLP/best_weights\n",
      "The loss after 59 iterations is E=226.91236064920957\n",
      "Wrote weights to file ./trialFunctionDataBasedLearning/baselineMLP/best_weights\n",
      "The loss after 60 iterations is E=224.2231279362908\n",
      "Wrote weights to file ./trialFunctionDataBasedLearning/baselineMLP/best_weights\n",
      "The loss after 61 iterations is E=221.52795398143064\n",
      "Wrote weights to file ./trialFunctionDataBasedLearning/baselineMLP/best_weights\n",
      "The loss after 62 iterations is E=218.82798056870146\n",
      "Wrote weights to file ./trialFunctionDataBasedLearning/baselineMLP/best_weights\n",
      "The loss after 63 iterations is E=216.12440912526574\n",
      "Wrote weights to file ./trialFunctionDataBasedLearning/baselineMLP/best_weights\n",
      "The loss after 64 iterations is E=213.4184984400774\n",
      "Wrote weights to file ./trialFunctionDataBasedLearning/baselineMLP/best_weights\n",
      "The loss after 65 iterations is E=210.71156209069864\n",
      "Wrote weights to file ./trialFunctionDataBasedLearning/baselineMLP/best_weights\n",
      "The loss after 66 iterations is E=208.00496555221622\n",
      "Wrote weights to file ./trialFunctionDataBasedLearning/baselineMLP/best_weights\n",
      "The loss after 67 iterations is E=205.30012298455523\n",
      "Wrote weights to file ./trialFunctionDataBasedLearning/baselineMLP/best_weights\n",
      "The loss after 68 iterations is E=202.5984937185497\n",
      "Wrote weights to file ./trialFunctionDataBasedLearning/baselineMLP/best_weights\n",
      "The loss after 69 iterations is E=199.90157848311415\n",
      "Wrote weights to file ./trialFunctionDataBasedLearning/baselineMLP/best_weights\n",
      "The loss after 70 iterations is E=197.21091543232288\n",
      "Wrote weights to file ./trialFunctionDataBasedLearning/baselineMLP/best_weights\n",
      "The loss after 71 iterations is E=194.52807603980608\n",
      "Wrote weights to file ./trialFunctionDataBasedLearning/baselineMLP/best_weights\n",
      "The loss after 72 iterations is E=191.85466092764773\n",
      "Wrote weights to file ./trialFunctionDataBasedLearning/baselineMLP/best_weights\n",
      "The loss after 73 iterations is E=189.19229568871236\n",
      "Wrote weights to file ./trialFunctionDataBasedLearning/baselineMLP/best_weights\n",
      "The loss after 74 iterations is E=186.5426267466261\n",
      "Wrote weights to file ./trialFunctionDataBasedLearning/baselineMLP/best_weights\n",
      "The loss after 75 iterations is E=183.9073172792445\n",
      "Wrote weights to file ./trialFunctionDataBasedLearning/baselineMLP/best_weights\n",
      "The loss after 76 iterations is E=181.28804321208474\n",
      "Wrote weights to file ./trialFunctionDataBasedLearning/baselineMLP/best_weights\n",
      "The loss after 77 iterations is E=178.68648927047266\n",
      "Wrote weights to file ./trialFunctionDataBasedLearning/baselineMLP/best_weights\n",
      "The loss after 78 iterations is E=176.1043450652722\n",
      "Wrote weights to file ./trialFunctionDataBasedLearning/baselineMLP/best_weights\n",
      "The loss after 79 iterations is E=173.54330117835866\n",
      "Wrote weights to file ./trialFunctionDataBasedLearning/baselineMLP/best_weights\n",
      "The loss after 80 iterations is E=171.00504521116412\n",
      "Wrote weights to file ./trialFunctionDataBasedLearning/baselineMLP/best_weights\n",
      "The loss after 81 iterations is E=168.49125776252058\n",
      "Wrote weights to file ./trialFunctionDataBasedLearning/baselineMLP/best_weights\n",
      "The loss after 82 iterations is E=166.00360830965613\n",
      "Wrote weights to file ./trialFunctionDataBasedLearning/baselineMLP/best_weights\n",
      "The loss after 83 iterations is E=163.54375097731736\n",
      "Wrote weights to file ./trialFunctionDataBasedLearning/baselineMLP/best_weights\n",
      "The loss after 84 iterations is E=161.1133201926337\n",
      "Wrote weights to file ./trialFunctionDataBasedLearning/baselineMLP/best_weights\n",
      "The loss after 85 iterations is E=158.713926235908\n",
      "Wrote weights to file ./trialFunctionDataBasedLearning/baselineMLP/best_weights\n",
      "The loss after 86 iterations is E=156.34715070822628\n",
      "Wrote weights to file ./trialFunctionDataBasedLearning/baselineMLP/best_weights\n",
      "The loss after 87 iterations is E=154.0145419446492\n",
      "Wrote weights to file ./trialFunctionDataBasedLearning/baselineMLP/best_weights\n",
      "The loss after 88 iterations is E=151.71761040607205\n",
      "Wrote weights to file ./trialFunctionDataBasedLearning/baselineMLP/best_weights\n",
      "The loss after 89 iterations is E=149.4578240836359\n",
      "Wrote weights to file ./trialFunctionDataBasedLearning/baselineMLP/best_weights\n",
      "The loss after 90 iterations is E=147.2366039474503\n",
      "Wrote weights to file ./trialFunctionDataBasedLearning/baselineMLP/best_weights\n",
      "The loss after 91 iterations is E=145.05531946705068\n",
      "Wrote weights to file ./trialFunctionDataBasedLearning/baselineMLP/best_weights\n",
      "The loss after 92 iterations is E=142.9152842257608\n",
      "Wrote weights to file ./trialFunctionDataBasedLearning/baselineMLP/best_weights\n",
      "The loss after 93 iterations is E=140.81775164582496\n",
      "Wrote weights to file ./trialFunctionDataBasedLearning/baselineMLP/best_weights\n",
      "The loss after 94 iterations is E=138.76391083685388\n",
      "Wrote weights to file ./trialFunctionDataBasedLearning/baselineMLP/best_weights\n",
      "The loss after 95 iterations is E=136.75488257729523\n",
      "Wrote weights to file ./trialFunctionDataBasedLearning/baselineMLP/best_weights\n",
      "The loss after 96 iterations is E=134.79171543754003\n",
      "Wrote weights to file ./trialFunctionDataBasedLearning/baselineMLP/best_weights\n",
      "The loss after 97 iterations is E=132.8753820538852\n",
      "Wrote weights to file ./trialFunctionDataBasedLearning/baselineMLP/best_weights\n",
      "The loss after 98 iterations is E=131.0067755643785\n",
      "Wrote weights to file ./trialFunctionDataBasedLearning/baselineMLP/best_weights\n",
      "The loss after 99 iterations is E=129.1867062201743\n",
      "Wrote weights to file ./trialFunctionDataBasedLearning/baselineMLP/best_weights\n",
      "The loss after 100 iterations is E=127.41589818875927\n",
      "Time for 100 iterations: 13.37 min\n"
     ]
    }
   ],
   "source": [
    "training_points = np.column_stack((intField50.x, intField50.y))\n",
    "dataBasedModel.set_control_points(training_points)\n",
    "start = time.time()\n",
    "dataBasedModel.solve(max_iter=100, learning_rate=0.01, verbose=1, adaptive=False, tolerance=1.0E-5)\n",
    "print(\"Time for 100 iterations: {:5.2f} min\".format((time.time() - start) / 60.0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAuoAAAD3CAYAAABLsHHKAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nGAzGCOT/AybAu/heigFvAAAAAElFTkSuQmCC\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x7f0eecff5198>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "evaluation_points = np.column_stack((field50.x, field50.y))\n",
    "prediction = dataBasedModel.predict(evaluation_points)\n",
    "plot_field_and_error(field50.x, field50.y, prediction, field50.A, \"trialFunctionDataBasedLearning_100.pdf\",\n",
    "                     [r\"Trial function after $100$ iterations\", \"Deviation from the numerical solution\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Residual based learning using a trial function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Initialized weights and biases for MLP with 20 parameters.\n",
      "\n",
      "MLP structure:\n",
      "--------------\n",
      "Input features:    2\n",
      "Hidden layers:     1\n",
      "Neurons per layer: 5\n",
      "Number of weights: 20\n",
      "\n",
      "Placing control points for 2 input variables.\n",
      "------------------------------------------------\n",
      "Control points per input parameter: [49, 49]\n",
      "The total number of control points is 2401\n",
      "\n",
      "The initial loss is 11493.059978343317\n",
      "\n",
      "Wrote weights to file ./trialFunctionResidualBasedLearning/baselineMLP/best_weights\n",
      "The loss after 1 iterations is E=11478.309182717705\n",
      "Wrote weights to file ./trialFunctionResidualBasedLearning/baselineMLP/best_weights\n",
      "The loss after 2 iterations is E=11463.644854983102\n",
      "Wrote weights to file ./trialFunctionResidualBasedLearning/baselineMLP/best_weights\n",
      "The loss after 3 iterations is E=11449.069868227964\n",
      "Wrote weights to file ./trialFunctionResidualBasedLearning/baselineMLP/best_weights\n",
      "The loss after 4 iterations is E=11434.586543965825\n",
      "Wrote weights to file ./trialFunctionResidualBasedLearning/baselineMLP/best_weights\n",
      "The loss after 5 iterations is E=11420.1967340369\n",
      "Wrote weights to file ./trialFunctionResidualBasedLearning/baselineMLP/best_weights\n",
      "The loss after 6 iterations is E=11405.901829637449\n",
      "Wrote weights to file ./trialFunctionResidualBasedLearning/baselineMLP/best_weights\n",
      "The loss after 7 iterations is E=11391.70274912158\n",
      "Wrote weights to file ./trialFunctionResidualBasedLearning/baselineMLP/best_weights\n",
      "The loss after 8 iterations is E=11377.599916365041\n",
      "Wrote weights to file ./trialFunctionResidualBasedLearning/baselineMLP/best_weights\n",
      "The loss after 9 iterations is E=11363.593233636988\n",
      "Wrote weights to file ./trialFunctionResidualBasedLearning/baselineMLP/best_weights\n",
      "The loss after 10 iterations is E=11349.682050683918\n",
      "Wrote weights to file ./trialFunctionResidualBasedLearning/baselineMLP/best_weights\n",
      "The loss after 11 iterations is E=11335.865130981563\n",
      "Wrote weights to file ./trialFunctionResidualBasedLearning/baselineMLP/best_weights\n",
      "The loss after 12 iterations is E=11322.140615891272\n",
      "Wrote weights to file ./trialFunctionResidualBasedLearning/baselineMLP/best_weights\n",
      "The loss after 13 iterations is E=11308.505987516824\n",
      "Wrote weights to file ./trialFunctionResidualBasedLearning/baselineMLP/best_weights\n",
      "The loss after 14 iterations is E=11294.958031311042\n",
      "Wrote weights to file ./trialFunctionResidualBasedLearning/baselineMLP/best_weights\n",
      "The loss after 15 iterations is E=11281.49279992744\n",
      "Wrote weights to file ./trialFunctionResidualBasedLearning/baselineMLP/best_weights\n",
      "The loss after 16 iterations is E=11268.105580467314\n",
      "Wrote weights to file ./trialFunctionResidualBasedLearning/baselineMLP/best_weights\n",
      "The loss after 17 iterations is E=11254.790868150207\n",
      "Wrote weights to file ./trialFunctionResidualBasedLearning/baselineMLP/best_weights\n",
      "The loss after 18 iterations is E=11241.542350521402\n",
      "Wrote weights to file ./trialFunctionResidualBasedLearning/baselineMLP/best_weights\n",
      "The loss after 19 iterations is E=11228.352907498514\n",
      "Wrote weights to file ./trialFunctionResidualBasedLearning/baselineMLP/best_weights\n",
      "The loss after 20 iterations is E=11215.214633518128\n",
      "Wrote weights to file ./trialFunctionResidualBasedLearning/baselineMLP/best_weights\n",
      "The loss after 21 iterations is E=11202.118888101668\n",
      "Wrote weights to file ./trialFunctionResidualBasedLearning/baselineMLP/best_weights\n",
      "The loss after 22 iterations is E=11189.056379361311\n",
      "Wrote weights to file ./trialFunctionResidualBasedLearning/baselineMLP/best_weights\n",
      "The loss after 23 iterations is E=11176.017280794544\n",
      "Wrote weights to file ./trialFunctionResidualBasedLearning/baselineMLP/best_weights\n",
      "The loss after 24 iterations is E=11162.991376070446\n",
      "Wrote weights to file ./trialFunctionResidualBasedLearning/baselineMLP/best_weights\n",
      "The loss after 25 iterations is E=11149.968221651756\n",
      "Wrote weights to file ./trialFunctionResidualBasedLearning/baselineMLP/best_weights\n",
      "The loss after 26 iterations is E=11136.93731488026\n",
      "Wrote weights to file ./trialFunctionResidualBasedLearning/baselineMLP/best_weights\n",
      "The loss after 27 iterations is E=11123.888255481288\n",
      "Wrote weights to file ./trialFunctionResidualBasedLearning/baselineMLP/best_weights\n",
      "The loss after 28 iterations is E=11110.81089019167\n",
      "Wrote weights to file ./trialFunctionResidualBasedLearning/baselineMLP/best_weights\n",
      "The loss after 29 iterations is E=11097.69543272162\n",
      "Wrote weights to file ./trialFunctionResidualBasedLearning/baselineMLP/best_weights\n",
      "The loss after 30 iterations is E=11084.53255447726\n",
      "Wrote weights to file ./trialFunctionResidualBasedLearning/baselineMLP/best_weights\n",
      "The loss after 31 iterations is E=11071.313445156244\n",
      "Wrote weights to file ./trialFunctionResidualBasedLearning/baselineMLP/best_weights\n",
      "The loss after 32 iterations is E=11058.029845779105\n",
      "Wrote weights to file ./trialFunctionResidualBasedLearning/baselineMLP/best_weights\n",
      "The loss after 33 iterations is E=11044.674059152838\n",
      "Wrote weights to file ./trialFunctionResidualBasedLearning/baselineMLP/best_weights\n",
      "The loss after 34 iterations is E=11031.238943781214\n",
      "Wrote weights to file ./trialFunctionResidualBasedLearning/baselineMLP/best_weights\n",
      "The loss after 35 iterations is E=11017.717896966908\n",
      "Wrote weights to file ./trialFunctionResidualBasedLearning/baselineMLP/best_weights\n",
      "The loss after 36 iterations is E=11004.104831803541\n",
      "Wrote weights to file ./trialFunctionResidualBasedLearning/baselineMLP/best_weights\n",
      "The loss after 37 iterations is E=10990.394151513101\n",
      "Wrote weights to file ./trialFunctionResidualBasedLearning/baselineMLP/best_weights\n",
      "The loss after 38 iterations is E=10976.580723538733\n",
      "Wrote weights to file ./trialFunctionResidualBasedLearning/baselineMLP/best_weights\n",
      "The loss after 39 iterations is E=10962.659855076226\n",
      "Wrote weights to file ./trialFunctionResidualBasedLearning/baselineMLP/best_weights\n",
      "The loss after 40 iterations is E=10948.627271240563\n",
      "Wrote weights to file ./trialFunctionResidualBasedLearning/baselineMLP/best_weights\n",
      "The loss after 41 iterations is E=10934.479096677775\n",
      "Wrote weights to file ./trialFunctionResidualBasedLearning/baselineMLP/best_weights\n",
      "The loss after 42 iterations is E=10920.211841061371\n",
      "Wrote weights to file ./trialFunctionResidualBasedLearning/baselineMLP/best_weights\n",
      "The loss after 43 iterations is E=10905.822388536655\n",
      "Wrote weights to file ./trialFunctionResidualBasedLearning/baselineMLP/best_weights\n",
      "The loss after 44 iterations is E=10891.307990822335\n",
      "Wrote weights to file ./trialFunctionResidualBasedLearning/baselineMLP/best_weights\n",
      "The loss after 45 iterations is E=10876.6662633863\n",
      "Wrote weights to file ./trialFunctionResidualBasedLearning/baselineMLP/best_weights\n",
      "The loss after 46 iterations is E=10861.895183911358\n",
      "Wrote weights to file ./trialFunctionResidualBasedLearning/baselineMLP/best_weights\n",
      "The loss after 47 iterations is E=10846.993092171579\n",
      "Wrote weights to file ./trialFunctionResidualBasedLearning/baselineMLP/best_weights\n",
      "The loss after 48 iterations is E=10831.958690458156\n",
      "Wrote weights to file ./trialFunctionResidualBasedLearning/baselineMLP/best_weights\n",
      "The loss after 49 iterations is E=10816.791043805437\n",
      "Wrote weights to file ./trialFunctionResidualBasedLearning/baselineMLP/best_weights\n",
      "The loss after 50 iterations is E=10801.489579453484\n",
      "Wrote weights to file ./trialFunctionResidualBasedLearning/baselineMLP/best_weights\n",
      "The loss after 51 iterations is E=10786.054085205877\n",
      "Wrote weights to file ./trialFunctionResidualBasedLearning/baselineMLP/best_weights\n",
      "The loss after 52 iterations is E=10770.484706552184\n",
      "Wrote weights to file ./trialFunctionResidualBasedLearning/baselineMLP/best_weights\n",
      "The loss after 53 iterations is E=10754.781942578149\n",
      "Wrote weights to file ./trialFunctionResidualBasedLearning/baselineMLP/best_weights\n",
      "The loss after 54 iterations is E=10738.946640752225\n",
      "Wrote weights to file ./trialFunctionResidualBasedLearning/baselineMLP/best_weights\n",
      "The loss after 55 iterations is E=10722.979990636082\n",
      "Wrote weights to file ./trialFunctionResidualBasedLearning/baselineMLP/best_weights\n",
      "The loss after 56 iterations is E=10706.883516471404\n",
      "Wrote weights to file ./trialFunctionResidualBasedLearning/baselineMLP/best_weights\n",
      "The loss after 57 iterations is E=10690.65906851586\n",
      "Wrote weights to file ./trialFunctionResidualBasedLearning/baselineMLP/best_weights\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The loss after 58 iterations is E=10674.30881301879\n",
      "Wrote weights to file ./trialFunctionResidualBasedLearning/baselineMLP/best_weights\n",
      "The loss after 59 iterations is E=10657.835220868878\n",
      "Wrote weights to file ./trialFunctionResidualBasedLearning/baselineMLP/best_weights\n",
      "The loss after 60 iterations is E=10641.241055165458\n",
      "Wrote weights to file ./trialFunctionResidualBasedLearning/baselineMLP/best_weights\n",
      "The loss after 61 iterations is E=10624.529358173242\n",
      "Wrote weights to file ./trialFunctionResidualBasedLearning/baselineMLP/best_weights\n",
      "The loss after 62 iterations is E=10607.703438226676\n",
      "Wrote weights to file ./trialFunctionResidualBasedLearning/baselineMLP/best_weights\n",
      "The loss after 63 iterations is E=10590.766857122264\n",
      "Wrote weights to file ./trialFunctionResidualBasedLearning/baselineMLP/best_weights\n",
      "The loss after 64 iterations is E=10573.723418384878\n",
      "Wrote weights to file ./trialFunctionResidualBasedLearning/baselineMLP/best_weights\n",
      "The loss after 65 iterations is E=10556.577156572897\n",
      "Wrote weights to file ./trialFunctionResidualBasedLearning/baselineMLP/best_weights\n",
      "The loss after 66 iterations is E=10539.332327547068\n",
      "Wrote weights to file ./trialFunctionResidualBasedLearning/baselineMLP/best_weights\n",
      "The loss after 67 iterations is E=10521.993399417968\n",
      "Wrote weights to file ./trialFunctionResidualBasedLearning/baselineMLP/best_weights\n",
      "The loss after 68 iterations is E=10504.565043742594\n",
      "Wrote weights to file ./trialFunctionResidualBasedLearning/baselineMLP/best_weights\n",
      "The loss after 69 iterations is E=10487.052126472965\n",
      "Wrote weights to file ./trialFunctionResidualBasedLearning/baselineMLP/best_weights\n",
      "The loss after 70 iterations is E=10469.45969817361\n",
      "Wrote weights to file ./trialFunctionResidualBasedLearning/baselineMLP/best_weights\n",
      "The loss after 71 iterations is E=10451.792983122312\n",
      "Wrote weights to file ./trialFunctionResidualBasedLearning/baselineMLP/best_weights\n",
      "The loss after 72 iterations is E=10434.057367061463\n",
      "Wrote weights to file ./trialFunctionResidualBasedLearning/baselineMLP/best_weights\n",
      "The loss after 73 iterations is E=10416.258383560113\n",
      "Wrote weights to file ./trialFunctionResidualBasedLearning/baselineMLP/best_weights\n",
      "The loss after 74 iterations is E=10398.401699149073\n",
      "Wrote weights to file ./trialFunctionResidualBasedLearning/baselineMLP/best_weights\n",
      "The loss after 75 iterations is E=10380.493097555647\n",
      "Wrote weights to file ./trialFunctionResidualBasedLearning/baselineMLP/best_weights\n",
      "The loss after 76 iterations is E=10362.538463475836\n",
      "Wrote weights to file ./trialFunctionResidualBasedLearning/baselineMLP/best_weights\n",
      "The loss after 77 iterations is E=10344.543766348972\n",
      "Wrote weights to file ./trialFunctionResidualBasedLearning/baselineMLP/best_weights\n",
      "The loss after 78 iterations is E=10326.515044540007\n",
      "Wrote weights to file ./trialFunctionResidualBasedLearning/baselineMLP/best_weights\n",
      "The loss after 79 iterations is E=10308.45839021718\n",
      "Wrote weights to file ./trialFunctionResidualBasedLearning/baselineMLP/best_weights\n",
      "The loss after 80 iterations is E=10290.379935050603\n",
      "Wrote weights to file ./trialFunctionResidualBasedLearning/baselineMLP/best_weights\n",
      "The loss after 81 iterations is E=10272.285836700452\n",
      "Wrote weights to file ./trialFunctionResidualBasedLearning/baselineMLP/best_weights\n",
      "The loss after 82 iterations is E=10254.182265933236\n",
      "Wrote weights to file ./trialFunctionResidualBasedLearning/baselineMLP/best_weights\n",
      "The loss after 83 iterations is E=10236.07539412398\n",
      "Wrote weights to file ./trialFunctionResidualBasedLearning/baselineMLP/best_weights\n",
      "The loss after 84 iterations is E=10217.971380872646\n",
      "Wrote weights to file ./trialFunctionResidualBasedLearning/baselineMLP/best_weights\n",
      "The loss after 85 iterations is E=10199.876361482298\n",
      "Wrote weights to file ./trialFunctionResidualBasedLearning/baselineMLP/best_weights\n",
      "The loss after 86 iterations is E=10181.79643410245\n",
      "Wrote weights to file ./trialFunctionResidualBasedLearning/baselineMLP/best_weights\n",
      "The loss after 87 iterations is E=10163.737646415113\n",
      "Wrote weights to file ./trialFunctionResidualBasedLearning/baselineMLP/best_weights\n",
      "The loss after 88 iterations is E=10145.70598182493\n",
      "Wrote weights to file ./trialFunctionResidualBasedLearning/baselineMLP/best_weights\n",
      "The loss after 89 iterations is E=10127.707345191591\n",
      "Wrote weights to file ./trialFunctionResidualBasedLearning/baselineMLP/best_weights\n",
      "The loss after 90 iterations is E=10109.747548204883\n",
      "Wrote weights to file ./trialFunctionResidualBasedLearning/baselineMLP/best_weights\n",
      "The loss after 91 iterations is E=10091.832294539636\n",
      "Wrote weights to file ./trialFunctionResidualBasedLearning/baselineMLP/best_weights\n",
      "The loss after 92 iterations is E=10073.967164943722\n",
      "Wrote weights to file ./trialFunctionResidualBasedLearning/baselineMLP/best_weights\n",
      "The loss after 93 iterations is E=10056.157602402176\n",
      "Wrote weights to file ./trialFunctionResidualBasedLearning/baselineMLP/best_weights\n",
      "The loss after 94 iterations is E=10038.40889749581\n",
      "Wrote weights to file ./trialFunctionResidualBasedLearning/baselineMLP/best_weights\n",
      "The loss after 95 iterations is E=10020.726174034375\n",
      "Wrote weights to file ./trialFunctionResidualBasedLearning/baselineMLP/best_weights\n",
      "The loss after 96 iterations is E=10003.114375010871\n",
      "Wrote weights to file ./trialFunctionResidualBasedLearning/baselineMLP/best_weights\n",
      "The loss after 97 iterations is E=9985.578248887901\n",
      "Wrote weights to file ./trialFunctionResidualBasedLearning/baselineMLP/best_weights\n",
      "The loss after 98 iterations is E=9968.122336208813\n",
      "Wrote weights to file ./trialFunctionResidualBasedLearning/baselineMLP/best_weights\n",
      "The loss after 99 iterations is E=9950.750956514692\n",
      "Wrote weights to file ./trialFunctionResidualBasedLearning/baselineMLP/best_weights\n",
      "The loss after 100 iterations is E=9933.468195553281\n",
      "Time for 100 iterations: 104.50 min\n"
     ]
    }
   ],
   "source": [
    "baselineMLP = SimpleMLP(name='baselineMLP', number_of_inputs=2, neurons_per_layer=5, hidden_layers=1)\n",
    "residualBasedModel = TrialFunctionResidualBasedLearning(name=\"trialFunctionResidualBasedLearning\", mlp=baselineMLP, beta=25.0, d_v=0.025)\n",
    "residualBasedModel.place_control_points(method=['uniform', 'uniform'], domain=[(0.0, 1.0), (0.0, 1.0)], step_size=[0.021, 0.021], growths=[1.3, 1.3])\n",
    "start = time.time()\n",
    "residualBasedModel.solve(max_iter=100, learning_rate=0.01, verbose=1, adaptive=False, tolerance=1.0E-5)\n",
    "print(\"Time for 100 iterations: {:5.2f} min\".format((time.time() - start) / 60.0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAuoAAAD3CAYAAABLsHHKAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x7f6ea01f72b0>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "evaluation_points = np.column_stack((field50.x, field50.y))\n",
    "prediction = residualBasedModel.predict(evaluation_points)\n",
    "plot_field_and_error(field50.x, field50.y, prediction, field50.A, \"trialFunctionResidualBasedLearning_100.pdf\",\n",
    "                     [r\"Trial function after $100$ iterations\", \"Deviation from the numerical solution\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data based learning with the pure network\n",
    "### no penalty term for boundary points"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Initialized weights and biases for MLP with 20 parameters.\n",
      "\n",
      "MLP structure:\n",
      "--------------\n",
      "Input features:    2\n",
      "Hidden layers:     1\n",
      "Neurons per layer: 5\n",
      "Number of weights: 20\n",
      "\n",
      "The initial loss is 22081.244781940826\n",
      "\n",
      "Wrote weights to file ./pureNetworkLearningNoP/baselineMLP/best_weights\n",
      "The loss after 1 iterations is E=20852.894843201993\n",
      "Wrote weights to file ./pureNetworkLearningNoP/baselineMLP/best_weights\n",
      "The loss after 2 iterations is E=19658.802820433983\n",
      "Wrote weights to file ./pureNetworkLearningNoP/baselineMLP/best_weights\n",
      "The loss after 3 iterations is E=18499.813774432292\n",
      "Wrote weights to file ./pureNetworkLearningNoP/baselineMLP/best_weights\n",
      "The loss after 4 iterations is E=17376.722346582577\n",
      "Wrote weights to file ./pureNetworkLearningNoP/baselineMLP/best_weights\n",
      "The loss after 5 iterations is E=16290.267047105555\n",
      "Wrote weights to file ./pureNetworkLearningNoP/baselineMLP/best_weights\n",
      "The loss after 6 iterations is E=15241.12452608688\n",
      "Wrote weights to file ./pureNetworkLearningNoP/baselineMLP/best_weights\n",
      "The loss after 7 iterations is E=14229.903873509395\n",
      "Wrote weights to file ./pureNetworkLearningNoP/baselineMLP/best_weights\n",
      "The loss after 8 iterations is E=13257.140996535873\n",
      "Wrote weights to file ./pureNetworkLearningNoP/baselineMLP/best_weights\n",
      "The loss after 9 iterations is E=12323.293124223275\n",
      "Wrote weights to file ./pureNetworkLearningNoP/baselineMLP/best_weights\n",
      "The loss after 10 iterations is E=11428.733491787892\n",
      "Wrote weights to file ./pureNetworkLearningNoP/baselineMLP/best_weights\n",
      "The loss after 11 iterations is E=10573.746258583855\n",
      "Wrote weights to file ./pureNetworkLearningNoP/baselineMLP/best_weights\n",
      "The loss after 12 iterations is E=9758.521716205649\n",
      "Wrote weights to file ./pureNetworkLearningNoP/baselineMLP/best_weights\n",
      "The loss after 13 iterations is E=8983.151845653681\n",
      "Wrote weights to file ./pureNetworkLearningNoP/baselineMLP/best_weights\n",
      "The loss after 14 iterations is E=8247.626285351806\n",
      "Wrote weights to file ./pureNetworkLearningNoP/baselineMLP/best_weights\n",
      "The loss after 15 iterations is E=7551.828774959855\n",
      "Wrote weights to file ./pureNetworkLearningNoP/baselineMLP/best_weights\n",
      "The loss after 16 iterations is E=6895.534143289006\n",
      "Wrote weights to file ./pureNetworkLearningNoP/baselineMLP/best_weights\n",
      "The loss after 17 iterations is E=6278.405912012533\n",
      "Wrote weights to file ./pureNetworkLearningNoP/baselineMLP/best_weights\n",
      "The loss after 18 iterations is E=5699.994589978349\n",
      "Wrote weights to file ./pureNetworkLearningNoP/baselineMLP/best_weights\n",
      "The loss after 19 iterations is E=5159.7367353676755\n",
      "Wrote weights to file ./pureNetworkLearningNoP/baselineMLP/best_weights\n",
      "The loss after 20 iterations is E=4656.954864209136\n",
      "Wrote weights to file ./pureNetworkLearningNoP/baselineMLP/best_weights\n",
      "The loss after 21 iterations is E=4190.8582832705\n",
      "Wrote weights to file ./pureNetworkLearningNoP/baselineMLP/best_weights\n",
      "The loss after 22 iterations is E=3760.544922490761\n",
      "Wrote weights to file ./pureNetworkLearningNoP/baselineMLP/best_weights\n",
      "The loss after 23 iterations is E=3365.004236263258\n",
      "Wrote weights to file ./pureNetworkLearningNoP/baselineMLP/best_weights\n",
      "The loss after 24 iterations is E=3003.121233471722\n",
      "Wrote weights to file ./pureNetworkLearningNoP/baselineMLP/best_weights\n",
      "The loss after 25 iterations is E=2673.681682762788\n",
      "Wrote weights to file ./pureNetworkLearningNoP/baselineMLP/best_weights\n",
      "The loss after 26 iterations is E=2375.378521830492\n",
      "Wrote weights to file ./pureNetworkLearningNoP/baselineMLP/best_weights\n",
      "The loss after 27 iterations is E=2106.819477437726\n",
      "Wrote weights to file ./pureNetworkLearningNoP/baselineMLP/best_weights\n",
      "The loss after 28 iterations is E=1866.5358767236341\n",
      "Wrote weights to file ./pureNetworkLearningNoP/baselineMLP/best_weights\n",
      "The loss after 29 iterations is E=1652.9926005692146\n",
      "Wrote weights to file ./pureNetworkLearningNoP/baselineMLP/best_weights\n",
      "The loss after 30 iterations is E=1464.599097254091\n",
      "Wrote weights to file ./pureNetworkLearningNoP/baselineMLP/best_weights\n",
      "The loss after 31 iterations is E=1299.7213404836962\n",
      "Wrote weights to file ./pureNetworkLearningNoP/baselineMLP/best_weights\n",
      "The loss after 32 iterations is E=1156.6945815189838\n",
      "Wrote weights to file ./pureNetworkLearningNoP/baselineMLP/best_weights\n",
      "The loss after 33 iterations is E=1033.8367122311283\n",
      "Wrote weights to file ./pureNetworkLearningNoP/baselineMLP/best_weights\n",
      "The loss after 34 iterations is E=929.462026188775\n",
      "Wrote weights to file ./pureNetworkLearningNoP/baselineMLP/best_weights\n",
      "The loss after 35 iterations is E=841.8951401362416\n",
      "Wrote weights to file ./pureNetworkLearningNoP/baselineMLP/best_weights\n",
      "The loss after 36 iterations is E=769.4848201001352\n",
      "Wrote weights to file ./pureNetworkLearningNoP/baselineMLP/best_weights\n",
      "The loss after 37 iterations is E=710.6174462927025\n",
      "Wrote weights to file ./pureNetworkLearningNoP/baselineMLP/best_weights\n",
      "The loss after 38 iterations is E=663.7298500240964\n",
      "Wrote weights to file ./pureNetworkLearningNoP/baselineMLP/best_weights\n",
      "The loss after 39 iterations is E=627.3212645864631\n",
      "Wrote weights to file ./pureNetworkLearningNoP/baselineMLP/best_weights\n",
      "The loss after 40 iterations is E=599.9641505854647\n",
      "Wrote weights to file ./pureNetworkLearningNoP/baselineMLP/best_weights\n",
      "The loss after 41 iterations is E=580.3136839518786\n",
      "Wrote weights to file ./pureNetworkLearningNoP/baselineMLP/best_weights\n",
      "The loss after 42 iterations is E=567.1157307947324\n",
      "Wrote weights to file ./pureNetworkLearningNoP/baselineMLP/best_weights\n",
      "The loss after 43 iterations is E=559.2131757908239\n",
      "Wrote weights to file ./pureNetworkLearningNoP/baselineMLP/best_weights\n",
      "The loss after 44 iterations is E=555.5505179900864\n",
      "Wrote weights to file ./pureNetworkLearningNoP/baselineMLP/best_weights\n",
      "The loss after 45 iterations is E=555.1766975477229\n",
      "Wrote weights to file ./pureNetworkLearningNoP/baselineMLP/best_weights\n",
      "The loss after 69 iterations is E=550.429157899282\n",
      "Wrote weights to file ./pureNetworkLearningNoP/baselineMLP/best_weights\n",
      "The loss after 70 iterations is E=545.7467160451258\n",
      "Wrote weights to file ./pureNetworkLearningNoP/baselineMLP/best_weights\n",
      "The loss after 71 iterations is E=541.2793559022576\n",
      "Wrote weights to file ./pureNetworkLearningNoP/baselineMLP/best_weights\n",
      "The loss after 72 iterations is E=537.0611315285721\n",
      "Wrote weights to file ./pureNetworkLearningNoP/baselineMLP/best_weights\n",
      "The loss after 73 iterations is E=533.1152669126632\n",
      "Wrote weights to file ./pureNetworkLearningNoP/baselineMLP/best_weights\n",
      "The loss after 74 iterations is E=529.4550478755589\n",
      "Wrote weights to file ./pureNetworkLearningNoP/baselineMLP/best_weights\n",
      "The loss after 75 iterations is E=526.0848735754569\n",
      "Wrote weights to file ./pureNetworkLearningNoP/baselineMLP/best_weights\n",
      "The loss after 76 iterations is E=523.0014120376931\n",
      "Wrote weights to file ./pureNetworkLearningNoP/baselineMLP/best_weights\n",
      "The loss after 77 iterations is E=520.1948096927674\n",
      "Wrote weights to file ./pureNetworkLearningNoP/baselineMLP/best_weights\n",
      "The loss after 78 iterations is E=517.6499110048823\n",
      "Wrote weights to file ./pureNetworkLearningNoP/baselineMLP/best_weights\n",
      "The loss after 79 iterations is E=515.3474506763123\n",
      "Wrote weights to file ./pureNetworkLearningNoP/baselineMLP/best_weights\n",
      "The loss after 80 iterations is E=513.2651874070337\n",
      "Wrote weights to file ./pureNetworkLearningNoP/baselineMLP/best_weights\n",
      "The loss after 81 iterations is E=511.3789545874995\n",
      "Wrote weights to file ./pureNetworkLearningNoP/baselineMLP/best_weights\n",
      "The loss after 82 iterations is E=509.66360944639916\n",
      "Wrote weights to file ./pureNetworkLearningNoP/baselineMLP/best_weights\n",
      "The loss after 83 iterations is E=508.0938679331243\n",
      "Wrote weights to file ./pureNetworkLearningNoP/baselineMLP/best_weights\n",
      "The loss after 84 iterations is E=506.64501788348974\n",
      "Wrote weights to file ./pureNetworkLearningNoP/baselineMLP/best_weights\n",
      "The loss after 85 iterations is E=505.2935077208203\n",
      "Wrote weights to file ./pureNetworkLearningNoP/baselineMLP/best_weights\n",
      "The loss after 86 iterations is E=504.0174120322389\n",
      "Wrote weights to file ./pureNetworkLearningNoP/baselineMLP/best_weights\n",
      "The loss after 87 iterations is E=502.7967788054125\n",
      "Wrote weights to file ./pureNetworkLearningNoP/baselineMLP/best_weights\n",
      "The loss after 88 iterations is E=501.6138659083417\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wrote weights to file ./pureNetworkLearningNoP/baselineMLP/best_weights\n",
      "The loss after 89 iterations is E=500.45327655779687\n",
      "Wrote weights to file ./pureNetworkLearningNoP/baselineMLP/best_weights\n",
      "The loss after 90 iterations is E=499.30200507944494\n",
      "Wrote weights to file ./pureNetworkLearningNoP/baselineMLP/best_weights\n",
      "The loss after 91 iterations is E=498.1494052569971\n",
      "Wrote weights to file ./pureNetworkLearningNoP/baselineMLP/best_weights\n",
      "The loss after 92 iterations is E=496.9870940508173\n",
      "Wrote weights to file ./pureNetworkLearningNoP/baselineMLP/best_weights\n",
      "The loss after 93 iterations is E=495.8088034978783\n",
      "Wrote weights to file ./pureNetworkLearningNoP/baselineMLP/best_weights\n",
      "The loss after 94 iterations is E=494.61019324858535\n",
      "Wrote weights to file ./pureNetworkLearningNoP/baselineMLP/best_weights\n",
      "The loss after 95 iterations is E=493.38863551815507\n",
      "Wrote weights to file ./pureNetworkLearningNoP/baselineMLP/best_weights\n",
      "The loss after 96 iterations is E=492.1429832969289\n",
      "Wrote weights to file ./pureNetworkLearningNoP/baselineMLP/best_weights\n",
      "The loss after 97 iterations is E=490.8733315396949\n",
      "Wrote weights to file ./pureNetworkLearningNoP/baselineMLP/best_weights\n",
      "The loss after 98 iterations is E=489.5807797997409\n",
      "Wrote weights to file ./pureNetworkLearningNoP/baselineMLP/best_weights\n",
      "The loss after 99 iterations is E=488.2672034449125\n",
      "Wrote weights to file ./pureNetworkLearningNoP/baselineMLP/best_weights\n",
      "The loss after 100 iterations is E=486.9350392414539\n",
      "Time for 100 iterations:  2.98 min\n"
     ]
    }
   ],
   "source": [
    "baselineMLP = SimpleMLP(name='baselineMLP', number_of_inputs=2, neurons_per_layer=5, hidden_layers=1)\n",
    "pureNetworkModel = PureNetworkLearning(name=\"pureNetworkLearningNoP\", mlp=baselineMLP, training_data=field50.A, beta=25.0, d_v=0.025, penalty=1.0)\n",
    "training_points = np.column_stack((field50.x, field50.y))\n",
    "pureNetworkModel.set_control_points(training_points)\n",
    "start = time.time()\n",
    "pureNetworkModel.solve(max_iter=100, learning_rate=0.01, verbose=1, adaptive=False, tolerance=1.0E-5)\n",
    "print(\"Time for 100 iterations: {:5.2f} min\".format((time.time() - start) / 60.0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAuoAAAD3CAYAAABLsHHKAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x7f6efd2b30f0>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "evaluation_points = np.column_stack((field50.x, field50.y))\n",
    "prediction = pureNetworkModel.predict(evaluation_points)\n",
    "plot_field_and_error(field50.x, field50.y, prediction, field50.A, \"pureNetwork_100.pdf\",\n",
    "                     [r\"Network output after $100$ iterations\", \"Deviation from the numerical solution\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### penalty factor 2 for points on the boundaries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Initialized weights and biases for MLP with 20 parameters.\n",
      "\n",
      "MLP structure:\n",
      "--------------\n",
      "Input features:    2\n",
      "Hidden layers:     1\n",
      "Neurons per layer: 5\n",
      "Number of weights: 20\n",
      "\n",
      "The initial loss is 27566.74086563324\n",
      "\n",
      "Wrote weights to file ./pureNetworkLearning/baselineMLP/best_weights\n",
      "The loss after 1 iterations is E=26054.08858856372\n",
      "Wrote weights to file ./pureNetworkLearning/baselineMLP/best_weights\n",
      "The loss after 2 iterations is E=24583.26152251178\n",
      "Wrote weights to file ./pureNetworkLearning/baselineMLP/best_weights\n",
      "The loss after 3 iterations is E=23155.28905810074\n",
      "Wrote weights to file ./pureNetworkLearning/baselineMLP/best_weights\n",
      "The loss after 4 iterations is E=21771.139639706245\n",
      "Wrote weights to file ./pureNetworkLearning/baselineMLP/best_weights\n",
      "The loss after 5 iterations is E=20431.713897546815\n",
      "Wrote weights to file ./pureNetworkLearning/baselineMLP/best_weights\n",
      "The loss after 6 iterations is E=19137.837763666295\n",
      "Wrote weights to file ./pureNetworkLearning/baselineMLP/best_weights\n",
      "The loss after 7 iterations is E=17890.255627195907\n",
      "Wrote weights to file ./pureNetworkLearning/baselineMLP/best_weights\n",
      "The loss after 8 iterations is E=16689.623586524944\n",
      "Wrote weights to file ./pureNetworkLearning/baselineMLP/best_weights\n",
      "The loss after 9 iterations is E=15536.502858112679\n",
      "Wrote weights to file ./pureNetworkLearning/baselineMLP/best_weights\n",
      "The loss after 10 iterations is E=14431.353403764519\n",
      "Wrote weights to file ./pureNetworkLearning/baselineMLP/best_weights\n",
      "The loss after 11 iterations is E=13374.52784039098\n",
      "Wrote weights to file ./pureNetworkLearning/baselineMLP/best_weights\n",
      "The loss after 12 iterations is E=12366.2656986951\n",
      "Wrote weights to file ./pureNetworkLearning/baselineMLP/best_weights\n",
      "The loss after 13 iterations is E=11406.688099986854\n",
      "Wrote weights to file ./pureNetworkLearning/baselineMLP/best_weights\n",
      "The loss after 14 iterations is E=10495.792923460524\n",
      "Wrote weights to file ./pureNetworkLearning/baselineMLP/best_weights\n",
      "The loss after 15 iterations is E=9633.450539779882\n",
      "Wrote weights to file ./pureNetworkLearning/baselineMLP/best_weights\n",
      "The loss after 16 iterations is E=8819.400190600341\n",
      "Wrote weights to file ./pureNetworkLearning/baselineMLP/best_weights\n",
      "The loss after 17 iterations is E=8053.247097503056\n",
      "Wrote weights to file ./pureNetworkLearning/baselineMLP/best_weights\n",
      "The loss after 18 iterations is E=7334.460387397675\n",
      "Wrote weights to file ./pureNetworkLearning/baselineMLP/best_weights\n",
      "The loss after 19 iterations is E=6662.371924305918\n",
      "Wrote weights to file ./pureNetworkLearning/baselineMLP/best_weights\n",
      "The loss after 20 iterations is E=6036.176138998011\n",
      "Wrote weights to file ./pureNetworkLearning/baselineMLP/best_weights\n",
      "The loss after 21 iterations is E=5454.930947552864\n",
      "Wrote weights to file ./pureNetworkLearning/baselineMLP/best_weights\n",
      "The loss after 22 iterations is E=4917.559846834132\n",
      "Wrote weights to file ./pureNetworkLearning/baselineMLP/best_weights\n",
      "The loss after 23 iterations is E=4422.855268398301\n",
      "Wrote weights to file ./pureNetworkLearning/baselineMLP/best_weights\n",
      "The loss after 24 iterations is E=3969.483261815594\n",
      "Wrote weights to file ./pureNetworkLearning/baselineMLP/best_weights\n",
      "The loss after 25 iterations is E=3555.9895632559906\n",
      "Wrote weights to file ./pureNetworkLearning/baselineMLP/best_weights\n",
      "The loss after 26 iterations is E=3180.8070851221364\n",
      "Wrote weights to file ./pureNetworkLearning/baselineMLP/best_weights\n",
      "The loss after 27 iterations is E=2842.264837412126\n",
      "Wrote weights to file ./pureNetworkLearning/baselineMLP/best_weights\n",
      "The loss after 28 iterations is E=2538.5982615806847\n",
      "Wrote weights to file ./pureNetworkLearning/baselineMLP/best_weights\n",
      "The loss after 29 iterations is E=2267.9609234996437\n",
      "Wrote weights to file ./pureNetworkLearning/baselineMLP/best_weights\n",
      "The loss after 30 iterations is E=2028.4374746180777\n",
      "Wrote weights to file ./pureNetworkLearning/baselineMLP/best_weights\n",
      "The loss after 31 iterations is E=1818.0577508638287\n",
      "Wrote weights to file ./pureNetworkLearning/baselineMLP/best_weights\n",
      "The loss after 32 iterations is E=1634.8118388032667\n",
      "Wrote weights to file ./pureNetworkLearning/baselineMLP/best_weights\n",
      "The loss after 33 iterations is E=1476.6658999286913\n",
      "Wrote weights to file ./pureNetworkLearning/baselineMLP/best_weights\n",
      "The loss after 34 iterations is E=1341.578508673477\n",
      "Wrote weights to file ./pureNetworkLearning/baselineMLP/best_weights\n",
      "The loss after 35 iterations is E=1227.5172299016367\n",
      "Wrote weights to file ./pureNetworkLearning/baselineMLP/best_weights\n",
      "The loss after 36 iterations is E=1132.4751391180487\n",
      "Wrote weights to file ./pureNetworkLearning/baselineMLP/best_weights\n",
      "The loss after 37 iterations is E=1054.4869751863398\n",
      "Wrote weights to file ./pureNetworkLearning/baselineMLP/best_weights\n",
      "The loss after 38 iterations is E=991.6446122230886\n",
      "Wrote weights to file ./pureNetworkLearning/baselineMLP/best_weights\n",
      "The loss after 39 iterations is E=942.1115453461064\n",
      "Wrote weights to file ./pureNetworkLearning/baselineMLP/best_weights\n",
      "The loss after 40 iterations is E=904.1361042748971\n",
      "Wrote weights to file ./pureNetworkLearning/baselineMLP/best_weights\n",
      "The loss after 41 iterations is E=876.0631389496399\n",
      "Wrote weights to file ./pureNetworkLearning/baselineMLP/best_weights\n",
      "The loss after 42 iterations is E=856.343961245448\n",
      "Wrote weights to file ./pureNetworkLearning/baselineMLP/best_weights\n",
      "The loss after 43 iterations is E=843.5443748252476\n",
      "Wrote weights to file ./pureNetworkLearning/baselineMLP/best_weights\n",
      "The loss after 44 iterations is E=836.3506790406728\n",
      "Wrote weights to file ./pureNetworkLearning/baselineMLP/best_weights\n",
      "The loss after 45 iterations is E=833.5735900776407\n",
      "Wrote weights to file ./pureNetworkLearning/baselineMLP/best_weights\n",
      "The loss after 67 iterations is E=829.1445561423831\n",
      "Wrote weights to file ./pureNetworkLearning/baselineMLP/best_weights\n",
      "The loss after 68 iterations is E=822.5938313076886\n",
      "Wrote weights to file ./pureNetworkLearning/baselineMLP/best_weights\n",
      "The loss after 69 iterations is E=816.1314059206309\n",
      "Wrote weights to file ./pureNetworkLearning/baselineMLP/best_weights\n",
      "The loss after 70 iterations is E=809.832159520032\n",
      "Wrote weights to file ./pureNetworkLearning/baselineMLP/best_weights\n",
      "The loss after 71 iterations is E=803.7570079609735\n",
      "Wrote weights to file ./pureNetworkLearning/baselineMLP/best_weights\n",
      "The loss after 72 iterations is E=797.9531551801053\n",
      "Wrote weights to file ./pureNetworkLearning/baselineMLP/best_weights\n",
      "The loss after 73 iterations is E=792.4547132518355\n",
      "Wrote weights to file ./pureNetworkLearning/baselineMLP/best_weights\n",
      "The loss after 74 iterations is E=787.2836148651344\n",
      "Wrote weights to file ./pureNetworkLearning/baselineMLP/best_weights\n",
      "The loss after 75 iterations is E=782.4507459975222\n",
      "Wrote weights to file ./pureNetworkLearning/baselineMLP/best_weights\n",
      "The loss after 76 iterations is E=777.9572315685843\n",
      "Wrote weights to file ./pureNetworkLearning/baselineMLP/best_weights\n",
      "The loss after 77 iterations is E=773.7958129055208\n",
      "Wrote weights to file ./pureNetworkLearning/baselineMLP/best_weights\n",
      "The loss after 78 iterations is E=769.9522626475575\n",
      "Wrote weights to file ./pureNetworkLearning/baselineMLP/best_weights\n",
      "The loss after 79 iterations is E=766.4067899745161\n",
      "Wrote weights to file ./pureNetworkLearning/baselineMLP/best_weights\n",
      "The loss after 80 iterations is E=763.1353965129596\n",
      "Wrote weights to file ./pureNetworkLearning/baselineMLP/best_weights\n",
      "The loss after 81 iterations is E=760.1111507243448\n",
      "Wrote weights to file ./pureNetworkLearning/baselineMLP/best_weights\n",
      "The loss after 82 iterations is E=757.305355817871\n",
      "Wrote weights to file ./pureNetworkLearning/baselineMLP/best_weights\n",
      "The loss after 83 iterations is E=754.6885930919965\n",
      "Wrote weights to file ./pureNetworkLearning/baselineMLP/best_weights\n",
      "The loss after 84 iterations is E=752.2316289609564\n",
      "Wrote weights to file ./pureNetworkLearning/baselineMLP/best_weights\n",
      "The loss after 85 iterations is E=749.9061796652144\n",
      "Wrote weights to file ./pureNetworkLearning/baselineMLP/best_weights\n",
      "The loss after 86 iterations is E=747.6855327270092\n",
      "Wrote weights to file ./pureNetworkLearning/baselineMLP/best_weights\n",
      "The loss after 87 iterations is E=745.5450285512954\n",
      "Wrote weights to file ./pureNetworkLearning/baselineMLP/best_weights\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The loss after 88 iterations is E=743.4624091711554\n",
      "Wrote weights to file ./pureNetworkLearning/baselineMLP/best_weights\n",
      "The loss after 89 iterations is E=741.4180440006422\n",
      "Wrote weights to file ./pureNetworkLearning/baselineMLP/best_weights\n",
      "The loss after 90 iterations is E=739.3950446118212\n",
      "Wrote weights to file ./pureNetworkLearning/baselineMLP/best_weights\n",
      "The loss after 91 iterations is E=737.3792820375695\n",
      "Wrote weights to file ./pureNetworkLearning/baselineMLP/best_weights\n",
      "The loss after 92 iterations is E=735.3593209715484\n",
      "Wrote weights to file ./pureNetworkLearning/baselineMLP/best_weights\n",
      "The loss after 93 iterations is E=733.326285555149\n",
      "Wrote weights to file ./pureNetworkLearning/baselineMLP/best_weights\n",
      "The loss after 94 iterations is E=731.273671278628\n",
      "Wrote weights to file ./pureNetworkLearning/baselineMLP/best_weights\n",
      "The loss after 95 iterations is E=729.1971169539808\n",
      "Wrote weights to file ./pureNetworkLearning/baselineMLP/best_weights\n",
      "The loss after 96 iterations is E=727.0941498156146\n",
      "Wrote weights to file ./pureNetworkLearning/baselineMLP/best_weights\n",
      "The loss after 97 iterations is E=724.9639156458691\n",
      "Wrote weights to file ./pureNetworkLearning/baselineMLP/best_weights\n",
      "The loss after 98 iterations is E=722.8069044769674\n",
      "Wrote weights to file ./pureNetworkLearning/baselineMLP/best_weights\n",
      "The loss after 99 iterations is E=720.6246809556362\n",
      "Wrote weights to file ./pureNetworkLearning/baselineMLP/best_weights\n",
      "The loss after 100 iterations is E=718.419626932405\n",
      "Time for 100 iterations:  2.90 min\n"
     ]
    }
   ],
   "source": [
    "baselineMLP = SimpleMLP(name='baselineMLP', number_of_inputs=2, neurons_per_layer=5, hidden_layers=1)\n",
    "pureNetworkModelPenalty = PureNetworkLearning(name=\"pureNetworkLearning\", mlp=baselineMLP, training_data=field50.A, beta=25.0, d_v=0.025, penalty=2.0)\n",
    "training_points = np.column_stack((field50.x, field50.y))\n",
    "pureNetworkModelPenalty.set_control_points(training_points)\n",
    "start = time.time()\n",
    "pureNetworkModelPenalty.solve(max_iter=100, learning_rate=0.01, verbose=1, adaptive=False, tolerance=1.0E-5)\n",
    "print(\"Time for 100 iterations: {:5.2f} min\".format((time.time() - start) / 60.0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAuoAAAD3CAYAAABLsHHKAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz\nAAALEgAACxIB0t1+/AAAADl0RVh0U29mdHdhcmUAbWF0cGxvdGxpYiB2ZXJzaW9uIDIuMS4wLCBo\ndHRwOi8vbWF0cGxvdGxpYi5vcmcvpW3flQAAIABJREFUeJzsvXlwJFd+3/l9mVk3jsLRDfYNVDeb\nfbB5NNk8ZqTRQXAkS7uyve5ZSmNJu9Z6OOtVWEeElvRotbY0Nj0mHdZhW5bIsSPGMfaOOGx7VoqQ\nVhKh0M6sZjgkuyHz1AzVKBRuoAFUFYC6qzLf/pH1sl5mZVZlVWWhCuj3iUAAqOPly6zKX37z934H\noZRCIBAIBAKBQCAQ9BdSrycgEAgEAoFAIBAI6hFCXSAQCAQCgUAg6EOEUBcIBAKBQCAQCPoQIdQF\nAoFAIBAIBII+RAh1gUAgEAgEAoGgDxFC/S6DEDJNCHmOEBLr9VwEAoFAcHdDCLlavSZd7fVc+pFu\nHB9xzA8WQqh7ACEkRgh5kRBCCSHXLY+/TAi5RQiZdjNOd2cKUEpnAFwDsG8nqNf71el41c/qWULI\nc16M12Rbr9s8FqsaSXbTFHXznM04dd+rLu9LzPK/q++1QNBPWOz1c8wWVB+73nyEpuO3dF7s93ll\ntX+9gN9nSuksgLMAHu3VfLpJp59nN45Pu2OKa0BvUHo9gcMApTROCPkCgDkAXySEzFJK45TSOIDP\nEkKuVwWyI9U72xiA+D5MeT+2AcD7/ep0PELIswC2AXwVwKPdOu5V4xUDYGfEXqOUPlJ93U0AXwTw\nKRfPWflU9TvGttm175DD2KbtCwQHAc5eP0cpfYl/ripiY9bHW8T1ebHf55XV/nVjGy7mYLfP6V7M\nZZ/w4vOc82QmHYwprgG9Q3jUveUmgC8AeK2VN1W9pp/ryox6iNf75dF4UQBpSmka+ufVleNOKZ2h\nlL5ifbxq7JLc69KoivlGzzlsgxfpXfsOOY0tDLTgsEEpfR7A5zpZmWpBpPfivDLsXzPnUTc4rNe6\nRhwGOymuAb1FCHWPYZ4Yp2XFakjDs4SQ64SQF6sPPwrdgD5dfS5WfT5V9fBcrS4xvU4IiXLPGQKv\n+j42dozb1hy3vbobCDehOW7Gr/7P5sXCauz2i73nxerrr1f/jroYs268BseYzfU57vGrAJ5m73ca\nz+4zcnMsXRJDvfcoyXkrnJ6z7iP7TjxbfajjfXE6bnZj22zf7fdwurrNl7n3RS3P9WxJXiAAMAPg\nOuB4/jCbxM6bKGfT7M6LnpxXVqz2r4EdcLu9aaJfP2LEfv/saGTDo077Yfc52Oyf4/EgTa5Vre6b\nW7va4PNk4Y3Xifna5/Y4Wvfd1oY6fZaNjp3d8cE+f1cFFiil4seDH+hf4qvVv2MAUgBi1f+vc4+/\nzr3nWejLrwDwMnsd9/yLAJ5lY0APi2DPTduNWX3sFvf3ywBerv59lRv3enXO15vsV7PxjTlW/3+d\nbcfNfjnsm+OYduPZzHkOQJR7/TT33HOWsU3jufiMTMfSxfeCWv5/lt9Xbr5XGz3nMLb1OHW0L02O\nW7Pvp5vvycv8c6idHy9avjMNP1/xI346/anaPurw3MvQV0UbnT/PAniRe44/76znZU/OK4d9s7N/\nhh1oc3v8vqdcHHunfXayD46fg83YzexMo2uVq31r8r1wut6yzzNqOZ7Pce91fd1y2G+TDXXxWVq/\nCx1dy73+roqf2o+IUe8CVI+BfB76F/0s99R1AGlS814n695shl0wWAiFcXdKa8uW1wHMWt4X516T\nRjUWjeoJJIwYgD8D8FSTOTQbv134EI8bpH0PtR2PAJgmhADAKKrHzSWNPiOnY9kKSejGmmfUxXN2\nbDfZVqv70upx47fv5ntyi3uO39eXAbxOCIlDN/SdxAcLBJ0yCuBtNDh/KKWvEELmADxf9Rry333r\nedmr88oNJjtQ9cS2uj2vwh+c9qPV62Ynx8PNvrVqV/nP83+EHnKJ6mt4W9fJdavOhrr8LDtlP7+r\ndy1CqHeJqiH/VHVZ7G3uqWSzk4QQcpVSOlsV/CzTOg3gVeKuKkEU5hPAzrClATwP/UbgaRdjNhrf\nFWy/Wn1fK+NVlxH/DNUkF0LItVbGq/7Z6DNqdpFoRhw24rt6kYTTc61upNV9aeW4tfA5uv2eJCml\nZ6tz/iwh5DVKqVMCrUDQbaah5xpNo/H5M1Nd9k9SSm/YvaDH55Vbmtk0r7dn0MI+N71u7jOeXiM6\nuW5x8zHZUJh1B8OTz7KH39W7EhGj7h2jqBdZn4K+LMa4AUumPXdXnubez4/zKvRwiJnq+z8Hcxzz\nDdSXWhyFHmfZiHh1zDgfY2ZDs/G3YT4RrZUEnPbL+Lt688EnXjYa02k8Bru4Mk+IEf9n81q78Rp9\nRh1jc2MRQ/VYNnrOJZ3sS7Pj1uy4t/s9BKrJe9Wb08+6eL1A0BWqXshXqudis/PnRejOjkYVS3p5\nXrXDfmyv2T7bzckLm9zsWuWWTubzVVgKBFTf2+p1y4qdDW31s2z3Ws7Y7+/q3UOvY28Oww/0JarX\nYInpqj43DXOc2zT02DDr4zFUY8DAxW1BP3H4WMjXbLbPxrxe/c1i8q5CX2p6DbV4Of6xGPQbCQqH\nmL9G43Pze5ntT3Xc11CLtavbL1Rj1aqPXef3r9mYTsfJ8t7Xqs8zA8jGYvv+Oswx/tb51X1Gdsey\nyXfianUMWt3faZvn2Haibp6zGZ/tS6zTfWl03OzGdth+s+/h66iVrExVx4tWX/ss932YbnZ8xY/4\nafen+h18sXpusu/ec7DE6FZfa2uvuedftvxv/a737LyymavJ/tnZgU62BxfXEjf7bLcfzT4Hl/Nr\ndF1pad/s5mN3PB0+z6uoXhNQyxVze92yjemGgw11+1lWH2vpWu4whiffVfFj/iHVgygQ7BssHIg6\nLBcLBAKBQCAQCEToi0AgEAgEAoFA0JcIoS7YV6rJLtPQE1661u5eIBAIBAKB4KDTauiLiJMRCASC\n3kI8GEPYcoFAIOgtrmy58KgLBAKBQCAQCAR9iBDqAoFAIBAIBAJBHyKEukAgEAgEAoFA0IcIoS4Q\nCAQCgUAgEPQhQqgLBAKBQCAQCAR9iBDqAoFAIBAIBAJBHyKEukAgEAgEAoFA0IcIoS4QCAQCgUAg\nEPQhQqgLBAKBQCAQCAR9iBDqAoFAIBAIBAJBHyKEukAgEAgEAoFA0IcIoS4QCAQCgUAgEPQhSq8n\nILi7UFUVqqpClmVIkgRCSK+nJBAIBIIWoZSiUqkAACRJEvZcIOgSQqgLug6lFJqmoVQqQVVVlEol\nSJK+mLO5uYl77rkHsiwbP8zgC6MvEAgE/QWlFKVSCZqmoVgsglIKQghyuRwopYhGoyZbLgS8QNAZ\nQqgLugal1PCg7+zsYGFhARcuXEChUEAkEgEhBIuLizh69ChUVa17vyRJdQJeGH2BQCDYf5gHvVKp\n4L333sPU1BT8fj8IIZAkCdlsFoVCAQMDAyiXy4aAB2C8RjhkBILWEUJd4DlMoFcqFcNYa5qGvb09\nvPXWWwgEAiiVSqCUolAoIB6PIxKJIBwOIxwOw+fzgVJqXBjK5bJpbN7gs7+F0RcIBAJv4e0wc6Yw\n0W2FPWZ9jlIKANA0TThkBII2EEJd4Bm8xwXQDXqxWEQikcDW1hYkScK1a9egqioURQGlFG+99RZG\nRkaQy+Wwvr6ObDaLSqUCRVEM4R4OhxGJRBAMBo1tsRsBK8LoCwQCQWfwq6GapgGAyREiSZLhhGFC\nnP+bh/eq221HOGQEgsYIoS7oCCePy+7uLubn51EsFnHmzBmcPn0aH330ERRFqfPMjI2NYWxszDRu\npVJBLpdDNpvFzs4O1tbWUCgUAADBYNDkgQ+Hw4bwp5SiXC6jVCqZjDkhRBh9gUAgaIDdaqidjWSr\npNbH7IR6I5zsLxtHOGQEAiHUBW1i53EB9OTQhYUF+Hw+TE5OYmRkBABQKBSgaZprQ6ooCoaGhjA0\nNGR6XNM0FAoF5HI55HI5pFIp5HI5qKoKn89neN+ZgA8EAsZ77Yz++vo6Tpw4USfghdEXCAR3C7yz\nhQl0u/AWBi/Km3nU28GNF97qkGHXgZGREeGQERwqhFAXtIRT/Pnq6iqWlpYwMjKC+++/H+Fw2PQ+\nr4y4JEmGCLdSKpUMAZ9MJrG0tIRisQhCSF0YTSgUgizLWFlZwbFjx0wXKH5bkiRBURTjb2H0BQLB\nYYHFjfPhio0EOoPZ80KhAFVVjeIAXgn1Ztu2s7+5XA75fB5DQ0O2Xni7FVXhkBEcBIRQF7jCzuNS\nLpexuLiIjY0NHDt2DNeuXYPf77d9fyMjbhXI7eL3++H3+xGNRk2Pa5qGfD6PbDaLXC6H7e1t5HI5\n4/G5uTmTiPf5fMZ7nZZe7ZZdhdEXCAT9Du+Rtos/d0O5XMbt27ehaRp8Ph+KxaKppro1LHG/cLrR\nYCWChUNGcBARQl3QEDuPSy6XQyKRwO7uLk6fPo2PfexjTb0wkiTVxTSy8bwS6o22HYlEEIlETI+z\nZNbx8XHkcjkjbIfVeefFezgcRigUMpKoGhl9JwEvjL5AIOgV1tVQoDWBTinF5uYmEokEisUiTpw4\ngRMnTkDTNMiyjFQqhbW1NQwNDSGbzdaFJfIhiZFIxCjtuB+0GwsvHDKCfkAIdUEdTIgmk0nIsoxQ\nKAQASKVSSCQSoJRicnISly9fdm2wGlUE2I/lUjuY92VkZMSIpWeoqmqE0WQyGWxsbCCfz4NSimAw\nWCfiWUlJAHUVDPhtCaMvEAj2E7Yaura2hiNHjjgmiDqhqqoR2hiNRnH58mWsra3VOT6YfRsfH8f4\n+Lhp++Vy2SgOsL29bYQlNnKI7AfNYuGFQ0bQDwihLjCwelzu3LmDUCiEdDqNxcVFDAwM4Pz58xgc\nHGx57H4U6o2QZRmDg4N1+0opRbFYNMJo1tfXkcvlUC6XIctyXTJrKBQy9tHO6OfzeWiaZurmx0S9\nMPoCgaBdrKuhc3NzmJiYcP3+UqmEpaUlrK2t4Z577sGjjz5qhDbarYQ2svFOYYmqqhphiU4OEWuP\njf2imRfeziGztbWFI0eOwOfzCYeMwDOEUBfYxp9XKhWk02ksLy/j+PHjePjhh011zFvloAl1Jwgh\nCAaDCAaDjiUlc7mcbUlJ60VHURRks1nk83nRzU8gEHSMF/HnLLQxnU7j1KlTePLJJyHLsuk1dqGM\n7dhyWZYxMDCAgYGBuv3gq3vZ9djgbWkn16ZWaeSFX1pawtjYmNHQz8kLLxwyglYQQv0uxi7+vFAo\nYGFhAclkEuFwGLFYDKdPn+54W07G7aAJ9UY4lZSklCKfz9uWlKSUGoabXXz4kpKim59AIGhGswZF\nbkin05ifn0e5XMbk5CQuXrzo+H47u+11ecZQKIRQKFTnEGFhNMyWrqysoFAoGCJe07S66l77BbPn\n1uNm9cILh4ygFYRQv8tw8rjs7OwYSUJnzpzBhQsXsLy83HURfZiEuhN8eUgry8vLyGazRjIWu+iw\nC5W1sZMsy666+VljKIXRFwgOH24bFDV6/507d5BIJBAIBDA1NVUXnmJHt4V6I3w+H4aHhzE8PGx6\nfG1tDZlMBgMDA0YsPKvuFQgE6kr0+ny+rthEuzGdHFXseAmHjKARQqjfJTRqUJRIJOD3++uMNCHE\n1nh4MRd+G4ddqDeCedKPHz9uepyVjuQTsNhFx+/31y398mUxRTc/geBww27UW61/zlBVFSsrK1he\nXkY0GsWVK1dsHQlO9FKoOyFJEvx+P44cOYIjR44Yj1NKjR4b2WzWuObxeUW8PQ0Gg32TzCocMgJA\nCPVDj5sGRU5GmpUi7Ca9Nu69xqk0JV9S0nrRKZfLRjLr1tYWcrlcXQUFfumXfY7svdYEKFVVUS6X\nMTw8LLr5CQR9Ci/cmAOl1XO0WCxiaWkJGxsbdQmirSBJkjGHbnQm9RJCCAKBAAKBQF11r0qlYiSz\n7u7uGnlFlFKEQiHbvKL9nHc7JSWz2SwGBwfh9/uFQ+aQIIT6IcUu/rxcLmNhYQF37tzB8ePH8dhj\njzXMoneqfe4l/Wrc94tWa8jzFRTsSkqyiw7zHLGqMoFAoC6Mxu/3g1KKvb09rK+v2yZkiW5+AkFv\n8SL+PJvNolAo4NatWzh9+jSeeOKJjmK3W6n60s8oiuJY3cspr8jn85kEfC9qwvO/rfNeWFjA1NSU\nbbKvnT0XDpn+Rwj1Q4STxyWTySCRSCCTybhuUAQIob4fsGVML2hUQaFYLBoXnY2NDVNJSUVRUKlU\nkEqlTEu/opufQNA7Oo0/B2q9LyqVCvx+P65du+ZJicN+DH3xkkZ5RSyMhnW55mvC5/N5JBIJk0Nk\nv8Jo2LxZt1j+RsxtYyfhkOlPhFA/BDCDnkqloGmaUXUkmUwikUgAACYnJzE2NtbSiddNoc5feA6L\ncW+HbndlBcwlJUdHR03PsUYoyWQSu7u7WF9fRz6fBwDbxk6KoohufgJBF2HOlsXFRRw/frzl+HNK\nKTY2NrCwsIBAIIBYLIbh4WHcvHnTM1trFxZ5t9jyRjXh3377bYTD4boVTacmed1A07S670snjZ2E\nQ6b3CKF+gLF6XPb29oxmPKxB0X333ddWgyKgO0L9MJdnbIf9EOqNUBQFwWAQQ0NDmJqaMs2L1THO\nZrNYXV1FLpczSqBZw2iCwWDDxk6A6OYnEDTCGq64uLiIkydPun4/SxBdWlrC6OhoXe4R87Z6ARvr\noIe+eAmzbUePHjU9btckz1oT3prM2ok9tBPqjWg3Fl44ZPYPIdQPIHYZ/6qqYmtrC9vb2zh58iSu\nXr1qqsfdDt3yqAvjXqNVo7pfc2i1jnGxWAQAIwGLv/CwkpKAfTc/3mOoKIrJeyOMvuAww25sK5VK\nXfw5E8PN7EOxWMTi4iI2NjYa5h55ac8Pe+iLl/Armk5N8rLZLNLpdF15Xmsyq5u8Aq+uKZ144ZnN\n9/v9wiHjAUKoHyDsEkRZTFwqlUI0GsWJEydw/vx5T7YnYtS7T6896kDrht2pjjFfUtKagMVKSvIi\nniVgMaMfj8cxNDRkupiJbn6Cw4h1NRSo92zKsgxVVR3PTZZ7tLu7izNnzjTNPeqWUOcF3d1sy9vB\nqUme1ZY2Ks9rVxN+P8IpG3nhK5UKbt68iUceecT0nkarqgJnhFDvc5w8Lul0GolEAqVSCZPVLnLJ\nZBKbm5uebdtroU4pRSKRwPLyshE+kc1msbOzg2AwuK+Z8/3CQRTqTvAlJXlYWUi+JjyfgMUaO2Uy\nGUQiEcOgi25+gsMGn+zfLEFUluU6+0spNRJEVVXF5OQkLl++7Oo776U958/Pfi/PeBBpZEv5ZNbN\nzU0sLCygVCoZ5XmLxSI2NzcRDoeN8rz7Bfse8rXe+bm7CYsUDpl6hFDvU+wy/gHgzp07RpKQtUGR\nnWHvBK8Me6lUwsLCArLZLDRNwwMPPABJkpDL5TA/P4+dnR0kk0mUSiXbBhShUOjQnqz9ItS7WR+Y\nLylpl4DFPEd37tzBxsYGVlZW6roJ2iVguTX6wmsj6DV2q6HNBBRfq1zTNKODaCgUwrlz5+q8sM3w\n2qNuV/5PCPXu0qgmvKqqyOVySKfT2Nvbw8bGBvL5vKkmvLUwQLdQVbUuTMeNF144ZOwRQr3PsPO4\naJpmdJGzSxJi8IbdCzo17LlcDolEAul0GqdPn8bg4CAmJydRKBQgyzKi0SgGBwcxOjpqVCNhxoZ5\n2lkDCgCG55WP2et1fHen9ItQ79Vx5EtK3rlzB7FYDOFw2PAcsQSsO3fuIJvN1nUTZBcevqQk382P\nHV/m4RFGX7Bf8E3G2ql/Lsuy0ftieXkZY2NjePDBBxEKhdqaz37EqAt6hyzLGBwchM/nQywWMx5n\nhQGYLV1dXUU2mzXVhOcFfCAQ6PiztBPqTjjFwrPvl3DICKHeN9h5XEqlEhYXF103KGIxjV7RrmHf\n29tDPB5HoVAwwnIIIYanlMdq8JmxsVaqYTF7zNhsbm4il8uBUopgMGiK19vvDnKd0A8eqH5IaAXM\nxp33HFlLSvI3c7znCIBtYye+pCQ7zxjM+N9NRl/QXbxoUFQsFrG3t4d33nkHp06damr73dANoZ7L\n5ZBMJjE4OGjrPLqb6AdbbgdfGMAK3+U6mUwaIYl8HXl+Zdut+G5FqDeaN/+b525zyBwMNXNIcfK4\nZDIZzM/PI5vN4syZMzh37pwrIdXL0BcWOxmPx0EIwdTUFEZGRmwrvLRT9aVRzB5fRnBlZaUugdHa\nQa6fuNs96jxujbvTzRz/XcjlclhbWzOVlHQqg8afh+VyGZlMBul0GidPngQhBDdu3MD09DTOnDnT\n9r7Nzs7i6tWrts/duHED0WgUs7OzeO6559rehqC3eNGgiCWI7u3twe/3Y2pqCuPj457Mz0uhnsvl\nsLW1hUwmg+HhYaTTacMGv/POOyabG4lEDozjpBP6Vag3wufzIRqN1oUkappm2FG7Ltd2yaw8Xgj1\nRjQLo+EdMvPz8zh+/DgCgQBKpRK+9KUvHTg7e/jPnj6EGfStrS1T5QzWoIgQgsnJSYyOjrZk5HsR\n+kIpNWIng8Fgw7rt3Sjp5VRGkAmvbDZrGJpEIoFyuWyItkgkgkqlgkKh4MlyX7v0WiQfNKHuhNuS\nkul0Gqurq6aQKl7EF4tFVCoV45j8yZ/8CT7+8Y+3Pa+ZmRl89rOfxdzcXN1zs7OzAIDp6WnE4/GG\ngl7Qn2iahmw2i1KphHA47Cr+nIdSath+SqmRIPrRRx95Os9OhTqb5/z8PDRNQyQSwcMPP4xisWgI\n8bfeegv33XefIfD4m2Wfz1cn4K3VSg46vd4XL5OF2+lyzT5bTdMMB8h+HhM7L/ze3h58Ph8kSUIy\nmcTXv/51IdQFzljjz//qr/4Kjz/+ONbW1rC0tITBwUFcuHCh7uRwi9ehL41EtKZpWF1dxeLiIqLR\nqGPcPI/dhaJbCUh8AqM16aZSqRjLfaqq4rvf/a5RgcRuua+bIlZ41M1061g0KinJr8ikUimk02mo\nqorbt2/jtddew+rqKt5//30MDg4anSJbYXp62hQzyvPqq6/i6aefBgDEYjHMzMwIoX4A4JfeVVVF\nMplEJpPBuXPnXI+haZrRQTQcDuP8+fMmJ0e/hDLyzphQKIT77rsPhBDbG0++Zrg1ZI1VK7E6TmRZ\nrhPwvXSctEs/2HIW+tEtGn2+fE347e1tZDIZvP3226aa8Nb+GvsBW1ElhGBvb69ODxwEhFDfB+zi\nzyuVCorFIt544w1MTEz0ZYMiO6NTqVSwtLSElZUVTExM4NFHH3UdTsKLcn5peL+XDBVFMUTb8vIy\nHnzwQQC1CiR2sc92cfBeGBprd79e0E9Cfb/hb85YiMHy8rIRakUIwQsvvIBvfvOb+NKXvoTd3V18\n4xvf8Gz76XTadMHb3t72bGyB9zjFnyuK4lpUVyoVLC8vY2VlBePj43jooYcQDAbrXtdroc47Y0ZG\nRvDAAw8Ycc7ZbLZlu+1U+YkXeKyBWqFQqHOc8Enj/Ug/CPVe2nK+JjwhBENDQzh9+rThDGHOMaf+\nGuy66lWZ5m+MPopPJG8CqGmZdDpd9/07CAih3iWsHhfA3KAonU5DkiQ89thjnsVNd9NIFItFJBIJ\nbG1t4cSJE3jiiSdajjvsJEZ9P+ArkPBYDU0ymTSaT/DJi3YlBJtxtxv3fqRSqSAcDmN4eBif/OQn\n8cILL+A3fuM3ej0tQQ9pFn/uRlQXCgUsLCwYNvTxxx9vaEO9dry4HY85Y1ZXV3H06FFbZwxfR71T\nnJr+NHKc8F07md3tNf1iy/fLU90IPoyRv+Hi4cNTWc4DCyHrdHX7G6OP2j6eSqWER13g7HFhDYrK\n5bJRCeXmzZs9nm1zNE3DBx98gJ2dHZw5cwb33ntv26KOz8q2PtbPNDI0LF7PLh7TLpHVrgRVPxj3\nXgv1fvoOsKXS/SAajSKZTALQvT3W2HpBb2HOlmb1zxVFMV5jZW9vz1QcwK0N7UYoY6PxWL+LO3fu\nNL2RsKuj7jWNHCes90I2m8XW1hby+bwh8MrlsknA79e53O2wEzc06mS73/No5oBsFJ7KKnvlcrm6\nmvDBYNB0bbU6x/6/iWsAAGVIqbuupNPpjoS628T/l156ydM4eCHUPcJNg6JYLGaKkWXGvd8qkQDA\nzs4O5ufnkc/ncf78eVy6dKljQck8Ovwd/0EQ6k40itfjE1m3t7exuLhoaujEjIzTxX0/6QcvTD/c\nLDBYwjHQvRsItgT7zDPPGDfs8Xgc09PTXdmeoDVYN2h+NbSR/bOKaj7xst3iAKyOulc4ecGt/S6e\nfPLJpudiL+02XwHsyJEjxuPr6+vIZDIYGRmxrQBml8jqJcLpUqPTwgBuKntZnWOKoiD/3/+vxmsl\nRQ8x5uexs7PTdhUlt4n/MzMzeP3114VQ7yfsPC6aphkxiKOjo6bYPh6vPSadQinF9vY25ufnIcsy\npqamkMvlcPToUU/G70bVl37FqewVi8fM5XLY2dnB3t4e3n//fdulvv1q6NQPXphul/NqBd6jns/n\n224ww7hx4wZu3ryJGzdu4Pr16wCAp556Crdu3cLVq1dx8+ZNzMzMIBqNikTSHsJanFcqlZbrn7MY\ndU3TsL6+joWFBQwMDDSsgtWMblfxYp7+fD5v6nfhBrswxl5DCIHP58PY2FjDCmB88zRFUeoEfLsx\n0iLfqEa37Hmjyl5vXvyY8bcypCD8X1/GrVu3UCqV8MEHH+DVV1/F1tYWHn30URQKBdu8kEb0MvFf\nCPU2sfO4FItFIwaxFw2K2oVSivX1dSQSCQwMDODixYttV55phJNQ7/YSaj9hjcfMZrM4f/48/H6/\nEY/JqiLsV0OnfjDu/SbU2Xnb6VIpAFy/ft0Q6Ixbt24Zfz/77LMdjS/oDC/qn1NKkclk8MYbb+DI\nkSN4+OGHWxYCVrqVTMr6XQCw7XfRylgHgUYhFqxsK1v5ZA1/mNfemsja6Dj1w41LP9hyYP/t+bfP\nPQkAkEMy1GQZCAEPPPAA9vb2sLS0hNOnT+Pee+/Fhx9+iD/6oz/CV77yFRSLRfz+7/8+Tp8+7Wob\nbhL/Z2dnMT09jRdffNGbHasMRlxWAAAgAElEQVQihHoLOHlc9vb2kEgkWo5BbBTX2C5M+LrZvqqq\nWFlZwdLSEsbGxhwvLl4ZILul18PqUXcL3xmzUUMnlnDTjeXcfrjA9JtQZ3PxQqgL+hNrudxW658D\n+orLwsICtre3QSltmiDaCl6KYUopdnd3Dftx77331iVvtsJhsdtOZVv57sc7OztYW1sz+i5YK9Gw\nJMd+sKN3q1AnPv24+6MKIqdCyC7pScfM6TIwMICf+qmfwre+9S0899xzeOCBB7ry/WX5Rl4jhLoL\nnOLPt7e3kUgkIElS2zGIXnvUWXfSRidruVzG4uIi1tbWcOzYsYaefy+XONlNBG/ke23Yek2zY+vU\n/plSaqpLzDee4Bs6sd/N6hL3+nNQVbVvuhfySWE7OztCqB8y7Mrltipudnd3kUgkkMvlcObMGZw/\nfx7f/va3Pf0Oe3F94ENxAoGAEYrZKYdFqDvhFCPNElntVj59Ph9KpRLW19f3vVY4P7+7Tai/fUVv\nRuePKlCCCvwRBeEHdZttLQzA2/NWr3nNEv+ZN70b9MeVsU/h48/fffddo972ysoKFhcXMTQ01FGY\nSDeEOotrtLtgFAoFJBIJbG9v49SpU3jyySebnkzMq+PFye9k3A+zwW9GuzdBhBAEAgEEAoGGy7mp\nVArLy8u2DZ3Ycm4/YE366RcOat1dgRlWLrdcLmNpaQl+vx9Hjx5tObyF5fBIktR22IhbmNOlHVRV\nxfLyMpaXl3HkyBFcvXoV+XweKysrnszN2hODpx88y92i0crn1taWYWtTqRSy2axRwtdaStLrRFZG\nvwj1/aqcdfPq9wDQRXpg0A+1bD5frPPoxJ47Jf6zMePxOOLxOJLJJJLJpKddpoVQt8HO45LJZBCP\nx7G2toaJiQk88sgjHTco6kboi534z2QymJ+fRyaTweTkJM6fP+/6ZPZy+dXOuB+kWMdu0I2LWrPl\nXFbyan193Qiref/99+sSWfdTOPdL6Iv18ziodXcFOnblctlqlNvzTtM0rK2tYXFxEYODg13L4bHS\nTjJpqVTC4uIiNjY2cPz4cVMoTrFY9NSWO/19mIW6E8xxEgqFcObMGeNx9l1jHvj19XVks9m6Er58\n6GInx65fhHq37fk73/N90FRdQygD9ds5+1/+CIDutOKr6mWz2bbPXafEf1YYgOUhvfLKK0in021t\nwwkh1KvwHhc+/pyVrsrlcvD7/a680G6RZRnFYtGTsRi88E2n04jH41BVFVNTUxgbG+tp0pCIUa9n\nPy9qTsu5b731FmKxmKmcpFNDp0gk0hVPSb8Ides80uk0JiYmejgjQTvYxZ+zH0VRXNld5n1fW1vD\n0aNHXXWP9vJ8bmXFtVAoYH5+HqlUylgttQq2bjhFrPt6N9tzu8+eX/lsVMJ3a2sLCwsLdSV8me1t\nlsjK6Beh3s2a8u9+4vtBJAIJgDLsR6VQMbzp/oj52sSa1/F0cn7aJf7zhQHYa7wuEHDXC3U7jwtQ\nqyNeqVQwOTmJdDqNU6dOebrtboW+bG1t4cMPP4TP58PZs2frPKutjuelF8Y61t1s2IH+WCYmhBhe\ndL4uMWvoxBJZrQ2d7BJZ292XfhHq1qXS3d1dXLx4sYczErSCm/hzRVGQzWYdx2Ddo5PJJE6ePOk6\nQZTZc69uZN2EvrDV0mw2i8nJSVy4cMHxHNyP1cu72Z63asublfDNZrNIp9NYWVlBoVCAJEkIhUIm\nAW/t1tkvQr1bvPcDPwgi6/sXjPrhC/mgVVQU90rGa5RA7fzbz+Z13eTg70Gb2HlcKKVGg6JQKGQS\nuXNzc56fBF6GvrCkoc3NTZTLZVy+fLkujq4dvCyfeDfVUXdLr4V6o2PPN3SyJs7wiaxO3iD22403\nSFXVrsVttoKXMY2C/YGthrptUORkd3d2dpBIJFAoFHDmzJmGotcOr4V6o9AXfrU0Fou5KmQghHp3\n8cqWW0v4MlRVNRJZ+W6dABAKhRAOh5HP5xGJRPrG8eElH0w/BdkngWoUvrB+rdAqKgKDQahlDWq5\nVPce3p7zjewOGgdz1h1g53HhGxSNjY3hwQcfrKuy0Y0uol541CuVijH38fFxTExMYGJiwhORDjh3\ns2sXVsOXhVWwY7C7u7uv7Z77iV4L9Xa2z+oSN/IG7ezsYHV1FYVCweS1t5Y1A/rHo2415qI8Y/9i\ntxrqpv65z+cz7D9LAkwkEkaTt2g02tY50Y2657ztZXOdn59va7W0W0Kdn2OvVwd7SbedLrIsY2Bg\noC7GWtM0I9dod3cXyWQSW1tb0DQNwWCwLozmoF1jv/vffRJUpZB9+rXCVw1z8YV8UAL6NSN6Kopc\nMofiXsH0Xr4nxs7OzoF1uhysT6xNnDwu1gZFjZY4+02ol0olLCwsYGNjAydOnDDmfvv27a52s2sH\nTdOwurpqJGNduXLF2Pe1tTUkk0lTfXAWF32QjctBoRurRHbeIE3TjERW1h0wn8+DUopQKIRCoQBK\nqVEbvleivVE5L0F/0GmDIlmWUS6Xsby8jMXFRQwPD+PSpUsdOzdYd1Kv0TQNGxsbSCQSGBwcbHu1\nVHjUu0uvVkf5al57e3sYGhrC+Pi4KXQxm81idXUV2WzWWL20XmO90jZefsf++m/9DQAAkfXjSlUK\nf8R+nkyks0RSwOx4OciFAQ61+nHyuLAGRawGrpsGRd2o0NKOYWfxkyxp6GMf+5hp7p2U9LKjE+PO\nN1Q6evQoTp8+bSTWMEE+PDyMcrmMe++9F0B9ljxvXKxlrrqV2Hg3sV8xjZIk2XqDWEOn73znO8b3\nxeuGTq3Ae2AA4VHvJ/hyuUB79c/L5TJWVlawvb2NoaEhPProo54JFFmWPb1GqKqKUqmEN954A+Pj\n4x13O/VaqFNKEY/Hsbq6apyrxWLR8Fz2QyjbftLrMEbAbM+dQhdZ0Qx2jb1z5w6y2WxdDw5me5v1\n4LCbQ6eOlvinfhRUo6AaBanuD9U0RI5EIPsVEImgUigbzzH4+HQ2l8PQE+NQqpxmDYpkWcbk5GRL\nNXC7VUrR7Zh7e3uYn59HLpdrmDTUTkmvRrRj3CuVCpaWlrC6uop77rnHaKi0uLhY522xLu86Zclb\nBfza2poh4NlFQgj41ul18hFr6OTz+XDy5EkjQ5//vHO5HDY2NoyyZoqi1Al4v9/vyUXS6lHP5XJ1\nVQME+wvrBu0m/tyJXC6HhYUFpFIpnDhxApFIBOfOnfN0nl6FvvDVZiilDRvStYJX3m62mpvNZiHL\nMh588EFIkmQkP965cwdLS0umpHNrCcLDSL8I9WYimRBihC426sGRTCaxtLRk6sFhTWS1299O8jQS\nP/Fjpv9ZuIta1jB4j75KSyRiCHK1pBoe9sF7hpBP5Wz3FzjY+UaHSs0w7xyDGSbm1R0eHm67Bu5+\n1TznoZQilUphfn4elFJMTU01TRrqRpykW6FeLpdN4ThPPPGEyWh0kkwqBHx36LVQZ1hj1JuVNWt2\nMWG/nS4mTlQqlTqvZa8vvncjzOtXqVSM49+OQGfVu4rFosnB4VXjH55OrxGFQsEIxTx16hSeeOIJ\nvPnmm54J206/x8ViEYlEAltbWzh9+jQikQgmJyeRzWahKApGRkYQDocxNTVl3Nzytpm/2T6MAr5f\nhHon9rxZDw4WB7+2tmZoLWslGkppyx71pZ/+W6AahSQTo2kRkarhLhrF6NQYJEUywlv8AyEERxTs\nLG4BAIIjuqazE+qMg7w6eihUCzPqqqrizTffxGOPPQZVVbG4uIj19XXcc889HS9xdkOoO4lgVn0m\nkUggGAzi3nvvrYv5bTRmuVzu+hx5rAbcroav09w6rSrTSMCXy2VkMhnkcjlTowlewKuqinK5fOAv\nEu3Sr0K9EW4uJnxDJ6BWFYG/oNjt92Ep53WQYedkMpnExsYGLl682HIH0c3NTSQSCfh8PmP1tNu0\n6yTJZrOYn5/H3t6ebShmrwVgPp/H/Pw80uk0JicnjfktLS3VvdbqeHHy3B5GAd/rzwnonj136sGh\naZpRiSabzWJzcxN7e3sol8t49913TfbWLvdo5Wf+DqhW+75oKq0K9Fq4y/DJEUhKbZ/48JbwWASV\nQk1THH9kyjQ3/vMQQr3HsFAXFrP4wQcfIJPJGI0fvEhM4ysFeIX1pOaTLqPRKK5cudLysvt+etSd\nDLgTdp1Ju2XY2PLe6OiorYBnhqVcLuO9997r2UWi14lXXsQTeoEXVV+aXUyYiOcbOlmrIhSLReMz\n9+IG7saNG4hGo5idncVzzz3n+Hw8Hve8ScZBRdM0I7GY96g3Q1VVrK6uYmlpCdFoFPfff/++hi21\nanuZt79UKmFqagqXL1+u21dmf3txjvI3ELFYzPaGySpO3X5Wh1HAH2ah7oQkScbnwUilUtjc3MSp\nU6eM0MVUKmXkHsX+82+hnC+ZBDqRJFBNhVRNGmWKY/TsPdDKFVBNg+z3YejkACp552ZloV/+beNv\nu8IAXvfC2S8OhVAnhCCdTiORSCCbzeLYsWO4cuWKpyeNoijI5ZyXVTqBj+k+evRoR95/rxOG7MZj\nBjyTyWBqasq1x8upvfR+ilVrfN7a2prRCvggXyTaRVXVvvCoA927aeMvJo0aOq2uriKZTCKTyeDP\n//zP8dFHH6FSqeDrX/86Ll68iCNHjrQ0x9nZWQDA9PQ04vE4Zmdnje8aez4Wi+Hq1auYmZmpe/5u\nx+fzuVodLJVKWFpaamn1lK3keV3xiA+9tINSimQyiXg8bpSDbOTlY+J/P4X63t4e4vE4CoUCYrGY\n7Q0E4Hy+dmLPOxHwXnf5bpW7UajbwWLUQ6GQqcz15j/8CahlDWVr00OJVJNGa6EukkwwfHocAOCL\nBFHay0EJ1boCSz5duvLedF/Y3DXYTqiLGPUeUy6Xce7cOSwvL2NwcNDzE0ZRFE9DSgA9ZKRQKODN\nN980lVjshG561N0acCf6veFRL7w8wrD3DruqCLlcDg888ACmpqbwp3/6p/jyl7+Mr33ta3jhhRfw\nIz/yI/iFX/gF1+O/+uqrePrppwEAsVgMMzMzdUL8+eefx+uvv454PI7p6Wnvdu4Q0Eyo53I5JBIJ\no2u0NSemEd0qt+u06kopNUosRiIR17lS+1FSkbGzs4N4PI5KpYKzZ882Lbawn52m3djmnZ0d5PN5\nbG1t9Wx1tNf2vB8cL/yNZep//5+glnSPOPOg697zWhw6/7j+W29kBACyX4ESChjPyYFaoyMAiJ47\ngcL2ju087JrXidCXHsO8XW69MK3iZY3cXC6H+fl57OzsQJKkli4wzeiGUM9kMpidnYWmaUYXvHbH\n2i/D7iVuLhJ37txBJpOpE/DhcBgDAwN964G/m4W6HewiMzExgQsXLuChhx7Cb/7mb7Y1VjqdNp0r\n29vbpuevXr2KWCyGkZERfPGLX+xo3ocJ9n10KjWbTqcxPz+PcrmMycnJlmPYgf3ri8GHM46MjNg2\n02t1TK9JpVKYm5uDJEmIxWKuvY52ttvrBnnN4G2zz+czusr2YnW0H4R6P9hzVVUx9G//MbYrqkWc\n177HuvDWTJ50nqNXzjTdTmDEHOIYPDJq2n+vhXqzMMZXXnkFgN7F/sUXX2x7OwxCiAI9WJ8cCqHO\nVwPoRtInG7fTG4Dd3V3E43EUi0VMTU3h0qVLeOuttzw1bF55YFjFmUQiAQC4cuVKS13w7LALfTkI\nQt2JRiWu+OSaRCJh1Ki1XiR6TT8Y9n76/PnvZ7fLebHxP/e5z+Ezn/mMIdwF9fAJ9oFAwOgg2i7d\n6ovBxuTDGScmJtoOZ/S6LwaDUort7W3E43H4/X7cd999dbkdzejnFdJerI5SSntuS3tlz3d/5e+D\nqhqopiFYUUFh9pYzD7oRg67q9dGJBJN3HQBGzh4zjS0H/ZCDfqiFEgAgEB1EfittO49bt24BgFG5\nixCCZDIJv9/fUehLszDGmZkZTE9PIxaL4VOf+pTxfzsQQgjVT6JHAXwKwLcOhVDn6aZHvR3D3iwm\nkXnqvao00akHhrWpjsfjCAaDOHHiBDRN61ikA/3hgdkPfD4fotFonVGwE/DZbBazs7N1FwkvPX2N\n6Aeh3g9zsKNTD0w0GkUymTTG4puOALoH5nOf+xyi0ShisRhu3Lhh66m52+Bv5CmlWFxcxPLyMkZG\nRtpKsLejW+V2S6USPvroI2xubnoSzuh1XwwAWF9fN0JwLl261Fa5YjY3O6G+X6E67dBNAd8PHnVg\n/8Ip9/7xs3oICq2JcdagqPY34ZoW1b4XvLXXoAt2ABi7eMoQ5EokBGgUlNZ/n0JHR6CVK/ANDaCw\nvYPBM8dAPvNPcQ3655DP542yvW+88QY+//nPY2trCz/90z+NS5cu4erVq/jxH/9x1/vaLIwxHo8b\nBQFisRji8bjrsa3Q2kmVA/DbAM4eGqHORCBb/vKaVg07H5MYDodx4cIFW48Fi2sMBAI2o7ROux51\nNt/5+XkMDg7i/vvvRyQSwcbGBvb29jyZGy/U+drIh02oO2En4N9++21cuXLFtQfe5/N5aoj7QSTv\nd7KcE9YLbadC/ZlnnsHNmzcBwBSDbuepv379urF0KtCF0/LyMrLZLEqlkqcdRAHvhXoul0M8Hkcy\nmcTExATOnTvnyXnlVegLpdQoUbu1tYUHHnig4xseJsrbqfrSb3gh4PtFqHeT7Of/AaimGTHiVpyE\nuU6t5CLvbZd9EqhGMXL+JORgEHLAXze+b3gIamELSigIX3QQ5d2M4xwJIQiHwwgGg4hGo3jggQfw\noz/6o/jEJz6B3/md38GHH35oOFDc0iyMka/YNTs7i2eeeaal8R2IAtihlL5+KIV6Nzzqbg0ma4O+\ntLSE0dHRpjGJXscgtjqepmlYW1vDwsICRkZG8NBDD5nm62UyEz8WL9jvFqFuhe13Iw98LpdDJpPB\n1taWrYDnY+DbuUhomtbzuuH9ItTtqgScOdM8VtKJq1ev4ubNm5iZmUE0GjU8ME899RRu3bqF5557\nDi+99BJisRiSyaQoz8ixuLiIcDiM0dFRnDx50vMVJq+EOp9gf+rUKRQKBZw4ccKDGep0GvrCx8iP\njo5iaGgI58+f9+R49nPoi1e0IuBzuRx8Ph92d3cPTYWw/D//WVCNVn80U0lFA0IAWvOkm8U6UB+L\nXksaZYzdX6t/zm+D96YHJ8aNMbRy7dwdPGMOlWHY9cQ4depUV0s0spCYTqp3caEv5wH8I0JIWgj1\nFsZvRLlcxuLiItbW1nDs2DFcu3bNlTHslVDXNA3Ly8tYWlrC+Pg4HnnkEVuvvpdC3cmIHybD3grN\nPDBOjX34zpzb29tYXFxEqVSCLMu2ITSNtiE86jW6Uc7LTnyzOEoAItTFBkKI4ZFOJpMol8t13WI7\npROhzneMBmCEM1JKsbCw4OU02w594R1GR44cMVYkbt265bk9t+Ye3Q323E7AJxIJ+P1+hEKhA1vi\nt/gvf0EX5JQCNsKc94Sb/q+Kdf0xc+IoAEiKbMSiW8cZuTBZeyHVIIdDqGSyUMJVh2Gb1yfennd6\nnWsWxsiYmZnpOJGUC335FqX03xNC5EMj1BndEupO8G2fT5482XKDJa+XYJt5NCqVCpaXl7GysoKJ\niQk89thjDQ1Gt4X6YfPAtEK7S6VOAr5SqRheHrcCXgj1GoepnNdhoJuOF5/P13JfDNbxdH5+HsFg\nEOfPnzeFM3bDlrXqyGFJrCsrKzh27FidffcyJ8iuGdPdbs99Ph9GRkYORCOn8m/9EihXmcUQ6ADA\nJXhqlfrrP/OW8+IbhIDIMkApJEU2nieSbPKw1zzptRsBORSAmi/CN6ofNzkcMsbWCkVjLgBAFP34\n+IYG4BsfR/nOHZDP/NO6OfL2fHd313V3dzvchDG+8sorhuOlk2RSjgcIIX5K6X87dEK9W1VfGExc\nZTIZJBIJx7bPbvHao+4k/JjHf319HcePH3ed5OR16IsQ6jW8jmlUFKVlAa+qKorFoiHmm3ngu0G/\nCHVrJ1Ih1HsD//3rZiij2+sEHx7YbsfodnFrf/kV3RMnTuCJJ56wte9eJnsetipendLInve6E6v2\nu7+sx31XkzNpRXXlqbZ2D62OZql9rplfa/Gus/fUjw0cefyK8RqJWzUzKsH4fEBVqEuhELR8HgAQ\njk02nTtvzzu15c3CGGdmZvD888/jxRdfRDKZxGuvvdb2tjguAvgEIeTwJJPytXe7VXdWkiSkUiks\nLCygUqlgcnKy5aY/VrpdJ7dUKiGRSBgtfVut2e61R90u+UgY9u7SSMB/5zvfgaIoSCaTRpa8LMtG\n7Dv73U0B3y9C3S70RQj13sCX2+2WR72ZUOdXH48ePeoYHthNTi59GyAStOW3IT35P9Q9b7XvzVZ0\nxQpp92jHnnfSo8Mq4CmluPbeH4L+tz8AVVUjrhyAqXqKIahNXnNdgxifH/N+Q6sLd9FjzGtjWT3l\n+jatITKSScBTTcP4tfshhcKgRb34BwmFQKtC3Ioc0W+MlaMTgKaCcjHq/lOnYGcheHvuhdOlURjj\n9PQ0UqlUR+Pb8CUAy5TS8qER6oxuiAlWsjCbzWJ+fh5nz571rL5yt1YACoUC5ufnkUqlcObMmbar\nEAjD3j16XSVAURT4fD4cOXLEJOIrlYoRA59KpbC8vGwS8PxFIhAIdLwPXpYn7QSrUO90uVTQOT6f\nD6VSyfNxG9ndUqmExcVFbGxstLT62An0z7+s/2E5lwiRjMfoX7yq/yZEV0VEgkwIYj4/zj35N1zZ\n927b88NYbtctXtrzZgJ+5Pf/jR5DTrkkT6qBAmaBbgORCCgrkGiIda4iC59rIElANUHUTL2n3K7D\naG0e1TE1AJRi/MmHzXPiC1iEI1AzeyA+H6TIAKQIVz5Uc+/U5Ovae5Fv1ANSACRCyEjvr44e0Q3B\no2ka1tfXsbCwgMHBQQwPD+PChQueNqphtXe9IpfLIZ/P4y//8i8xNTWFCxcudHRsumHY8/k8Njc3\nMTg42BdNf3pFr4U6YJ9koygKhoaG6kSqk4CXJKnOy9OKgK9UKn3jUecTwPuhgcndCl8coNVYcjfY\neerz+TwSiQRSqRROnz6NJ598sq3P323eB/2z/6j/wb+WcuEDgK5sqPk546wiRP+7VAC+9Zou0tj7\nqkKeEgL5ib9tDN+tKl61KQnHixdIv/ciKFtp16jxt1/T4KNU/0o0FONSnVgnRNLDXnjvOCfEee+4\nSbDz309LcyJrRXR+u1Sj1SRSroqLpmLs8QfZJPVfgVrIixTW9YA8MAg0uDkmRyZANzdAXK5wHbQw\nRkLICIBfB7AF4I1DI9R5WKZ8uxd/VVWxvLyM5eVljI+P4+GHH0YwGMT777/flSYZXoS+ZDIZxONx\n5PN5KIqCxx9/3BOR4aVhLxaL2NnZwXvvvYeRkRHs7u4ay3vvvvuuIfJYyMVhF0n9KtSdcBLwqqoa\nn2M7Al5V1b6ogFAul/ct9ljQmG5X8eI96sx25nI5TE5OduTcYA3snM4pQ5zzMPtqJ9jNL7QMVv1N\nJKB6DTHNuirk6Te+Yjx0EQCS79W0PyfsqSSbRH0zCCFGYxlKKQYHB5HL5e7ac8itPZf/y6/rnzm1\nD02BVhXivKh2uAabRLfuwq5/kU3lFtP7uTAX9hgv2CWZq9giy4BEQSuqo5fd6lFnr6Maxfj3PApa\nKoEEQ6DFapJoKGSEv7RD+Yebl7U9aEKdUpoC8PcIIccA/L1DKdSZcW9VqLMlT5Zwac2YdxPX2Cqd\nhr7s7OwgHo+jUqkgFothdHQUb775pueZ/Z2QzWYxNzeHbDYLv9+Pa9euoVQqGcvJb731Fu69915T\nwmMulwOltC7UIhQKHRoBf9CEuhOyLLsS8CsrKygUCnUCvlgs9sXFncWAAv1RtvJuZj/6YhSLRczO\nzkLTNExNTWF0dLTj85E5Xqw3nrYC3YqdYDcN4mDTqcXRY3jjnbZTe33NOy+BUA30//1P5gLXThCC\nCwCwLeEYqoI/A7wln0YqlcLa2popnnpgYACRSKQvQty8wvfHr+jHsiqyL2kU+FCriVpLBRU+PpsX\nz7xAr77A9Rx4Ecw/ZhLbqI3PvOrGb/79fKy5cc2Xqq8xV3khiqx74qu/rduu/a9rMK2iYvT7ntQf\nCwQBSdZ/l2vRBGQoCuzt6v+EI0CpaN7ZQBDIZ10dF+vNcjqdxtmzZ129tx8ghBwB8H0AbgL47UNz\n1thVCnBbe5cteSaTSWPJ007kd6vtdDse9VQqZbSptcbMszG9CCfoRKgzgZ7P53H27FkMDg7i3Xff\nrbsYEkIQCoUQCoUwPj5uPK5pGgqFAjKZjJFUk2dZ39UkR3YhCAaDPRe9rdIvQr1bYSeNBDwLoUmn\n09ja2sL29jYWFhbqbsz283PlY9R3dnZEfHof4LVQZ/lG8/PzKJVKOHv2bF2SdSdY7bkrgW5F09qu\nHa1vtA0nDS/2rcK/ESp0wVb993Hpr2vCrQggAyO8orX58GX/bDzF1Fy7G5qGk2w7cdgfA6sgNj1m\nEdfsOVa6kN++5TFTiImDODe9rpFIb4KTA65Wz5yJbbN3nGoawMobQjZEO5uDrZedE+K8AK8LibEI\nfKunnWoUIz/4iZool7jrTUgPdTHCX4aGgd2d2vNDVV2zm649Fh2rvsc57KUbPTH2mQiAYwCeBXDh\n0Ah1BqXUtaDe29vD/Py86yVPWZY99+60ItQppdje3kY8Hoff78e9995rKya6HYfYjEwmg7m5ORQK\nBZw9exZjY2MghKBcLpsMzc4v/y8A9PZbO1/7XQz/8/9Qt+1wOFznbdU0zejYubOzg9XVVZOnlhfw\nvSg36JZ+EOqNlum7hSzLGBwcNGpQl8tlHD9+HJFIxBDw1s91PwS811UCBO3DPluvhLo13+jy5ct4\n5513PBXpQM2Z05JA9/r888j2N8TJ66vaCOpmCYBO83VcQWge3mF6jXV8qxh3GMNRRDcS6E7baQYT\nt04NhphwtiZ6GnO3VCZeEJEAACAASURBVGexCHZWy9yYN7c9Jy87E9Qs8VSvAFMbh1lf03GQ5bqb\nieHveUL/wx/Ujx3TO5FBwC7kZXxC/x3kOroPRUEDYZBg2NWKwyHoiZEC8H9TSpcA4NAI9VZq7zJv\nNKW0pSXPbizDurmpoJTizp07mJ+fRyQSwaVLlzAwMOD4ei9LPrYihphALxaLOHv2bN1xZRUBCCEo\n/LOfhexXjLJQALD7K38fQG0Zj3L7EH3xS6ZxBgYG6o4BH2rB1wtXFMUk8gYGBvoiJrofhHo/hHiw\n1R+rgOef3w8Bb/WoHzDDfqhgn2GnVUT4Dp18vhHD63NQlmUMzf6B9+K7Gc3CZlqhhdCL2nuafEbN\nbh68EuVO23IS5zZjNhPoTWHbZ6K6wbWYiWPzg0adw+q/1vKI5uuytZyi/lazYAe1Txh18rLXxtbq\nbxYs3ndrwihBTbybzt1A+92FacA+NNIpPv0QCPUCAEoICQI4cWiEOtDYC8N3lQsEAo7e6EYoimKE\nXnhFI1FNKTUabQwPD+PBBx9EiCtj5ISXHnU37O3tYW5uzlhKdmqvSwjBfX/4Rez+4ReNzmU8tfJO\n7ESvvWbncz9TfU1tv7SKaizrjf6rLzuGWpTLZUPAs+8Ai0cuFApYXV01hN5+xlD2g1Dvhzk0C9Ny\nK+DX1taQz+dNKzEsMTkUCjXdTz4M6AAadgEH3+DNrkMnYN9ZsxPon39Zj9vuBbxAt6nS0TXaFeiN\n3uckiNsVzo225eRFB3QR6zIkxRSGYwn1IdznYfVm62+thaCYtsmJZiJpAOfNNo1v42U3x5kDAOsO\nys/LLLAhma+vQE3Ag73P8h42xfrt6dflwad/CMjugLIQF8tnQYMhQFJACYG0l4I2NAap4C4OvREH\nVagTQgjV725+AMDPAFgEsHyohDqDF+qapmF1dRWLi4sdd5Xbrxh1fs5jY2O4evVqS402ut1EibG3\nt4fbt2+jUqkYHvSGr/9n/0C/G2eGS9OqXnXmOai9lmq1JBfWQAEAiCwZhoK9nEgy0s//z+ZWyKaS\nUBTjv/Wf62LUSqUSbt26BVVVsba2hmw2C1VVEQgETOEz4XC4K3Hc/VL+r9+FuhNOAp6FRmWzWezu\n7hoCnhBim5xsutBV4VtDt8ONGzcQjUYxOztrtJXmmZ2dNXJMrl+/3vZ2DivW76TbG8pCoYBEIoHt\n7e2mDd6YPffi3NaTMLtwHtWVa2xxdaGd+HCLR3ffsYtJdyuYedwcK4sYN4lmN/MyPe8g1m07dZqT\nPNm2DbHOPc5v20j+tNRB199v72VnOQ8EGqhmusjWeceJ5XpkJI9yx0WS5brjwyejMq975Ac/qT8X\nGQb1+UGKeplVzReEVK6GvEg1CaoN2gtpdXAEkjWxtAnWLtMHJUad1i5CHwH4Seix6k8eKqHOVwrI\nZDKYn5/3tKtcN4Q6v7TLl4U8evQoHn30UVNd51bG7KZQ393dxdzcHFRVxdmzZ13dqe698L9BkmU9\nho0r9UQp5c/VqrEghlDXveay5Tn9eGmAKamoVg7KEo8HYPsXf9L0XnYDcK663VP/Ri9hRilFsVhE\nNptFJpNBMplELpeDpmkIhUKmGPhOK9D0gze7H/C6M6lTaBQv4Pf29rC+vm4S8KVSCZubm9jd3UUy\nmWzbAzM7OwtA71YXj8cxOztrtJxmfOELX8Brr72Gl156yfZ5QQ3meGi02pXJZJBIJLC3t4fJyUmc\nP3++6bnJ7Hkn1wWjSorZy1AvdtsVv1b7YGcvmglSp+PQTMA3E6b8fJzm4HSjwItaHqft2Xm3bQU9\nn4Tq8ubGIoz5eG3wiZFcsqYuci2ecus2eaHM9tdOtEsANTqDWrzrpn3jPOxyNaTG7rO1dP/m66Cb\nEz0t3nEZNsdfMnnaSXWV2/C0Gy/jQm4kCaEfeBpQda2kBiOQ1DJoIAxV8UMq6lEJ1Ppd5pJMtUAY\nUn6vft/Y8wNRlCbOwumqYfWoW3tk9COk+qFVxfp5AOcA/AWl9I8OnVAvFApYX1/H5uYmYrEYnnji\nCc/CGbrVRZRSing8jrW1NRw7dqzjTniyLHcl9GV3dxe3b9+GpmmuBToAZP/lz0HyK/pJrGngmyQQ\nmE9YIlcFbM2GAWCxcCzGjglyUhXlZo87ez0ohVYBJKU+C916A7D183+3+rcGtawfu0D1ZxTAxO/8\nHvL5vKmtM1+Bho9/dxsnrVkM6t3Kfq0sNBLwmUwGu7u72Nvbw6/+6q/inXfegSRJ+OY3v4lLly7h\ns5/9rKkiUSNeffVVPP300wCAWCyGmZkZkxC/ceMGrl27BgC23naBfc6RnU3ky9NOTU3h8uXLrs+p\nTu05/fr/pYsLqxDkRbu11KGb0od1G3Ipluve16KAtxXUlhsQN3OwbpcXrHbvoRbPsdP2nDzN/Ovt\ntmWdm9M2WLx2VRDXedf57VtuEkxx39Z9YzcrRs36+v0mCufAcogVtwp3ArnuGJk828brAGhaTbCz\nqkK2FVrqPe5WT7sxT+69xjGTgcAnflB/nc8PTa7PBdMCIWiBEEg1yVgq5qGGhyBXQ1608CAqwQEo\ngEmsq8EIlFIt+fR2ZQCR5WVT4QhGpVLpi5K/rUDNyTgnADwE4B8SQrYPlVBndXGPHj2K8fFxTE1N\neTq+10K9VCphYWEB2WwWsiw3XKZtBa9DX1RVxa1btwDUl4JsRu7XfxGkuk+E6It9NcFd/Q3Z8CgA\n5mxy4lN04Q5YOqjJXBILZ+QAQ8xrFRWST7HxZNTPs/Z+CZJCTUmuALD1c5/WBb5KEdA0BFCLqQcA\n+YXfNnlprbXCBwYG6irQCI96fyBJEoLBIEKhEGKxGL785S/j137t1/D93//9mJqawocffthS8nE6\nnTaFgW1vb5uef/vttwHonveZmRkh1pvAuoiy/BxW/Wp+fh6KomBqaqqtZe127bnRQMgqvNrFJFRt\nPM1uxX1dUmKLAr5ZbLvTPOy2a7eNZoLdOj+nVQiHCimm1zvti1Uk223DJhymLnYdMHm42Rh1gpYd\nC7ubIssNhLWMomnXuG2Z5mK56SCys3gnfNOi6m+TNrQ0JzIqx1g87byDy9gGNECjUH7ob4KWCyCV\nMtRABKSil2TUFD9otVY/Qw1EIBfr49ErQd2RUokMQx2egD+7XfeaUmQME2MTRt5ZIpEwbuZZ9TBJ\nkpDP5xEMBjtKSG8WxtjseTcQQgIAngDwJqW0AOCbAL5KKd0jhIwcKqEeDAbx+OOPo1wu44MPPvB8\nfK+EerFYxPz8vFG3PRKJ4MyZMx7MUMerZNKdnR3cvn0bxWIRV65cafliWPjXv2TKCKfQIFXjXIgM\n3XNQtRnGbE13+7Lu+WYGjRkYqZZQwwt3Jv4BQC1V6hJWdS+8bgwl7v/q4Fxsu2bzXl38SwoMEa+p\nnKH6Jz+HkKYhBBgx92y/U7/yktGtU5ZlQ7irqrovuQSC5lg9tul0GkeOHMHly5dx+fJlz7fHck9m\nZmZw48YNEafuAN/0iFKKjY0NJBIJRCIRXLx4sWH1q2a008CO/sWrzSurWOugWyuyNBOqboS/nfBo\nRUg3G6+VcBk7D3WjbTQb20lMW7dhDUtxmo+Tl93Jkw/UkierY1s93ABAJc02jrwWT27jhWb75bCa\nYX2PnXgnmk1YDgCwcBSreOertrBtWco96m+vCnebyi52n4GptrokQfqhvwloKjRfEJXwCJRytfAG\nIagE9Dwipah7yMvBIchqCWoggmJkDL7q43bfjVJkDIgAquxHEICyswkAiEajdZqEFY64ffs2dnd3\n8ZWvfAVf+cpXkM1m8bM/+7O4fPkyvvd7vxdXrlyp244dzcIY3YQ5uuQsgL8L4C+hV3yRAFQAgFKa\nOlRCnRACQkjXutl1KoDz+Tzm5+exs7ODM2fOGHGUS0tLnpbJ69Sjnk6nMTc3B0IIzp07hw8++KCl\nesPF335eF7bMk17VvFSj5lJVhjNDgwQ9QYXIPuM3UF0SrKgmEc7GgixDU9Va5jl3QVQkYghmtg3m\nCSCSDA21MBk2HpHlWuwfYPYYaFYvhQZFsXrya94Ifq7hf/GPEOLG0ioqKACiUYQAvPVz/0dPuvh5\n1b32MMB3JQU6K88YjUaRTCYB6OeStQrS2NgYYrGY8dq33367LaFOCBkGMAhgk1LaWrZVn8NsOaA7\nSNbX1/Hd734Xo6OjeOihh1w3s2uELMuuhbr2za+yiRmVK1jIHl/JghKiiyv+Oatdd2PnNQeByWjF\nW26/hGgO33A7XlOR7SCqrdtw8rg7hazw77HbRl08dwOPvt287bZhF/LCedsJkQDZ4uG2E8v6m2ui\n2+442HzWunfcLsFTtb1xYJu3m4/JK85yuOyOh03ce31FF31cvooMeerHjGfKgQEQTUXFF4KvUjJE\nOgBUAoP1sekAyoFByIUsSuERSJqu3fLhcfhLGX1ash7WUhg4AiXoXK3P5/MhGo0iEAjg7NmzeP75\n5/GTP/mT+Pmf/3l8+tOfxgcffIDFxUXXQr1ZGGOz51vgHIBfBJADAErph4SQTxJCZiil2qET6kD3\nYrTbDVNgia3ZbBZTU1O4ePGiaSwmrL0U6u3cqKRSKczNzUGSJFP5ylbKmJX//f8J4vOBaFqdEHSy\nLfxyG9U0SJKv+r/GJa9YqBpLuXpXTzUNkGXDGFIAksIL8doIWkU1PP12LZjZN0c3buxxXZTrse0E\nkiGiJfN7KyqUgAJNpZBkbptV7zsrKckL+eP/7l/UXqdpSJU1pIx5U6gv/IYh4L2sQNMPoTf9Eqfv\nZTmvZ555Bjdv3gQAxONxTE9PG2NGo1Fcv34dN27cMB5j8eptcALA4wAkQsgigG8AuB/AMqV0o91B\n+4VKpYKlpSUsLy9jeHi47eR6J9w6dLRvftVkvCgsN+1GCIwu7nghYidK7LCWrXMst1g3uSbeeitW\nsds0rMZmvEZhLdY5ON4kOHjcreNYEzLZ63iPuFP4CgBYkx75ObB5O90cEEv+ARO7Fk8+L5ptPd2q\naohtp6oqxn41yxug1BzPzs2D3UTYzsdy82CXOGo412y+Q9TmO6jXbJehPvW3IaslaESGFjCXj84P\nTsBXrpW0LgYGDfENAGV/xIhVLwzfA0l1p1vks480fN7avG5iYgIf//jH8fGPf9zV+IxmYYzNnm+B\n4wAGKKV8LFCJVu+6DpVQ7zd2d3cRj8dRKpUQi8WMDp1WFEWBqqqeNeFp1fOfSqVw+/ZtKIqC8+fP\n19UhdyvUy//hnwBE0u0bbMQ1g1t606mOS6gpxo55wDWgZoiqf5Oq54BC0osuoGZgiCSDMu9FFcOr\nT6kh7oGa99uoJFMV4rYGlRDI1cfZ69gY5tdSyH5zkwq+fq1kY2hr2weUgHnuyud/CWWNYlul2Ebt\nhoa9b+I/fhXhcLjlG71+anbUa+yEervlvK5evYqbN29iZmYG0WjU8LA89dRTuHXrFmKxGKLRKG7c\nuIHt7e224xqrXhcVwGcAPAO99m4OwJfaGrDPyGazhtOgXC57XrVBURQUCjadETm0N/5rvdi0ilyn\nmOFG8JVhUC/+G8F79K12qqnXuNmNg6v4+AZztRObdXNwSOa0eprtblbskjLZ652OvWQjavnX1jm/\nOw+RYV5tQiTQapiniernbz4UNt59O+Fu42VnlVnsklLtw2QA002MXYgMB7Fer6rHpPhDPwEAqCj6\nCpes1Qvtsi9kEusAQCUZFSlkeV0Y8AGBwo7hVW8Xq1A/AKUZ/xLALxBCXodemlEFYCwbHiqh3m7t\n3VZpNi4LHQH05ZBmnrlWlmDd4Db0JZlMYm5uDoqi4MKFC3X1qBluhH/lS583Ql1soZwo55JZmFHk\nK71AMx9b9nCdB1wjIIT3eHD1XatJqvrf5kRTSkitukz1cWJ6nWyKX2dhM3psOlu1qXnSiWQW+Xx8\nO5+kqlU0yL7qfvgU7jV6kqru6ZeM1+tj6u+XFAKpWq0GMgEgQS1rkBSCrc98urZvWq2GvKZS4+/z\nf/DHsCKEeg2rUO/0xvnZZ+s75rGEbP75NkNeJOii/K8ppd8lhPxaNeloEMAlAFvtzbq/iEajGB4e\nxtbWFnK5nOfjsyRVO9Rvf03/gxCQOiVnoZ1KLu2O4cJjTyhtHHoD1HtNG3nk3Yh348bD4TrhGDJj\n/3Dr4zi8ngl4W287e6/lxkFWzK93DI/R7bBdzHx9+UJLPD2zeXzJRatod9IYDo/zXnu7rqO16XM3\nESzOnsXVN0hGrc1fXwUt/PCnjWNRVkLwVWpiXJX9oJLhHUPZF4ImmeVmwT+EQDkDK8WgOcw2Ez6C\nUHEHAJAPDGOwcsf+uFiPR/U4dTOMsdnzbqGUvkkI+TiAfw19ZdQH4K8A/DFwyIQ6j9dd5xhONX0p\npUgmk4jH41AUBefOnXMd1+11lZZmoT/b29uYm5uD3+9vKNAZhJCG46n/6QUQizglkmIYB90wyLq3\nwbqf1gz+agiLMZaqmmLdmVgnMqCVKyCKxXPNRLaDgCeAEfMuWT3isgxI3Pw4kW3ngWdlH1m5R4mJ\nbz5mVdO9XmqpXBXhZgFeGwvQqqEyLEyGahpkfy2cR6voDaKMhk+c90SP2SdQy7zB10U9kQjm/s6P\nmMJvGEGV4jsALvw/r6MX9ItQL5fLB6mc1wiAaQB70DvXHSGEDFFKVwC82dOZeQizO93KOXIqDqB+\n+2smb7nb8BUGL5T5x4CqF7yR95fh1FbeBbx33nHuslwfbgPUVjqtDXtsN2TjaTbFvXMrDqZQFUvp\nQiua1uBmgNSHvvDztmLM3eH41XmbbWLMG3ncgaaefGKtDlNdDTaEMmATmiLVfQdYeKdj1R5+BdnB\n4w6Ac2Cx+HguTIj9dlgNZqK98NSn9CEkBZVq/HhZCQEV3ate9OtJ3v6qeM/7hxCo1G62M8ExKFoZ\nRd8AgsUd5IIj8KndSbNJpVJdC2N0er4dKKW/Tgj5XQDXAKxQSm+z5w6VULerveu1CGDGnQl1SqnR\nlj4UCrVVicBrj7pdwyN2IzE3N4dAIIBLly65nifflMmK+nsvct4B3cAbcW5VTzMzDHzMOTNUYEmk\nhBPLxqQ1I5mU7QOL+6aqLoypRiH5zIKbyLIpadW0XVUFUWRdsBse8errqhVmDCPHN2eCHtfOh8mw\nbVo9IbXa7jASVmV/Le4egOF150Nu2GNEMnvzAaBS1Cwx92wOkvG/Hh8vc/tlTr5lFWskuRZDz2rG\nf/RjP1wT/UzQq7Wbk8szf4Zu0KyRzX5hPaf7nBiAf0spXSGEyJTSOCHkewghZUqpO3fTAYBPJt0v\noa6++ftm4dWGt9wUxtJC3LpJ4BMHIW2XSOnUXMl2co1FvMkDX02KtZ1HI4+vVdka1VAaVLexim5+\nO07j1DXMcRHLb90eq4Pv1uMONBfu1hfahN7o10guVIYX7DYx5YCNx71uczbhQcYb7GuhW+PZ7UNl\nqsK9Gh6z99SnoUky/JU8ikoYMq2dQ2UlhBKpOTxKSggqsXjSA8O2/5flAHxqEXv+UQyWkravBZrH\np1vtdzqdxvHjxxu+x4lmYYxOz7cDIUSilOYAfN36XO+vkF2CCXUvqgNYx61UKqCUYn19HYlEAkND\nQ7hy5UrbHjkWo+4VvIee1Ryem5tDMBhsSaAznEJftNf+FYjPZ3jJjeVWVvWAzyDXNNNr+fAWAhkg\n5pOLeeFRFdb666rPaVWjo2lG7Lk+0aqwZPHjRh1Yo+yMaUGVWLapsbnwr6nOU63eJJhuFqqhM7Si\nQvabQ1n42u5aRTUEu9GQSWWe8GrTB4W7mZEUzlNOjARVa+gPE9ssbEb2K9AqmlGn/v9n792D5Tru\nM7Gv+5yZO/cB4BIASVAgReKCT/ENvkTZci0jKM4mG1c2kop57G5cSZYVe23Z2pQedpxUbSqbFJWs\nk9Q6dkq7VdmUS6mopHWSXa9jRah17Ei2LIGwaIl6kMDF+0UAF/cxc+fOnNPd+aNfv+7TZx73Dkjw\nEr8qFObOOadPnzMzfb7++vt9P6uTt21pqYyebHDzfj5FJwxaTmMLSuWNDLLQrPyP/+rH3fUVG8Kd\n117LE3/y/2IzMakS7lsNCtQ7nQ5mZ2ff5R4NjN3QKOC8UsoOHAW0rde2i83YKI4SMVAXR39/uIZ7\npLDssmV/RwD7FswTi79RdOuBLGcUgD6kL6lzJgH9OKAYwEBGG0gCS31YfFwNc65PXHOORJJqnDQ5\n6L5YeUgKQKc822mBo9Tnb4c7AuCt5SKzq78pTbmAl6ikQLclllK69lSSaKqAEplE0FAQFQnRamuv\nY8ElywAGFHwKDApNEWrSXVdYhiKbit7jvo1oX8vWjxPxKu3Kygo+9KEPjd2OjVFljFsNpVTtD35b\nAnXqvTvpyLIMFy5cwNWrV3Hbbbfh6aef3vJkYNLSF8uoX7lyBYuLi5iensZjjz22afCRAurq//wf\ntc5aKp0sAwQ/cMYRDBhusHcyFvKjNOx74GturQ4T/WEZzCQgdG4JB5cI/EkJJTl4I2KfAAdmuZXr\nEAsuZRj0bKpZ0brbYS64M9J40JrBWstVwkHVTjQCuQxgHk5WH6/7L/plMAmQpfe45U6L7j3jKeDX\nx/vvVT6VB32gD8+iW4BxhqxhdY4ZRCGRmWMsMBe90u3TaGhgjyzDG4c/BsY1Q6+E18nr/ik8/Wd/\nglTcLNKXOPlos0ul71B8D8AvMcaeBvBNAEsA9gH4/rvaqwmHZdRvtPRFHP19/+a4ORt0XEzpvGtc\nNKpBpRbZYIBPQPeoSahJnX2U0Fr5O3VO+yfRwFdsKkfRxLuou98jgu5Bh9RNAsiKwcC+UVeZlOML\nUK9vT7VtJTvuNXkWcUYczhJAWVmwnWDas+iZFq9UWLkMuW+MXh8NK5GJnW3MKvWVf+0VB6wBoM9b\nmBZaZ15wD8D7mU4UFcjQlJ4/WGvsRktWixzZ6OVpsrPd2hMw94Mizjfaikb9ZoltBdSp9+6kB3ch\nBM6fP4/Lly9j9+7dE7UKmySjrpTC8vIyrl27hizLtgTQbcRAXf2z3/IPkwxgAv6HbTLclYpm63aJ\nT5Q+2QWEfVfed10z5d5DNk7sBAyAlwqsQZxgGgyqKJ0kJpCpZFmojw8SVgVcwSRbNZXoz13V1Ejr\nDhg2neduXzsRoRaRoa5d72sBeNUxRjrwLosSeasZOMxYa0crdVFSOQBO26Ce7gAP7oWKknVtXzjR\nyOt2/YBsde3ZVG6eWVZmFLVVSLBMO//wjEH0BfJGhr/8mb8S7FdumKJRpW7nGoBnj30T71bQgkc3\nO1BXSl1mjH0TwG9A2zP2AFyBSTzaLmHH8kHSu60E5xzPNq5B1ftTVfsU6c/duDSOb/pALXXN30BV\nrqE7kNivqhFPsuIEJGs2v0ZuMwDAK+a3u9eJqGXgbwSAd6xyxL7XAdlBfaN6+NQ9CGQt1QTRpEyG\n6ulT7Q6ygKz4pPt9gOp9dhKZumtO3P8KW2+Iq4s/+wsQ0JZuDdZ3+3SzOUhkyBHiLRGRZd1GmAvX\nz1oQ5tlp9et9vnUFxCStdt/JYIy1AEilVD/etq2AOo1J6Rqtl++FCxewb98+3HPPPa4c/KRis77n\nNKxW3jLos7OzI5v6DwsL1NXv/7Z+I7NJkzWMBVNgKhxoXAINbxhQ7RNaKuMUbU8ILZmxTQeV1BC8\nVkI4NxW7imblLTQplfZZqaqMhhZoYlwCJFGW6u0VkeXEWnWU1QGPN0xSaFFGzjChpIVxhnKj7/Xt\nUWEmc2Mc0A8q2AGwya+yFGhMN5xMJvWcsSCdgm+ehxpIK5nh7mOnKwvc7aOkRHPWTFpEpJM3LHy5\noVmRrEkmHGbfv3jxZ9xxomtlQzppV5YKL/zoT6sXMKGgg/vKyspNb+ellPo6gK8zxg7qP9Xiu92n\n91qIv/jD8TToCcA77O9UDJSN1EWsOx54noRGPBEjy1yGylYS8puooNJQPXwcowL4lNsMTbgMkmJT\n50qcJ1WEqG7iNAhkJ9hv1/0YKA+aEETSm7jYkUp9DiZqk1LtOVP5AImJ6MmXfgEZPNFVqCYarI8N\nNQ0GhQbz+KVEAyIB3PssBOFt7AQkMM3Tjk4rjb2YlavBe/P77knuS6MoisCx62YH6owxpjQT8TyA\nVegV0yC2HVBnjDnpS79fmZiMHEVR4PTp07h8+TL279+PF154AXme48yZMxPXS2ZZNtTPty4oQJ+b\nm8MTTzyBZrMZaKi2Gpxz3PbnXw0TPQGAGc05AZJuUBDwA0+mXzObtEN9ZRssrFZKQbzRuAeMumXf\nqTsB4Fh4VZReusKZP1dcXc6eC4AqCoBa8ZEEUi2XoaWSw7LJNAIAGyWdMsdwQzP+CYtPpZQuFCUV\nsmYjAcDhAHhc9dVtF5ap9kCemSqtsV+8FCpIUKXnADhEX69+WEtJH0ZTSFxkdBIrd+/xKS2VcfKc\nwjDyzTAhVwoFTm590dGDez5npD89fe2NaY6jh35a969QSSAPAB859e3K9YwSSilnVXmzD+w0lFIn\n3u0+3Ki4kXa7xV98fShIZw5kGjmYec2UdP+7vkV/+w1VNtoB5Ng2cFDUubSkYhSpTR1DHFcsRf3k\no1ZOA/i2a4C7bdeuUFRccYABSaW0P3Ua/RowGkdKLjMO4073s3aNrmtR43bF1Y6lsb69tt0B0hto\nfXuSaXfPOn/OYEU7/lxjRxmzz08++stoooBQGTIm3P9d6WUqhWogN2C9K6fR5IRx53MQKkMOjZvW\n+Q7MyLXKdayy29CC1rYvN+8IrzEpgk3HJGtivMPxCIAfAwF4B7ANgbqNRqOxKe/dXq+HU6dO4erV\nq7jnnnvw4osvBl7TeZ6j15usjdBmXF+UUnj77bexuLiIHTt24IknnnDJrEqpiWreHzz7p0Cj6aUj\nlKUgiZowLi61S6Wi9MiSjq90H9ucEFU9u1QBQ5BKXmVWS97wfVQQPknUAlvL8lvGnvaZ+uESlt3+\nbhhswicZcJl2s/MwUgAAIABJREFUu5FmZYRlmT62MkkgEwxqrWjug4Tuhi4w4egXt18MwGmyqN7J\nsPBND/A18DfguvTJq9p2i+gWjW5eFMIckxuQzxC73RTdglRezWDtKjnZxzLzopDgDftZVh80lkUv\nNgQasw2XaMsyDp7573HW1MC+LATynRlEV6IxbUBUqaAKhW/f/6Lv45r/TalCn+Nnlo5Wzg+EoPC9\nBNTfL1FniztuFN/7f/SLEQC/imbBChxgTL+vlAPwfv/EmEcAPAX87hieDwf4A1jpyumGAtNIelEJ\nfY2VqpyJ/iUtKOsmPyRJNpTkpNtSZGKiGAsnKrbvAeMd9ddNFDB4MhSPx4z71ymnmvhYGhXwG01S\nYrAd69tT7Qbsd+Izse0lmPYAvNt9MrvinJ4kBs8SzvGTn/o7AIC+aqBJWPOu1Ox4g5UoVI4GK9FV\nMxAmYa0vmwAHchToiBm0eA+lypEzPSavsttIWzPocg/6xRZhaQzUu93ue8V6twHNqENFWr9tB9Q3\n6727sbGBkydP4vr167j33nvxwAMPJIvB5HmOTqc+GWIzMY5GXSmFy5cv4+TJk9i5cyeeeuopTE+H\nFb4mWeRJ/dHveqlLsMRJkmIAr/nmXLPpw/IDScKmf4+AxtQxmWWMMw/arXbbDLDKusuAAPkBbTm/\nd6vrsI4tQoA1yGBGH7ilqLi/2Pd5DPoB14YsBbgp+KQz+zNvh2WOz0hRCOp3rqMK0nXzyu0bFGky\nRZkYmfxY73X7P2XsbXuZBdXmHIOW6q3toz0njbJnWPQGd5OAeOm67Hkw3Wjp80rhJyY88/ej3CjB\nM+bY9sxo6JVQYKUCzM+gXNXnbezIHdNut31r//P+bhpW/qPXvhtooJeXl7F///7aa74V70zEdrv9\nfn/TQL0oCqgf/vHIUhfLmtO/R5eekFDKF36xfw8B+IyMH4rxegV9YpwZVgDJAd5a//Kaa6SJkABS\nGvI6jboD2PG9j1l3dw3m/+C+yeo1uK7wBEim/R4i3bFMcswoD3PRiT3dY4BNrDaDvlDw7KKaQFph\n3FPXyDJAirRLTATeVVRFNSXboc+PH33kl8HJkkNfNSCEqUDKq3ilU86AMYUpw6ZvyClINe1et7gm\nOK/3dyJjCjO5d4Zpl9OYI3+vFDuxs6FZd8HykRNJgSpQp7mLN3nMAKhWgMI2BerA6MmknU4HJ0+e\nxNraGg4cOICHH3544IdaVyRjKzGK6wsF6Lt27UoC9EmH+uP/Tc/iAT1o2IGMmeU0pQi7MYJzRyUb\n3bRrdOtB5ImqkFKGrIEFuMIMVJKH2vdGFshnAAJki8Iz9bbrSrmsd1+wiWjJlWElcjMxIImlSion\nuwpmKVLCAn+qSwc8MIb9Opl2lTLyl5b3RNeWkzzQuoP7c3pQbQF86P6iE1LNKkClYIgG7bIUyFsN\n1zeru6cF5SywTktm9Huir/exchjrBkPda+x+PM+AHIGnO5XClBuluwd5S/u/563QGhINu6/+DeU7\nM/CcQXQl8gZzbLosFWAY+HK1RGZex4XRVlZW8Nhjj1Wu71a8e7FZi8Z+v49Tp07hvo2z9eC6AkIN\ne07/dqCS/m4GRwXc2+OHRHBuxip9cZtI31gM2l1jEXgf4BQzMOGT1sow/aqeKw2Ka883yCc+lgUN\nkONsTusO6NUD2whCBn1YHoFtNj7ONR0B7Dr2n35u9nkIVIH0IJlMimkHQumNXbVN+aTb5xBZ6Xj9\n+V8FBNDK+2YXjr7Q43ejBqQPi65sYUdWxaHL/R3IefW3vVrswGy2ijZ2Yq2cwWN3jwa2y7IMnPje\nA3UxbEwBSLLA2w6o2xgG1NfW1rC4uIiNjQ0sLCzg0UcfHWnWdSOA+iBG3fq1nzx5EvPz8xOxgxwl\n1De/4h1MGAOj/bMPNunlKS6MbWI1WSYG+qYdx8QjHLwq9ooqJHHsgAdUl/ysDh5wyrbQDlIFCaqx\nqwwTwoH2oK+l7Za5L4q5gY9BQIGD86qeXTP31WVSZln9Zq7ZdotQJQskN7IowSOfdkacW2Tpk2j1\nueg2/T+VvsQyFlnYBM9GMKGhjL5zfTHseMyeK6kgCiqrCScO1uPdauPtfjz394lWWgU0I6+TTvX5\nlAHp9n/R1xInawlprzFvadDf3KWBPaY0654BKNZ0H5u79b1+7vvfQq/XqyQfbVbT+LWvfQ3z8/M4\nduwYPve5z9Xu98UvfnHg9lvhYzN2u3SF9NkdvcHsaDzsW+BVpyEfkZ2rA9iDYhxwT9tPym6AYCWQ\nvpdurwbUGga7IkGJYxwHlwHsf7Kfg5xnBlhUJicfdLJRm8Q5AODHrimp5w9t28lDifSRsu2pax7G\nftvgQIVpT61gcOl90mt83ZWSgFR4/blfhpQcnElslE0nZ8mYHjv7Msc0FxAyw7ph2Cl470lttGGB\n/XTupcLX+n5cXS+nK6x6q9HFld5uNA1wvyj2Q6jRfm82KKPe6/UmavxxI4LIXBp4vwD1YYz6ysoK\nTpw4ASklFhYWcNttt421LHKjGPW4TaUULl68iFOnTuG2227DoUOH3hGADgDyT7/mGXLzsFJZDQNC\ng/NQiwgQoJsYaERZBfpAaKPo2oaeGNigx7kiSpZ1yGCttVjM/CqpizeQMsoVB5pUkihxlLHh7kcJ\nIOchOePYds9kO4BsGCrqRJNloSe8BfhKSmRTTQ9grZSlkTuAzZtEc5p7Fl4K4bdxXinMBADlRt/Y\nWVYTVzMjnaGSmbzVdCsOVDIj+qWziRTmIRMURZISWVNfK2Xo40RZmKJLSkozKdDXIwXzo1Vu95Xg\nGQPPskAuwzgDyzw7LwsBIRRYxtCc9640h77z/wEIrRmBzWvUjx07BgA4fPgwFhcXcezYsWSluiNH\njuAb3/jGLaA+JDZjt7u+vo6TJ09idXUVh3b0gJ0Yar9YsVykAJiPmMSZBJKD2NyaZoZpw00kAX2y\nwarUxh5flfakAbXulz9XbXLpKHIaF+OB7lrnmUFRMxmonWwMskwMd/T7JXXq5hnIs2hVICGRic/t\ngujbs6gdd0wi6TbVFpXc1EhkwBkggD9/6jNowUhXSk1eNMxYapNIGRS65RQKmSFn6c/LgnQA6JZT\nmM57aBctB8BtLPdDy8arxe5kewBw4cIFzM3NYXZ2dmDtDer68h7zUG+gpmjdtgXqsffu0tISFhcX\nwTnHwYMHsWtXtTTtKHGjpS8UoO/evfsdBegAIL79f4QyFsYBxcAUD/OuJQHZVAaSZXBA2YJmIULW\nKl4+jvXuKSalLEI5jAOuhNXPsioLllh2ZPE5zHkcgKcJoDDLhW6/eEVAagCvfFKndqRhUUIsEPu+\nB4lMILdISoAAfH9ffV/jYknW7cUy/LJfIrMsu0k45RTwAxC9wgBvRSYTuWPPvX97pp1vhIw+uwyi\nbwolkQkAfWDzXMtl7Ht5y7DjbtUhczIbe858KoMomAP6VBZj9xFSoTHT8Lr6KRasANj8AVFI8CwH\nbygoIZ3l5cYXX8V3v/tdzMzMOHvUTqeDVqu16cH9K1/5Cj7+cV3BdWFhAUeOHNlSSelb4WMYUG+3\n21hcXMT6+joWFhZwP66hDgwGMSnLRfCqHGMUPXzCDWVseczAfqWBVDAhqJHX1IL3SmNVMO/amJB/\n+kD5zDj32byuFGkCqqvAyXYI6E19Tu56OZJse0rHX6dvpwmkCcmnbi9x7XGiqCGtaidiZiLw50/8\nKgBgtecltQ0uUAjuwHpf5uiXGRQYpvIqDuoUU+44Gt1yqrJvKkoZySMNm/6BmevodHQtm06nAykl\nWq0WZmdnMTs7i7m5OczMzIBz/l4rXkdDKKWSTiXbDqjToNaFrVYLDz30EHbs2DH8wAFxIxn18+fP\n4/Tp09i9ezeeeeYZTE2N9uWui3HtzMrX/gVYUraCMLEHVB9pBzzDaCqpf/wEVNMeuP3L0lRxi4Bv\nLJGxTHze8IMP43oAk6ICdmHdUnI/KLtjlHSa9ar7Affse/TwVLIMpDLU6ooy++46lebwYh911iCJ\nmwaI0/aCfbOsNmEVjGtWmvbRFEICANHr60qqtrvCu94wZK6Ak5XTMIDo64EszzTQJxVVZVmV78iy\nqrtPSWZiuQydDAAWyJsJtmnPSmWyZiiLsZ7ujLNKImzW8MC9WDc2j61MJ5tyBTQ4eMZQbAg899xz\nkFKi2+3i4sWL6HQ6+OY3v4lf+7VfQ6fTwec//3k89dRTOHToEF566SWMEsvLy9i92zNC165dq+xz\n7NgxHD58GK+++upIbb7fw34v8zxPWtiurq5icXER/X4fCwsL2LNnD3o//ObIRYyCZDuwwAYu/rs2\nRgD7MWtv3wPTAH8kxngUMB/p7Wv17SRZs46dHzQZCJxs6qwpkQbYIyezBrEJ2UvKT74GsIe7JRjq\nCoivuzeEbY+TU+3xtB+j6NsrfUxo2oNzRKDcvkxOQPTGP3/sl9DuNZBzGZiU2egJ4+ZSZsi5BINC\nr8yRNzT73hUNdIscSjFMN1IAvlkB9qv9FloJsJ+K3bt3B2OrUgq9Xg/tdhudTgdLS0vodDru/fPn\nz+PcuXO4fPnypknZVAyTNn7pS18CAJw4cWKsMZ7pgehf1m3fdkDdDuyXLl3C+vo6Ll26NJHqnDYm\nXSVPSumAQrvdnghAB8a3Myv+4utek2AHkAEOAYqCa2YeZ1IY6jhitiObMSgJRbOyqe1WbAxu27VM\nfSroJMICeBdZOLjmPJxIWPBuz2/YcPe/UmDNZnWAFwKMR+DdylaKAsjDSQgzr5lS7nwOaFobRyCw\ni2TkfAwapDPjwe5XLXQfmAWsQhipjP+OZkT7rYTQBZykRJY3KxMWUUTJomYbb+ZQpXDVUDVQZmA8\nrwBm3sgdw18Lqs0kgYL9sDKql8oAgOibRNEpf5yWEnnvdr2f7n9jpuHfz+0zTUEWAo//0b80l8Yd\nI9NsNvHMM8/gZ3/2Z/HRj34Un/nMZ/DGG2/ghz/84chAfZRYWlqaWFvvh6B1MdbWvPfy8vIyTpzQ\nFvJWwggA3R9+q5ZhTck9LKBX8RhlIgb8DKr63giMbkpmS0GmimUSro9mXDCWibUyHDs+MfK37ly0\nn22PTFDq7teAYkAqSmCsk9eMlVw6iH0fB8ATht9NkOy9o5/zKMm3df0a1p9UkinVqGd5mnEHMFxa\nQ1Y1U/r4OFHUyV6iCQDngBD4wwOfQd7T7ZWSo5n5z8cC9MJIC3Mefv9KxZEziW5BpC5FjsaUHq8L\nmaEvwue2UBk6RRWXLG9MY645Wh4KYwytVgutVgt79+71l68UvvOd72DHjh14/fXX8Qd/8Ac4efIk\nXnvtNTz88MP47Gc/iyeeeGKkc8QxTNp45MgRHD58GAsLC/jUpz7l/h4llNbKvn+AOgD86Ec/Aucc\nc3NzeOSRR7bsvXsjQkqJCxcu4PTp09i7dy9mZmbw0EMPTaz9LMsgB2a86+i/fkS/4Jl+cGUNv0xm\nE0bhB2JmGfOERoxR1oHIXnyZaQ3gGQHSKhh0SttQwDiwmM0wA0ySNacSGcqElIV/L2tUwTsNKreh\nx9jXLKu6z9iEnMKw7xa4E8CshM3szEjBIgO4M3ss+a4G4L90dpH6dqrgM3BJs1ZGQ+QlvpsSvNHQ\njDqVC0F/H8E5eJYFOnkAYI08SFjV52GG5fYymNhhxrVBpDI2RF/f26yZa3/00n9Xrb+71arr/bSD\njA2ecy97gQfofLrh3WOin33RLfDw//0NxBHbeXHOcejQobFlK/Pz8w6ILy8vY8+ePcF2y6bfitGD\nAvWiKHDt2jUsLi4iz3Pcf//9ji3r/ujPzAE1YC8C6UAaXNYmZdrtNW0PixjgMySkCINArbVrHEHD\nPZihH95X3UiNtp0eX+f84vpRNwFIna9GT44xATyViwDk2ePdbkaSy5Bnim1jJHeZgMCK9zX7WKa9\ndlWhhmmPk1jp+zYqTjEsnYxq/vzDA58BAPRLjmZuxm/76JIMG6UesxtZ+L0pJUfGNau+Uuh9mnko\ndylkhtWNJloN/f56v4FmyxQ96ueYaXo2vVtUEz6XjQTnxYXR69YwxsA5xx133IFPf/rT2LdvH5aW\nlvDpT38ab775Jvbt2zdyW3EMkzYuLi5icXERr7zyChYWFrC4uLjpc8Vx8yHYLQZjDI888giyLMOx\nY8cqiWLvdkgpcf78eZw5cwa33347nnvuOTSbzeQy+VaCcz7U8rH//T8CtV+MC3zYoCBdaxvT7Ilj\njQjD7YcaAiojTbriZkKQEbmMMkvCgvjp0nYpCB/GtFsGXcoQxAMR+076GmviBz2MrQOLNGBWKqBh\nBkjbBSFcsYlw2VP/r4pCF2sK2tXXqIpCM+nGQ54xn7iqzIpAkLBaCqDR0PtI6fziqRtNcJpSgE81\nAamgOHWysd1VgCkfzRuEQechtyg2+mCck4WZUAZjzwWlDNPu9ep5KwsmFqJfmq+D92CPK5ra9qw0\nxl4bi/SRjHMU3QIP/rM/TH58sZ3XZuPll1/G0aO6oNLi4qID5dZFxg7kS0tLWFpaqk02vRVhKKWw\ntraGy5cvQwiBhx9+OJAwrv/42wMYTh1JDbltn7F6prrSGVkBQ8PAvWXug3F0E9Ic/cYQFtiOJ3Wg\nd5jExl1fnZSlbuz3THJFBhPd2/QEYECXor5UKpfKxOQhxToPuTe6H5Z9zsMxf4Ctpd7M0/1wFxGz\n+TWTHFp4qdo5fWwM2C3ZkzpfimE3iaO/f/evICdTTwvW+2WGjUIfY5WNvZJjKpfolRx5U5+r0zMy\nRdLVUjLkXGG1N+VYeBrLG9PJ9692prCj5YH7WrF1VQHgNepTU1N4/PHHt9zWIGnjK6+84l4fO3YM\nL7/88pbOR+PmQbATjNgpYNJ+47aoUqogUl1QgH7HHXc4gE5jkiWyh3mz9974EyhCOTKoChhlSkFl\nHlAqEODrOx0w78HxUkDxLMnGV50WwrBSGkXAu2PYpYBqkB+yTWQliaXBw1eYE6S83lNV4KTQlVhJ\nspW/1qrOEZn3a6+ElBrAI6tm3BtHFQBgzWhgsg8UpUL23W03k5xCgpkJBYMG7qHu1txbuwoBwNlL\nlgJQmmUH9GSCW8Bv9pGFThblzUYAlJmTB+nEVqWMnCV6YFnJDODlKjqBVHoATqwolRCu+qq+Deaz\nJffOThykUJXCTVRW425zKXHgK/+8+tmYiO28Nis9O3ToEI4ePYojR45gfn7egfCPfexjeO211/DJ\nT34SgNYxLi8vb+oc77cQQuC73/0upqenMTs7iyeffDLY3vnJd4azowAs8LSe45tKGE2xpKPIXuK/\nR3Bhce+NAOhrwXxCejgQpCrlXW7qEg8TAHToSoXFzimyYwADz2j/Ewy3i5SLy5huM1QuU5HJ2O0D\nXHF00yMw/wPZ/AiE22PpmBpPolLsPJXHVJh0few/3/8rYAA2Co5WQ0IqYKPPsN7LkGcAZ/UT12sd\nj1um8upnWkqGTi/DjAH0G0XmWPXrnRxzLf3+2kaO1lyZBO6F4EmDmmERy5JXVlZw4MCB8RvaQlgC\nZpIkzLYE6gBcAtI43rujhk0oHcWfU0qJc+fO4ezZs7jjjjvw/PPPB57NNjjnlcIrWwnbXiq6P/wW\ngqoyFkAzynpzcBUCO7ev9S9XhoE1gDwIC/LhB1xdgtsu4fpgSkJZ7Th1kEmBaPBg8mD7w8j53KBr\nBzQDwkKPXjuQkf8zaMvIWvssA8pZTkB8DpRlKJ9JPZDy1LVUE0pdyWdBrj+rVhGF8XrnTT+hUFJ7\n5bIGhzTfe1e4SUl/LsDp3fU5FSAlmNGss2AVwOjHTSJssOgilZ6wNhthe1TuI6WeLBidem6qj9p9\nqHe6Zt8z/xswWvzAqaZvPd9zLe0n1U7jJFXr60617qmIXQI266EOhKyKjddee62yT2q/W1GNRqOB\np59+Go1GA9/+9reDbZ03vxv+foEKe2tjFE02DQoq/f4jSFxSSaPm+zmIuQ/eTYyRA6NutY/8WJNS\nG3osAcDjsfE1505cZ7gya167CVSChKD7U1Ipek7ofaPVkq0kq6Z06tH2+F7WJs0CUHYiMej8MSnk\nusXD/4Hwlg9KRk09/xXwe3t/2RXZ6xUMfSNd4Tz9jSsNB3VtLUeeKcey04+4lAxNAMudHI1cb+j0\nOGanfB+ud6pw80q7hdKsSC+vNyoa9XFkL0BVxjjueG6TQWksLCzg8OHDQ6WNNo4cOTK2WQBjbBeA\nHQCupJxfth1Qj713J+3QAowG1IXQVkJnz57FnXfeWQvQbVgGfFJAvY5RX//xtwHDcnvGIvNsOfwA\nE//MWQXM25MlGCKqQ3ee5aLyQHVyGgBQHLbqKZPCta8YBzKAi8L8HQ3Kwu9LlzxjLbyy+8vIrYUm\ns2aJn4QD72QApElbdmCgWmx771P6d8rAO919eD3IeciO28mLOVaDXwLcLfNukzijgk6BVr4svIuN\nMtuIZl0JYQB75vzd3blN2IkAbzQAzrR8yd5Pu09f2zJaH3i6AsWN9lj2/e8zazYCoM04M4w7N+cs\nCdNu9OnNvOIyY9n3fEqvDtzxP/3vGBSTBOq3YvLRaDQGr14ynxDKIuacRrCyNyQ0qRAmAo4C8OOk\n0Zi9lyYXqHIJRppTW/DHnaDKMtcy0fSwBDM/qk4+YJsT/U5HPYBPTZoseK840NQw9iNXYqVBmfcY\nxA5hvAMgToiqQSsqFfA+yO+cMVTY/lQyahwx6+/aQhKw/68znwbW/aY8A4T0j/CiZJASaDX9udpd\nXrtYsFF4MH5ltR4PWZkMALQ3uGPVU9ET2abYdKAK1Me12h1EoAyTNgIa6Fs3mHGSSQHsB/ACAM4Y\nOwPgTwA8BuCcUurytgPqNMatZjdqDLJoFEI4Bv2uu+4aCtDjNidVRSsF1FePfw+cSEksgHaA1j7w\nAMdmAn4Q5S6xlJn3lWfj4QddrgRkLKuBkWbEg2hKI55g46EUZKBhJ6x5jYOMymIAbfY37bgJiX1A\nsvRnymwbtu92zBElAvCeRQNjxQISepCPvd8BD6TLwq8AkPvBLItvQDyDZby5SxhSogzuhe2zZXOc\nzCZv+GMUC1lyq4NvGE27sZW01+CKLDWIXSYQFFoCtEUkb4aFnpx+3twTuw+9V7F+nucZRL8ELfyk\nbNEnc4206qoF8rad3f/gdzEsaIGMWId4K97dGCQFnH3wOfe6/ZZetRiFOR9FUz5qddCw4SormtJ0\nJyUubrIxWJoSALO66xveU7PjAM21aUXBSEFqWq1lkkeVzgABGK8WYRq8f7XfqU4mmPdYLhMng1ai\nuuIbPAeHyGTofbKrLkkbyGCVN75GS+rUSGMqfTDtmO3/S/PvADI8vNcHpppArwCmieJvo8/RNbxu\nVOcPpfDadQC4tqb/yBMcY6enT9YrGJp59Tt0dTXD/Jy/N+eWZ6AUsGduc7gtxahPykd9mLTxyJEj\n+PznP49XX30VS0tL+OpXvzpy20qpHzLGBIC/DeBlAC8BWAfwT4BtyKgDCJwCer3xlk5GiRRQF0Lg\n7NmzOHfuHO666y58+MMfHiuJdZimfNyIpS8rJ76vtfWJxMsYHGmmxbxWEtLIZGSs64QG+TEbLyuD\nFMBkCcXzgOliIHIVwA1wTBRk+Zew8e4k+vNlUnuKUzZEuV3K6kCnJJigTD/dpJNZ6dK1nYiohLsM\nKwvPvivlPeFtDNCrJwdaQIN0OlFzlecYUBT6Oq3to5JgWdM/NGDsImmxJqnIdviEJ/tx8UiLzri3\nrwTAmGfa7ZVxZw8JMOZfW8mMs2VshLaNvuAThySsvwPxZmIQVEAUPlHUyloYD5NQgTBB1SaVKikx\n/+o/qX4GiaArWbcY9Zs76vJ45h54Jvi7/dZroydsGkcW58wykuY91U4C+I4A8EcpXhQz86kVA9+P\n0fqfgt6eAInHehaCd9Qw8kiD98H9Su9fe19SCasDjqkD78HK7KAVGytZsUWiKgw//JhJt9dOFBPn\njyd5dcmo8eSlzkFGIgDr/zj/RYhCodnwx5Zl2NT1VQBQyI1Nbt0tKUqGPFMoBUOnq9+LfRBo9AoW\nvJ5qKLQ3ODob9b+Nq2sN/JtPtusbrQlKugCTH88HSRsPHz6M69evj9We8U9/CcBbSqmfMMb+nlJq\njTG2A8CHAFwF3gdAvd0e/8MeFhSoU4D+gQ98YGyAbmPSQN22J6XEyukf64eb4m7UssusDFp6oqCX\njRU4mCL94BoY0qVGyfTyLVciZLmJXEW/wdyxirDYkhtpC+2wGYR0m/5Xb5NcGc/CpWEGb+0YLFsa\nzTxNQrXAEvBaeHpqMtApwlAo5wsfDcBKQlEfdqqrFyJdyAkIXWfs35xMRKwzDRAuW0pRleS4a9AO\nNSzlUGPAsRJCF3NS0hWOCrTuQKB1Zyw3fu4GENuCSUqvRjByDv3MMRMkI5cB5+B2SdvpzXW1VHAG\nbnXzQOAuA5PEqkoB5dpSriAT48yz8X1zfmcTKYOH7Y7/sqo1rAsK/t5jlezeV2HH3VFWKClwX3vr\nWGV7yoHFjmujAnwaDKNJYwKJh3tzgG7ZrXKOtjqQSpSt9sGOoSmQXd9ve3/8vjVa9lqmfzIAPmbU\nK/cyyj2y9y7cbwTGOyk1STP8gxxl0u0kzp9KirVB7YSDA2vkMZw7sP6P+X8SgPKiVJhqMqxvKDAG\nrG/oxWA7fPb6eruNXh+Yjkyxri7rXltQH0dRMrTX9WshFeZm6r+Ty23uWPW6yx81YkZ9bW0NO3fu\n3FqjNzZuA3AYwBqAMwBuZ4ztVEqdB/DndqdtD9RvlPSl3+/j5MmTOH/+PPbv379pgE7bnKSenjGG\na9euodlb1WAzGmQ1OyKh4PXqCpkB0mHyJWMWzIcPDBENNvaHTtkXOxGwYX+HlcIjjINLaDbesjrG\ngYDJEoqFD1emPIj2DWk2PgDpAJxTDMsq561MLmy1PntBtkiP3S5FZPWWuf+ZFFX23Z5LECcZpyG0\nAH6AYwzrR10xAAAgAElEQVSgj8kTkxI7sas44GTeAx4IvdkBx7Rr28dGAHKVWX5lxCWGZVlQlIle\ntyoL2MJH+nj7nSG63EKvbvCpZnBflFTOZQbQchmAOMXQ+y+V66OUEhmR1QBwPuyMM5dwOmrQvt4C\n6jdX0M/GjuejAHUaOx4I3RcocB/FW73SJ5oMWcNAJ49DWuKRah+w41+9JMONxXRFscZRhkadTl73\nsU73nZam0EmHf44Qxj1ofIsA3pFFZH+lvJEBIXEC2cyQyY45xDcZMd5Jtj2QnNRJVfz11d1X3wEP\n9JMTBUrc1OUxxKCdc/wj8bextmZXSIH1LpBlDOtdhSxj7pEiZPoRVJQKGWfobgDdnjKLtQox1Fnf\nAHbO2tcMnW49iAeAK9eBmQFmfJth04EqUFdKjeXO9y7EAoDfUkqdZ4xlSqlFxthPM8YKpdTbdqdt\nCdRt3AjXl7Issby8jOXlZdx3331bBug2JsWo26qsp0+fxqMH9rshTT8kQmDLpXCDq5W3hKyD2U+J\nBIMR+qIzZYZbqnMHnP49lrzYwVQxbfvIpIDkHvT6pFDNxgcV+ix+jRKRuCJMOlkBYEJGDLt9uPJA\nekP18E4jz03bJpEVnFdZJ8N6h843of5fJVxhnAyH0+tFOIDzLLSVtNuEqOrR7XntPlKm3WaYAhPQ\nXuuRe4Jlw6zePUi+Ilp1AFCFTsq1vuuAKUQkiE2mEGDW49xKlhgz+npzzkyDeU4LKmVZmOQKn8Dq\n7Sr9pWVZBqUUVCnGZtNprK6u4uDBgyMffyvemZgk8RID929961t4/E5SuXoII00B30ge6lFC+NAY\n0aGmIrWp6fcgnXw86Rg5YbQGvNO/FRCM+ZWCT6adFMDVBEokDYklJTEwJWNoMsG2jg1H+HwKzoEq\n2w4gdGOpK7xE2qLFlhwIr9G1D5TG2Ouk50lMFP7ba3/TvBLIRszMpCrMXl9husWw1pbIDODO8zr2\nXIGTc1xfVWgOmEtfSahDltv6xHPTW6PUJ5nn9w7FbuhlkvNKOSlDAWCD7rStgfokGfWyLHHmzBlc\nuHABO3fuxD333DNRf86tAnWlFN5++20sLi5ifn4ejyx8MBjGKbuhwMCVgLAsKM81uwwFu7RmdeaZ\nLCF4I7IryxyI9+1KKEZmsjb5lPsHhWIZmDKSl2iJMtbJAwCHcJOL+OcrmZ9YeKBPQDrXEg8tu2kG\n+koGZSwlSYlpfRONMw182+b8MmbpQR5CsaQFqAHv9jgOyNLZdlH2RDEGZldW7KHBgz4Lq7La9xnX\nfS+Nvj9VtVXJQOvOzDVbnbqWtsBXVwW8zj12hQHA8swludo2NDtvijMZ7TmLHGEsO27dZcA5GKnW\nGlyXvZ39UldVVRJ5o+F07vrStM5d9PqY+Y3frtzvQVGWZeC0dEujfnNF7OI1SeLFyhY7nQ6usH24\n9957HemyeuL1QZ0a3riVXTDP9o7M1o/Jzg9rbxhAdW/VtledYNTJVeJxOgXa9d92kI0sJO12ngW/\n/7See4gMJOjXAKkMMHg1gkoeUxOZVFJoBUzXg3AAvtCSbzTqv5UzRs41CU37q5f/fX3FBmALqSpg\nvSgUylJhaoqjKBQaDYZeX7e7saHBeWfdlt/Q7HtZhhr3OM5eFJiaSn8v2usKu3cCV5ZkJKvxMpvO\nusLcdDjHGTeKosDMzIy+biFudjYdAL4H4JcYY08D+CaAJQD7AHyf7rQtgbr9cCZhz1iWJU6fPo2L\nFy/i7rvvxosvvoilpaWxkwaGxWalL0opXLlyBSdOnMCuXbvw9NNP4/qVy1qeRlxYKkCXbLeAlCaR\n2uOELzPpWOtKsSKlQKU1dj/NkGceHCsN5t3xlrEPALnpsywgeNXlJU5AtSA9SEo1jDwXpb5GJc3k\nwC4f6sRW/bAj9xISyHz/lL02KXyCJkn6sQmtqjFVWQYOpDOx84soHcNvz+GlNSVUysUGsMVBfcKn\nBd+KgPIGYROUQuCBX0qgQRJO6f+Ak8s4qQxl262m3OjdqcOMSxpV3iIysI5knLjMGC924y5jJwCM\nseA+aXZcX5OSyjnIcJNUmuVN375UUBCY/sI/xLgRa55vSV9u3piUPNAC9PPnz+Ouu+7C/Pw8PvjB\nDwYrozsPhoWVVk+8PlLSJ2CY7lFcY6iUzfw9CvjW+6bAczjKD0qMTSa+0n7RP5NtjKk3pysLdP8a\nzXut7ztChtu3U7N/4j4NZNvjpE56K2rar5WpAKE0pq4dSqbU9T+aLARe8YQK/2/O/7uuC1JosF4W\nCtkUQ78v0Wz6c6x3Bda7ApzBTYazAXIVIPQqsHF9WdSy7TTOXa5H4KtthYwDl68B/9FLm88rpOP5\ne4F0UUpdZox9E8BvQNsz9gBcARCU0d6WQN0G57yytD1qFEWBM2fO4NKlSw6gW+Zt0npyQDPq4zjU\nKKVw7do1nDhxArOzs3jqqacwPT2N8+fOGYbcg+mYodEgmlb8zPR7iUz/5CBNVxvtwAEFGTDqRt9n\n7r9kzNk7gvmlRgbltO520K70D4SNp4OeTRy1mnri3sCUhHROLF5aQhNbaf+YLIGEhh2oJnK6QVkJ\nZwHptPcWvNuJg5V3cMu+CyjeDAZXDzh1ASSqU/SThRIsy8IJkjA6fXtfXYJrrGUvDQsfJ7Ka1xym\naJO1XMz8PpbxkjoR1LLljCdWEGwYZxqW5/q8RNGEotRgPW84i0m9jdhVKkk8wDL3dWPG6YU3zPcl\ny7QchjM0P/0PsJm4kXZet2KysVVGvSxLB9D379+PF154AXmeY2VlZaj2PQbuKye+X7NnWmqSCsZG\nk66kKMbkuJwCpTX+6QO911PSiujv+mscD0TXVmmtS1QdwWmm4kU/gGnXx5L2WHgvNLMf3v+KRCYh\nUwnAdF2hI7IyUHGOoe3Db4vPpwv96bHz197462Cs52QoFnxzxrBhhBTdrnnP7MMjYN7rCUxNeXLH\ngvteX2KqybGxITEzzbHeVeh0RAXYdzoCs7MZ+oW3ayxLYGVV46Xp6eoKs2XyJxF0PB/XQ/3dCqXU\n1wF8nTF2UP+pFuN9tiVQH+S9OyyKosDp06dx+fJl3HPPPXjxxRcryyc3CqiPKn25du0ajh8/junp\naTz++ONuqefMuUsAcoIaYZxcfDKNZnv0ZmnZa0gIArItM1NJFlUi2E8f6/XroR7RNeaOldyz6/Zo\nn1zqBx6pvG+v3Z9L7c2uB32/P/1oaMKnY+i5Z7kpc6/PqbdxUUJG1ozmBvkEqUDDaFYVEsWRNEiv\nur5UmP+M3i97O7hzT1FBIqW2jlSmfX2QXuFgqURUyuKLUk80sjx0lLGvbdKpBcqM9F0Kn7DabLpE\nJmZlNWbyExRQoiy9CjOUrF6d2e+QqtIzyvi0s0YE5M35aNVUDdrHSyyMY6sFMm7FjQ9qDtDtdsc+\nPgbocV5Rnudjyw53HXw8+NsC92GOK0Go4YA+yc7XNZeyJiSrnPbvFAh2K6qp8wdvEKlFhX1W4Xnc\n2JmYLAxMvgz7P6jvrs/UhKCymlHjpkITdcn2pJc7kWlWJjlxs7G8pa7QUuW+pEG/uz7a7wiwf/Z7\nP+d4GClDzTg9JBVS6SexxU29nsDMTI5+X0ae6xKddon1rpHFjqB97/WVA+l1sXRdg3sA+Js/tQRg\n8xrzuHjde2ksV0qdqNu2LYF6HHXeuzSKosCpU6fw9ttv1wJ0GzcCqI/ysLh+/TqOHz+OZrOJRx99\nFHNzcwCAk+euAKguO2rAHM9gOYT+Wbp9JMJBnkFqD216lArBvGIMXMlAQmOZeT9PMGCaAHwKeDl8\noilgB0UQGYwH6V6CE16fIEAaMAmyPLwWxTRIFhELbPWHIm+G7wGgCaV0G5OiOlA7JiS2hfQsOoAQ\n2EfgPXgg8SqAtyA5SH6V0H9HOkoY+0sA4WoAKRLk38uCh1wQ1jKSymwAL5URBlRTq0qnq9crGUoI\nfx+iFQG9m5mQFYXTtNtzKCWdmwxjWtNOde2WDbr8b/wCZtttzMzMjK1JjIF6u912v6tbcXMEBeqr\nq6sjH0fziuJVURqTMB2IgfvRo0dx/+5Wzd46apMF6bYEmI/BsydgEgA8eibUJZHqfRPPyDrpTIql\nJn/7vKQQ9A4F7/ScEeOerDZL5TJ1SHQY+B2g4/fPtpprjiYn/iQhsK7IVWxUKqTWaNNp33kejPlf\n+Mufg13JcByLVI7t7hcCzUamD8mAohBgjKHZzJw8pt+XAZO+vl5ifb1AlnFwS4oNkcXYsKz6yVNd\nNFsZGkYWI4T/jm/0FGamGd6+UiDPOTod/bz6wQ9+4BJCZ2dnMTc3h7m5OczMzIxUtX27Vpne9kA9\ny7KB3rv9fh+nT5/G22+/jQ9+8IMDAbqNSWjf6/qZiuXlZRw/fhxZluHhhx/Gjh073LbjZ5cAZOBM\nQkaDOmfhABwzJtyw7TL6GjAww8YbWQgkRAz4lW43qECqFATLnQTFSziih4OT2sC4vhi5SuJBkanS\ng3T6viwhWQj8dOJrKINx+5IfOX0ousRWwIxyum9xVzSbwysVUz0rXwRyHWZGxWRRJ7uPKEPpjJPU\nGAaHfpyx5AUgzD0ttmTAP4d3i4mWTVlZ+m0uaYtq2Q1oyRsR25R5yYwUOmE3YLwNqCf76CRTM/FS\nEjxvOIcYp3svC61tb1DNJUlyBbQ8SEADeWk+WSVx5V//BXTbbVy9ehXr69q4d2Zmxg3ws7OzmJqa\nqp2ox5KH94Cd1/suxrXbtXlFly5dwv79+2sBuo0btULa3LfgVjsBYHnxB8E+IQhFBYTq91MAMiX9\nqAH0wwBo1A8g7Meo0pn4uJS3d+wSUwHddS4vdJ/KO3RjBGoTbHt8Hr1fjYwo0q3XatsHbSfnT5IU\nsZ6dnG9Q322f/+53/yqaTUBJhbIQyBsZir5Ao6m/70VfgHHmwPrykl6RyjKGbseeQrfZ6Viduvl8\n7Mq7Ug6sA8BGt0RrOtdJqjXg/fKljeT77XaJqd16vD17vh9s+8zPdQEcglIK/X4fnU4H7XYbZ8+e\nxfr6OqSUmJ6eDsb26enp0ApYyqB43SQZ9a997WuYn5/HsWPH8LnPfa52vy9+8YsDt28mtiVQH8V7\nt9/v49SpU7hy5QruvffekQC6jUGgerORkr6srKzg+PHjYIzhwQcfrBj3v3X2uh+gLcYjoFyo8OEU\nO79YbbkCg1IMjClwSCeJ8cekHgIKEn5Jj0psyEk0Q+4y/DmUYshgGHbm24IK/dZdE6zq08ugUGbN\nykAviP+7raJK2XhfedBaTnr7QXsvGGSgtdcbzV2243Y88ZGAyL2XOXWisW4H7uFkjyX69uC+KqWf\nEdCWmTHzFXi1B300rLWwTjNZlWkHtFVlJVk1Aul5I1jqdSGlf5jY4kwVDafxcKdOONYlxlpWmr4p\nq3tvNvU+NQ80lFYOk7tTQCrIf+8/w14Ae/fuJV2UWF9fR7vdxvLyMs6dO4der4c8zwOGZnZ21gG0\nSdl5DRvIv/QlbRt54sQJvPrqqxM55/sphgF1CtDvvvtufPjDHx6JhbvRK6S9Xg+Li4tYXl7DgQMH\ncOedd7pn1PWTP3THjO7hPoCNJlGnI69o44Fa6Qc9r2+3hlkdM3F1sJZ/hGTVeKWv0v4Atj2Sj1Tu\nX7B/AkCDTDgiX/d4u96HnNdtTyShBr7s9v9q3xVj+MU/egmc9dFbt12mYNrr1K0MZoN8b6yTy1aD\nttPrSbTbGnxnY5IdGqTrYIxhamoKU1NT2L17t3tfKYVut4t2u421tTVcunQJ3W4XnHM3tpdliX6/\nj2azOVEZ47FjuvbC4cOHsbi4iGPHjuHQoUOV/Y4cOYJvfOMbt4D6OGFZGDoI20JFV69eHRug29iK\nBr4uKFBfW1vD8ePHIaXE/fffj127dlX2/9GZVViAlTEZgHLOUmDbD1yWwY4Z+BjcKzDHytOIwbwC\nA4cMkjlte0oheq/KznNSFIlq1SmAl4adzpSukGcBtz1PoI+3IJ261tDzKekkM/SZwyEgmQfHVKIj\nqXYb0K/tBMI4yNB7y0URVm11LLI5vJK0Cp8sm+UuiVQXpGIOwFdkMraPpTmW8zRzY+aAcSKtfUww\n5xNvmXbute50Apk3yLas+pAstbe6T0yF/psWS7LuMhYgCwE07T6mTSGckwy1krQhPvF3kQrOuQPj\nNIqicAzNpUuX0G63IYSAEAJzc3M4f/58xapxnBg2kB85cgSHDx/GwsICPvWpT7m/b8XwsONtHaCm\neUXjAHQbN4pR73a7uHDhAq5du4YDBw7g4Ycfrjw7bjvwocqxFLzHMY6FIo1wFTEBxFPgHahli3Wb\no+nH65nmqozEvUzIXKod4MH/dWy77Vuwolw5tnquQCJTI3upHlS/gsHoxKKOZU85xiTO94t/9FLl\nNEW/RKOZV96bajUq28tSILOruaRwnDuuZ5I/Zz2JsbFRotXSx1hWfWOjRKfdD45vNv1vr9PuY36e\nFOQDcG2pcEmrQqiR80MYY5iZmcHMzAzuuOMO974Qwo3tZVnijTfewBe+8AWsrq7i3nvvRavVwuOP\nP46nnnpq0zVvvvKVr+DjH/84AGBhYQFHjhxJAvUbFdsSqKe8d3u9Hk6dOoVr167h3nvvxQMPPHBT\nLXHneY6NjQ1873vfQ1EUuP/++2tng2+cbmvGFgqMKZQEXGsAHoLtDLLi6JJFUhnGqsDdNBAwyAwK\nwlguuveYcsDdMvMBWDdHZhCBzMbLU0pzKg4ogDMNOqXSMhmnl1dAyavsZ6a0DIYGVyJ4zw6Uet9w\nkNb7e/DuJCxGBuN08DWJqbG+H1JAGO92+4BI6dHpgy4rexUG3bWpJKz7jF4tICsByMBEAZk3zQMh\n8TCRtHhR5gC4AkgyqQHWnGjWDRD3WvnMJKX6gkaOjbda9Myw+YyFAN9eUtEn7jSGobeDZ24mBaV2\nhdHbzWdPElTVmIl/gB4H5ufnA82iUgo/+tGPMDs7i6NHj+LLX/4yTp48iWeffRYPPfQQfv7nf94N\nzsNi2EC+uLiIxcVFvPLKK1hYWMDi4uLY1/B+jTof9VES/0eJPM83laRaF/1+HysrK7hy5Qruv/9+\nPPjgg2ORO+OCd2AEFrzOlcRuH1M6E59zqH4cIQiP+6ffCEHpWLaQlnlOsOxxxdTKsSNIZByTn5D3\nKDbMGYZVwbo9nzuZXTUmn0ts62va+Vu/9zyADlpzLTSbOfr9Ek0DwFNgnSaX0u29XompqRz9vsBU\nqx4K9nolpg3Yv/p220lj2qtmAt3gSbBvoyiV06kDwMpyD61pf74vfKqfOmzkyLIMO3fuxNzcHC5c\nuICnn34aX//61/Hrv/7ruPvuu7G6uorf+Z3fwW/+5m8mSc9RYnl5OWD3r127Vtnn2LFjOHz48A1Z\nLd2WQJ0GYwynTp1Cr9fDfffdd9MBdADodDo4fvw4Op0OHnrooeALEcdfnuq6QUMZLbmNOAHUgmYw\n/doGZ7IC7kG2W3AZM/W2/UqVOeXfNw0E2217gmXB8UoxZEygBGF5oXS/6SqAAjJWRiy+TqDikCij\nJFELun3/vFawZNXE0Zh5p+x87HLDlHLg3e4b+KcrVKwlGaSzkdSSIf8/AHBZVqQz7rVJXlXBpEJo\nTatLUk3kXxiZDBjT2w17Tu0puTAyF6GLP+njiNZd6OJJynqz22ukvx8L0LmeVLnjpQSaxGmmLPQ2\n15b5zlHm0xRjssmpQSEmc3NVKWvZ9HHDap/37NmDT3ziE3jqqafw9//+38eXv/xlvPnmm2i1BicD\n0hg2kL/yyivu9bFjx/Dyyy9v/QLeZ2HB7jiJ/6PEpBh12q/p6Wncd999uOuuu7bcLgAsySnMzs5i\n3759AIYDd2AQC24BZhW8hw2MIZ3BAKlNHUgeNVGVuMvUFS6qVPusJGXWnAfm+VRzXGXFMAGubd9S\nspdg9SCafAyrhJrSsP+tf/qs+3OjvYENAIwz9NZ7phmGDfva3MMuELjA0PG72zb7GH16sB9jaK9u\nIMs41td8+0JIB9b1sX3HvBc9ETDqNK5f17p1zhg2uiWmprItg3QasTFAp9PBSy+9hOeff35i5xgU\nS0tLN6ztbQvUe70eTp48icuXL2PPnj04dOjQRCUrnHNIKbf0gFhfX8eJEyewvr6OgwcPot1uDwTp\nr5/cMMy4+VHBg2ehGDLKWliQrgChQgbcgmSY42PtOmXqXTuIl089mE8lsVa82yGhFINnmLVcRijN\nmCvF3HEWkNt+cUiUqlGR8HAAwrDw/tzCM+bmPoBlhs0P7wGYSVaNB+fIvSa8Dq2vr7BSEfsOEJZJ\ncc3GG8mQ1vObw4ztZNAv85qZyq723tsHqVQczBRtcl0mvdUAnUMxonM0A7CtK8CkcNIcxph/UFgw\nXvqiTB4oN5zuHgCYKKAsU0+tH4XwEhqqaadsV0obn+XerpGy5lJLetDvo/y3fiXxqWw+aKK51TQ2\nm0089thjEz2PDSuJeSeXTd/rYcftfr+PjY0NfOc73xk58X+U2CpQp9p4O3E4c+YMZOzosYWwzxsb\nKdYd2BzzrqM+kbXiSOPG5wGMOFArm0m5zASrjzGYrXlux+3ShNzUqkHd8XQiUbGorJHGVJJdayYX\nQR9J/4aCdjKhYkrhb3zlSYhCBPXsyqJE3qiHcEW/QKNZJXF09Wi7Aj7cEU8fU8+Yx9FZ62PnrhZ6\nvRI8Y+i0+9jolsgb4ef12X979Loxo0RRFAFQX11dHUujbnOIaCwsLODw4cOYn593QHx5eRl79uwJ\n9rNs+o2KbQvUy7LErl27sHPnTvR6vYnryq2l19TU1PCdo+h2uzhx4gTa7TYOHjyIvXv3gjGGN998\ns/aY1xb7AJGXcCa17Jgw4QIIgDVXDIIMhjYh0oJ0pTRbTZlyQA+gUnH3vpQ80L37pUIEYN72C8oq\n1o1e28hlADiJje6/ZUxrmHQTlOmhSa9U655i8/2EQFTOr++HrDDm+r5V/eIZVCCncYMo0+8DcACe\nlsXm8A8Qp39H5o5RzpmFMFpKab06z7S0RWlg74C/1DaQFLzb15no+zY5AcL2ASbD8zIlIfMpM6HQ\nDx9uALhtlxZAUkb6wpRybSjO9S5K6n95rp1plAplLu7z5q49V6yJZ/pmWaDfiJaTywLFX/vFyme1\n1ZiU7+6wgdzGkSNHbiWSbiJOnz6NCxcugHM+tgZ9WGwWqFP7x5jZH6cuxigxSnurq6tYXNLJdQ88\n8IDL09gK+663JXJeUGXQa4F7TRJmcA6w4P9afXucuJlgspOrBikvdUKo2L7UudyM5ByTkMW4fkb9\nc/tEgD52igGAT/yjBwDojNGi5xnoZmvKgXULootegcbU4NoSZSHQmEpP2HrdPqamQ3npRrePVvze\neh/Ts8Oxz+p1LynLG37//+o/HHro2LHVKtN01TOOl19+GUePHgWgZYwWlFsLSCttXFpawtLSUm2y\n6WZj2wL1HTt2YHZ2FlevXkW7vfmStHVhB/dxgPrGxgYWFxexsrKCgwcP4tFHHx1pAvHdEyVi55Wk\no4sdDwz4lkAAfBWRuHhGXEEo6vpCwsmDNWte0Ry6FUnlwbziEDRxhwECLGD4Yx09oDXzbrCO2P74\nWjOIStIrg9KSHoSJnZpJJysKyrL0AiLQy1vtuYxkMHbSI5JFoaBkYEnJmGeGqF2ka4sw7/59r3/X\n1pAM0pU0DR92mZLJJFUFy85XfdztoM+V8L7rpj9W766QGQAutd7dnFPR81h/ds6NTjy27GRw9pC5\nPo931cmdrSSACKCbe2F82dFo+u0AIASKf/U/xo0Iuiq2Fd/dYQM5oBkb6wZwK5l09GCMYe/evbj3\n3nvx2muvQQjxrgJ1IQTOnj2Lc+fO1fqz25yjSQXnvBaot9ttHD9+HGVZ4oEHHgh0uFJKzO6/H0Jo\n7+wsy7C6ugq+ennoOWuZ8Lpn1phSmTodOk3+jJNAK0CX/l/TJu13suongNizPeVyk3SOcf1i1clA\n3M/g3MP7zgBsPPhR/M9/r412W//7L35b440sy9Df0Iy0/d+Cdfe3aStm1ZVUDtAXvQKMMzRbw52v\nBrHq3U4fMzumoJRCZ20Da8vraEzlgUTGxo0A6UC6yvRmNelxHDp0CEePHsWRI0cwPz/vQPjHPvYx\nvPbaa/jkJz8JQI/xy8vLEzknjW0L1Kn37qQz+oHxBndrz3X9+nUsLCzgkUceGZnh//ZxARW7s0Ra\ndFNo3v3NI0263ccmdbpET6ZQKh4w4irSnwN+ikDBPN1HKhjAn5482LahQllMLLOJIyMMhwQz1dMU\nSlUFzCnHmoyJisOM1cWLylefG5aeSFds5VYWyl0AOK16ylJS971EyUO5jmXfU23Z+5zS1lvwDgAi\nIxp7U4GVsvNKX2QV4Jc64VZPChqufecDL7W3urKuK/6G6X2F3ldlWnaDRPY8K/pacsPhGXflJzTW\nVpKVhQHoWfrhZbWZBrgX/8p/kLzHkwi69LsVRn3YQH7kyBF8/vOfx6uvvoqlpSV89atfndg1vB9i\ndnYWnHO3kjkpS01g9LFcSomzZ8/i7Nmz+MAHPlCpcEpj0ha+WZah3w/1vN1uF8ePH8f6+joeeOCB\nQDYppURZlg6g53nucqE45zh48GCysNfqmZ/U9mHchFX3vpP78ZARj+1fyXkou11huYd5uA/sN9F/\np9pMsOz+OmqcY8hxtf0k0pg6nX08ZsvHPoYmgN27d7vP9mu/pT/bbreLtbU153Zy/fp1/PY/vRfc\njLGylOA50O/20e+G3xsLtjfWjWacc6dxb822Ala96OvvMGXV+xsFGGPodnrodja8Fr6j2+AJcA4A\n62s9/A+/Onrez7gRA3UhRG39nM1EinF/7bXXKvsMYuY3G9sWqNsYtUjGuDHK4G6tIAfZc8VBgcOf\nvqk5cUkY6YwpSMkIcDVsOZ3Ic1UF6gG4Zw7YOkmwZHE1d7Oncjpy3biR2AQTAxmw8gBCMK/ofpFD\njeNnzvwAACAASURBVAHI+tpDxxgLWu21cCYgVFhVVZ/Lu87Ya7X2k/G+9px0cgIgAvUZaVtCqATD\nzhLyGOX7XrKqz7vWw/sKq3TVQReAygLwrrgB9Co9sDPOwSJ2Pt6HK/0dFUbeIpkH6VCeXZd1endj\n22hdZQKpDOCBvCyBvEGsJD3Tb/XoTnZD5TCM+yqq1pFGlPqhlU9ukB0lVlZW8PDDD2/6+EED+eHD\nh3H9+vVNt30rcMOIl2FjuZQS586dw9mzZ7Fv376BAN3GjZS+eF/2Zdx///1OOmn7am1HAc/s//jH\nP0a/36+1+7Wx84MPJd+vA/C1LHRNYmpFLlPj6T5QipPQk1cqiEbtDetfpVopkGTZK2Ddbaxq2Svs\nfYJhr7t/5cM/A2bG0DgPw/qGz87OuveOHj2K3/3vPuBqSLTb6+h0OpBSugJwn//vAZ5lkEK4/+P2\nNzoavK+vrSPLstCb3QJ7xt37dHusmS96JbKZJoSQ+If/qS/8daMiBuo3wkb73YptC9TrLL0mFYMG\n96IocPLkSVy5cmUspxk7GOd5jj/+MQN1d7GseUkAuGQAVMieMwPkAcqAq2gfP9hZ9zut3Q6LHwVM\nvQPLnp2XKsHeWzxPqqLagU0ivgf6CmlCacC6KwKwmUKhQlmL7o8MdPhAmom3+wZOOcrLY+KEW992\nFpzTWVQmWH3OhN7HsuWOldHHpMC7ZcNtlVXrCS+5Zr8ZVGg9aVhxwFpEVtl5xTJYp5kwUZVMkoR0\ny7XB+1buoiS4LFxCqXatIZpKw7wzA6pV1nCJqjDOMsxq1gFSiAnuuwRTMEvmtHBWAVgLTilRfPgT\nuFERJ4NPskDGrZhcpOx2J91+KqSUuHDhAk6fPo077rgDzz///MgMHS14NIngnKMoCrz55pu4evVq\nhfiRUjoWHdDPkqIocPz4caytreHgwYO1eROjxCgAfpD3OZWdBK9TiakjatsH+banmGpqoxi7dSUT\nZxM2j/pF9RoDi0ezT22l1YRGnfa7/+BP63HRAHX7PbK/A13EKEqkVcoVCaJjmFIK6+satP/Xn15D\np7OKjY0NZFnm6k3MzeliQX/jsxeQ5dUVYimkY8llKSEhnQ6+LMpAWlMWJZpZE1IIfOk/f2fH0qIo\nXCVgpRKTqfdwbHugbpdKJx0poE59fTdTTMkO7n/yZtMw03QgIgDTREYAtw0OBEWGGINmoY0awurE\nqQ6cQ0G4BEunSg7a1moEhVL648wWiMD6Uf8/SENPE0/do4z0J3SRYchYdYXAtknZftcm48H1lSZ5\n1Sa52uuUYNqC0lXM9JpI60gTny+W7rj3DfPu2iADRQbt3W5BtH1QxV7v7j6Z96gUJnaSob7vVt+u\nHRN0NVYF7TSj25SE2TbsfRYWdnL/230kIDOTf6GkZ05s4rF1lsmbcC4ubk6ni0Mx4T2D7a2sJKfa\nCzcyF+o0Uz7/1yuf+SQjZmCuX7++aY36rXhn4kYRLzSUUrhw4QJOnTqF22+/Hc8999zYUptJSl/K\nssSlS5dw+fJlPPTQQ/jwhz/snitS6t+svSfWHebEiRNjreRuNuoA/NrpH1XeC0wBImA8ELC7g2q0\n6vY9Mh7XO6qwgMmOJS614NqdpFpszp67IouJGPwKa0+dYpTCuR0Lmg1//XUopRxrPjc3h+npaTSb\nzQp4B/Rnrgywj/EGY8y1Q4sE0QJwFy9eRKfTwWf+HYHp6WkC4OfQarVG+u689dZb2Lt377tKdNDx\nfG1tLSnteq/GtgXqNm6UZzoF6il7rs2cN8sy/PFbM2AM4ApBkSInczHvZ0yhVF7yYkG4kJyAaAYh\nGJG3aA/TkBFnUEx5+2swPXZQKY0Cci6d5Ea/GfbB9k1xD5p1vyJm3hyr69eECaqcycCRhjGTICqr\n9zJjREtPpDVSEZBuJgc0ydW+L1HjJw8E4D04Z5TEGibMZu6+0m0cUvvEKz8RgNK+8LVadeoiY48h\nUplM6YRXWzxDP5gMuJfeE55OPCygz0Q/lLkYnbtiHHoNxzD8pmqdZtIb7qHElAKXBSSnk5KQOWQG\ntKtA6871l4QmifJM69UBz7abEE//tcr9n3TEQP0Wo37zBs05ulFAXSmFS5cu4eTJk9izZ8+mALqN\nSUhfrCb+3Llz2LNnD+644w7cfffdbrsQwt0Le3/OnDmDS5cu4YMf/CAOHjz4rtUM2XHvI7XbUiDe\nxkAnmRGTRittpjzNSTt0H7pfLcsOJFn1WotH14+oPdJW9thLuJfsK6XE+vo61tbWsLKygnPnzqHX\n66HZbDrwPjMzg1arhbNnz7pk41HYd6C+AFy320W73cba2houXrwYsO+zs7POpCOWfk06wXszQcfz\n7TaWb1ugfqP1Sbaa3cmTJ3HhwoVNla6O4yJ/FpCmCCORpmRcaecU6TXgpQPC5mAFLymwMhUnOfHg\nXiggAM3Ms+SAlbKE/WKGcY/z/spI8pKxCMzDuPZZBtoCam5lNjTDndgmOpLB+70DdJVATxCoJMWC\ncX3dOoS7Zs+Yg1x+/L4ErwXvHApl9HNhTGlXGcUr+7tzJNh6DfjTunfdjyz5kHAAnlRO9W3IoHBT\noDOnbjNZJL/JQqcCwbjxkDfnz6z8ykhujLOMYsxrzisPOatFbxL9OSr7ahvIyKsdgHx8tGqgW41b\nQP29F41GA51OZ6JtKqVQliX+7M/+DLfddhueeeaZTdnu0tgKUKeSm3379uGFF17AxsYGTpw4AUCD\norIsAwb14sWLOHfuHPbv348XXnjhpivqB/icrZWVtBRHKYXOABAfRCIRNSVNSerYkQDtA9j62iTZ\nIS4xYF42mbJ2tG1lj/6VyuVxzh2rTaPf76PdbmsrTuMgZ0H3hQsXRmLfLTZKse8zMzOYmZkJ2Pey\nLB37funSJbTbbQgh0Gq1XB97vd67/p2blNXuzRjbFqjHMaqx/yghhMC1a9dw+fJlHDhwYCK+vv/i\n9QZxTwm3lSKUwMSXYYGt1YwDhl1HCHTBw7EtYypoW8tkEkCdeIbb8ykVVndn0BITIJTdlJFkJYsY\ncpoQC+h2BYhMJ5L25JE0Rlq2nzDpApotB8Lze395L78Jr8u3Y7dbVj/WwQOa2S6RB/vqa5LOE77C\nsjAk2frYR15rMP1yMIfUri1GskMfQJksoBhHzPH4SYZn6HXbKloOttVOc+NK00Tg0uAcbozri5G6\nqCxkqjxw95MTmWUB486UclKZkEXX+lEmJ+/QVBdxgYxJ2nndiskGYwxSyoky6kopXLlyBSdOnICU\nEo899hh27tw5kbY3482ulMLly5exuLhYYfQtY9rv9yGEAOccWZbh8uXLTkP/3HPPDU1yfTdCCIEz\nZ844SeiDDz6YfBYzxjB3X7WQkxACa2traCyfiw+oPqwQMujj6tj1RlWrtw8qoKb066iRxNQ4xGQf\n+pnKOQZFs9lEs9nE0tISWq0WHn/8cTQajZHY95mZGSeRAUbTvgP6u7xr165gbFRKYWNjw9lGrq2t\n4Y033gi07/bck3ReGRR0PN+K1e7NGDffr3pCQQcCO2hu9QtDs/9vu+023H777Thw4MBWu4r/65gp\n6c5h3FP8thQoD7ZDA3uaHMqZgpIRQOYh4OcJEM2Yl9dQlxmRYOqpDCfn0jmTeNDrr8MC+ox5MG9D\naZReAc5cVXXpnCsUkrDItiCT4uEkxbDztE3dbzNAkWsBvBu4dYOxbSjFKlp1Cuod8276DwVkcSEm\n4rZjAbxtP7g2on3X2+hkxlRgJWGXhDMICCNNoR7q1NrRSmjs+5IUO+JKGjcYPQkorYTGWj4iA4eA\nNLpzfW4W6uKtd7oB8V53Cq3p5DlsciqgZS7Ou90xWxys7L9jbDpQZdSllDcl0LkVfjzfahVRQIOM\nq1ev4sSJE5ibm8NTTz2Fn/zkJxNdure64XH6c/z4cezatavC6Ftgtbq6ih/84AeYm5tzk4w9e/bg\nmWeemahd5aTCav3PnDmDD3zgA3j++ec3LQmdn58HIpmGdThZW1vD2toaHpynq5Q1MphN6Njj7QFY\nd2+GoH0U/fq4IL0oCiwuLmJ1dRUPPvhgAJzr2Pe1tTW0221cuHABnU7Had8tkN4K+z49PY3p6Wnc\nfvvtuH79Op544gkwxtDpdLC2toYrV67g5MmTrt4M1b5PT09PnIG/xai/x8OyMJsF6lJKnD9/HmfO\nnMGdd97pliKPHz++5b793lE/wMpoXOdGOy4VA2c2MdRv19pwFhALGdf7S2W3G1AeEKgha6+lIyoA\nr4BOVi2lB8EWdAqjs7ZgPtCQK2YSNROEBw+16+Ytd5w7D9Hn+7oQ3s1GX7ufSFidvGXiLZvuJg9O\nIlPVkVMZjVLc68DNeUoip5HQbHzFZpJ5NryMQboB7wAgoyRX3a7ZFun1bf9i5xn7vkSODKWfFLDo\nvIwjk2WQrKofTrnTrDPTJ/rA4eYBU/IMmWG3bdKqW9FVyjDt2tZRF0Hyji6upyRxlAtptPC2M9Hn\nIAXO3/YQ5lZXMTs7+47oHScxeb8V72xshVFXSmFpaQnHjx/HzMwMnnjiCecSMYkJwGZiaWkJb731\nFmZmZvDkk0+6/gChF3qWZfjIRz6CS5cu4dSpU85T/tq1a2i329ixYwd27tzp2NN305rOTjwWFxex\ne/duPPvssxP/ndEkyTvvvNO9b6UhFrxbmdTs7CwWZgyDnNCQJxn2ceQwNQw7tZS0580f/qmxrlUp\nhfPnz+Ps2bMDVyTiaDab2LNnTyAxGqR9t+B9M+y7XeXhnGPnzp3BypRSCr1ez7HvV65cwfr6urOZ\npAB+q98T26ftJmPc9kB9KwlIUkpcvHgRp06dqthzTcJ+66vf8SC9wpQzz67b17GcjiZ9+oTO6iBk\n5Swe9IasfcYR6NQBL4uxA4xQntWibXEwJ4OxYN7JRAiYZwyQ0lslCsWdzIe2J6PEUyWZk7YACLTe\ngLar9KBXT2xExNq7wpeRPj9jcACbWkdqtjyUrrgandZFBra/GrxnXFRY/MDW0YB2976Z0AgQj3r4\n7XYSIVXMZHiAb4s2xSCeWakLryarMnMl1FFGb9PsuNWoA6i1hYSROnFbyISZT9U83Oh+3IB9W02V\nS1GpEMhkget3fAii3cb58+cr/r/239TU1EQBSFmWaLV0AY7tZue13cI+gDc7lluA3mq18NhjjwUe\n1MA7D9RXVlbw1ltvIcsyPProowEbmipWZKuP5nmOp556yvXfgiALTC9duoRut4ssy7Bjxw73752a\n/K6srLj7/OSTT7rf1zsVzWYzKBAE6PvZ6XSwbO7R2toaNjY2nJ3f3Xffjfn5eczMzKC3+Be1bafc\nXoJIaORjGU7j4Y+MdT3Ly8t48803MT8/PxFp0yDt+zD2fWZmBo1GI8m+2/Gzznmm1Wqh1Wph7969\n7n0hhNO+x+w7BfAzMzNjs+/Ly8u45557xjrmZo5tC9S34r2rlHIAvS77f6uFN77y7SkCbqtguno9\nZHavGDKuAoBsI0gM5QDBcFAS5DhWC+6r2nKGjHtdOmXqpfJAUQKOlU+Beb2PZ5Sryage5Nr9spix\nt++T/gDaRUY5m8bwWqRUzo5R98fqFKvSHwABOKb3lgJ4WvSJM+klOcqvErhCTvDyHt0/howL70JD\nOpwRCUxJfONdH5Rn5/0d8eBdu/l4T3h9jJWxMC//qchhSCEk33DQBq3Cat1laBvW7cV+8NbHPThP\nlptlZb0fFyXK+57BHPTyrV1ulVI6FmZlZQXnz5/H/8/em8fHcdTpw091z+i+5duSLcu6fMS3nYss\nS+INLLs/duENb7ILy7tvIGE3uxxxSEh2gYRkc5CbAAEcQgi7HAZxheyGBSUcCQ67lpU4xI4ljW7Z\nsiTrmpFmpJnurt8f3VVd1d2jc0Ya2/P9fPyxpo/q6pJUeuqp5/t8p6am4PP5JPC+EADitPPKz8+f\nVzvpWLyYK6AeGRlBIBCA3+/H5s2b41q2LRZQHx8fR2trK3RdR3V1tSRf8ALokUgEbW1t0DQNVVVV\nLg29CIKWL1/Oj8diMc4s9/T0YHx8HJRS5OXlSQA+UUx3OBxGIBCAruuoqalJqd8lRVH4+7Jqroqi\nYP369TAMA6FQCIFAgC9w2Bix/73ml6nWI/Ef6GDXWcwFpE9OTqK1tRWapmHLli2uhWWiIx77zoD0\n6OioJ/vu9/vR19eH7OxsDtxnq31XVdWTfWe7IuPj4xgaGkI4HAYAF/suYjInhhkdHcVFF12UuAFa\n4jhvgboYs/VSF5N5SkpKps3+Z8k9c43vHpbbYxIUWxtuM8BmnxgolUGlqDdnMhNnGOLWnQXu5cRU\na5EggHunNSNgSnA0nbVpyWQgynBkmYr9UBvMm1dRSWYjgXlrcSAy0qJvuzhGXkmwBigMD691VWDe\nGRNvgEBRqOs92XRMhcWELixEAEVKoDX/N9y6e4ENZ5IZwGTe2T264SGdIfZugQi0TfbalMFw5p9C\nus6Utbj1+BY3b0tsHONjPlOF6TBvLVSI6tJV8kUJ1c0xVETHGcqlN+aiQIVqxGAofpNx53Y/Psvu\n0WaZumgu8oaGkJOTg+zsbGm7lbFjpaWlfMLXdZ1P4gtl389nTeP5GrPdUWHMrqIoqKurmxE4Jguo\nM4YxHA6jra0N4XAY1dXVLsbXWU00Go2ipaUF4XAYVVVVc/7Z9Pv9KC4ulu5jwIvph9vb2xGLxZCd\nnS2B99l6ZwMmE8t001VVVdJ7pVJomoaOjg4MDw+jqqpKAqTiAkfTNL7AOX36NMbHx/n8IoL3zKo9\nSZEX6bqOrq4uDAwM8KqzSxXiAmf16tX8eDQaRTAYRG9vL0ZGRuD3++H3+xEIBLgcKScnZ97ad1a4\nKd6iYWhoCF1dXYjFYnzRIEp1FEVJ6HxeX1+PoqIiNDU14bbbbnOdb2pqQnt7OwDgmmuuScgznXFe\nA/XZeu+K2f+FhYXYtWvXjFt28/kl/fdXbJCu6yYgV5ksw7BBueGQyrFz4jG5L5CZc6sdBsAZa8/A\nraT7Fm2tichSm+ElizEvdD7PZurNS9yaesbMa0K/KJU187bTCRXaEhJSFTu5levU4VWwSd4ZEJNc\nAXOMZa26e6HA7lE5026/CXOb0QxFYt3N623LRvatZCy8KYGxE29ZNVbmN284dPXmmBAoRIdGVb4z\nwN6T9dF5rw7VBeKdSarMZ948z/zaGYi22wVMhxvAlsM4k1Ntn3VrYuYMPeHMPKHUTEJlibBr6lBq\nbbf29va6mBOvZCfATpxiE77oQODFvov+vyI7JuatpIF6asds59tgMIhAIABKKaqqqmbt4pMMoO7z\n+RAOh9Hd3Y2xsTEOvKarJqppGlpbWzE6OorKykrp+oWGCLxYsN+dUCiEYDCIU6dOYXJyEn6/3yWd\ncWqSRSeX2traJdXFxwtmddnT04Py8nLs27dv2n76fD5Pf3Gm6x4dHUVPTw9nlkXw7hyjuQTDIO3t\n7Vi1atW8E28XI9iis7S0FBdddBFUVZ0V+87m8/k4z3j97AK2ZGdsbAyxWAyvvvoqPvnJT/Lk1VAo\nhG3btmHNmjXzetempiYAwP79+9He3o6mpibs2rVLuub+++/HD3/4Qzz44IOe5xMR5zVQZ+H3+xGJ\nRFzHxez//Px87NixA9nZ2UnpwzO/yXRpzA3DAoxMSqKYoFwE6uxn1jDMrxkod7LRLAzKHF6EY1Y7\nbrAvT1gM0IuSGrdVownuRUtItiiQtebeYN7JzIv9kJNiZe27eRV1LTYUrsg2gTuXtyimc4zdF1tK\nw1pjoYpjQ0UmmUlQZODO2mGSHBHAmxaPgkREYN7N9gHNS/ICdzVX9kyFGIhZMhjDOsasN31Mj+7h\nNGMy8XayqvMcY9qZXIbfD9nWjBAKCEWPnP0z7yG2haOlQWf/u7Tt1kfGnIiskcj6DQ8PY3x8HNFo\nlE+8jK2ZC/s+MTEhse+s+l44HIamaaCULjj5aCbWZabz6Zh9MGmU+AedyRd0XUdVVdWcrdl8Ph+m\npqYS1sdYLIZIJILXX38dVVVV2LRpkwTQndVEKaXo6OjA4ODgnBIGFxqie4fonc0AUCgUQmdnJyYm\nJkAIQV5eHgzDwNjYGMrKylIaUA4NDSEQCHD56nz13WLi6qpVq/hxcYy6urowPj7Ox0gE8DPJi8bH\nx9HS0oLMzEzs3LlzwR7+yYpoNIrW1lZMTU258jymY9/ZGLE5GAAfz5m079Ox74At2cnMzMTk5CQ2\nb96MV155Bddddx0qKyvx61//GocOHcIzzzwzr3c+dOgQ/uzPTCeyyspKNDQ0SEC8vr4ee/fuBYCk\nzuvnNVAXvXeDwSA/TinF0NAQ2traXNn/c43Z+LN/4yX7F4/JABgQtY/LAN3sv8m8s2DnbW05oKom\na83OqarNwDNwz4C7qC0Xn83BtgOUQ2Dz7UUB05C7FwXsOmdypPm+Nphn7+YE8yyY1Ma2gGQLFrem\n3Zkgyr8VVuElyeudA1z5Wio8yzxu5wBIIewWePmzq47kT1E6Y1CF7wyICyuvAk5SmzBBtpTwarUp\nWkSKY25aHxL4iC45xjDLSSZriecoY7ngW8/X+D1iRVOFuxmYoVoWj4ZVWIkSYlU7NaTfD8XQYShq\n3NLj8Vg/Mdmpt7eXA4fpkp0Am31fvXo1Z9+Z9v3UqVPo6OjAo48+ipMnT6KgoAAHDx7Etm3b5vQH\ncybWZTasTDqmDy+73YyMDJ5kGYvF5iURcba50BCrVPv9fmzbtk3SxXtVE+3t7eVF81IF+Do1y5RS\nDAwMoK2tDVlZWSguLsbAwADOnDnDZSHs31IDTZYH4PP5sG3btqSRb166bpYgGQqFMDAwIMmLxPyA\n7OxsaJqGtrY201rSYbeYSsF+Rnt7e1FZWYkVK1bMehE5G+17T08PotHogth3UcaYmZmJ8fFxXH/9\n9Qv+3o+OjkpyrqGhIen8kSNmrkJTUxMaGhqSBtbPe6AOyMmkM2X/zyVY9bnpVuoHGzLgRKOmDEWW\naHj2ndoLAS9ZikIA8W8LET4z0Gx4MPQiIDcMGdzzd7MAuDtZVe6z7lhcKLD19CJItkEwFa6D6zrm\ncMOCUsCnUvM9BKAMmMmgMsC2hR1Oq0tYTitiqArTZBPbzpsACjWZe2fbdv9tK0mWzOpXTRmMmDAK\nmPIY0eOeENkakznPsOCgmZpOMmIiLWvbR9wsuSE82xwjw1MOxK4XpTLScyF+jwwYFlNvQK5gypxs\nmJOMLjDuopUZS0BVqeX+oqgoLtuIuYSoW5yOfe/u7nax77m5ucjKypImfFbJz+/3Y+vWrXjmmWfw\n9NNPo7u7G5qm4Vvf+haWLVuGqqqqWfVvJtZlpvPpmFsw4uX06dOIRCIuzfF8YqFA3TAMdHd349Sp\nUygrK8Oll16KEydOSAl2zmqi/f396O7u5lKHpS7BHi9EJxenLFSUhYyMjKC7uxtTU1PIzMyUwPti\nWEZGo1G0tbVhfHwc1dXVS1LwJl6CJJMXhUIh9PX1IRgMIhaLobCwkDPQzIYzlWJsbAzNzc0oLi5O\n2M9oPPadESjx2Hfmv+7Fvk9OTnIJDmsrWQs0Z5SWlmLXrl1oaGhAfX19UnTq5zVQZ8HKTh85cmTG\n7P+5BJvc4wH1L7+QwTXoznCCUwlsKrCABbuactkLY8oZky7e55oIvdhqD9be+feJAXxG7MR083mA\nt1berobqrmxqMudiX8xFh7fPOrUWJ4LlInHLajSDuBxtAKZfl3cA2PgbHosHOHYoeBuMoRcBNWwA\nzoaQEpb8S6EzQO1M+nUCbYcER9TFs0UAOycmnLL+EEKhAVKSKnOaYSy5+Z5C9VaLteeafmpItpCM\nrbd153LiqpNxN8fD4Ew7fzei2GDekrmYRZXsokulayuQqJgN+97T0+Ni3/1+P06fPo2ioiKuEf7l\nL3+JPXv24KabbppzP2ZiXWY6n47ZB1uYNTc3o7a2lkudFhrzBepMA93V1YXVq1fj4osv5n8PVFVF\nLBZzVRM9e/YsOjo6UFpamhSP8UTFbJxcvGQh7HcwGAxifHwc/f39LkcVJg1JBPBji6QzZ86goqIC\ndXV1KaWXF+VFfr8fQ0NDWL16NcrLyxGJRDgwdSausn8ZGRmL/j7RaJQ74SyG6wyAaZNIxYUgI2PY\nz10kEsHAwABqamqg6zoCgQB6e3tnpXYAgIMHD7qOVVZWYv/+/SgqKsLw8DAAcx53EgKlpaWorKwE\nABQVFeHIkSNpoD7XUBSFswHj4+PYt29fQm2jppvcv/i8HwB1uTQRgZ4mjJFlzLJOQRQCVWCaeWKo\nIIFhkhi3naPAqHosEJjchFLK71UVVhxJZu4Bi41n8hYHU6+qdp9Y17yeKem/YbfHe8zlMoChM8bc\nBOeiHMcJplnfxEWCFwsu2jaJY2VKdwRJBuuPIIORwLuHowyT3xgGcS08zLE1x5nrvS15iyK0p1iJ\nsIolS1GkZ1kgmIFjC2wbXHkOft70WjF17k4dOmDr4VWi21VOqdwGY95Nnb03226+GwUg2C5aib+W\nQEa6TqUGTyxduXp+CT1zienYd6YlHRkZQWZmJp5++mkcOXIEQ0ND2LJlC971rndNu/BOx9IFIQQD\nAwPo7OxEfn4+1qxZk1BHjLkCdUopzpw5g46ODixbtkyqsQHYbi/d3d1YtmwZ8vPzuUNKQUFBymuR\nF+Lkwn4Hly9f7nJUYaxyb2+vZBmZl5eHgoKCOVlGMjlOR0cHVq5cib1796YcI81icnISLS0tMAwD\nW7du5VJbJiNiIRYkEncoWOKquEORDImUWFxpw4YNWLly5ZIueuIlkU5NTfEFL+vfn//5n6OgoACn\nTp3CP/7jP2JwcFDKu4gXN954Y9xz1157LRobGwEA7e3t2L9/PwATtBcVFeGaa65BfX09P8b06omO\n8/ovEvuhq6mpwfHjxxPu7Rpvcn/0Zz6YLDiR7BcBgGo26BEBK2d6DSpXEVWIKUsRtBzEYTPiG5vZ\nkgAAIABJREFU/D1SBG277ZZiPZ+5eVjMvSYJ06knS68b9r2EECgwgbtzV8BwyGfYMTFU1da02wmh\n8n0adcplrOcbTJJjv7CpX6eeuwcm0PeWu9iyG1tOoyimBp/LeRyLBOfCyAXgKZPfmADd7Cdx3CM7\nzuhgjLYJ0sWiUOzd7cRO9o0gLp27KXWx2HlDThwFzKRbZgspAnmz20yzbhU6ou777cWC7ckusfBg\n1pJsAaKDUsKLLq0StjiXIoLBIJqbm7F8+XJs3boV4+PjAICSkhLceOONGBkZwVe/+lVMTEzgne98\n55zanol1mel8OmYXy5cvx4oVK9DV1ZVwh5bZ1sUQHcKKiopcFr6iF3p5eTmCwSDOnj2LQCDAE5kB\n4OzZs5xVTgVNOiBbA1ZUVCTcycXn8yXMMpIVjMrNzcWuXbtcdU5SJeZqtxivIJEoCzl79iwmJiZ4\nZU9xh2IhuzNjY2NoaWlBYWFhQoorJSsMw0Bvby+GhoZw0UUXobCwEIcPH0ZOTg4uv/xybN26FceP\nH8cnPvEJfO9731vQs3bt2oXGxkY0NDSgqKiISxavuuoqHD16FJWVlSgqKkJ9fT2GhobSGvX5BCEE\nmzdvTtpE6AXUH/6JDwalUBUCTQDlYhcMwxvEisDevpi6tdK6zYg7nBIBANRCcQzcKypxJYWCyvpy\np8UgCy5B4f2XNdaUgnfCXgTIzLz1GnxnQNLCO5JhucuN1UcxVAXCooGx1oBhuGUtJsMug03zPnlH\ng/1vUOKwqqRCRVNhkNnCh8gONDKTLo6ffI+5eyFr2QmxmXnz3YnHvYKHOmy9u3lOcIyhss6ducaY\nOwhWRVXhXg60CYVGHVVSxbYA+IlmMvYiky6CdZ4Aq3NtOyhQtnrpvIBjsRgCgQDC4TDPSfn5z3+O\n++67D5/85Cfx5JNPLhiMzMS6xDufjrmFqqqglCbFSnE2dTGGh4fR2tqKnJwcl0OYV7GicDiMU6dO\nATD/4Ofn50uscnd3N9fhMklIQUEB8vLyFhUkiRaGa9euXdSE1pksI5mfeSQSgd/vR3Z2Nh+zuro6\nVwGoVAnG9re3t2PNmjULHlMvWYhYU6K/vx+BQACaps3ZF1+cIzdt2pQQWXCyYnh4GC0tLVi1ahX2\n7NmDsbExfOxjH8Pp06fxgx/8gMtQEhlejPvRo0dd55PloQ5cAEA9meH8g/GFehWUGhYwlmUvzuRG\nBeY1LJzAHnAz5S7mXAD2htWWqhKJtTefLX/2AuXU46AIFHXDfZ9BAb/PfLa46FBVN5hXFOJOPLX0\n8SLQFbXx8rt6M/GaQxIEyCBfIUTaVTAErTnvryIvhgAzUdWU1lCpyJNBCXwqtSwNmUxHGBshEVUM\npn2nhnVOkL4AhGvVxbFh7SrE8omXdisoFAqHVIYx2zZ49jmcaFgxJgjyGtM9R7V91R3SGQpiSmYs\nIG8ICw2nvEZ0kiGgKF+9NOwxK17W0dHBdaunTp3Chz/8YRQWFqKhoUHall9IzMS6xDufjrmFWBeD\ngbVEth0vGHvr8/mwZcsWCch4AfSpqSk0NzdjamoKGzdulJIavVhlBrhYomEoFOJaZSYHYVrlRAaz\nJ2YF/lJFL+9lGckcUs6ePYuioiJQSnHixAmIdojs31IzwaFQCC0tLcjOzsbu3buTxvarqorCwkLJ\nLSbeIsfn80nMO9vJOX36NLq7u1NS2y8Gs4aMRqPYvn07srKyUF9fj0cffRS33347rrvuupTteyKC\nOEuvzhBzunipg5UhVxQFhw8fxqWXXprQb2ZXVxdUVUVZWRnu/T6J27aqEp4caiaFWoy3pJ8WwJYX\nUCWMNaYW+DRZe7Md87hPtT/b7YIfM+Uuch8Ng0J13Gdq0AmoQWUduAeYVxW3E433GLDn2UCcSWu8\nxkEucuSReCq06ewPfzeBYRd95J1SFHcb8RdMhNjg3Wzbe9dEHAeFuH9tuL+69SxnISO7TSpdb0tM\nKHyK4XmPneRruPpiV4c15Gsc/ROBOHOJYcedmnXFukZWzgM+oi0JUI9EIjh58iQyMjJQXV0NVVVx\n8OBB/Md//Aceeuih84HRTsQkdk7N5YDJ/GmahpGREQwODqKuri6h7R8+fBiXXWaXeWc2f4ZhoLq6\nWmJvndVEWeJoR0cHgsEgKisrF5ToKmqVg8EgQqGQJAlhAH42FXi9gi0+srOzsXHjxhkL/C1VUEo5\nmCwrK8PatWtdhZfYIoclkOu6juzsbL47sZBxmkvEYjFut1hbW5tSbH8sFpPGKRgMIhKJIDMzE6tW\nrUJRURGv6JxKQSlFX18furq6uDVkV1cXbrnlFpSVleHBBx881wvVzeqH8oJh1JlFYyJXtz6fD7FY\nDHd/BwBoHIANaJoNeA2DgDoyTE2wavCvFUEKwttRbFlJVDfg87tZe42xxYYJ9hVCOIhnQF/XqQRY\nzcRU+ZjiYPdjugXmndcRuPX0gp6bGlZyrALENHs8YpoIkGWga+rg7XZFJl7M4nbKaHhCqkNCA1gM\nu3XcZQXpaUtpJ6U6Nemmr7stTWFsNktwFZ/J+yRUWAWs5E/qkO1YdoeskBEh1CGHYX1h52DZQTLn\nGOH7ACp5uht8jEzu29wNUDiTrhADGhw+7uydrV0VZsfIbCBF4K6DuJxkVOiLDtJF94fa2loUFxfj\n9ddfxy233IIrr7wShw8fXjTLrnQkL2aqNL2QoJQiEokgEAhgcnIS1dXVLk21s5qorutob2/H2bNn\nsWHDhoRou0WtMrOwY2xpMBjE2NgYent7MTk5OScrRNHJpa6uLqVlDqxg0XRsfzxW2auSaLIsI0Wf\n8WRo+xMRfr8fxcXFvNCb3+/Hli1boCgKQqEQhoaG0NnZyR1VxIJNyUpcnSkmJiZw8uRJ5ObmYs+e\nPQCAxx9/HD/5yU/w2GOP4Yorrlj0Pi1VnNeMOpvYFEXBa6+9hpqamoTaDA0MDOArv7C3z51stchm\nO48BNkMtJooydhsQGWWZgRcnAZ7g6fg9ck4UlFIojDmXZCpEAvLO/puyFTcbrSiEy23Yezvfl4F5\nMQwK+HxuBt/rOgamWcRj4kUW3hnx5heRjRedY8T3FsM3jZmAyMA7gb3zOAP/PsVmxqkAktl1Xqw+\n65NPsUGyGHYVVPd5Z4KqkxlXHW2piqzXZY4zXgmmPkUTEl4BH9Ex0PWmpJVkHrjJ+gM2OjrKk0Ur\nKioQiURw77334rXXXsOTTz6JLVu2JOW5SxQXJKOuaRpisRimpqbw1ltvJVxC9OqrryI/Px+hUIgn\n/k1XTRQAXxiWl5djzZo1SwJopqamJOY9HA5zqYPopNLZ2TlvJ5fFjImJCbS0tEBVVVRXVydscc3G\nif0Lh8OSRn4+lpEsb6G0tBQVFRVLLruJFyIzvX79el4Azus6sZqoOE7OiqvJelfDMNDR0YGzZ8+i\nrq4OhYWFOHLkCG677Tb85V/+JT796U+nbPLwPGJWc/kFAdQJITh+/DjKy8sTWv3rs88IVnTMVcWw\nv54OlFBKoQq6Cyfo5e06mvCSn7BDsl2hDO7ZAsBZ/Eh+FuHvQg0b3DOpjf08NzBnbTmZeTFEmY0Y\n4jHGwovti++nKuazvNxkXO16jdU0f0e92mCLBSfDLiazisEZZWdCKQSgrbpBuNQmkUE764f5/lQ+\nPhPAd54nBv9MHOBbtIE0n2lIAJzJYOxnuwE9AUVdWbZLKxkKhXhRCnGyX6iXspgIVVdXh9zcXPzi\nF7/A5z//edx000244YYbUsZVI4FxQQJ1XdcRjUZhGAaOHj2Kiy++OCHtRqNRdHR0oKenB7W1tSgr\nK5PmblasSNzR6+vrQ29vL9asWYOysrKUswWMxWIIhUIYGxvDmTNnEA6HkZmZiZKSEgmYplK/mS1k\nKBRatIJFmqa5pDOGYUhuKl75AZFIBK2traCUorq6et6VzRcjWN2BvLw8bNy4cV55CNNJjMQcgZkS\nV2eKkZERNDc3Y9WqVVi3bh3Gx8dx1113IRAI4Mknn0RNTc28207RSAN1wPQvBYCWlhaUlpYmzHv3\nX56WXQcYw8v+VwVw4MVYi8CehWJp2dkxzoCLbioOJlts1kuDbT4rPrh3voPruhmYekBYBAjv6byM\ngXmx/+wd5Pbj90/xAPnOa23G333M2X9JRz4PAO+Z9Eo8xp9QT9bevN6pSXe3Z38tMOAejDv3QVeo\nK1nW61mmrtzN+ivEcI2/C6QTw5Nl31I+PdPh1EpOTEzwP4zihD+TVlJ0VWAM0ZkzZ3DbbbfB7/fj\n8ccf5wVYzsO4oIE6IQSvvvqqpCefT2iahs7OTvT396OiogKDg4OoqqrichBnNVHm5d7V1YXly5dj\n/fr1KcugOp1cysrKQCnF+Pg4Z95FUComrS52QqlhGOjp6cHp06dRUVGBVatWLal0RLSMZP9isRiy\nsrKQl5eHSCSCiYkJVFdXJ9TLP9HBEnCDwWBSNPNMJuYkY/x+v6uw1UxkiZgsWldXh6ysLDz33HN4\n4IEHcODAAXzoQx9KOTlRgiIN1AEbqHd0dCA7O1sqWTuf+PRBWxvJ2F8vyYcYTnDKjomhKorLnQXA\njABf1MVTwwSz7Hm2LIax6VQC9yJDLMpw2NfTMfXm/bOT3bA+ut4ljtTEi3UXrxf75iwgJZ4X2+WM\nsuotRWKFn5zvHM+BRlxQiPd5AW35PHUdY/1QFdlhRu6fYywcANunUnkMCeVaehHAizp5sX0A8CnO\n3Ambabe91u0EVLkPFNvWze8PvJg4x0C8qCl1aiWdyaI+nw/f/OY38c1vfhP33Xcf3v3ud8+rH+dQ\nXJBA3WkOMF+grus6enp60Nvbi/LycpSXl0NRFBw/fhxr165FQUEBd3JhAH14eBjt7e0oKirChg0b\nUnbrnfm8t7e3Y9myZVi/fv20wNsJSoPBIDRNk6pjFhQUJCXJ0FmwaN26dSnF8IthGAZOnTqFzs5O\n5ObmghDCQanIvOfm5i75Dh4rxtXZ2Yl169ZhzZo1iwpy2W4Om8udha3EXQqvZNFTp07hlltuQUlJ\nCR555JGUXgwlINJAHTB1aSzZg1KKdevWzbut274W9bA6tDXehiVnYZpzdpwoRNKhm8y53Yantjue\nfYoQzl8+T535NJIcWxajuFhuT0bcQ6LiYl7jTAhO8C1q5r3ewQmyvbTx8QB93ONxqrUy8O5cwLjH\nl/XB/t+ZvOqWwsSXybDzgA20ZfaftektlXFq0p39tH3W3aCc++M72nDp3kE9wbzT+WVnReL/OIma\n0vHxcUxMTCAWi0HXdaiqiomJCRQWFuKuu+7CJZdcgrvuuisppa7r6+tRVFSEpqYmz4IWM51PQqSB\n+jyAOgNb3d3dWL16NdatWycx4idPnkRhYSGKioqgKAqvbN3W1obs7GxUVlamdDJyopxcGFPKmHdx\n4Swy7wvJOQkGg2hpaUFOTg42btyYcm4jYoh2i1VVVdIijYFScZ5aSsvI8fFxNDc3IycnB1VVVSlh\ntwnIC0IG4CcnJ/lOxcDAANauXYvDhw/j0KFDePjhh3HllVcmvB/n6lyemvt2CYxEee8e+FIEgGxF\nqKoEmoXUeDFL6wIGZHVQs/wk649CYIiFkIgJedgCgH92VD5ioJYdVyw7DhHMenmhw7DHAIbJ3OvU\nkHTn1CH4dtolMj29FrOvUyxaWIeTmbdZZPsd4fKIV1XiekdCrPHiY2XdD3thI+9MyPdTaiaq6la7\nrF/8uEPXDphJoszLXe6f81mm9biqEF6wSrxP191tmO/E3sEu1uSlcxerrZrXUKFwk3mOMe6ACazN\nqqymq4xKbD9zgzKpivBwa7h4HymxLCGtY4rFnFPZ8UYH4d7qPsUQPNvtBNg9G5LD1rAiH8uWLcPY\n2Biam5uxevVqLFu2DI2NjXjiiSc4uAoEAnjuuefwN3/zNwntQ1NTEwBg//79aG9vR1NTk5TEONP5\ndCQnCCFckjJTMIaxo6MDy5cvx759+yQAw1xc8vPz0dHRAUopMjMzeZGdVLPac8bExASvfJoIJxdC\nCHJycpCTk8PlY5RSKWm1r6+Pj4/IvM/kEDI5OYlAIIBoNIra2tqEVwtPZESjUbS1tWFiYgI1NTWe\nPwN+vx8lJSVScq6u65iYmEAwGJQKEYm7FIm2jNQ0De3t7RgdHUVtbW1Cc/ESEWLSrmEY6OzsxMDA\nAKqrq0EIwXPPPYeXXnoJg4ODWL9+PQ4dOoS6ujqsWbMmYX04l+fyCwaoMyvFucbExATuOCiAXSGo\n4ZR4EEiVflxlRgFF8CbnQJ63R0EVwuUchmaY0hoPcK8JvohOcO+U45hJgyY4jlkVgtjawSyaROUJ\nQ4cA5M0EUAbSGXh2JnMyME91Krlpm2APrl0FQLZKZNdKiaK62Wfdg/wjCnglUZGFZwsCkZFXVSId\nZ6GqRCqYxI8r7kJMrG+aTj0BuapYVpgOMM7u06n8bPGdWWEjUQ5DKbGAuM18i7aRpp0jAIvx1lwV\nVwlUobiTtMC0NOzsFh+h0A0CWBaMsPpBqSyR0QzF4dlOcPHG5BKzzJt4YmICW7ZsQW5uLl588UXc\neeeduP7663HTTTeBEILOzk4uc0tkHDp0CH/2Z38GAKisrERDQ4M0ec90Ph2JCy+73emYWCYDaWtr\nQ1FREXbv3i1d7yxWtGLFCuTn5yMQCGBqagorVqxALBbDyZMnpQRD0UllKUNMvty4cWNSnVwIIcjK\nykJWVpZUKEx0COno6OCl7Z1OKpRSdHV1YXBwEBs3bpQcdVItWIn6U6dOYcOGDXMuBKSqKgoKCiRg\nH88yMiMjwyWdmcuzxFyd8vJyDnxTNcRk0X379iEcDuPee+/FsWPH8P3vfx+bN2/GxMQE3nzzzYQv\nNs7lufy8B+os5uq9G4lE0NbWhif/a50FfL1++GW5CNUp14F7BSEEmoX8DACKIJPh11BIAF8xKC88\nBFgAytG+C8jqsGQpRGLqNeezFJt9BmQgK+JXlphIGOp2vrt1k25BdCazkcG/PFYicGZAHpRKrLcq\n9Q/C/QDlVUptIC9q4XXDkh0RSG2IEhqR5bc17G7mnZibFxJDLwN+8CqzYlKx+Q4m2y4+AwCopXM3\ndAuks7EQniP7rIvgn0laYAFsWR7DEnd1Slxe9woRKqdSYlZMFaqksu+16APPDnCG3YrLqqYvub6Q\ncCaL1tbWYnBwEB//+McxOTmJ559/HmvXruXXJ6N0NGDaPooAaGhoaE7n05H4YDuk0wF15sGdm5uL\nHTt2SJIVr2qiLJltYmLCE/SyPIpgMMjBv67rnCVl4H0xtOu6rqOrqwsDAwMJ822fb2RkZKC0tFQq\nbS86qfT29mJ4eBjRaBR5eXlYtWoVfD4fdF1PyURc0W5x3759CdPME0KQm5uL3NxcKcldlPcNDg5K\nVojsZyqeM9bExASam5uRlZWV1AqoiYhYLIaWlhZeWTQ7OxsvvPAC7rnnHvzTP/0THnvsMb4Tk5ub\nmzBHJzHO5bk89X5TkhR+v58XqZgu2HbX6Ogovv3bzWCAlPJiQlRKYKQGheJTBMbZZJ+dTitiUSPA\n0oXrhksbTiVQSeCEQjoggV5P33Wrqqgm0N5OcO9k/6lh2UU6mG8G5hl7y0CkqBXXrfeROqvHcZZR\nYLVn71AwJj5esP4wTbwpM2H32wmtDLwzpliPW4SKgWnreoE19wo2xs4uel3PGHBKqVSsyXyueJ0N\n4HXYGne+YLCYdmlhwBdrBIpiL2pEtp23wfIkrLZZG1w6AxO064adZ2CA8P6bL2Gy7ea7E860E3jv\nKiQqxGTR3bt3w+/349vf/ja+9rWv4fOf/zz++q//OnkPT0fKBiE2YRKPeBkdHUVrayv8fj+2bt0q\n5Sw4q4n6fD5omoZAIICRkRFs2LABmzZt8gS9YgEiFowlDQaDGBoaQkdHB2KxmATeCwoKEgagnE4u\n+/btW/LERa/w+XwoKiqCYRjo6+vDypUrsX79eg5KmRxEXOiw8VoqsBmJRNDS0gIA2LZt26LlIojy\nPhbiQufUqVMuy8jc3FwMDQ0hGAyipqZmUWws5xtiYitLFj1z5gxuuOEGZGVl4b//+7+xcuXKpe5m\nysd5D9TZRDYTo65pGjo6OjhL8eiPVgLQoagKR2gcpAufxdAt5ORMKFVVBRTUdb0XIOafRWSG+L7r\nTIDMQK/i0JKztp0oUxGebViyGF03XHp6132EAAqR9ersXshyE90hFXIy8k4m3nWt8J6iJp4oBFS3\nF0YK6LTgXfzDSxQryRd21VbAlq54SWTk97fHTCHWuwvSFfa1U+NOnEOpyVIVUeNuMvTyM8XFhAHA\n0EWAbS0Q2NgrJujm4Fw3Qbm0E2CBdmLp1O1r7XaJUBlVfAefQpPCpouVRWtqalBSUoLm5mbcfPPN\n2L59O1555ZVF17QWFRVheHgYgAkCReZwNufTkZxgIJtFKBTivtZOTblXNVGmk+3v78f69evnJRkQ\nWVKxeiizrBsZGUFXVxei0SiysrI4cJ+rPtnp5LJ3796UZKNZTExMoLW1FYQQbN26lXuMs2RUFuJC\nZ3h4WBorcZdiod7c04Wu6+jo6MDQ0BCqq6tTohAUW+iIAJwlY54+fRqdnZ3w+/1QFAUdHR2SdCaZ\nReXmGqyyaE5ODvbs2QNVVfHUU0/hW9/6Fh544AG8613vWtT+nMtzeer+tic4nBM7C13X0d3djdOn\nT2PdunW4+OKLceO/jYIoJiOqxUxAwl1OKJHkLYQQaILQmRARtFKLFTd4G4AJLo2YfY9CZBbbcIBy\n9mzNkMERUdzPNhzXiCy97ZaicLxs91EOhcDRJysBVAGoANJF/bzoOuNsjzHy0zHxnGFXzIWNoduM\nOGPhzQ+CvMgC71YL/CtVtZl3SaZjfWlI1oKmfCZeYSd2v6nnt20uQey8AMMaC6t7LvtFcRxU1Twv\n5jywhYKpsxclOkBMl11leKKnCN6thFOFCP0VAL7JlpunVEWUGJlMvHQtNYG9Avvngg2jSigur048\nSGfJomzLORqN4t5778WLL76IL3/5y7yE9GLHtddei8bGRgBAe3s79u/fD8CcyIuKiuKeT0dygxEv\n4XAYgUAAk5OTqK6uRnFxMb/Gq5ooIYTrj9euXYuLL744oay0mIjJmEKx+NfY2JhU0l4E716AdHR0\nFIFAADk5OdixY8e8nVwWI2KxGNrb2zE2Nub6XnhFvIWOWCjt1KlTmJyclLTcLGl1IYCUUor+/n50\ndHRg7dq12Lt3b0ruTrBgSbgZGRm47LLLuLWhOFYswZdVpZ2Lj3kigy2CBwcHUVtbi6KiIrz55ps4\ncOAALr/8chw+fHhJCkSdy3P5eW/PKFp6/f73v8fll1/Oj/f29qKnp4dXl6OU4sZ7RkDi/FA7E0q9\nJgqn5EX0MBc/O9t1tuHSrjt917307V5te0hwnDGbPsVrnyXFSvaTPovltxYYTFbjZVMJuLXfYn9Y\nP0ybS0gFosQ2xJip+qnz/eTiSM5x9hobWevOjrF3URV7rJyadrGyqrMok/2u7ueZfZE/i9eyYz6H\nTzwrfiQWXRKfqxBbxsLfidjnxPCpFFfWRV3jsZBg0oOJiQleWfR3v/sd/uVf/gUf+MAH8IlPfGLJ\n2cODBw+isrIS7e3tuPHGGwEAu3fvxtGjR+OeT3IkgjI75+ZywLbbZWw4AFRVVaG0tFT63XVWEyWE\noL+/H11dXdyzeyl/rpwuKsFgkANSJgE5e/YsFEVBdXX1gp1ckhli8mVFRXIKFrGkVWYZGQ6HeZXj\nuQJSZg2Zm5uLjRs3prS2W2T8a2pqZlz8AG7LSOZ2J1aDTlYy9MjICFpaWvjv2OTkJB544AH84Q9/\nwJNPPolt27Yl/JlziXN1Lr/ggPpll12Gvr4+dHZ28upygPkLceM9o/w+l4f2DL7mDNwzq0NisTcs\nxOqi8n0WWHVo38VQVWVOCwTb49xaJAgJpSJYZo4y4n2AKZ8R+y0WTZKeKUlKZJApFUlS3F7ybMdA\n7JvYDgO45rVxxiXOpByvgqnP7724Mdls9/iy57IFgrNwk/htEIG7p3adnVe9wbj5PG/JDSHEVfFU\nzC0SFwHyffKzzcWAu0qqea8TlMtt+RSK3LHfJIyt8aosOjw8jM985jMYGhrCV77yFf67mQ5XXLBA\nnckqBgYGUFhYiO3bt3sCdLGaKNOPl5SUoKKiYsndWqYLJuGZmJhAVlYWNE2D3+/nrHsi2OREhSjJ\nWbFiBdavX7+oBYs0TZMKNTkBaUFBAfLy8viCLBqNIhAIIBKJoKamJqWtIQHwpGVGJC6EFWeWkSKA\nT6RlZCwWQ2trK6amplBXV4fs7Gw0NDTgzjvvxEc+8hH8wz/8Q8oWs1riSAN1wAbqhBC8/PLL8Pl8\nfMJWVRW6ruMjdw3P2A4DmuL/LKhBTQcXL022ECLYNQwKVUBD1DA8mfx4wN4ZTiDtLGpEiBuUi2Ae\nkAs0zfRMr3eLtzDxup+B4HiLBrFf093vOh5nMvP53UWdzP7Jz2DjorKFhyf7TVxJqk7w7pzrFIVw\nIO3e5ZCBu6lvdzDfilxcytk+k9TwNhRZ1x4PzIsA3WblqeMa4F1bTevDWCzGE52cVefECX86MOSs\nLOr3+/H9738fTzzxBD7zmc/gmmuuSQkgksJxwQL1cDiMvr4+ZGdnY2BgAJs2bQIgO7mwYkUjIyNo\nb29HXl4eNmzYkNKyEU3TuH3hhg0bsGLFCv47MB2bzKQzM/mXJzqCwSBaW1uRlZWFqqqqlClYpOu6\nND+FQiEYFnk2NTWFtWvXory8PGX66xVsfvT7/aiurk5aX8V8CvbPKTMSK0LHa4Mli27YsAErV67E\nwMAA7rjjDsRiMXzpS19KqBf6eRhpoA6Al6gNBAIIh8PYuXMn8vLyuF79///s4LQSElO2ocIQNBeK\nh5WIl/xDlnnMDOT5tdOw9yKYZ+BeBOXsaydzPh95C7vfS7YiXsfOsXeUZSXuZzhtKUVjeSfaAAAg\nAElEQVQ2fjrGXwzViTgRH7z7fIrEiLNvpRO8i/0VWXlxl0AVFxNxwLtXdVWfzw3OxbaduwBOUO50\nhWEsu9gP56LCKZHh14nHFfevtKrI975nR8R1jRgs0UmsZKhpGrKzs6UJPyMjA729vejr6+PJom1t\nbThw4ACqqqrwwAMPpFyhjhSNCxaox2IxaJqGiYkJdHR0YOvWrS6AHgqFuJ5348aNS6KHnW04nVxm\ny5yK8gbGJjP/cgbek1HOfnJyEm1tbZicnDwnWOmhoSG0traisLAQBQUFHMTHYjE+P80nwTcZwWw3\nBwcHlzSxdWpqCuPj49LCkLkeifP51NQUTp48ySu2qqqKb3/72/j617+Oe+65B+95z3uWpP/nWKSB\nOmCC19OnTyMzMxOBQAArV65EcXExfD4fPnjbaRd4ZrIPESyKnwEbhDP2NR6YlNoVwP1017uAqkMX\nr6omuIwnSXGy8l5gXnxP3j9HO2LiqVe/zHeyky+dmnnnuDjtGNkzWHKpCLydMhpnMA28M2YD3kXN\nu/ksWdYSrx1bJ+694LHBsdwu+1pV5Iqf/F0EqYwoFeI/WwLbLt9nH4vXttlf63/nDoCQlMradzLp\nhAB/tXN6kB4vnGzN8PAwQqEQMjIy0Nraikgkgu7ubrz66qv40pe+hEsvvXRez5kpZioJffDgQQBA\nW1sbvvCFLySlD0mICxaoa5qGWCyGyclJvPHGG9i8eTMyMzOhKArC4TDa2tpgGAaqqqpSGkQ6nVwq\nKioWrJl3SkHGx8dn7ck9U+i6zhMEU71gEWDuvLS0tHCNv9NukSViiuTC5OQkMjMzXWzyYrzn2bNn\nEQgEsGrVKqxbty7lElt1XZd+toaGhhCLxZCXl4fnn38e5eXlOHToEC655BLcc889ScupOA/n81n9\ncJ33ri+EEK73W7VqFfr6+tDW1gZKKShdx7fFWDg/K9T9C6MLji0wTBDvZLNd9wjeJIR6gV7zOczj\nVyHeemrJ+UWRi/kAADHcf38VyeOEd1v+7LAxBADVp3AHG9Z90yaRHbJbFZ1tmK2jYri16YrVJruO\nubnogh+hYnl9Uw/Zj0LMiq3S+/nY2JnHRb27u9iR+Zn7yhvWe0tGJlaeAZGLF7EiSZI23bJLMUEv\ngelsY3u6AwAB4QWReJ+JCa7FJGPu4sIWU5bQn1c1Fe0XpSEQ8h8cbjO62A9xOIX7WftRjUiA/v/Z\nE8Z8g7lfZGRkYGRkBKqq4pJLLoHf70drayt++tOfIhwOgxCCm2++GXfffTeuvvrqeT/PK2YqCd3Q\n0ID9+/ejsrIS73//+/nndKRuhMNhDAwMoLS0FAUFBThx4gSi0SgMwwAhBGVlZVi7dm1K69CT5eTi\n8/lQXFwsJRyKAKu3txfj4+MAIIH3/Pz8uOCd7Uh3dXWltHc7C03T0NnZOaPdIiEE2dnZyM7Olny8\nxQTf/v5+hMNhnpfDxiuROxXMv50QktKuPqqqoqioiBOf5eXlKC8vx+DgIM6cOYOGhgZkZWXhpZde\nQmdnJ3784x8nfIFzIc/n5z1QB8xV1q9+9StkZWWhpKQELS0tuOuuu/DILRsQjer49Bdt1lwsSkSI\nAk2P8a+9wqldV32qYIdoSL/QPMFJ8KbmfuuQiyTpcFsxOj0PFUNomxpQiOKpL3eBeUJMZC6E6lNh\n6PIxr90WaniAZ0Uu1ER1Uwaj6waIUNpeIYSDbINSqyiSe0fC0Bnjb9swGgaFQokneDdiBi/yBAhW\nibq1A+FhuSgXOZJ3LMRqrQohoKBQVOLSuBsGBXRq2VaagF0hQExjz7IsLXU5kdjp5U4Is5+UWXGq\nEBCrTcreCYAmeq5bhZNMhtx+fy7hAWtffF/wyqgM2PNqptaP//+7b/4g3Rxfu3w7qyw6NjaGO+64\nA729vfj3f/93Xk00HA7PqhjZXGOmktDt7e08u59l+qcjtaOnpwcf+9jHMDg4iFWrViEWiyEjIwP3\n3XcfioqKEAqFcOzYMV54iMlACgoKlhy8T0xMIBAIgFKKurq6RXFyYQDL6cnNpA2nT5/mBXWc4J0l\nthYWFmLPnj1LPn7ThaiVLisrm7fdolcBIlFm1NXVJe1UiEmrc9mpMAwDXV1d6O/vR3V1dUp5dnsF\nSxadnJzERRddhJycHPz2t7/Fv/7rv+Lv/u7v8Oyzz8Ln83GTgGTsQlzI8/kFAdQffvhhtLW14QMf\n+AAyMjLwwQ9+EM8//zweeeQRZGdnY/v27dixYwd27tyJu58SKtnptoc6LCDNADmrSEp167wOKKoK\nzVFUycmKGx6e6jx0SJOLKBchHgDZCeYNRS5Y5GTl44F5s0iSDJQIIVCsvnIZjAXmJXmNARdbLN5j\nvpM3Q272GdwQXEpsZc8W30dRoeuGJNOhFiOvCQm+1KAg1ATJWsyx+FCIJNNhPWcyE9Y+679d0VWU\n5gBazPZ4p1TwqmfXEMBQCIgh5g6IYwJQYkluRDtFcQwsaZCLabeAu1hsyc2ce4B2YSHAGHnFqufF\nWHpCgGsvXhhIj0QiaG5uhs/n45VFf/SjH+Hhhx/Gpz/9afzt3/6tNJEnS0M8U0lo0X6rqakJ1157\nbVL6kY7ExZYtW/DSSy/hi1/8Ig4ePIgrr7wSfr8ft956K86ePYuKigrs2rULO3bswNq1a+Hz+XD2\n7Fm0t7dD0zTk5uZKDiqLAT6npqbQ3t6O8fFxVFVVzcpiL5mhKApfvLAQ80x6e3sxNDQESimKioqQ\nmZmJUCiUNEu/hQazW8zLy0vKgsLv96OkpESaS1jSajAYlKqHziapnunmV65cmfI7FF7JokNDQ7j5\n5psxNjaGn/70p1i3bh2/nhCStEqjF/J8fkEAdQBYtWoVvvOd72Djxo38GKUUo6OjaGpqQmNjIx55\n5BH0NjcjNzcX27dvx86dO/HTP9jaWQbQWTglGAZ0yRGGKERiqc2Kmvb1xAGkKTVg6DoUa2VOBYDM\nkqWYNMcpySHE7b1OPRIFvcC87mDXAVNOosUcLL+mmwsH3ZaGxNPIU4FF1vmYeCQuQpGYYOYSo1hD\nK9o3ajGd7xA4dfxmBwWtvQEO3sXviQpIMhvWhmLYvTMLLZnX6xZjrgtaE75LoAsVXSHo3xWTKYdB\n7QWExOqbwFpVIVVOJYKUiena2WdVJbwJDqytfnJpDGsfdqVTVbWZdp2yBZhQ3MgQE1SBv71k/iDd\nMAz09PRIyaKdnZ245ZZbsHbtWvzmN79Jicp/zmBbqCI7k47Ujne+85246aabJCBkGAYCgQCOHDmC\nl19+GY8//jiGh4dRWVmJHTt2YNeuXSgvL4eiKJ7gnQH4RAE9p5NLXV1dyuq6FUVBVlYWTp8+jUgk\ngm3btqGoqIhXDmW7Y7quc0s/Nl5L5UMu2i3W1tYuak6CqqooLCyUkt/ZYicUCmFgYABtbW2SBWJW\nVhb6+/uhKEpKy1xYhMNhnDx5EllZWdizZw98Ph+++93v4stf/jI+97nP4X3ve19K/jyfj/P5BQPU\nWXEDMQghKC4uxlVXXYWrrroKgA3ejx49isbGRuSG7uar9YLqr0ruL4CDGWd4V/hfTCJ1stteW2VE\nIdAFdltk6RnoJx7g2pSgKJLGfrZg3glmFWIy7IyR50y1A2gTlbhArwpZd80ZayrvFPB3ddwPmBIX\nQ7gOEJh3XdagU4MCiiAh0m3wLkpyGPOuwwTKos87A+PiWIosv1Sd0+ojfz+DcFtLNj6GTm2/dKsP\n4nxmVmS19flM167HhCRRS9fO3lXUxjNG3O8z22AMu/1NYPoWeWQZaGeAHbBZdl0HPnTF/EG6WFl0\n7969oJTi8ccfx49+9CM8+uijePvb3z7vtucbsy0J3dDQcK4kHqXDirq6OtcxRVFQU1ODmpoafOAD\nHwBgMp+tra1obGzEr3/9azzyyCMYGRnBxo0bsXPnTuzYsYODdwZGFwreDcPAqVOn0Nvbi7KyspRn\nTcWCRevXr0dNTQ2fH/Ly8iSJDqWUg/ehoSF0dnYiGo0iOztb2qlIpv0hIwROnz6NyspKycpyKYO5\n7uTn53NLQkopdyjq6elBZmYmKKU4ceKEJDPKzs5OiXcAbFnOwMAArywaCARw8803o66uDr/73e+W\nxJ3rQp7Pz3vXl0QEpRQjIyM4evQojhw5gqamJrS0tGDV9mfN84YTbArMslAAyX2NINVQiMv20SlR\nccplvDzd+b0Ol5l417E/IM5E2OkcXPi9Hk42M1VClaqOeoyJ043GeZ+zyJLiMbl5ucLYRYy8i0e5\n3GEc7+P5HAGMO5+lEAJFtZNRnUmphLjHSiEiK892F9g55ziy/9m4yO2w/omP4O4yDvtGVSH4//5k\nfiDdq7JoY2Mjbr31Vrz73e/G7bffvmSexWyn7MYbb8SDDz6I/fv3Y9euXbxkNGDmr7At03Mo+SgR\nf9EvyLkcMMF7S0sLjhw5gqNHj6KpqQljY2OoqqriEki2EGDaZMYkTwfek+HkksyglOLs2bNoa2vD\n8uXLUVFRMS9HGNHhibmoTE1NISsrywXeFwpGmTvKUhRYmk8MDw+jpaUFK1asQEVFBRRFcVWlDYVC\niEQi8Pv90k7FYnvjAyYAbm5u5uOraRoee+wxvPDCC3jiiSdwySWXLGp/xDhP5/NZ/UKkgfo8g1KK\noaEhzrwfPXoUoex/dTHWLKbzITc/z+zNLhcEip/c6lwAmB7nlpyGGhzEey0EVGdJSniDeKeP+nR9\n8vKUB4SETAiAcw7g3Wmj6ATVzgVKvEWEVzu88FEcAO+8T7ZOJAJjLY7TNH13AHdVcVs+2vfK4NwG\n4Mzq0r348BpCQkzQ/uF3zM+CkW3vrlu3DmvWrEEoFMLdd9+N5uZmPPnkk6itrZ1Xu4mM6UpGNzQ0\n4P3vfz9KSkowPDyMH/7wh+fCxA6kgXrCQ9d1nDx5Eo2NjWhsbMRrr72GYDCI6upqzrzX1dWBUiqB\n99zcXOTn50NRFJw5cwZ5eXnYuHFjShfUAcwFSEtLCzIzM1FVVZVwGQazPxTBKLM/FMF7VlbWrMC7\naLdYU1OT8rKRyclJtLa2Qtd11NbWuuwhvYIVtmL/RG98sRJ0MhYnYrJoXV0dcnJycPjwYdx+++14\n//vfjwMHDqREfsJ5OJ+ngfpiB2MoGPP+2xN/LrnITBderjJOdlvxMe26zdI7wTwA/ovMkkfFtpxt\n874zVl9Vp11QAOClub3acxVamiV4l7Tm/DnEewEjAHEn2J+ukqvoMOP0eBefKfVBVVx+7mwxxHzk\nvaqomiDY3gUQGXUbaNsMO0scNZ9J3M8UmHZ2DXsn9jz7WtmfnTniiH0DTPB+w1VzB+mTk5M4efIk\nfD4fampqkJGRgeeeew73338/PvnJT+Lv//7vU2Yb9zyNNFBfhNA0zQXex8fHOXjfuXMnVFXFiRMn\nsGXLFs6eOxNWU4lVn5qa4rrumpoaKaF0McIJ3iORCDIyMqTxEmUgmqaho6MDIyMjqK6uXvJE3JlC\nzNPZuHEjli9fvqD2NE2Tig+JlaBF6cx8f8a8kkVHR0fx2c9+Fn19fXjyySexYcOGBb1DOqaNNFBP\nhWDbof/7v/+Lx/5jhasSJgsRcAIm2HaC/OnAvP155msAcPeX6aqNsmeyfsRj4s3+K7JLDXEDYvHd\nvML7+W4wzt4z3vO8pChumYkNsp1SGVmi4iErUmwg7bUAUlWFWy3abcrAPV7BJUJsFj+eNMaruBJ7\nJ/YsQGbUOesuyGL2FL/MJ/rZlCH3Shbt7e3Fpz71KRQVFeGRRx5Z8B+mdMwq0kB9iULTNLz11lto\naGjAN77xDQwNDWHjxo1Ys2YNd5upqamBYRgckBqG4dK8LzZ4Z1UvBwYGUFlZieXLl6fMYjoajXIg\nGgwGEQ6H4ff7oSgKxsfHsXbtWmzYsCGldf4AMDIygpaWFi57SpYsh9lriuy7mOTL5vOZknzFZNHq\n6mr4fD7U19fj0Ucfxe23347rrrsuZX5GzuNIA/VUiubmZnzlK1/BZz/7WSxbtgwDAwO47mOdnvp2\nFqLO3QuAzxbMO5l4sW3za5n5no6Jdx6bCbzLfZs7eJ9LFVLiYPolNj0Og+7dDmO2bYmO1CdnVVeR\nzRakR/EkOC5GXWT04+jeFQF0O7XszncSK6GKwZxjVAW46V2TiMViUmU+ttXqVckwGAzi5MmTKC0t\nRUVFBQDg61//Or7zne/goYceSsoW40xV6Fg8+OCD054/DyMN1Jc47r33XmzatAnvfe97oWkaTpw4\nwZn3Y8eOIRwOo7a2lmvea2pqePGhxQTvImO6Zs0anjibyjE2NoaTJ0/yKqETExMIh8NQVTVphYcW\nElNTU2htbUUsFkNtbW3S7GanCzHJl83n0WhUyhNgzjOUUleyaGdnJw4cOIDy8nI8+OCDSdm5SM/n\nnpEG6udSUErxjvf/Ia7GXQyZebbBvOpT3cyzwIizz055ithOvOcoROFONV5JrV6heIDv6Zh3L/cZ\nds5La+5sR2TbxcWNC1gLUhPn4kFxfPbStbP7RDmL3Ib37oAkvRGaFRcLipBAKjL7Uv9V+zl2n6RL\n4POxJGH5uKoSfOzdk4gXzlLRLDEMsJPG8vLycNddd+HKK6/EZz7zmVnpL+caTU1NaG9vxzXXXIOD\nBw9iz549nnZbLMP/V7/6VcL7kMKRBuopHrFYDMePH+cJq8eOHUMkEkFdXR0H79XV1dA0jf++eRUd\nWgh4HxkZQWtrKwoKClBZWblkNoqzDSbLmZqaQk1NjasglFh4KBgMujTcrPDQYoF30S2HyVxSiYEW\n8wTEMYtGo8jPz0dXVxcqKirw29/+Fj/72c/w2GOP4YorrkhKX9LzedyY1Q9M6ojnLvAghOA39ZdK\nx97+vsMA4GLNTe9tczIy9Bj/WtfsAk2sOicRvNRN8GraH4ptAQBxFAwCZCbegFnoSCqyZH0pVkgF\nbNDtrHQaLxRFMW0ULU9xXoyI9d2yXPR6nmwdaR23rCsVGJ5SI15dVbHBtqEbln2jBfypKU9xJs7a\nz6H8iOjdThQCgwKwbBPtZ1Kz2BLro3BS0wxbmuJTLctF0+5R8lZ3SFu4zaNBrUJGNmOv61ZBJipL\nYqYD6ea9diXDgYEBBINBbNy4EQUFBfj973+PJ554AidPnkRxcTFaW1vxs5/9DNddd920bc4nZqpC\nl450pHL4/X7s2LEDO3bswA033ADAlHi8+eabaGxsxI9//GO8/vrriEajqKur4wmry5cvRywWw5kz\nZ9Da2iqBdwbgZ5JUhMNhBAIBGIaBLVu2IDc3d9rrlzoMw0B3dzfOnDkzrSzHq/CQqOHu7u7G+Pg4\nCCEu8J5oGcro6ChaWlpQUlKCffv2paT7DCEE2dnZyM7O5vM1pRRbt26Fpmn4wQ9+gIceeghnz55F\nZWUlDh06hA0bNqCsrCzhfUnP5wuLNFBP4fjtjy+TPv/Je1/hX4sgmMIsBESduNhQQF1+6zagNz8r\nZvVLXZcKIamqCkNzNQi2ZmBstQlMHfIdfWbwLkp0DMj3c0ZEOCyy+IZVDUmsvGq27SjQxDzUHfIi\navVf10QpEIHoj04geLzr9oJAVRUYunyfIVQfVYUKpjprl3myGxQGX3zISagMuLPCSjpj6wXfeFj3\nUmoDb90qemRQANRcHGia6fzCiinpMD9/8v9MYTYhJovu3r0bGRkZ+MUvfoF77rkHN910Ez7ykY+A\nUsqT0pIRM1WhA0yWZv/+/eedZ246zs/IyMhwFWKJRqP44x//iCNHjqC+vh7Hjh1DNBrF5s2bOfO+\nYsUKRKNR9PX1oaWlJS54j8Vi6OjowOjoKKqqqlKyuJgzmN3iypUrsXfv3jkDXp/Px4kFFs6qoaFQ\nCAB41dDZLni8IhqNorW1FVNTU+fEIohSiv7+fnR0dKCiogKrVq1CKBTCfffdh0AggB/96EeoqalB\nMBjEsWPHkpZcnJ7PFxbnBVBnlai8Yra6qHMhfveTt0mfr/irl/nXXpIZChGQm+DO0ITz1ACxmF4x\niKJYlVCte5gjiQC+qWEtDjwk9kRROJjmoQMqREcZG1QTKj/fyWKLXu9idVix8qrJgCuucSAKgQ65\ncqj5eF2yoqS6IGOxQL8U1kfN0CWpitRXRa56qqpyfwyFgAifdQj3WsDdkIbCHHcRtLNKqJLOXnhX\ng5r6dHttYhZTuvk9UcwUlFJeSKS6uhqlpaXo6+vDbbfdBr/fj1/84hdYtWoVv36p7RdZ8Yt0nD9x\noczlLDIyMrB7927s3r2bH5uamuLg/dChQzh27Bg0TcPmzZs5887A++nTpxEMBhGNRqHrOpYtW4bq\n6upFd3OZa0xMTKClpQU+ny/hVTrjVQ1l4N254BHBezypEaUUvb296O3tTakiS9OFV2XR5557Dg88\n8AAOHDiAr371q/wdCgoKkiZ7mW2k5/P4cc4D9YaGBnz0ox9FW1ub61xTUxMAYP/+/Whvb5/2j8C5\nGC//zP2LxcA702gzNtl0FlEk8A4AUAxJSkOpwTlhEauaR2XwbgJHL1bCcAF4l2wG4Pe7qqVChwK7\nXd1aFABygqoErgHo0CWAD8iLAwrRopBA13RZ/22Nl6g9t9uzNe26wNwr1mcmkxGTX0XQbrP0bkkM\nf4ZKYOiyjaLOq5qaz9c0ClVROLtu9ofKVVgNudLqre8TVmdxgiWLlpSUYO/evSCE4KmnnsIzzzyD\n++67D+9+97tnbCORMVMVOsa+pOP8iQt5LhcjMzMTe/bswZ49e/ixyclJvPHGG2hsbMR3v/tdvPHG\nG9A0DcuWLUN7eztuvfVWXH311dA0DX19fVw2I0pA5ssiJzI0TUN7eztGR0dRU1MjMeHJDEVR+A4E\nC8MwMDExgVAohP7+fgQCAamwFRu3cDiM5uZmFBcXp6zMRQyxsmhNTQ2Ki4vR09ODT33qUygpKcGL\nL76IZcuWLWqf0vP5wuKcB+r79+9HZWWl57kLURflBO9v+z+/5V97O8xYAF0A0QZkeQpj3okFFMUE\nVF2Q4Nhaba9kHgOAnORJLMkNS1Tl7RDF0sTbGnG2mBBBuxgqmJ5eZuN12O07pTLm84XKodY5VQFP\nUhUXAwZslxV2nGna2SuLjLm4MDCHnungHYm61j2aoZtAn2vUrURaaoJ2trNhUMolMaKOXUw+ZbKf\nf71u+jwBTdPQ1taGUCiEzZs3Iy8vD8ePH8fNN9+Myy67DL///e+XZHv32muvRWNjIwCgvb2dT+Ks\nCl17ezva29sxPDyM4eHh8xq4XSiRnsvjR1ZWFvbt24d9+/YBMNnHa6+9Fn6/H9dffz3+53/+B9/4\nxjcAAFu2bJFkM5OTkzh9+jRCoRAopRJwXyzwTilFX18furq6UF5ejurq6iVnpMVE1DVr1vB+iuD9\nzTffhK7rKCwshM/nw8jIyKysD5cqxMqie/fuBaUUX/nKV/C9730PDz/8MK688sol6Vd6Pl9YnPNA\nfbqYjS7qfI9Xfv52/vVbb72FyspKXHXNH/gxJ3g3WXjGQAsgz2LeAe8EVMBm4IlC3d7ngl5elM04\nmXameVcMoR8cXMugXQzWBk9MZd2G3b7IuLP2iUI4oGYa93hMO6EKIGjQRWtE1hZj1+3xtZ/BklAZ\n+80XAIJtI3tXJovhCiKmYdcp2N9VnZrsOks+ZSy7eY2Cz/zN9CBdrCxaU1ODyclJ3HnnnTh8+DCe\nfPJJbN++fdr7kxm7du1CY2MjGhoaUFRUxCftq666CkePHsU111wDwKxUNzo6umT9TMfiRHoul4PV\nLdi2bRs/xlyZjh07hsbGRjzzzDN48803QQjB1q1bJdnM1NQUB+8AJM17opMvx8bG0NLSgoKCAuzZ\nsyclKlzGC0IIcnNzMTo6irGxMdTW1mLFihWIRCIIhUIYHh5GV1cXotEosrOzJeZ9KavRxmIxnjN0\n0UUXIScnB6+99ho+9alPYf/+/Th8+PCSVnNNz+cLi/MaqKdDjk2bNgGQwTsLxrzH83UXpSzcylEA\n78wG0qxsakiyGd2A5D6jCGDbBu3UBd5dBX8kcC0w+ZZ8hoFss4+CPaIHc8M19LqdlMptKx0MNQwT\nuFPDABWlNnyhYoNvRVUkaYw1TBYzbgNps20ZtGsOyYrZPcoTUtkCgJvWKAS6YbL0IstOFEwL0icn\nJ9Hc3AxVVXmy6IsvvojPfe5zuP766/HAAw+kxPYuKxEtxtGjR13XeF2XjnScz6EoigTSAXOey8nJ\nwaWXXopLLzUdxJi/9uuvv47GxkY8/fTTOH78OBRFwUUXXcSZ95UrVyISibiSLxcC3pm/eDQaxaZN\nm1x2i6kYwWAQzc3NKCwsxN69e6Vqs7m5uTxHh1kfBoNBjI6OoqenB1NTU8jKyuLAnYH3ZO4cOJNF\n6+rqMDExgdtvvx1vvPEGnn76aWzevDlpz59LpOfz+cd5DdRn0kWlww4neBclM86wwbygF7eAsyib\nodQQmG8buTMVvGKBZRu0U0kjz0A7v88Bru32ZNcbIkhcREcZkW1XMHO7wiUCuNalPnEJjFjhVbCY\nNNsHoFPJEYaz+FLSJ4FBbY90cUfBTIql8PlVXvHUqV+3Ho47P+htke2VLDowMIDbb78d0WgUzz//\nPNauXet5bzrSsZSRnsvnF4whvvzyy3H55ZcDsOUdDLwfPHgQx48fh6qq2LZtG2feGXjv7e3F+Pg4\ngNmB99naLaZSMEY6HA7PalEhWh+uXLkSgDmuU1NTCAaD3HFmcnKSF25i45aVlZWQ8YhEInjrrbd4\nsqjf78cLL7yAu+++G//8z/+Mxx9/PCUKQqVj4XFeAnWme4qni0rHzDEd685iJs27ySIbEnA277MQ\ntU+c5AX5C3FfazLUHuAakGQyIttOHE41or6deMhTAACqGyCLr2YY4Ay7fVzol+mTKJ/n6wBbFsPY\ne1HHDoh+7m4dO1EItJjDdQZM62+y8Xd9yBukO5NFFUXBs88+i6997Wu4++678U/9s+oAABT4SURB\nVFd/9Vee96UjHUsZ6bk88UEIQV5eHt72trfhbW8zncQopRgfH8drr72Go0eP4mtf+xpOnDgBn8+H\n7du3u5h3EbyLCauTk5Po6OjAypUrsW/fvpQHiqJ2njHS8wXRhBBkZWUhKysLK1as4McZeA+FQujr\n60MkEkFGRoaUK5CTkzPr57KFUH9/P08W7evrw6233oqsrCz88pe/5IuHdJwfcc5XJq2vr8cNN9yA\np556iuucdu/ezbdUDh48iMrKSrS3tydlS2UmyzB2PlnPT4WYjn0HZM91MUTJitf1iiRfUeJe56xk\nytv3ef+RYG2Jto/T3SdWSCWKd1VSRSGuSqy2U429OIhXAZV9rTqqqIruM5w0V7zb+fyHdJf+U0wW\nraurQ15eHpqbm3HzzTdjx44duOeee5Cfn+85TulI2UgEPZmeyz2ef6HP5WIw8N7U1ITGxkY0NTXh\nxIkTyMjIwPbt2znzXlZWhpaWFoRCIWRkZPCy9SLznoqAPRQKobm5Gfn5+aisrFxU7Xw0GuXgPRgM\nIhKJQFVVyRs/NzfX9beJJYsuX74cFRUVoJTi6aefxrPPPov7778f73rXuxbtHdKRkJjVXH7OA/Wl\njJnK4jJLsV27dqGhoQElJSUXTCazE7wTx0TtAt0CyJbBqwzanUmkgF1B1Xmv4gnu5Xa8bB9FQC3q\n5wHbjlI8LgJ30ZvdafHIQLgTZEv9stqT39nZjnzfJ/5ygE/4mqYhNzcX+fn5MAwD/f39WLduHdau\nXYupqSk8/PDDeOmll/DlL39Zsn9LZMwEeNjvDQAOyNIxpzgvgfpSRnoun11QShEMBvHaa6+hsbER\nr776Kl555RVkZWVh//792Lt3L3bu3InVq1cjEokgGAxKzHsqgPdYLIa2tjaMj4+jtrY2ZYiKWCzG\n5/FgMIhwOAxVVTnjPjIyAk3TsGnTJuTk5OCPf/wjDhw4gLe97W248847kZOTk5R+pefzpMas5vLz\nUvqyWDEby7BPf/rT+NWvfnXBbdd6SWemLdAkFkgybEDL5DW6Aag+VSrEZLclylhscG8ohlt37nie\naPvInsm07E6tOWDbPTqTTwHzjxjrL1EUl8WjZuhQfYpk7Wi70MB+5zkknT74D34Aa+z3pxQjIyNo\nbW2Fpmnw+/247bbb0N/fj1OnTuGd73wn6uvrk1ImGpid3/X999+PH/7wh3jwwQfTNlzpSIlIz+Wz\nC0IICgsL8ad/+qe44oor8Nxzz+G+++7D+973Prz++us4cuQIHn30UTQ3NyM7Oxs7duzg/1auXImJ\niQn09PQgFApBURSX5j2Z4J1SijNnzqCzsxPr169HbW1tSmnn/X4/SkpKJHejWCyG7u5udHR0IDs7\nG729vfjoRz+KwsJCDA4O4t/+7d/w3ve+N2mOM+n5PDUiDdQXEDNZhu3atQuVlZUoLi7GU089tdjd\nS7l4+WdXwDAM12QsAnjA9G0HIOnLRUcX3XAy9LrEkDuTXSUHGQdwZzp6wE5CNWD2UdptEoA083xX\nrP4xMK/6VW5J6aVjJ8S2rHSCbwbaDZcG3XSSscfGSpKlBI/8k+zl65UsOjQ0hIKCAui6jg9+8IPo\n7e3Fhz/8YXz84x/HX/zFXyDRMRPgqa+vx969ewHgvKkumY5zP9Jz+dxDVVX85je/4fP5O97xDrzj\nHe8AYM5Fo6OjXDbz8MMPo6WlBbm5uZJshoH37u5ujI+PS97miQTv4+PjOHnyJPLy8lLeIpJFJBLB\nyZMnkZGRgUsvvRQZGRkYGxtDdnY2rrjiCpSVleGXv/wlnn32WbzwwgtJ6UN6Pk+NSAP1JAZLhLrj\njjtwww038Mn+Qg6vSderwioA/Ml7X7GLLomuKhIYl6umis4votOMyLTr0D0LJLEEWBgKd4th/eUs\nd0znwFnyZSeKeU6qTGr6mRsGhQLDclK3pTOsgBFfEDiSTnm/KHUVNHKCdK9k0e9+97t44okn8NnP\nfhbXXHPNorBHMwGeI0eOADCZmoaGhvTkno5zItJzuXfEA9GEEBQXF+Oqq67CVVddBcDe7WtqasKR\nI0fw0EMPoaWlBXl5eS7mfXx83AXeGfOem5s7a/DOcnSCwSBqa2ulyqSpGqJrTm1tLYqLi9Hf34/b\nb78dmqbh5z//OS/QlOxIz+epEWmgvoCYyTLs4MGDuOOOO1BUVITKykrU19enf5DnEL/7yds8j//p\nNa/yryX7RgHMK5bHu5iEKspSWKVS87hbhuNVCZWDdtFhxgL6jGU3YoakY9ctFxe7b6a9o2jraMpt\nbJ07NSggFD+iBoXiU2DoBhRVwWMftwtXeFUWDQQCOHDgAGpqavDyyy+jsLBwliO+OFFaWsq1vvX1\n9WldYzqWPNJzefKDEIKSkhLs37+fS4copRgeHsbRo0fR2NiIL3zhC2htbUV+fj53mtm+fTtWrFiB\niYkJdHV1zQq8i/7irKBbKslc4oWYLMqq0D7zzDM4ePAg7rnnHrznPe9Z4h66Iz2fJz/SQH0BMVNZ\nXDFYklI6Fh6/qb/UdSwWi+H48eO4+b4pV+KpNxuvuBhxdi0Dxsxa0tB1CbQrlozFy8aR69YhA2yW\nJGoYFKoHGWRXcTU/67phsvHULHRkaGYbX/xkNr9ncHAQgUAA5eXlqKmpQSwWw4MPPoj/+q//whe/\n+EVe9GQxYybAU1paypnIoqIiHDlyJD2xp2PJIz2XL00QQlBaWoqrr74aV199NQATZA8NDXHw/p//\n+Z8IBAIoKiqSmPfly5d7gvfMzEwMDg4iP///tnd3MVGdaRzA/2ewUgbsAhINHxYzdEgtBsdZUWKN\nSZMxaoxtaobYNDXpxWKv3Ei0TqmWGy8saNpu17rbwbQ1RJKRY3rhRdNy3CWm0osZpsUSYwlztFra\nBCzyaZkKnL0YzmEGhx35mK8z/9+NwBDm6OCfh/c87/OuSJo2l+A57uvXr0dmZiZu3bqFmpoaWK1W\nXL9+PS4HRjHPEwML9UWIdCzusWPH0NDQAJPJhIGBgbiMFEuVHdlPPfUULBYL/ntp5mOPHj1CV1cX\nPB4POjo60DP0N+0xZWoqsLKtHowUtJk1bXpsJBA6D376Ex9ri1GLdrVIV7+WIWh+vDoz3ZBmwOTE\ndJsOELK6Dsw066SlBVbjgcDppGlpBvzzSCaAmZNFDQYDrFYr0tPT8d1338HhcMBut+P69etx++EU\nqeCx2+0QRVH7mNrfuFCyLEOWZbS2tqKiogLZ2dn49NNP0dLSsri/CKWURMhygHkOBPI0Ly8PO3fu\nxM6dOwEEivf79++jo6MDbrcbV65cgc/nQ05OjrbyXlJSgi+//BKVlZXIzs7WptOo4w7n2zYTC4qi\noK+vD7Isa3Pc/X4/Tp48iba2NnzyySdx3ZzJPE8MHM+YxCKNFAOAqqoqbUe2zWZL+R3Zf/75J7q6\nuvDNN9+gsbERiqJg1apVeMb8r+lZ5XONiXx8Brz29qx57LPnsAef3hd2/GKYee5qO0zguQw4d2xF\n2M2iDx48QF1dHXp7e3Hu3LmE6JsNN+969jzs3NxcuN1u1NfXL+q5JEmCzWaDKIpwuVxoaWmB0+nU\n+5xrjmfUIeb5/CiKgv7+frjdbjQ1NeHrr7/GunXrsHz5clgsFlitVpSXl2PlypUYHR3F8PAwxsbG\nYDAYQua8G43GuBTvwZtFzWYzli9fjra2Npw4cQIHDhzAoUOHsGxZ/NdSmedRxTnqeudwOLBjxw7Y\nbDZIkvTYKowoipBlmb2UYRw9ehQ2mw27du2C3+/Hjz/+qK28d3Z2YmJiAi+88AIsFgv+c3NnaNEe\nYQZ78GOzW2uEoOI7WNqyNG2VX/06arH+79q/YGRkBLdu3dJ6ZA0GAy5fvowzZ87A4XDg9ddfT4oe\nzGhxOByoqKjQ7SrjLCzUdYh5vjA9PT14//33cerUKeTl5aGvrw8ejwdutxterxe3b99GXl4eNm7c\nqPW85+TkhBTv6rzy4J73aOVp8GbR0tJS5Obm4v79+zh+/DiGhoZw9uxZPPvss1F57mSRQnnOOep6\nxx3ZC3fmzBnt7fT0dGzatCnkACC/348bN27A7XajwPAPdHZ2YnJyEmVlZdpoMbPZDL/fj5rTj+Zc\nkQnebBo8f13tWwcCRbnWDqNOszEY0Hg8B5OTk+ju7sbQ0BDWrVuHrKws3LlzB0eOHEFhYSHa2tpC\nvgdSlSRJqK2tjfdlEC0Y83xhnnvuOZw/f157f/Xq1dizZ482fladn+7xeODxeOByufDzzz9j1apV\nISvv2dnZGBkZwe3bt6NWvA8NDeGnn35CXl4eNm/eDEEQcPHiRZw9exZ1dXXYt29fSi+4qJjnoVio\n6xx3ZC9Meno6KioqtJ47RVEwPj6OGzduwOPxoKmpCV1dXVAUBWVlZSF9kn6/XzthbnJyEkajEQ3N\neQAeb6FRv/bscD5fl4v+/n74fD4UFRXBbDZjYmICH330ES5fvowPP/wQ27dvj/4/RAKTZVnrlZRl\nWdv0x+9z0ivm+fwJgoD8/Hzs3bsXe/fuBRDI3N9++00r3pubm3H37l2sXr1aW4ixWCx45plnMDIy\nAlmWQ04KnW/xrp6GOjY2hrKyMmRmZqKnpwc1NTV4/vnnce3atYSbzhVrzPO5sVBPYtyRHTuCICAj\nIwNbtmzBli1bAATC/o8//tBW3r/44gt0dXVBEISQ4r2wsBANf/djeHgQIyMjWvEe3CcZ3Is4Pj6O\nzs5OGAwGbNy4Eenp6fB4PHj77bexZ88etLe3R+0kumSi9vRarVbU19drm5r4PU7JiHkeO4IgoKCg\nAC+//LI28lBRFPz6669a28zFixdx9+5dFBQUYMOGDbBardiwYQOysrIwOjoaUryrOb5ixYqQ4j14\ns6h6GuqjR49QX1+Pr776Ch9//DEqKyvj+U+RMJjnc2OhnsRivSN7LpEmFagaGhp0dbtWEAQYjUZU\nVlZqYasW7z/88AM8Hg8+++wzdHV1wWAwYP369dpqTVFREcbHx9HX1wefz6cV71NTUxgdHUVJSQny\n8/MxPDyMd999F93d3WhqakJpaWnU/j6RXkf18eBNRfHEACc9YZ7HlyAIKCwsRGFhIV555RUAgTz/\n5ZdftJX3CxcuoLe3FwUFBdpCjMVigdFoxOjoKPr7+7Xi3Wg0Ynh4GE8//TQsFgsyMjLQ3t6Od955\nB1VVVVGfzsU81w8W6kks0kgxk8mE7OxsiKKI33//PSqh6vV6AQA2mw2yLMPr9YadRCBJElpbW3UV\n7OGoxfvWrVuxdetWAIGwf/jwIb7//nt0dHSgsbERN2/ehMFgQHl5uRbiHo8HBw4cwMqVK3Hy5Elc\nu3YNDx8+xEsvvYS6ujrk5+dH7bojvY5erxcmk0m77T7X60xEC8M8TzyCIGDNmjVYs2YNXn31VQCB\nzaD37t3TRkV+/vnn6O3tRVFRESwWC8rLy+F2u2E2m7F582YMDg5qd2EnJiZw+PBh7N69O2Qa2FJj\nnusLC/UkF+43YXVsUvDj0fpt1eVyYceOHQAAk8kESZL4H34WQRCQmZmJbdu2Ydu2wGmriqJgbGwM\n7e3tOHXqFLq7u7F27VrU1NTAbDbD5/PhxRdfRHV1NWRZhiiKuHfvHt54442oXOOTvI4OhwOtra0h\nq31EtHSY54nPYDCguLgYxcXF2LdvH4CZSS7Nzc04evQoioqKcPXqVVy5cgVGoxHp6ek4fPgw1q5d\nC6/Xi/feew8XLlxARkZGhGdbGOa5vrBQp0WJNKkACPz2brPZFj1jVU8EQUBWVhampqbw2muvobq6\nGoIgYHR0FN9++y16enpw6NAhAMD27dvx5ptvRvV6Ir2OVqsVJpMJOTk5aGxsjOq1EFF8MM8XRi3e\nHzx4gLa2NpSWlmJqagp37txBc3MzPvjgAxQXFwOAVkBHE/NcX1ioU9SpG6Tocbt27Qp5f8WKFdi9\ne3ecrmZuap9sbW0tqqurtaAnotTCPA9PEAScPn1ae99gMMBkMuHEiRNxvKrwmOfJhYU6LUqkSQXq\n6gsltkivo9PpRG1trXbgkiiKuu9PJUo1zHN9YJ7rS+zPzSVd2b9/P2RZBvD4pAL1Y6Iowul0YmBg\nQNvkQokl0usYzG63azNuiUg/mOf6wDzXFxbqtCjqBpVwkwqAQAioG5/ChQQlhkiv47Fjx+B0OrUf\n0okwzouIlhbzXB+Y5/oiKIoyn8+f1ycTxVKkubFOpxMA4PP5uBGKktlSnDHOLKeExjynFPBEWc4V\nddKF4LmxargHkyQJNpsNBw8ehCzLkCQpHpdJREQRMM+JZrBQJ11wuVxan506NzZYcJibTCatf4+I\niBIL85xoBgv1FCGKIhwOh9ZX6PV64XA44nxVSyfS3NiDBw9qfXherxebNm2K6fVF2//b1CWKIiRJ\nQkNDQwyviIiihXmu3zxnltNsLNRTgCiKsNvt8Hq92sgml8uFkpKSOF9Z7KlHJevptD1JklBVVRX2\nsUi3kIkouTDPZ+gtz5nlFA4L9RRgt9sxODgIWZa1Qw3UHj+9iDQ3ViVJku42HtlstjkPq4h0C5mI\nkgvzfIbe8pxZTuGwUE8Rly5d0sZqAQgJeT14krmxTqdTmx6QKiH3JEeCE1FyYZ6nXp4zy1MXC/UU\n4fP5UFFRASBw61RPqy9A5LmxkiTB4XCgpKQEOTk5Ub2WSH2E7DMkosVgnjPPKXXMd446JSlBEEwA\n3gLgnv6zRVEUZ3yvSn8EQbACMCmKIgqCcBCAR1EU75M+vojnbVUUZUeYj9cDaFUURRIEwT793PyJ\nQpTEmOexEY88Z5bTbFxRTxGKosiKojgURREB5AK4FO9r0qn9ANT7szKA2UtdkR5fEoIgqGdCuwCo\n98RNAPR/j5hI55jnMRP3PGeWEwv1FCAIgkkQhJbpt20I/NbP85+jIxvAQND7s3dBRXp83qZXVzZN\n/6m6CgDq6s706z64FKv3RBQ/zPOYimmeM8spnGXxvgCKiQEArqDbZW/F+4Jo6UyvqomzPvbXoLd5\nS5xIP5jnOsUsp3BYqKeA6dUWMeIn0lIYROBWNBBYbZm9NT/S40REc2KexxTznOKOrS9ESytsHyH7\nDImIkg7znOKOhTrREvo/fYTsMyQiSiLMc0oEHM9IRERERJSAuKJORERERJSAWKgTERERESUgFupE\nRERERAnof9QYGReipfgFAAAAAElFTkSuQmCC\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x7fb7e3721940>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "evaluation_points = np.column_stack((field50.x, field50.y))\n",
    "prediction = pureNetworkModelPenalty.predict(evaluation_points)\n",
    "plot_field_and_error(field50.x, field50.y, prediction, field50.A, \"pureNetworkPenalty_100.pdf\",\n",
    "                     [r\"Network output after $100$ iterations\", \"Deviation from the numerical solution\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data based learning using a custom function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Initialized weights and biases for MLP with 20 parameters.\n",
      "\n",
      "MLP structure:\n",
      "--------------\n",
      "Input features:    2\n",
      "Hidden layers:     1\n",
      "Neurons per layer: 5\n",
      "Number of weights: 20\n"
     ]
    }
   ],
   "source": [
    "baselineMLP = SimpleMLP(name='baselineMLP', number_of_inputs=2, neurons_per_layer=5, hidden_layers=1)\n",
    "customModel = CustomFunctionLearning(name=\"customFunctionLearning\", mlp=baselineMLP, training_data=field50.A, beta=25.0, d_v=0.025)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Visualize custom function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAuoAAAD3CAYAAABLsHHKAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nBEIEQoQ6gUAgEAgEAoEQgfw/TqfFqbqv58wAAAAASUVORK5CYII=\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x7f6ea01d4a20>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "evaluation_points = np.column_stack((field50.x, field50.y))\n",
    "prediction = customModel.predict(evaluation_points)\n",
    "plot_field_and_error(field50.x, field50.y, prediction, field50.A, \"customFunctionLearning_randomWeights.pdf\",\n",
    "                     [r\"Custom function with random weights\", \"Deviation from the numerical solution\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "The initial loss is 106.69989434100859\n",
      "\n",
      "Wrote weights to file ./customFunctionLearning/baselineMLP/best_weights\n",
      "The loss after 1 iterations is E=104.14263177946617\n",
      "Wrote weights to file ./customFunctionLearning/baselineMLP/best_weights\n",
      "The loss after 2 iterations is E=101.4669958326886\n",
      "Wrote weights to file ./customFunctionLearning/baselineMLP/best_weights\n",
      "The loss after 3 iterations is E=98.66338353437207\n",
      "Wrote weights to file ./customFunctionLearning/baselineMLP/best_weights\n",
      "The loss after 4 iterations is E=95.7212175517689\n",
      "Wrote weights to file ./customFunctionLearning/baselineMLP/best_weights\n",
      "The loss after 5 iterations is E=92.62885012542628\n",
      "Wrote weights to file ./customFunctionLearning/baselineMLP/best_weights\n",
      "The loss after 6 iterations is E=89.37346485455804\n",
      "Wrote weights to file ./customFunctionLearning/baselineMLP/best_weights\n",
      "The loss after 7 iterations is E=85.94098163872204\n",
      "Wrote weights to file ./customFunctionLearning/baselineMLP/best_weights\n",
      "The loss after 8 iterations is E=82.31597401890099\n",
      "Wrote weights to file ./customFunctionLearning/baselineMLP/best_weights\n",
      "The loss after 9 iterations is E=78.48161458060717\n",
      "Wrote weights to file ./customFunctionLearning/baselineMLP/best_weights\n",
      "The loss after 10 iterations is E=74.41967461932666\n",
      "Wrote weights to file ./customFunctionLearning/baselineMLP/best_weights\n",
      "The loss after 11 iterations is E=70.11062172830356\n",
      "Wrote weights to file ./customFunctionLearning/baselineMLP/best_weights\n",
      "The loss after 12 iterations is E=65.53388827500139\n",
      "Wrote weights to file ./customFunctionLearning/baselineMLP/best_weights\n",
      "The loss after 13 iterations is E=60.66843375265886\n",
      "Wrote weights to file ./customFunctionLearning/baselineMLP/best_weights\n",
      "The loss after 14 iterations is E=55.49381116123431\n",
      "Wrote weights to file ./customFunctionLearning/baselineMLP/best_weights\n",
      "The loss after 15 iterations is E=49.992103396156395\n",
      "Wrote weights to file ./customFunctionLearning/baselineMLP/best_weights\n",
      "The loss after 16 iterations is E=44.15138300088996\n",
      "Wrote weights to file ./customFunctionLearning/baselineMLP/best_weights\n",
      "The loss after 17 iterations is E=37.971899451872424\n",
      "Wrote weights to file ./customFunctionLearning/baselineMLP/best_weights\n",
      "The loss after 18 iterations is E=31.477306639500352\n",
      "Wrote weights to file ./customFunctionLearning/baselineMLP/best_weights\n",
      "The loss after 19 iterations is E=24.735622719706416\n",
      "Wrote weights to file ./customFunctionLearning/baselineMLP/best_weights\n",
      "The loss after 20 iterations is E=17.9002181220176\n",
      "Wrote weights to file ./customFunctionLearning/baselineMLP/best_weights\n",
      "The loss after 21 iterations is E=11.296493671494169\n",
      "Wrote weights to file ./customFunctionLearning/baselineMLP/best_weights\n",
      "The loss after 22 iterations is E=5.634784127796669\n",
      "Wrote weights to file ./customFunctionLearning/baselineMLP/best_weights\n",
      "The loss after 23 iterations is E=2.689639587814777\n",
      "Wrote weights to file ./customFunctionLearning/baselineMLP/best_weights\n",
      "The loss after 29 iterations is E=2.491412933888189\n",
      "Wrote weights to file ./customFunctionLearning/baselineMLP/best_weights\n",
      "The loss after 43 iterations is E=2.4641036193878154\n",
      "Wrote weights to file ./customFunctionLearning/baselineMLP/best_weights\n",
      "The loss after 44 iterations is E=2.1704337835855885\n",
      "Wrote weights to file ./customFunctionLearning/baselineMLP/best_weights\n",
      "The loss after 53 iterations is E=2.125789934478305\n",
      "Wrote weights to file ./customFunctionLearning/baselineMLP/best_weights\n",
      "The loss after 54 iterations is E=2.0328283100868467\n",
      "Wrote weights to file ./customFunctionLearning/baselineMLP/best_weights\n",
      "The loss after 64 iterations is E=2.02853700835548\n",
      "Wrote weights to file ./customFunctionLearning/baselineMLP/best_weights\n",
      "The loss after 65 iterations is E=1.9634580018945718\n",
      "Wrote weights to file ./customFunctionLearning/baselineMLP/best_weights\n",
      "The loss after 66 iterations is E=1.9374435096159088\n",
      "Wrote weights to file ./customFunctionLearning/baselineMLP/best_weights\n",
      "The loss after 74 iterations is E=1.9226582495414892\n",
      "Wrote weights to file ./customFunctionLearning/baselineMLP/best_weights\n",
      "The loss after 75 iterations is E=1.8914285082070768\n",
      "Wrote weights to file ./customFunctionLearning/baselineMLP/best_weights\n",
      "The loss after 76 iterations is E=1.8760080024708865\n",
      "Wrote weights to file ./customFunctionLearning/baselineMLP/best_weights\n",
      "The loss after 77 iterations is E=1.8747789432457556\n",
      "Wrote weights to file ./customFunctionLearning/baselineMLP/best_weights\n",
      "The loss after 83 iterations is E=1.8681855438009038\n",
      "Wrote weights to file ./customFunctionLearning/baselineMLP/best_weights\n",
      "The loss after 84 iterations is E=1.8504656941342037\n",
      "Wrote weights to file ./customFunctionLearning/baselineMLP/best_weights\n",
      "The loss after 85 iterations is E=1.834330172357951\n",
      "Wrote weights to file ./customFunctionLearning/baselineMLP/best_weights\n",
      "The loss after 86 iterations is E=1.8225115136730767\n",
      "Wrote weights to file ./customFunctionLearning/baselineMLP/best_weights\n",
      "The loss after 87 iterations is E=1.8160160492366513\n",
      "Wrote weights to file ./customFunctionLearning/baselineMLP/best_weights\n",
      "The loss after 88 iterations is E=1.8139747565539472\n",
      "Wrote weights to file ./customFunctionLearning/baselineMLP/best_weights\n",
      "The loss after 90 iterations is E=1.813888025611446\n",
      "Wrote weights to file ./customFunctionLearning/baselineMLP/best_weights\n",
      "The loss after 91 iterations is E=1.8111628133452475\n",
      "Wrote weights to file ./customFunctionLearning/baselineMLP/best_weights\n",
      "The loss after 92 iterations is E=1.8052288829066478\n",
      "Wrote weights to file ./customFunctionLearning/baselineMLP/best_weights\n",
      "The loss after 93 iterations is E=1.7967018463346984\n",
      "Wrote weights to file ./customFunctionLearning/baselineMLP/best_weights\n",
      "The loss after 94 iterations is E=1.787080501549547\n",
      "Wrote weights to file ./customFunctionLearning/baselineMLP/best_weights\n",
      "The loss after 95 iterations is E=1.7780121704455474\n",
      "Wrote weights to file ./customFunctionLearning/baselineMLP/best_weights\n",
      "The loss after 96 iterations is E=1.7706485156084912\n",
      "Wrote weights to file ./customFunctionLearning/baselineMLP/best_weights\n",
      "The loss after 97 iterations is E=1.765325540513375\n",
      "Wrote weights to file ./customFunctionLearning/baselineMLP/best_weights\n",
      "The loss after 98 iterations is E=1.761612906379481\n",
      "Wrote weights to file ./customFunctionLearning/baselineMLP/best_weights\n",
      "The loss after 99 iterations is E=1.7586247158955375\n",
      "Wrote weights to file ./customFunctionLearning/baselineMLP/best_weights\n",
      "The loss after 100 iterations is E=1.7554192286748267\n",
      "Time for 100 iterations:  5.24 min\n"
     ]
    }
   ],
   "source": [
    "training_points = np.column_stack((field50.x, field50.y))\n",
    "customModel.set_control_points(training_points)\n",
    "start = time.time()\n",
    "customModel.solve(max_iter=100, learning_rate=0.01, verbose=1, adaptive=False, tolerance=1.0E-5)\n",
    "print(\"Time for 100 iterations: {:5.2f} min\".format((time.time() - start) / 60.0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAuoAAAD3CAYAAABLsHHKAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz\nAAALEgAACxIB0t1+/AAAADl0RVh0U29mdHdhcmUAbWF0cGxvdGxpYiB2ZXJzaW9uIDIuMS4wLCBo\ndHRwOi8vbWF0cGxvdGxpYi5vcmcvpW3flQAAIABJREFUeJzsvXtwJdl93/c9/e77AC6Aeey8dgDM\n7uzMksvlDnd2uaRCJ+aQkRVZlcfQlMopWbYlMrEqFblcXpqulOIkZlwrVUJVueTKLiWbZcUpUZwq\nWbESS9xRsUhHInc1OyIpLrlL7gAXwAAYYICLC+C++5U/uk/f08/b9zUAZs6nCoX7PP243ae//Tvf\n8/sRx3HA4XA4HA6Hw+FwDhfCQa8Ah8PhcDgcDofDicKFOofD4XA4HA6HcwjhQp3D4XA4HA6HwzmE\ncKHO4XA4HA6Hw+EcQrhQ53A4HA6Hw+FwDiFcqHM4HA6HwzkQCCFXCCEvE0KuHPS6HEbGsX/4Pj9a\ncKE+IryD/mVCyGcIIdeZE6E0ZLvzo1rHjMt7xduGl8e8nPnQ87cIIdfGucyE9Qhs7zj3NyHk9ZjX\n5r3j5Fr4eEl7L6adyP4b87Ycit+Pw+kX77x6hRDiMH32y95r10fQfl/nwoM+lx5UH99jHfxtdhzn\nNoALAJ4/qPUZJ8P+nuPYP4O2yfv9g0E66BU46nji6S0An/IOfvr6PIBXALw2RNtXAMwDWBh2PTMu\n7zMAtgH8HsbYaSZs16ccx3kg28msR2B7x7W/vY5sHkBch/ZVx3E+5H3uFoAvAfhUhvfCBPbfOI+d\nw/L7cTiD4DjOAiHknwF42XGcX2Pf80TsfPj1Psl8Ljzoc+lB9fE91iFum6sHsS4PiFH8nndGsiZD\ntMn7/YODR9SH50sAXmVFOuBeDDCcSC8B+PyQ69YvJQBVx3GqjuPcHMcCkrbrgE52f3sB3MKY9rfj\nODcdx4kcC17HV2E+V4Un5tPeS1gGK9LHduwcst+PwxkpjuN8DsDnhxmN6kOkH8S5NPY+Po0Duq4d\nKA9D38j7/YOFC/XhuQ7gRsJ7rziOU/WsC3e8aAY8a8wO9YcRQkreUOQ1772X4UY7SgA+4b037332\nCvNZ9nV/Gd7ja4SQV71hXvrZxKFOb10+wSyv1zqHl3edEPJquE3GunHdO9kj2+V97i26rD63M7Lc\n0DpcIzHbH97elP1Nv3udEPJKzDpcJ4R8NWn5PZhHNJJUYSIXSe+FtzG8/4belqT9Ftf2KH+/hHOB\nw3mQ3ITbryedM7QvpOdKyTumX0k4Fw7kXAqT1seHzv2xXGOStpl5r5S0HXG/Q8z2Je4P9r3Qb5h0\nPUvdtqx9acrvGb4uph0nPUnqN5N+y7R9F7d/8ICPVU4Ix3H434B/AK4AcACUMnz2FQCfYZ6/DuAK\n894V5r3r3v9X6WPv+TyA10PtvhVaxqvse6Hv7/RYx5dD65i4zinLm/cel0Lr9jLcoebIdoWXNeB2\nzids0x36+3jLvZayvan7G8BnQtvwKj0OMh4vTuj5Z+DaW8LreyXtvYzH11Db0mO/je33Q8K5wP/4\n3yj/vP7JSXjvVQBf7XHOfAZuIIa+x55r4XPxQM6lhG2L6/P8c3/A5WW+xvTY5qQ+IfF3iGm7V9/S\n7/Ussm09jou4vpT9PdOui5mvVQnbHeg3M/yW/V7vH+ixyv+6f9yjPgSO49wmhADANGI8doSQkuNa\nFnrxKoDXCSELcA/0JH/kdQC3Q68tEEKuOd1hzLeY9yoYv789vDw64fFvwLWTAABStomyzTweZDuT\nJlp+CMA15nfqZ0j7OoAq6U6WqTDvVeF5/JyQ7akP4tZ7OsN7cWynvAf0vy397rdR/X5ZzwUOZ1xM\nA/hzpJwzjuO8Rgi5A+BzXtSQPd7D5+JBnUtZCJz7XiT2oK4xSduR1nf1084g6xC3bf32pezvmXZd\nHOZaFek3M/6Ww/Igj9VHFi7Uh4cOk8YJimtItsWwVBzHueANM32WEPJVx3ECkwZJjOXBo4QjfLAT\nQq5kFLp9b6c3pPgn8Ca8EEKu9rNe3sNKSqfW64LRiwXEiO/QDWDkvX4X0u+29LPfxvD79TwXOJwx\ncw3AP/P+p50zN71h/4rjOLH9/AGfS1np1Y+N7RrTxzan/Q4HwUivC8Ncq5j1CfSbcG82w4zktzzA\nY/WRhHvUh+dTcE+MgJAm0VR62wgesOyM+88TN9PAbcdxPsu8XkVXrE3DFf1hwT4N92ZhHKStcy9+\nD6HJj0wEIrxdYUa1nfRCS6Mivhcw4fNx+zuwzSnf7ZtwR+dF5m72ei8jw2xLr/02zt8v6VzgcMaO\nF4V8zTv/ep0zrwD4HNIzlhzkuTQID2J5vbY5bp1G0Q8Pcz0b1fokXRf7vVaFies3+/0te+2fw3as\nPjoctPfmYfmD22m/DNevdh2Mv8x7vwTPd+a9/1XvrxT6nv9duENfr3qvUZ/dNe/z173/9PUrcIeV\nXkc3FeCO9/2S176DZG8f+/1rGdY5dXlMm68w3y/FbVe4rWG2M2aff5XuU++Pbk/c9qbt72vozh2g\n3/0qMvjTvc+/7O3/VxD0Hl5h2n+Z3Ya091J+u/lhtyVtv43790PCucD/+N+o/rzj7hXvfKTH28sI\neXS9z0bOmdD7r4aeh4/vAzuXYtY10OfFnfvDLA89rjGh/Z+4zXHb0et3yLh+A1/PwtsWtz5x+zPh\n94xcF9OOk7g2YrY9SUNk+i291xL3z4M+Vvlf8I94O5HD4XA4HA6Hw+EcIrj1hcPhcDgcDofDOYRw\noc7hcDgcDofD4RxC+s36wn0yHA6Hc7CQEbTB+3IOh8M5WDL15TyizuFwOBwOh8PhHEK4UOdwOBwO\nh8PhcA4hXKhzOBwOh8PhcDiHEC7UORwOh8PhcDicQwgX6hwOh8PhcDgcziGEC3UOh8PhcDgcDucQ\nwoU6h8PhcDgcDodzCOFCncPhcDgcDofDOYRwoc7hcDgcDofD4RxCuFDncDgcDofD4XAOIVyoczgc\nDofD4XA4hxAu1DkcDofD4XA4nEOIdNArwHm0sCwLlmVBFEUIggBCyEGvEofD4XD6xHEcmKYJABAE\ngffnHM6Y4EKdM3Ycx4Ft2+h0OrAsC51OB4LgDubcv38fjz32GERR9P9oh887fQ6HwzlcOI6DTqcD\n27bRbrfhOA4IIWg0GnAcB6VSKdCXcwHP4QwHF+qcseE4jh9B393dxdLSEi5duoRWq4V8Pg9CCJaX\nl3HixAlYlhX5viAIEQHPO30Oh8N58NAIumma+Mu//EvMzc1BURQQQiAIAur1OlqtFgqFAgzD8AU8\nAP8zPCDD4fQPF+qckUMFummafmdt2zb29/fx5ptvQlVVdDodOI6DVquFhYUF5PN55HI55HI5yLIM\nx3H8C4NhGIG22Q6fPuadPofD4YwWth+mwRQqusPQ18LvOY4DALBtmwdkOJwB4EKdMzLYiAvgdujt\ndhvlchlbW1sQBAFXr16FZVmQJAnfnH4eADD14z9Go9HAvXv3UK/XYZomJEnyhXsul0M+n4emaf6y\n6I1AGN7pczgcznCwo6G2bQNAIBAiCIIfhKFCnH3MwkbV45bDAzIcTjpcqHOGIinisre3h8XFRbTb\nbZw/fx6PP/44fvSjH0GSJFiWhf9w8qrfxg+e/kk4RrSDrwF48u1/h93dXayvr6PVagEANE0LROBz\nuRwkSfLXxTAMdDqdQGdOCOGdPofD4aQQNxoa10fSUdLwa3FCPY2k/pe2wwMyHA4X6pwBiYu4AO7k\n0KWlJciyjNnZWUxNTQEAWq0WbNsOimc5+NgxHP8/5cfv++uR15oATv3oj9BoNLCzs4NGowHLsiDL\nsh99pwJeVVX/e3Gd/r1793DmzJmIgOedPofDeVRggy1UoMfZWyisKO8VUR+ELFH4cECGXgempqZ4\nQIbzUMGFOqcvkvzna2trWFlZwdTUFN7//vcjl8sFvsd24m898zEo07L/XqfiDnlS4c4K+EAbjGD/\nwcWfjLzfAjC3cBONRgOVSgUrKytot9sghERsNLquQxRFrK6u4tSpU4ELFIUKdkmS/Me80+dwOA8L\n1DfO2hXTBDolTpSPUqj3WnZc/9toNNBsNjExMREbhY8bUeUBGc5RgAt1TibiIi6GYWB5eRkbGxs4\ndeoUrl69CkVRYr+f1onnz2sw9qOTjICoiI+0y4h3IhN8b/5a7PtNAMff+X/RaDSwvb2NRqMB27bR\nbDZx586dgIiX5e5NRNLQa9ywK+/0ORzOYYeNSMf5z7MgCMJIrC+jJulGg6YI5gEZzlGEC3VOKnER\nl0ajgXK5jL29PTz++OP4yEc+0jMKE9exy0UxUaDT95RpOfZz7Y1OJvHO8s6ln4p+FsCxO6+j0Wj4\nth2a550V77lcDrqu+5Oo0jr9JAHPO30Oh3NQhEdDgf4FOuUgI+qDMKgXngdkOIcBLtQ5EagQrVQq\nEEURuq4DAHZ2dlAul+E4DmZnZ/G+970vc4fFduKs8JaLYuB/knCn0O+qJ5XIa0A3Ah9ZfoJ4B4C/\nvPCJ2NcbAJ764f+DWq2GjY0NNJtNOI4DTdMiIp6mlAQQyWBAtz9NxHM4HM44oKOh6+vrOH78eOIE\n0X44akI9iV5eeB6Q4RwGuFDn+IQjLpubm9B1HdVqFcvLyygUCrh48SKKxWLfbdNO/O2f/KtQJxSo\nE+7rkibCbFlo73X8z1LRPgjU+x6OwrMWGjppFUDANhP3+Ecf+OmIwG8BOMOklGw0GjAMA6IoRiaz\n6rrub3tcp99sNmHbdqCaHxX1vNPncDiDEh4NvXPnDk6ePDmy9qvVKsrlMgzDQC6XgyiKaLVa2N/f\n958fZXpF4eMCMltbWzh+/DhkWeYBGc7I4EKdE+s/N00T1WoVd+/exenTp/Hcc88F8pj3S69oizqh\nuKJ9wvLFuzqBkQv4OJKi8Gm8/eR/Gvt6HcDUD/4wNqVkWMRLkoR6vY5ms8mr+XE4nKEZhf88DTZx\nQKFQwFNPPQVZltHpdLC9vY1arYaVlRV/DpCqqpHRx6R5TEeFtCj8ysoKZmZm/IJ+SVF4HpDh9AMX\n6o8wcf7zVquFpaUlVCoV5HI5zM/P4/HHHx96WYN2ROqE4v1nX3P/syKeJUnQJ3ni40S8XBRRX3JF\ndloUnn1O+dHTPx1pLymlpOM4fsdNL2ZsSklezY/D4fSiV4GiYaGJA9bX1/HYY4/h3Llz0HUduVwO\nlmUhn8/DcRzUajU8/fTT/jp1Oh3U63U0Gg1sbm6iXq/3HH08ytD+PLwd4Sg8D8hw+oEL9UeMpIjL\n7u4uyuWyX6Do0qVLuHv37sg9h7npHBqVBsyWGzlPgkbVk54DXREPAFqp26G1qm3/MRXnceI9LNzD\nz5Oi8OEIfJJtJvw4nFKSfV/8/v+NnZ0drK6uotVqgRACXdcjhZ1EUcxUzS/soeSdPofz8JG1QNGg\nNJtNlMtl7Ozs4OzZs3jppZcgiiKWlpZ6etQJIVBVFaqqYnp6OvBZ0zTRaDTQaDQio4/0BoAV8kfJ\nRhO375Oi8HR/8YAMJw0u1B8R0goUlctlKIqCubk5lEol/z1CSGznMSgzv/4/AaoIJa9CyQOSKsJs\nW1Dy7vudejsg3qk4jxP0ccKdkj+hw+rY0LxNof9ZAU9JE/JJKNMyBCm+o2xtulH+fjPSvPf+n4m8\n1gRw8t1/j3q9HkgpqShKJBrFDifzan4czsMNvVHvN/95VmjgptVq+YGbcKXnYSaTSpKEiYkJTExM\nBF63bRutVsuPwlcqlUC/x/Z5NJXuUe6/shR24gEZDhfqDzlZChQ988wzkQJFAPxUhONAUqPCWMmr\nvninn7EMG+39VnwbnlgnQraOSSupvmgPEhTwrGhPSyHpr0dBhFlzP6OdiPdftjaT00kCyQL+h0/9\ntWhbAM57KSW3trbQaDTQbrcDKSXZwk70d6QjKeEJUJZlwTAMTE5O8mp+HM4hhRVuNIDS7zka9k2H\n39va2kK5XIYoin7gJu7zgiD46zDKyqRsHxZet06ng0ajgXq9HkilS200tPJ1o9GApmkjvXE5CAZN\nKVmv11EsFqEoCg/IPCRwof6QEuc/NwwDS0tL2NzcxOnTp/HCCy8EivuEict9PgriRDor0MOoRc37\nD4iyAMtw1ylOwIuKEHhsddzPSpoEsxXt1ABXwHcfd1839hs9tqQ3VMSnCfg4q0zcc/a1Xikl6cWM\nZpVRVTVio1EUBY7jYH9/H/fu3YudLMyr+XE4B8uo/Of0hj38Pdu2sb6+juXlZRSLRVy+fBmFQiG1\nLSrKe0XZRwVro5mamgq8Z5omms0m1tbW0Gg0cOfOncAkftrvHUUbTRy9ovBLS0uYm5uLLUgV15/z\ngMzhhwv1h4ikiEutVkO5XEatVstcoAgYvVDXp4JREqMRjGTHCfg01KIGNZQpUlIl1Ldqwde06GFO\nX2PFe1jMF8+568uKd2qfsZrx+0UquNtAI+xJn6HvF+b12M+yFho6kTUpD3z4vXcv/2fR9gCce+9r\naDQa2NjYCKSUlCQJpmliZ2cHuVzOj0bxan4czsExav857c9p328YBu7evYu1tTWcOHECV65cCUxk\nT+Mw5VGXJAnFYhGlUgmapuH8+fMAujYaGoVnJ/En2QePet9FR8xpekhK1sJOPCBzOOFC/SGAdug7\nOzuwbdv3/VUqFZTLZQDA7OwsZmZmhi4TPShr/+2n/MeSKsFsm5BzKvQp97EguutFo+T9iHbaHoVG\n4MOvU5Ii65F2Y6LwfvQ9YqGJZqChop0+ThPv4c/0stD0Eu9xfP+JT8a+Pvfd30elUsHe3h7u3buH\nZrPprkNMYSdJkng1Pw5njNBgy/LyMk6fPj0y/zntz5vNJpaWlrC9vY2zZ8/ixRdfhCT1JwXibJGH\nreARa6M5duyY/zq1AFIf/P3799FoNAI2GrbfO2o2GvZmjDJMYScekDl4uFA/woQjLvv7+2i326jX\n636BoqeeemqgAkXA+KwvacRFyYGozYWKcEnNfggreSVWvJst04+wU3GeFIUPC3f6mlpU/JsNAL6Q\nb4YmsLLiPQthga+dUGJFP2uhYRF1ITH6T99ffPa/AADsht5rAjj94z9GvV73h5VN04QkSREbjaZp\nqYWdAF7Nj8NJI2xXXF5extmzZ0fWvmVZePvtt/3MXhcvXhxYgNLI7YOyvowSQggURYGiKBEbjWVZ\nfgR+f3/fr0gNJNfCOGzECfU0BvXC84DMg+PwHWWcnsTN+LcsC1tbW36UpJ9hzCTGIdRZYU0f0/+2\n6YpPQRL9x6IiweqYfpScRckr6NS7kex+RDvbRm465y8PADChorUXzRADpHvdKaIqwfJuBvSS2o2+\nM1H4JuLbH5SJJ3Po7HbXi05cbW90IOpup80KdvpaL5IKO11Y/BM/J/zq6irabXd7klKrpVXzYyOG\nkiQFoje80+c8zNAbW9M0I/5zKoaHieY6joPt7W2Uy2XU63WcO3cOp06dGvq8OkzWl1EiiiKKxWIk\nuOU4DprNpp9SkrXRyLLs93WmaaLdbh+ojWbYY4YyTBSe9vmKovCAzAjgQv0IETdBlM1zWyqVcObM\nGVy8eHEkyxulUJdzKuScGnguyBLau/XgMqVsEWcq4JW8axFR8or/2ijQJrx1nQje7CQJ+MT1ZEQ7\n+1xnJrCGBTyNlvcbfY9DPRlvoWlvdG9waNQ9TtAn8d25j8e+/vTKN2IvZtQTyop4ejGjnf7CwgIm\nJiYwMzPjt8er+XEeRsKjoUA0simKIizLGkh02baNe/fuYWlpyR9ZpefXKM4bVpSzgu6oC/UkCCGJ\n2WhYG41pmvjhD3+ITqcTsN5QIU+zcD2I9R13+2lReNM0cevWLXzoQx8KfCdtVJWTDBfqh5ykiEu1\nWkW5XEan08Hs7CwuX76MSqWC+/fvj2zZoxLq2//oFxLfk/NupJyNohv1lv/aICh5xU/ZqE91o+Xt\n/a7I7mWdSRL9+WP5yOvUlGMbFsSY9sJiPe39QAQeABEJGt4SzJo1EvEO9Bbw/Yh3+lkiE9ye/48j\n7//EvTdhGIY/pLy9vY2VlRU/pSQt7FSr1ZDP5/0OnVfz4zxssJP9e00QFUWx7/7XNE3cvXsXq6ur\nOHbsGJ577jk/m9MoAy/s+TnK9IxHjbCNZm1tDR/84AcBdG00jUYjYKNxHCd25PEw2mj6hR7HbK53\nSlZbJA/IRDn6R8ZDStyMfwDY3NzE0tISVFWNFCgapGNPY9TWF0GWYBu9I95UvFMk3Y0+m802rI4J\nURnssFWLauQ/K97pzULW9qmYp9H3YD73qH0mTsRnITfj7Y8Z5sUZNwNNZ9eEMhltV86JMBpW4vMk\negn4XuJd1EVYzeBy/r/HXnDXoSjBalr46Oqb/nuWZflDypubm9jY2MDq6qqfUjJ8MWPTiWbt9HnU\nhnPQxI2G9oqssrnKe9FqtbC0tIStrS2cOXMmdoLoKPtzassJv/aoCfU00mw0bFGn1dXVWBtNeOTx\nKGFZViQNZpYoPA/IxMOF+iEjLuJi2zZWV1dx9+5dTE9PpxYoGmUl0WE79kajgXK5jDM5FYAK0bO6\n2IYJQU4+9JKi2ZKu+qKdPh9UvHe972rkNQAB73tW2FEBgLHPAADcx1S80yh6nHiXc64YNRpG5D0W\nt4CTGngOxFdgdduNF+tKQUKnln4DpZ+Jzndorva2AcUJ9z894wp3IrtC5SPlb6NQKGBzcxPz8/PI\n5XJ+gRN6Mdvc3ES9XvdTSiZlZghX86PnEI3w8E6f86Bgi4wNkv88S+Blf3/fT717/vx5PPnkk4k3\nAKMW6nEedU5vCCHQdR26rkfeY4s6hUceD8pGMwhxQj2JJC88Pb54QIYL9UNDXMSl0+lgeXk5c4Ei\n6mkcFYN27Pv7+1hYWECr1cLs7CxEWYLlRdIlT7SzJxO1usRBBTgr3FlBLelqIAIv5+Pb7Ne/ruQV\nkNAE1vgCS1JfNwqueKeityt+W3vu/ySLDLXoxKWbjCyjpIIIAvTp7mvNSjPT+mUR7ZSweCde1pvm\negdW04Ko9+6oBYng20+85E9+Fb/zdbctpsDJ9PR04DtpmRniCjuxKSXpeUahnf+j1OlzxssoCxTF\n9eeO4/ipdx3HyZx6d9xC/VFnFPuD2mjYkXKgO/JYr9dRq9UCNppwUafD8Lv0I9ST6DWZ9VEKyHCh\nfoAkRVxqtRoWFxdRr9dx/vx5PPHEE5nunA/S+uI4DnZ2drCwsABCCObm5jA1NYW9/+W/AVKi50DU\n6tIPoiIFotiiIsPqGIltZhXVbrvBbaeZZ1jRLyoSGtu1yHfZ5STdZACApMkwWwb0UjS6QkkT5+F0\nk7Iuw2hGI/H6tO79776WJN6pWGf/A4DZynYTqJ8K2mdqC03IxeA+lyakWOvM957/qwCAq3/5p4nt\n9xpSpr7Q9fX1QErJcESKTSlJz0PDMFCr1VCtVnH27FkQQnDjxg1cu3bNL6QyCLdv38aVK1di37tx\n4wZKpRJu376Nl19+eeBlcA6WURcoCgdebNvGxsYGlpaWkMvlcPHixb5S747Do35Uhc84GKdAFkUR\nhUIhUjGW7fNoGt1ms4k333wTsixH0kmqqvpAfrNRCPU0etlo2IDM4uIiTp8+DVVV0el08OUvf/nI\n9bNcqB8AtEPf2trC5OSk/zqNkhBCMDs7i+np6b4LFD3oiLrjONjc3ES5XIamaZG87aIn0sUEsR62\ni1BYiwv73Gx61hFG8Ca1Edcm266c12HUmz3Fe5zgZmFTR1Kfelz0vV9ou8G88i1/EmySQJd1GWbb\nyxyjiv7j8GepeCcC8QU8K97TIuty3rPn1A3/eVLayokngzat1rb7HVEX/Nzv4Xzvt1/4jwAAV978\nD4nrEIYdUmYzxwDwJ7M2Gg1Uq1Wsra35ZcbDE7va7TZM0/Rvjv/4j/8YH/3oRzOvR5ibN2/is5/9\nLO7cuRN57/bt2wCAa9euYWFhIVXQcw4ntm2jXq+j0+kgl8uNvECRaZq+9XFmZgbPPvtsrG0ia3uj\nIM6jznnw9p9wn2fbNvb393H16tVANpqkCfy0zxu1jcayrAOZHBsXhd/f34csyxAEAZVKBd/4xje4\nUOckE/af//CHP8SLL76I9fV1rKysoFgs4tKlS5G75qyM2vqSNrxp2zbW1tawvLyMUqmU6JsXdS/j\ngDeRVNQBq9mCbVqBrC5UbAuKDLsTjAizr0m6a+sgkgjHzxLjiktR6dqCaGSdvmZ14v3ecl737TLu\nc/htCpIQiaqnwUb34/K+U/FObzIkLdnGlIQ2Gb1At/dbsVH0fqq7UvqJvCeh5GV06sn+ev24AiK6\nF4X6StOfnCpIwYscEQn+4qWP4blvfbOv5cchyzImJycDN8ZAfJnxarUKy7Lw3nvv4atf/SrW1tbw\n/e9/H8Vi0a8U2Q/Xrl3D/Px87Htf+cpX8IlPfAIAMD8/j5s3b3KhfgRgh94ty0KlUkGtVsMTTzwx\n0mWsrKzgnXfewZkzZ3paH3vBrS/j5TCMMFDrB+D2eaVSKdFGQ/s8WpmV2mjCUfhBjrlxR9T7gY6o\nEkKwv78fKXJ1FOBC/QEQ5z+nhRG+9a1v4eTJk4eyQFFcp2OaJlZWVrC6uoqTJ0/i+eefh6LEZwpJ\nQtQ1iAhmSTHrTQieqI4T6/46xdz1q1NF2KydxJtkygr3zOvmiWg5r480L7uoSAHrTDdlJBXvIqyO\nFXlMnwMIvBZuN3pjEK7k6kbWZb3/fZI/Hr0Ba1S67dPoehbUohIQ8cXZHIhA0NjqtqdMSrA63eN4\nVGI9jrgy43fv3oUgCH6qyC984Qv40z/9U3z5y1/G3t4evvnN0a1LtVoNePC3t7dH1jZn9CT5zyVJ\nGlmQpFaroVwuY2trC8ePH8ezzz470gj9KOBCPcphEOpZih1lsdHEWQfZCHwvG41pmrFC/ZvTzwPo\nFuQTJBLIAjYu6HpWq9XIjctRgAv1MRGOuADBAkXVahWCIOCFF17oW+gmMc5Oot1u+xePM2fO4MMf\n/nDPoa32b36uZ7tEFOFYFuSJAhz2QueJ7X4RFAl2x4xYZ+S85k8wpZF2NhVjL0FOBAI5rzFC3n2d\nthm2zmRtN05kD2ubIQKJjbyPbYxFAAAgAElEQVS3doOR8bCfPWynSSM37a4zOeYup77ltq2ERLsg\nZjsmc8c0iIqAxnZ025W8jLevfRzvu/knmdoaFtM0kcvlMDk5iU9+8pP4whe+gC9+8YsPZNmcw0kv\n//mwo5mO46BarWJxcRGWZWF2dhaFQsGv0jsKRu1Rt20b6+vruH//PgqFAnK5HGzbPhSC9SA4DNtt\n2/bAkews1sF6vY5KpYK7d++i1Wr5NppwFJ6NqP/Z7IcBAI7hpva1zeANHs0C9iAE+87ODo+oc5Ij\nLrRAkWEYfoGiW7duHfDa9sa2bbz99tvY3d3tmf4rjKBpEDTAbkXFlyBLQWEegzxRgEPLzVPhTQSY\njf6sGDRyTyeYshNN2ZuBQfKzy3ktYJ0RlWi11XT/uxyx5fiRd1Xx8863dht9r1sYVrxrk8w+YMR5\nnPedvdlImqyaP6aHRjvc9TVbJuScDKNhQC26N6TU9qIWlUgKytyMBiIIaFaasC0nIPwflFinEaQH\nQalUQqVSAeBGe8IXSM7BQoMtvfKfS5Lkf6bf9ukEUU3TcOHCBd+e1W63R25lHEV7lmVhbW0N29vb\nyOVyOHXqFAzDwP7+Ptrttn9doz5o+neY0wmOAtZ2clAMWsm2F2nWQRqBpzaaZrOJVqsFRVFw99qn\nAp8nsgBRRiRlLwB8+4mX8OH3vjXS9Q6P+lSrVS7UH2WyFCian58PHOi0cx9VRH2U7O7uYnFxEc1m\nExcvXsTTTz89ePaCYgGOYUIIuTOogCcpEQAiy12x7iHlotFiu9PNvEKj6oInkB1moqkgy7CZ9mjk\nPRyBD6y/Z1XJWilVzuvMY/d/10c/YNGjYxOBYlFEIJEIeRy9ouSsYCeCgOZOIyLG/VGEkG1GySux\noh0A8sfzgectsfe6ssvVp3VfxBOBwLEdiLKAd/7aJ3Dp37/es61hMAzDF+rjGt6nQ7Cf/vSnfWGz\nsLCAa9eujWV5nP6gkzjZ0dC0/q/fiLplWf4E0ampqdg5PqIowjDSayn0A1tNdBAMw8DKygrW19dx\n/PhxTE5O4sknn0S73fbPl+3tbVy9ehW2bfvpBFkfNIBIOsFcLndo/MzDcFgi6g/yZkEQhFgbzcLC\nAqo/+7dAShKaTbcmCa2bAbj1NejcJAB+UoFRE/bK7+7u+hbHowQX6kMSF3Gxbdsv4zw9PY0PfOAD\nsbP0Rz35c1gcx8H29jYWFxchiiLm5ubQaDRw4sSJkS2DyBIcT3AKmgYIwY7NbrXhGAZIzAQWIssh\n0d2tdCpPBDsKKUcCkXd28mkvJEZoS57eNOtBoZnFv05vGIDupFXWl9+1zUSj6lmgEXK6Hzo1t72w\nMI+LkieJd30qF2xbJGhUho/m52bcdolAUF3Zg5IPdj1+tJ25yOhT7k1EazdogVr89F/H3Ff+3dDr\nlAQbUW82mwNl2GC5ceMGbt26hRs3buD69esAgI9//ON46623cOXKFdy6dQs3b95EqVTiE0kPEFri\n3DTNvvOfZ/WodzodLC0tYWNjA6dOncLVq1cTJ+sdlgJ2rO3x7Nmz+PCHPwzLsvDd7343cd/QOR75\nfPCG3XGcwETGSqWCRqMB27YDExmpgD+IzCGDYtv2IyfUk6j+7N/y62oo093fkBXkrAVGmhBh7ln4\n9hMv4eTr/1fARjPM9hiGETi/dnd38eSTTw7c3kFxdM6CQ0ZcxKXdbvtlnA+iQNGgOI6De/fuoVwu\no1Ao4PLlywNnnqHYv/frAFxhngSJsb8ImgpoaiDybQN+VJ0K7rjKpqxwB+Ij71azHYmqBxsJdgq0\nTSmvM49pZ9xN7dhvdVRBlkKFmkIeejV5lEVU5cB2UpSCl85xorsN7b1sIjs8SZeNbuemo5NJ44hL\nBRnH1Hl3Mk+j0oCcc9M6ktANm5JX/Am3uZkc6vfrfsVWYLxi3TRN/7wdxVDp9evXfYFOeeutt/zH\nn/nMZ4ZqnzMco8h/LopiqvWlXq+jXC5jd3cXjz/+OD7ykY9kmvR3kEKdVpauVqsR2yO9qekXQkhk\n8jZtr91u+xH41dVVNBoNWJYFRVEC4j2fzw+V/WZcPIoR9TDv/vQn0d7r+CI9jDIpobPrnifajIzW\ntgFtRobRsCBNiMgd01D7O38X5F/9S99GY9t2oJAd/Z/lGAjbGLn15REgKeJCyzjTAkVZfdyD+hrT\noBH9LMunw68rKyuYmZnBc889B02LphUctAMSi4zYZ7zqbFSdTiZNQyoWItFw1vfOCvSwWA9bZ0TP\n4iLqqi/KLS9aHhbvcTcDgfXK61BKrugPR96FGNEuKFJqVJ8Kd6XYvcGgUfJIW6HtTEKdcEW2CgCE\noLPvCvcsE0eTJoLSqLv7OI/mTj3WAkOtMon2mGPuTsti4Sk9Xsr0uVHAZiw4qh07pzfhdLnD5D9P\nEsF0giidm9SPhXDUWbyytre/v+/bHufm5nD58uXIOo866wshBJqmQdO0wDwNx3HQ6XT8CPzGxgbq\n9XogEwn9ozdaB8WjLtSpSFcnFL84Hs3wJefc/lSQRWjHRNiG+75Scq+TtKgeZXZ21n9Mb+LoMXDv\n3r3AMRCeyEoL2QGjE+pZitKFa2B87nOfwyuvvILXXntt6GAMF+oZSPKfb29vo1wuQxCEgQoUjSOi\nTquTpp2shmFgeXkZ6+vrOHXqVGrkn3bIA3VAogjQEQdJ9j3qRJLhmF60PHaiafKdsh9R924oBM0V\n/oJhxraVBr1JED2ri4iuQLaawbaoaA9HfsNIeR2EMPs+BxiNZiB9ZNoNQCSyXOjeOImqgub2Xux6\nsZ9JE/BKMcc87tpv2CJKSSkcJU2OtfvoU3nkZtz1bu40Um8AaOSe3c7cTB5EEPwKr0o+fjRBm9Rh\nNA04tnsxXvn5/xzn/vW/TVzWoLCTwnZ3d7lQf8iIS5c7rLhh+0fHcXD//n0sLi5CVVXMzc0NlBLu\nQUfUd3Z2sLi4CNu2MT8/j6mpqcR+/0GlZySEQFVVqKoaOQ9pQZ96vY6trS1UKhUYhoHt7e1IBP5B\nVOR8lIX6nf/qpyBIIiQteD1S8jKQR2IxPIpjO4ER03d/+pN46g+/BiB4E8emsgVcIU6LOlWrVayu\nrqLVavkZbIBuRiVJkgbqz7MUpYsravfaa6/hxo0bePXVV/taXhxcqKfA+s+/973v4dlnnwUArK6u\nYnl5GRMTE0PZRMYh1KmvMc7b12q1UC6Xsb29jXPnzuGll17qOYmHdu59n/wx7bIC3f9YoRiJqDtt\nNloue9/tPdmIet7ZNbVb7dgJqb2ghZqAGL+7J/DTbihY5JwOUH0skIDfvVdkPGzTUSaCNhTCRMmT\n7DKiKsNqx28/jd4rRVdEk/2gVabf4kz6VM4X4c2d3rYbtajB8CYb5WYKEGUB9a1u1hxBEqHkFV+c\nU2gxp+2//19j5ov/Z6Z1G4SjmneXE4Smy6UTIhVFwYkTJ0YqrGiBopWVldQicFmhQZdRESfUHcfB\n1tYWFhcXoShKIOtMGqxQDwv2ByVYwwV9NjY20Gq1cObMmUABs7t37/oVOVkPfD6fD0Rfh+VRFerl\nn/sZ2JYT6aMpkiZC0kSYLQu2FfyMXlLR3u8MvGxJkhKz0TSbTT+N5Jtvvolf/dVfxdbWFn7xF38R\nzzzzDN7//vfjF37hF3ouI0tRuriidl/60pcilsdB4UI9hriIS61Ww8LCAtbX13Hy5El86EMfGrpA\n0TisL3Hiv1arYXFxEbVaDbOzs7h48WL2FIsDDL86f/QlENUVuqzoBlyxHgdrgREKbpYYUdV8m0y4\nnV74YlrzfiPvf9pk1TgE2bWrxPrdW23mc66gDkTTE2Anq1I61X0vGm74baXdCIiyBMsT+GyUHADa\n1Zp/A+BH3BMuIOGJrGxbRCCRVJOx2xOKtIuqAqvd8S0yuRkB++tVyDkltmhTd5vcfZc/lkdrt+l7\n1Fm0Sd2/AZBzw51/cYQvtkc17y7HJS5dLrVTjEpUdTodrKysoF6vo91uD1QELo5xTiZl5yUVi0W8\n733vi0z8TIPdd+HHBy1YJUnCxMQEJiYmAq9bluUL+N3dXayvr6PljcKy9olBU0ke9HYDD16ol3/u\nZ4K1ODQJDqMXJE0MPO7U3etE4WQetQ332qIWFbT3O5A0CWbLhJKXoJeGm8BPJzPruo7JyUk8++yz\n+OQnP4mPfexj+O3f/m28++67WF1dzdTWoEXpFhYWcPPmzVS7TFa4UPdgIy6s/5xOqGk0GlAUJVMU\nOiuiKKLd7r+oTxpsZ1ytVrGwsADLsjA3N4eZmZm+O5KhfJKS7ApXxgIDAPBENxXtaekZKUTXffFN\nt8Dp0+oCuJNVSUh0W41m9yYho/cb8LzzluVbZkToMPdqgfSQbJpIOyVLjDpNIwJuW2ZIH6dNLo20\nVSoEJocaoYw1kqbAbHUgab3bVCfdizdtj0bvzVYntkpsGPqZiTOu2KU+c6tjJlpcADe6nlT4ifXI\nj5pwOq9qtYqTJ0+ObXmc8RDnP6d/kiSNpN9lJ1uePXsW+XweFy5cGJlYG4f1xbIsP+o/PT2dOC8p\nK+P2ro8SURRRLBZRLBYDr4dTSW5ubqLZdPspmkqSzQWfdP1/1IT63V/4L/3H2oQKs215Il0MJCMQ\nJBG2aUFSJV+oA65YpwkYBDm4TwVJxNovXsfp37ox1DrS4nUUQgjOnj2Ls2fPDtVuFqg4f/3113Hz\n5s2hUu8+8kI9LuICdPOIm6aJ2dlZVKtVnDt3bqTLHpf1ZWtrCz/4wQ8gy3Lmocy09voW6mYPm4mq\n+RF3n3bvnOphCHOBId7DJPFOI+gBC4wkAaYJMSTcBZ3A3Nvv+tL7Se3opYkUc93MMFmIZKxhou5E\nyAXsMmLIly4oMuyU9I5yXodSjKaFTCPJj64UXWuLOpn3RTvAWmTSxb8+7e6fzn4jIPRlXfYj6DRf\nvVrUYsW6NqkHJpaO0v4Snny0t7eHy5cvj6RtzvjJ4j+XJAn1eu+RoiTotaHdbvvF6wghWF9fT7Qd\nDsIorS+maWJ1dRVbW1soFAoji/qHOcxCPYmkVJK2baPVavkCfnt7O5BKko3A5/P5R0qor/1i19JB\nRTqLrMuQVPc8aNe61pbctObXxWChmcX21vaQPzZcxjmWURSvG6Qo3WuvvYbp6Wlcv34dMzMzWFhY\nGGodHlmhHhdxcRzHL1Ck63pA5N65c2fkJ8EorS+2bePevXu4f/8+DMPoeygzCZpFJivO138HYO0t\nScKbnjym6T3WAhMMiQpfvIdzrSevqwCi5wIRfLtPywy1iMgz092UkLIMa7/mW2bSrDNsRhsAgZsA\nMedG70GEgMUlLVouqAocw/CFu8RYa+xqd2JpL7HOet3lYg4ya3Gpd6PcSTi2HYmgU5tMwCKT8UKl\nTuYhSCJaO7XEz0iaAklT0N5r+NEZmt9dm9R75rEfhLgsAdyjfriho6FZCxQN0u/SCaLlchmyLGN2\ndjZiiaKBl1EJ9VFYX2je9s3NTZw4cQKlUglPPPHESNYvjqMo1JOgnvZcLofjx4/7rzuOg1ar5dto\naCrJTqfjawl2MuuDTCVp2/bYc8/f/+9+DkQgIALx7YqAa10023ZiMgLA7bdp8EUtapHEAxOnJ+K+\nNjBsf84WsuuHpKJ0adeG559/3ves37lzB5/97GcHWX2fR06ox0Vc2AJFMzMzePbZZyNFTsZRRXQU\nEXXTNP11P3bsGE6ePImTJ0+ORKQDA1az0/OAHdouKpgtqyvSAwuKuah6UXf/nXYrNp2j73uP2ZdC\nvuDeDPjrpsNuNruC0ouq90LQVMAX3TqIJMPac4UyFeepOeO9ibRiTgeIELDLJKWa7IVScjs1eoNj\nNpqwWh2ImpJqswlDRbuMYGGnLEWdKOpkHhAEt56AJ9olXYXVNrz/Hch5HWarG12xTQvaVAH6jIDW\nzr67DjnVrYKa1+BY7g2iPjMB4+52ap763X/8dzH5v/525m1OItyZ8/SMh5e40dAs+c9lWc4s1G3b\nxtraGpaXlzE5OZkaABmHVWVQ0dtsNlEul7Gzs4PHH38cL730EkzTxHe/+92RrR+FXceDjig/CGhG\nEV3XA9HV7e1tbG1tYWZmJpJGUJblSAR+HCMa446ob/33fxMAoORVtHab/jwhSfPOv5jruF7Se2ZL\nS2L9M5/Cqde+OuDaBmti7O7uDhR0SSpKR4vWAdGidleuXPGj6hcuXBi6kN0jIdSTIi7hAkUvvvhi\n4h3XYRPqbIW7M2fO+Ov+3nvvHYpqdhFUDRCloIBPinYLXhSeflaSXTuNqrm+9xi/u0+adcbrLITi\nRMSeY9dqrmi3rPQ22OY84U4kGY5lujcA7Hpk/B1oO3732mi6E0FDkXbBO/aShLyU0/1Jr6ZnubE7\nZl/+drkQEiEJ4p9OGKUdsKhrsNquCFcn8yBC0Y+009fT0KaKMOrNxBuDibMzkXb0mQk0t/cg6aOb\nVBqOqPP0jIePYQsU9SpOBMDPDrO2tpY5eUDW6qTjhCYOqNfrmJ2dxaVLl/z9Muq87HE8TBH1QZAk\nCdPT05E0gmwueDoyQ4MC4Uw0iqIMfMMzTqG++/m/49lW3OeyLnvZXrrHlFp0g2udegeObUGQRKhF\nFZ16t+9mEwJ0v6eivd9G4eQEzJYxspoZbOBlmMQAcXnQ2aJ1cUXtRlnI7qEW6kkRF1qgqNFoZC5Q\nNI4MLYN07Gyk5Ny5c5EKdw8ipVffiAmHmap5lXgYwsI7IUtMoA0K7dxa3knexzCXUGSG3CQZTt2N\n8PoWlwydn0BHYXT3e/Z+DY5pJGa6CXyXsaYIOR0iY3GxgMT0kkmRCokR3IQQmDW3YxRCedZpppkk\n5EIOcsGNuJt+YSgzk/gGaKS9iM6ua3GhE1m769/dTnXKneRl1KKdtJzXIOc1tCrBPPKjFOlAMAID\n8Ij6YYJNlwsMnv88LaJO+9dKpYKzZ8/2lTwgyw3AuNjd3cXCwgJM00xMHMCF+nhJ86grigJFUSIR\nXZoHvF6vo1KpYGVlxU8lGc4FnyWV5LiEevVzvwCr41aQFhUJju1EUi0qedUPtGiTOhrbNahFt3/W\np3Jo7jT869X0/HFf4McFZ7RJHZ368BO+2f1xlIMuD6VQ71WgSBRF32OY9c51XKkUs7ZJq8U1Go1I\npIRlnCm9emG98QcgGiMQO63MnmUfVQPYFIcdRrhTwUsvnOx20mg4AGh69/OmAcBrw7a60fowoQg4\noRNeQ+LfsaxMwhugkXLWQuXaXKhNxjHMRP8963WXJorBz3oR936QCkymFEFAZ2c3EmmnqSUFVYEd\nI8TlIs0AQ2B4E0lZjz09HiUmuk5RJgsQRBHt6n78+ukqbG//a1NFtHb2Y/O/h61Bck71O//O//b3\nofyDL6bshd6EI+qNRmOoXNic4aHVoLP4z7MQ1+/u7e351TjT+tde7T7IiLrjONje3sbi4iIkSepZ\nWOlBiOh+5zQ9TAwymTQpD3g4leTa2hparRYEQYCu65Fc8FSM2rY9sqx0lMY//Xt+QMQ2LYiqArPp\niuiuLz06ahvOzuXW2Ei+iaDzjyj5427wrP3rvwL1H/7GwOtPf5OjPN/ooRLqdJIHhXZMq6urWFlZ\nweTk5MAFih5UznMWx3H8anGO42Bubq5n9dMHXc0uDVuN5kIVOiHRbFtdAc7aUahw1vPuf4WJnHd6\nTBD1BboHjboLQneZFNPoHbUHQIoTgGG4fnlZBiwLzn62aDmLoGmA7nVWOmDV9v186X7+9B72KiGn\nQ2Q6PKvRDETcw1HzOJQp98JABDfanhZVdz8X7GDlie45ZNbSM2hQmwwAqKUiVIGgcW8bckFP9NJr\nU0U/q4yc7/72SjGHzn4jcft+8IMfRPIh93PxNE0zkq7uUfDdHjZoqlzTNP39P6xAp1DvNyt0BUHA\n3NxcX8GbMOO4RsThOA42NjZQLpeRz+czX9PGcRzTNlkLEo+oD09aKkkq4Pf397GxseGnktR1HY1G\nA6qqQhRF5HK5kUTXbdNy0yvqqt8nS7rqZ/hSiiQ2k5hS0NGJGR0F3JFWOi+pF+3dWmTgfRCO8ujo\nQyHUaaduWRbeeOMNvPDCC7AsC8vLy7h37x4ee+yxodNRjaMTThLBNPtMuVyGpml48sknI8Ub0to0\n+qzCOcg6srTbbZTLZdBcAg61uoQnlAKwc8WIP1wwmehrTMEgR5JA2H0fcwOATisYFQ8L6HCHRcU7\njZr7dhnme5IMWClVQ4sT/k0BgVvcKZwRJq4aK4s0NeVPjiWSDOwHrR2BCaq2ExuBdyeoutEL6pOn\nkWdf5KgKnAS7ilTI+fvd8tLWJUXVRU2F3emEvp8HkcRYwc5WeGUpnD0BAGhu7iTaV9SpIqxWf1Xr\nzp0751/E7t2759+467oe8IImXcRGkc6LMxyWZcEwDFQqFWxsbPjpD0eFbdvodDr49re/jWKxOFR1\naZZxRdSpAGQntk5NTcUmPThouFAf7029IAgoFAqR45WmknznnXf8uXfNZjOQSpLt+7JG3Wv/xM1W\nQnOhK8VcrFWF1ttobu9BkER3pFMUoE7mMxXNi0MtFdCuJmcF64Vt24Hfgwv1A4ZaXahn8e2330at\nVsO5c+dGVqCon0wBWQmf1GxHPGgJ6gcZUW82m1hcXES1WsXs7CysdgHEMiEYPbxloYi3LWsR64lg\nBO/QqVh3PBFFwtuoaHAkGYTeHPSbllHTo8+peM+YXouoGojG/F7MOlDBTrybRSchIkx0vVvdVAfs\nZnfSDfFSNaYhMBduIS/A2t3zbxzCYl1QlMhNhJjPB24GqLUlMLk1dDMl6ipsw/R98UQUgb1azyg9\nAOROHUMnxg5DvFzqymQBnd2an35SUGRoM5OwO4YfVRd1DfCOUfW1X0UxZH+hBU1oJCqcD5kV8O12\n2/eoG4YxdGq1GzduoFQqJVano+8vLCyMdPLRUca2bTiOA0VRAhH1YTFN058gats2rly5MnR1aZZx\n1cUwDANra2tYXV3FiRMnxpYDfRDC4vRRHn06yDzqNJWkoig4d+6cfwNHXQb1eh2NRgM7Ozuo1+uw\nbRuqqkYy0dAgReeL/8DvU2n2L1rfAujWugiPbOozE5HXJE0JTDhNIn+yhPZuHUpRj1TbHoS4xACj\nroXzoHgohDohBNVqFeVyGfV6HadOncIzzzwz0pNGkiQ0Go3eHxwA9gIybEc86glDce3V63UsLi6i\nVqthbm4Oly9fhvmdr/mRdFvR4AgiiG35/4WwXYWK9ZgIOuCKd0eXfEFOjGTh7UgySDhqzUbMgfhJ\nqux3ws+BqHjvhSQHffM0vaTXaZLQKGBSOkfHsbt+cd3rsPLEj5YTz3oTyEWfgFjqeh8DWWkyIBWZ\nqE2/350o+LaaXiilIoxaA7AsyIWcH8mn3ndBCYpluZBDu7LrLofmly/kEy047MSscD7kdrvtX8TW\n1tZQqVRQq9Xw9a9/HT/60Y9gmia+8Y1v4PLlyzh+/Hhffcrt27cBANeuXcPCwgJu374dSNN1+/Zt\nzM/P48qVK36p6WHTeD1MyLI8ktHBVqvlZ/eiGbL+/M//fOT5rSVJClgvh6XT6aDdbuONN97A2bNn\nU7OSHQRJ5wKPqB8c4cmkbCpJFsdx0Ol0/Ims6+vraDQaME0Tz9/+t65IFwRIuupW386YqlfKqejs\nxn9O9vpq6m8HPAuNrqZG3Wma3n6JE+rco37AGIaBJ554Anfv3kWxWBz5CSNJ0kgtJYBrGWm1Wnjj\njTcCKRaHYZwR9f39fSwsLKDVamF+fh7ve9/7YvezEzNh02Y85o4nQkUgcwpDR9bgyOxzV7xTIe8k\neMUd0XtdI4AgAXa3EyFAvLUlKS+7ogUtNHFR+3BaRolJScl0lkQTgBYTLc+SGaY4EWibjbaHIYrq\nRssdx58QK+bzQD7vFlzyLC5ha46QkAVGzDMTgRpe9pd2B2LItiJoamC0gIp9ItR9PzqNxgia5o8Q\nUM98nNhWpibR2dkNvKZOT6K5sQ3A9cs7phXIdJMFQgg0TYOmaX4+5EajgQ984AOYm5vD1772NfzO\n7/wOfv/3fx9f+MIX8FM/9VP4lV/5lcztf+UrX8EnPvEJAMD8/Dxu3rwZEeKf+9zn8PrrrwcKaXBc\nhhXqdAI+TVXIZvcaV7rdUYy6tlotP/OMKIq4cuXKoZ3QvL6+juXlZSiKgnw+7xcCGpU/+ihxGIS6\nZVmZ9jshBKqqQlXVQCpJ+//4x24/LQh+AT1RVQBBgCBbcGw7YFU0m21fwEuaew1TJvOwWh2/0rQ2\nVUBzu2vppDYZbaoIKa+jtR3s21mUyTw6u3U0/unfQ+5/+BdZd4O7bjHF67j15YCh0a5RRWHCjDJH\nbqPRwOLiInZ3dyEIAj784Q+PbKb2OIR6rVbD7du3Yds25ufnIzliAcBSchCsTtef3rNhEZasuVV2\nPES04BASsLiEYaPntl7sWl3gZZkButF8MVn8OqIMKACg+lF9AkSj6lR4x0XcabQ+V3DFeCtj1FlW\n3Ci4Z5PxO/cWjZgr3YqtZnJmGEHP+SLcoVlpDANE6T2cL9CiLV7bVnU3kredQjQVDuNLF/PueovF\ngi/4eyF5cyysej1SsIpFnpxAe3MLUk4PfE6ZmkRrY8tPFQkA+skZ/8YpyVPfL5ZlQRRFnDx5Epcu\nXcIHP/hB/MZvDJZxoFqtBs6V7e3twPtXrlzB/Pw8pqam8KUvfWmo9X6YoEJjkFSzjuOgUqlgcXER\nABIn4B+2uhhAd6Ryf38fs7OzeOqpp/Dd73730EWoaRXsWq2GnZ0dXLx4EYQQ1Ot1bG1tYXV1FUtL\nSwDgZygpFAoDTe4+ShwGoT5Mekbnt//HwHNRU+CYFogkwrGjx6Agib5Fxc/y5UXiRU2BzUwk1Wcm\n/BFSNqIOANrMpF84KQ4iCtBPHU98P4mjLtQJIRLc8irkoRDqbDaAcc28H0VEfW9vDwsLC2i325ib\nm8PTTz+NN998c6Qd8RKRD0YAACAASURBVKisLzTjTLlcBgA888wzkRRSlMY73wYhAiypa3kBXMEs\ndVwh54gyiGWkimdTcz3ukLvRd9GqwxFFEMuKjZoHlqdooHvSJgRCh+kQhN6HuqNogewypNMKRcfl\n2AmyPqxVhgQj5omE0koSms5RF4CUiHkY4i2b5FwB7lCbjBL1tbOpH/3VYDuwRj2TvxwABDqpKc4a\nQ2+AmJtQaXICViP9hkZ//AzMqhtlEXO6f5FQp4PHn1gswKoH95FULID83q/D+hv/MNP6h2EvtuNO\n50Xb//znP49f+qVf8oU7p3+oeFxaWkI+n8dTTz0VyZjBMq66GIO0Sa8LnU4Hc3NzgZHKUdfFGAZ2\nDtXMzAyKxSIuXrwIwzAgiiLy+Tyq1SpOnjyJyclJf25IvV5HrVYLTO4OF/l5GAS84zgHPoowsFD/\nl//EFdlEcAM2hgEiCGCPvDSfuVzIw6jV3Qxfhltkj62HIRfyqdcUUddgNeNtY8W5M/1uDYCja30h\nhBDHFYXPA/gUgD97KIQ6yzgj6oN0wjTCs7CwAFEU/RRgbLuWZY3MezhsVMdxHGxtbWFhYQGapuHM\nmTOwbTtRpPfCklzha4syBNuCaKZ7OB1RCkTJLS3vi3wQwZ9kyop2Vqzbogzi2VssLe8/pggtM/Vm\nobseMqAGOzzSaUXTOyYhin7E3JE8r319L1MqSCiqG72n/nQ2K01GSC4PetkjxNtnzGTWNIRCAQLi\nPe3dybAxk1DRzRoT225Oh9Pp+BF5u9lE2i0qK9IBQCpNdgV8MZqlI2lEYFCGjcCUSiVUKhW/Lbbc\nOAC89tpr+PznP49SqYT5+XncuHEjdsLpo0ZYsKVFKk3TxN27d7G6uopjx47hueeei6TXjOMg0u2y\nsJF/NjVkmFHXxaDZY/oRc7Zt4+7du1hZWQnMobp161YkyMTmUWfnhoTbY1MMUgFPCIkI+CxFfg4L\nhyGiDgwwoffL/7NbH0QU4XjXy3A6Ximng54tdsfwo+0A/PTC4boc2swkhBingFIqRhII0Kra6swU\niCSivbXjvj5EQbtwMoDd3d2BdcyDxOmeVA0AvwngwkMj1GlaKFmWRzqhh9Jvx87mus3lcrh06VJs\nhIf6GkeVfWDQiDpd38XFRRSLRbz//e9HPp/HxsYG9vfT851aogIR2WwHVLgDgC1KECwTghUcCmMn\nooZft2UtEHFns8PYMQLcF/n0M3r3N3CggXRavk2G/Vwctl4IiPTIBNle5Ce61h3Wox72tcdRCPrT\nUdsDFDfy4UMFfhg6mRV6MBONl/WFyF4HGzq+BS86D0JgdbL9vuLEhDu6UtuHY5gRz3rgs5Ml2Jub\nEPN52J0OiChC0NzzQCpNBm4UxJy7DaxAZ9Ng0teTUlBmIXyhHVaof/rTn8atW7cAIOBBj4vUX79+\nHa+99trAy3pYof1Z2BrYarWwvLyMzc3Ngeb3HJRQdxwH9+/fx+LiInRd7xn5H9ecoyxC3bIsrKys\nYHV1FY899hheeOGFgPChorzfrC9pKQYbjQZqtVqkyM9REPCHRaj3g/C7r8CRJT/wIuZ0OIYBRxJB\nRBGk3T3vlMmim/J3ouDPJRJUxU0bDHeukLHXTacoF3KwmskZ4HKnT6Czm64tlOmpSDrgrIRrYjiO\nM/JiUGOmBGDXcZzXH0qhPo6IetYO07Isv8DS9PR0z1y3o+6I+23Ptm2sr69jaWkJU1NT+OAHPxhY\n3yzC3xZkOESAQ0QQx122FBM5tz3hLTBi16aZYphIumi2Yiekxi5b1mALkr9cABBCPz8rwtnoO+Da\nXWwiJGenSVu2GpzgJRitzAWUAHQj7oSAtJrxIjup4y9MuP5sDcD+rivSgaiXXlGCz6kvXs8YpadD\n8KUpQBDgNKIRcxp9IarmT0wVC0VAIN1Jq35qx+D2yCdO+J+hIp0iFAqw9vZ9kQ64NhrWuy6VRhch\niRsqPX/+/MDtXblyBbdu3cLNmzdRKpX8iaQf//jH8dZbb+Hll1/Gr/3ar2F+fh6VSoWnZ4yB9uf0\nAlur1fyMU+fPn8cTTzwx0FD/uKwvSX0v28/2k3p31NaXLP25aZpYXl7G+vo6Tp8+nXgTFJczfZg8\n6kkCPq1KJyveDcM4UD//URPqwu++4valluUKblH0RTcRRT89YxxSIe9aVbzP06QCcmkC1v1KYC6R\n+/lcJCEAi1jIweqRIUz43Vew+ld+3k8p2evGPNyfD3Js9EqvCyCSrSvLd9JgrC8XAfwjQkiVC/U+\n2k/DMAy/czt16hSuXr2aaaLSQQl1djjz2LFj+NCHPhQb1e/Vse//+DYgRIWpyUbOBQmCY0E028xr\nyUKcjboDbpYYmYl2h8V293PujYItM9vB2mViIu6uVYa5caAede/3FjotEMvMNEnWljVAYURlSkpJ\nuj7+DYSmA9SfToT4ia1JTDDR2X4sMpoOInhRc+qHp8Ke7kOaFceL1pN80VtFAru603MRQtGdROqE\n1otoWreTn5gASfDjyyeOw671V/TCMS3IX//XMP6Tn+/re+PwNMaJ77feest/zK0uUdi+VpZldDod\nfwK+bduYm5vDzMzMUKJoXBH1cJuWZeHu3bu4e/duaj+bxKitL2n9Ob2G3bt3D2fOnOmZ5IBec9n/\nwOjTMyZV6bQsy08vuLOzg0qlgk6ng+3tbeRyOX8Caz6fh6IoYxfRR0moS3/wz+HIMoiiuP0rmyaZ\nufGlhfOEmFTCkhdA8QMn3ve0mW6fKZeKMDybi18NW1X878Zl9ErDNE2sra2hXq/Dsiw/2xA74kJH\nfdj+fBDvfq/0ugBw8+ZNfPazn8WdO3cyf6cXjPXlzxzH+S1CSNYUHUeHcQn1JNgcvWfPnu27wNKo\nLxi9Ihqsp/PkyZOR4cwwvYS67Yl0h8Rvs81M4rQk9wJFLzui5Q5p0ag6jaIH/ebu445eCghq0WxD\nsE2//aTlA4Cp5CEwEXfiVUIVmCg7C+uTp8LdTylJxTcjov0iTKHf0ZG76RwddLPSBJclB1JG0g7T\nzk34vnzS7iN/v961q6DNVFtlc9bLSvQmQM+5/nugt9j3MsUIpankSbOhTDVU4DvViivSwx/Xc6np\nJgOfzedjs80I+QKsdiVTG3Ec9SwBDxu0guj3vvc9FIvFvio090KW5ZHXxRAEwe972cDN6dOne/az\nSYzD+hK+PnQ6HSwtLWFzc9MvEphF1MTZkh5kZVJRFDExMeEfExsbG2i1Wjhz5owfga9UKlhZWUG7\n3fYnvLJ/oxTwR0WoS3/wz7tPLAuCrsPudCDIMux2pzuyaTsgqgLBMGAzCQCoeE9sf3IC5m43HaPE\nRNdJzDwiKuBZ9NMn0VzbcJenKL79hS1YRCvS05u1jY0N1Ot1vx9vt9uQJAlra2sAkGoxiyNLet1r\n164FEgBk+U4ffIAQojiO852HTqiPK+sLhZ6MtVoN5XIZ+/v7OH/+fCBHbz+MuiNO6ijYaEnacGaY\nXkLdFIMnnkMIZLPtRrZjMq3YRPRFs+V91xJk3w4jWa6YjfObs1iSCosEBR8V0bYgQ7DjiynZggwi\nufvI9m4cBLNHJVUGNjONI0oQ2/GTJ+OKMNmqF4HITUBsZo8SO57FxvEEtlDfc8W2ZQKil6c9Nn2k\n3o2OtBquB13OMOGSetOpzYXaasL7U9EAo9O9OWg1u1VVExBK03ASsuEIhSKcdsv3SwqeoBcKBTid\nTmACq6BpfiRHyOVhe+sqTk/DqlRSi0AlEZ58xIX6wUAIwdbWFn784x9DFEWcP39+5BUFR5XzPIxt\n23j33XextbU1ksrYoy5gx072bLfbKJfL2N7exuOPP55ZoLNtxVUmPeh0kpIkBQQ8xTRNX9Rtb29j\neXkZnU4HkiTFCvh+OQpCXf7DfwFQ+6C3rkSW3d/dsiDQKHlorg99nYT84oKmAQKB0+lExLuQ0xPn\nXrFVstnCf8pjJ2FsbQFwxXoahBAoigJFUSL9tGEY+M53vgNZlvFHf/RHuHHjBsrlMn7iJ34Cly9f\nxs/+7M/i4x//eGr7vdLrjuo7KVwG8DFCyMMzmZTNvTvqMs7sMnZ2drC0tATTNDE7O5tY9Ccr41xf\nwI2WlMtl3L9/H+fOnes7Z3vaheL+0o8R160bUnBoV7KyTwbpyPmAh73XJFUq/C1B9nOyW4IMwbEg\nd+qwUyLtFEsOziGgHvk4e01k+czEVigCxGZ3cowjuSMC9KaDzUBjK27+dluUIVgGhOa+a4Vh878n\nVW3NT/jvC8zyIpVRWbQcHNoxt5oxXnYtKvTzhe56NKloj4nGs5+nnzUNN898eHRLkkAKE0CzDsc2\n3aqtnY7rjwRAihNwdqtujnine9yRqRk4m/dANK0bnd+J7wTFmDz/WYizvnChfjAUi0VcvXoVa2tr\nYxE/siyPVKhTa06j0fCj/6NI1SeK4khHiAVBQLPZxPLyMnZ2diKFoPph1B71cSNJEiYnJyOZPwzD\n8Cex3r9/H+VyGYZhRAR8oVBIHRU5aKHea7/LX/stv5+FYwOWWxjP72cz6AJB1/1J/gIzMkoUxc0U\n5ol1gZ1XJIl+hpg0pMlgxFvQVNit7EE0FlmWQQjB2bNn8cu//Mv46Ec/ii9/+cv44he/iHfeeScy\nD+KQ8mUAdx3HMR4aoU4Zx4lCUxbSghQXLlwYWT7OcY0AtFotLC4uYmdnZ6hJV1kiOpYgQbRNWF4E\nnbWZWEQCRPc/fd0mIhSrBdE2XIGd0q7D/p6ia5cRbDN5kiWDoXTTgpkgkLzIuSNIILbJCOhu3ndi\nW65HnhHJotkCbCvVp05FtsVklRGMVvLIQEiE23rRt9f0NaFVLzJ+em94knpFJTl6s0EEOL49RgCp\nVbNF2fW825H7gj3o5YfKCH09794INPaZ90xAVbs3EoUJ94aBxYtiCZMlV7yzy6arn48fvhTyTMc7\noKMvLNT39vZGZrXg9AfN1kA96qNmVP0uW615bm4O1WoVp0+fHsEauoiiiHZ7MLESptFoYHd3F3t7\ne3jiiSdw6dKloa6XcaI8zlpz2JFlOVHA0wg8K+BlWY5E4GVZPnChnuTDlv/kX7n9rmX5GcaIJPtz\nhAC3/7TrwVFeEvKl03oYgq6DSOnnjiDLft50aXoGxuam+7pnY5EmS/5riW2cPAV7qey2UZqMpATu\nBZvXns43mpiYwAsvvJDp+73S647qOynsABAIIVMPjVAfxwnCFtEoFouYnJzEpUuXInlhh0EUxZFe\niBqNBprNJv7iL/4Cc3NzQ3fGqZOPRC9HOhEgCO5nJDvbtpiCDJMR6VkPxI4c9JtLZis2ak5fYz9r\nepF+ao0RmUh/WpYZOrmV3jSIyCakDW0iYMERekwu9ZdH/ekeccLdkZWIJ95WXKuLQwgEz5/up7oM\n56aWFHeSbKHkb5uQYfKqk3MFcURkx5Gf8KLrCZ26pruiurPtPqY3FVrO/dutBES6cOKxyIiBkMu7\nGXAAoB5M9dXvhNJwpcrDUMDkUYVNDjBqLzkwfAG7nZ0dLCwsAICfA50Qgvfee2+o6pBhRjGZtF6v\nY2FhAfV6Hblczq/WOop1C18bDnNEvV9kWUapVIoE5QzDQK1Wi/iiDcNAs9lEs9n0J7KOqj5KFuKO\nO/mb/8YNmggiYMX32bRYnqAyo8PeY7sdvfYQTQPRAMdLg+1YFogsuxYa2wb6+f17BAHDUXX5m/8G\nxsf+Zvb2PQaxMfaTXrfXd/qFEDIF4H8HsAXgWw+NUGehndug3sDwTH1aROP73//+gRbJSKNWq2Fh\nYQHNZhOSJOHFF18cycUiTajbMdaMtqCDrWcm2R20xVxEwIeFdFvKdUW12LXLUGtLWIzT52x2GVP0\ncnBb2SJQpqTFppSkxZni0kkCrnB3mG2PS0UZwHErvrmpJN2bBDqhtRcOEWCpuW6aRNNwveox2LLq\nT5C1Vb17g9BuBiesRpbhefY90S4wE2bpDYGjaIGbBxqVJ+1o5+9oevcmQs+7kf1GSr7cqRmAVpHV\nmLReevoNsTA1k200ICOGYWRKmccZP+PO4jVIRJ0tBqeqauzkVpqicVRCfZjrQ61Ww507d9BqtXDh\nwgXMzMzg3XffHZmQThLlD4tQT0KWZUxNTUWE3w9/+EMUCgU4joN79+75Ap5mJmH/xiHgwxN75T/7\nPVc027YbBFEU95pABEDwqrmHR1xDNT0EVQPyBXcOkGUFEgEQTfPFOuDNFap1+3lpegbmfTdqzta/\nEEL+f2myBHO3mrptRBQhHE/3rKcxiFDvlV4XcFMx3rp1Czdu3MD169cTv9MvjuPsAPjbhJBTAP72\nQynUw7l3s9LpdAITLsMz9UftawSGH4Ld3d3FwsICTNPE/Pw8pqen8cYbb4yss0wS6uW7m5B6BOpt\nCDAF96Q0BcUXhHKCkGYnmpqiAgfBBdDvxUXQLUHyBTUV7ABN7diKWECcmJsMU9Lc4j6CDNE2Aukk\n/XX0POWR78HNcCN36oFsNEnYjI/fknWIRtPzp6d/z8p1/emiuecJ6bj8693ts7Su4BU9L3zqumlU\nhDcCUftIhhp4RaCoVz5lIqmTK4I064AkRDzujqICigpS2wt+SVG7Aj6MF9mH4d7wOHoehEbU8wWg\nXoM92d+wo2ma/vk+yqgop38OS10MIDqySovBpbU7SIaXOAaZTLq3t4c7d+7ANE1cuHAhED0f5eTU\no+ZRHzeCIGBiYiJgoXEcB51Ox7fQrK+v+6kFVVUNiPcsucHTYPssX6QT4vajrUZ3DpNAXNtL+FqT\npJdEyS9+5xihyaSFIqzdqv++UCjC3t/z0/JK024fLOg539pIbY3yY57wpml6TzwGZ38PyunT/vwo\nYeY47NW7fe+L8M1ytVrFhQsX+m6nV3rd69ev4/r16z2/0y+EkOMA/gqAWwB+86ER6uHcu4ZhZCol\nDQDNZhPlchmVSsWf/R4n8g+67DQLO/Qa9szTNkdRhSutYzcdGZYjQiCe7QXJF1QLEgQvMaMh/v/s\nvWuMJcl5JXYiIh/31q3urn5yuofD6ce8h0NSwyGH9K4McTWy9uf+ICAItgHDgMQ/NmDphwitbZgE\nDBAkYMOwBWFFeYWVJRiSOQssIGi9C/bKWppcSuSo+RA4tDTTVf2Y7unHdFdVV91HZkbE5x8RX2Rk\n3rz16lsz0835gMatyoyMjHu7buSJE+c7XyvZ1E5f1wbpVqhwHYPs1BQNcG+larDfRiaQZJ1ER9XH\n8qp7K5393sP7S+qEGHajAWrbSAbjMgKvJu3BoE5olUXlWHQywcoSAAQovMdYx26TDGnhtIIOGEfF\nmVrVU/XAPxCyPmTl9fctiUvbJ94sHAQJCeUtH6nLqpHvly/A9v2uQsScU5JPVXG1/QOAUpCTaRcc\nHjf1B04XX06AqgBUAsr7dTITu8uoBDC6BvBl4cbIDPscXTDiiDXq6+vrH+jT3wfxXtXFAJrF644e\nPRp2VreK97KA3draWvBzPnv2bCeDuB9APQbnP8tAvUujLoRAnufI87yxYGoD+GvXrmE0GnUC+MFg\nsKNnubUWn5gsA68te+Y8nKiL8PX6dZ4RECQuIVjSmCROsthiv8XCACgmjaJz6lCHDESInUlgDh0B\nZpgCcCSPfhh0b2vGvR37URPjXY4BgJMAfh3AMw8NUOcgoh0D6o2NjZCpf/r06W313PPOwOc+dzoR\nExHu3LmD5eVlZFk201d4npPxrL5sy+/FkoQWafhZCosEFSxkkMJYUpAiKi7kf2etOpEABJBQFeQu\nXbIXjkrlMCKBIg2CQGpnS1444RWYdqXJIuAe3ysG/kW6CGV1AMIJdiavqXKX/BgnswLOmrLNzIdz\nSQ/knWiUl9WEhNcWWA9j9YWWCCKw8+1EVqdLd9fGchrJbjBaB9/49tiYlXcFoLz/fIdTjO0NQFJB\njjcgjGlWb2WP+P4iZFU4kB6PrzeAqHaXOGcXlyA33SROxx4BZT33+2D3Wf3x5P6BNeN7GzwPv9t1\nMYC6Muf169dx8uTJXXmgz5vM2YlG/e7du1heXoZSCk888cRUUmS7v/18NsT2jz9rsZtk0q0AfFEU\nDQA/HA5hrUWv15ti4GMAf/Dv/rLuJ0ncbiiPJ/WyF7LuZ5bEtKMXWfoWk+YuKX8H+gNg855LSGWg\nH2nZ5cFtqkUf+xDwzs3698NHgW2kL7uNh6AmxiqAf0VEV4Gd5/C976OLUZ8VzEYTUUis2ckXbD8e\nGjuZ2IkIt27dwsrKCgaDAZ577rkt7YXmyep0fS5/f3VtW9kLAGg0GXcASAgNsM5hqGbctUgBkToW\nXlikFAFcz053RSVzxOuHxJYu0dUztqYlR2GQX6T1NjbLa7rcaGK2Wqt8xzp1K10BJU5mJeEAaoJu\nCU64TqXb3iN4xkfXIOpXVdskfQoJmznrRiW2TxA1vQEgBBQz55ElZCwLMguH6jZRsLTHHjzaqdM3\ng4NQw6YExh44DLlaOwTY3gByMnQSHSLQwoGGlaNdXNp5Vdco2oz6AzaxP1TB885+u4jE4Cq2suXK\nnLuVIuwHo94FfIkId+/excWLF5FlGZ5++ukdFXTZL+nLB4z6fOwZhRDo9Xro9XoNxxAiwmQyCQD+\n7t27GI1GAcD/nL3eMSD//5z3mkn4LHnpkhUm0S4uJ/uPpufxhvtWkiDwVgutStdHTwB3tnZ3AYDk\n5KOzT/b6ELv8Hj4EQH0CgIQQPQCPPjRAHdiahSEi3L59GysrKzMTgbaLJEkwHu+iRPsOYquJnYjw\n9ttv4/Llyzh06BA+/vGPo9/vd7aNY95FMrpiYnMoThr183ImSljaAniShBYJQL6aKAmAUuTSJ3FC\nBbAeRyW44I4/IFJkVLu9tGUygAOqleo1gPpWjjTM3k/Ja7Zhzpmp10kvjGM3yaxdNpNt+Upoz37v\naR+qmoQx2tbiJQb3Ju2DpELSAuyUuC3NhiwmcwmobPPITjjWO8TUF5MH7DIUfGImPg7TG7iiUB3F\nnWyaA2kOWYwgTPNv1QwOQt1rVhilxaXOnYSZ4X3i1YU/x8Yzn0O/39/2IRonYz2AE/sHscvgebIs\nS6ysrGBtbe2+rGyB/Ze+xAmt/X5/W9KmHR9o1Pcv9tOeUQiBfr+Pfr+PY8eO1cdf/0sIu+FMEXgu\nF9LN3UI25ndKEohYY85s/HZ/rwsDB+rjPK9ev1nBerC4tdTFa9bR4SIDwC0mZpzTSyeQvHNt6zG2\nr3lAgboQQpD7An0OwH8J4AqAtx4qoM4RA3VrLa5fv44rV65gaWkJL7zwwp6dHd4tjXo85qNHj+LF\nF19EnuczethZn+9GlLbWs1kSSMXsz8qQCpKYwtYgLxGAhIFpbfbEEhoAKES0YBEpEjTlMl3R5UjT\n9nfv8oCPFwKpKSDJbMmE6yktfbOAkiA7dX2sh1diekFhWx71JnXOMwQRQLhRsx1QGiB/G5cam/Wh\nhUsSVaUD4qSSJlj3YfLBtraTpr8Y2ghjHKj3k7rNF6BG6/6+9d+47S82XWa8uwsfs72tHWFM1ofy\nrPrFixcxHo8hhMDCwkJj+zgG8DHA2MqCayfx6quvYmlpCRcuXMBv/dZvTZ2/cOFCyDFpJyN9ENM7\nefsFgv72b/82eKA/++yz932P/ZC+WGvDrury8jIOHDiw5+fYPOwe474eBh/1ecW77aMuXv9L9xwR\n/gmVpCDj5C3CGufUNRk6okUagCwoX3BzqLXNZNJY7tIV/YHTtltTy10WFoFRR4XtttXxge0JUdtf\nBPqLEBv3uhtEBfp2YtHYrjL9oGjUqf7y/D2A/wxOq/7Zhwqox04Bm5ubWFlZwbVr13DixAl88pOf\n3BXY7Yr9AOrxxBbbQp44cQIvvfTSnkoZz3MybscPViYAesiVlzhAQsGG1zgKmznZi//TmwXcYxCu\nKQGhKTtJWtcZnxnK1xgoGEQSGwFkKIJ9ZLCRbD0/tMy8vMZ9Vix72QqET9LFAJZrmUytf2+M0x8v\nIxCeep26IBOSV41MG0mwTg9fOccaPe6skGpUFpJYddoPY25YTJoKtoM5174QFAN8KxNnUen70Emv\nlrBkA5AQSMrW9mfM2ucDWKGQFB2TNrfpHYCaRFVbPfAnqaAXj7j7+Z0Pm/aAtIdk4w7MwqFOLb/p\nHwhVYPXgEJLN1eb5pAcsAGp0Dy+88ILr11qMRiMMh0NsbGzgxo0bDQBfliVu376Ne/fu4e7du3tm\nYC5cuAAAeOWVV7C8vIwLFy5M2XR95StfwTe+8Q187Wtf6zz/QdTBxMO8LO3W1tawvLyM0WiExx57\nDKdOnZobwJo3SSKlxGQywV/91V/h0KFD+MQnPrGjXdWt+puXfJP16MPhEGVZBunNB0B9f8O8+X1X\nPVtItyMrRHi2MWtOWQ+wtunwJRMH1qWCmLGrQt5TXVRlzZAzoO8Ppu0c0yzM2zQ4CMGyxcGBOoE1\nzdx4AIg4L+nw8en+ANCJRyGGjrwJ7l7hzZsdVVBtM+rtGhnvxxD+j8eD9acAPAHg20T0rx86oD6Z\nTHDjxg3cvn0bZ8+e3ZPOcFbsVxVRIsLy8jLefvttnDx5Ei+//PJ9jXmWrvF+4969ewC83aJNoAEY\nElCCkEgdwLqmGcmflMDYGlDHwJ3BehuEW0gP3kWdpBpdx+3bUSJ30hoAaQdD3Tk+ZsJJICV3DTPu\nXT7ulcoD217JHLkeTVVpnRpX5Pse21S2HWvCmJI6STTVnjnn5Ft2jZFp8IJnb3gASDoeGkZltQVm\n2g/vK9FFQ+veDp25JNF0vO6SUv1YTdIL1+m8ZuFjq0mrMggQTO9AJ/tu0j5M2kc6biYU6QNHA4uv\ne4tADxBbgIDq0An3XsY1KxPsLOFAyuLi4pRUwFqLzc1N3Lt3DxsbG/jSl76EH/3oR5BS4jvf+Q6e\ne+45fOELX2hsOW8Vf/qnf4pf+qVfAuDcN86fP98A4q+++io+9alPAUAn2/5BdOcc3c+cyIn4Kysr\nSJIE586dw9WrM2U6UwAAIABJREFUV7G4uDhXcDUvoG6tDbJHY8yOHGd2EvNkvIuiwPXr15EkCXq9\nHi5duoTJZAIhBLTWGAwGWFxcnEp6fFjj3SiQZi7+jduRjUwFeC62PC9HjDappBMMA75yta4Cm24j\ns4HAvAMh6ZTS3BXGmwydLbAxznaXQTUcWGcSR2xVYfvESW8VqQDrTAdkMQI9dnbLOZ4OLLkaIduE\n1vqBq4lBzS/mowA+AeC/FkLceaiAelEUuHDhAk6cOIFjx47hzJkzc+1/3kC9LEtcvnwZw+EQSil8\n5jOfmcuENm9WxxgTvEPlkec722ibQFsJJeq/tURu/Vkx405Ug/BUNsE7gKmEVAbu4T5txp0UVJSw\nWlHWOJeLiV8UeCDpFxZxkmsl3DVdCa0MbmPZDODYdgCoVFMPz5IaI5rylQbT7kG73kK+UiX9hpwG\nQMPysR1a9WClQlY5NrwtjTGqBtw6yUFyoday+/u09fLlgmOZs8m9Rn+x/aTJBpCm6NwJ0PkBqFn2\nmJmfWDn5N11AYmZshc4xpJTo9Xro9/s4e/Ys/uiP/ghf/vKX8Qu/8As4c+YMXn/99V35Yq+trTWc\nHO7cadqPff/73wfgmPfz589/ANa3Ca4iuhcmmYhw8+ZNXLp0CYPBAM8++2xYqL399tv7UhejKHbn\nXBSHtRbXrl3DlStXcPz4cbz00kt47bXX5gLSgfm4smxsbODNN9/E5uYmTpw4gTNnzsAYgz/78RLE\ngpv/nzrsnmtXr14NSY/9fj+Ad3YteTelIvsd+8moT668jrQaQ1hdW/vGOUTRXMuF71hiKKx2Sfdc\npTqSYVLWc8C7o7ZGvBigNKpNMqcic/rgUST3trZm1AddQq0+dBzJnes7AulAk1Enovf9Lo8QIgfw\nGQB/TUQTAN8B8H8R0YYQ4vBDBdR7vR5efvllVFWFn/zkJ3Pvf15AvSgKrKysBN/2wWCAxx9/fA4j\ndDGvhKH19XW8+eabKIoCL7zwAt68M4C2AoZEkL4AjlWPg1l2bes/LwsglR3bXK1rq+gaQwo95Ysc\n+SRVfhV+QWBIgs2LiARSWYVrpbChfQz0K78rwDr5VMTvJZmZ0GpQ6+pTNCunmqAh9xIeX+ipkjkM\nEuR26zLo4/RALXeJCjtJqu0rY3lNpXogb72Ttm0fWwC5TAdhYcOs/KyovANOVjZlLDrpQUWM+6R/\nGGkkh9FJry44leSQnk1XnQz6QgDrJq0nXqMyKF+R1qQOtOveQWTD6cmcpIIeLCHZXEXVO9h40FT9\n2h5sMjiGnUCcNmO7traG48eP4/nnn8fzz3cvTu8nOPfk/PnzoardBzEdey16FOf5HD58uDMRfz8K\n2O2VJIlljx/60Id2ZQm5m7ifZ8PGxgYuXryIqqrwxBNPBNtAAPizH9f6XyKB//fK6TBHkxIQCQEG\n+I8OrGBzcxO3bt3CaDSCECLkizCAz/P8gQTw+2XgUF7+WyhrQEKEeU622HTAAWuWwxA1x8LGANTW\nj8PlEAFoJpr6sGlvulhgyyKYuJ5H6562vwg53oRdOBgcvtquXgzEdxrF4nHkm7e3bRdr1Eej0cwC\nZe+jOAfgPwXwAzjHFwlAAwARrT5UQF0IVxZ3v7x37xcAj8djrKysYH19HY8//jieeuopSClx9erV\nuVZCvF9GnQtnCCHwxBNP4Cc/+Ynz5r2jAyjXNgmAPNxXmCnQDgDas+WV5UlCwZJsgP1ZUdqmN3ss\nlzEkw6vyQLzy7Q0kFLVkMp455744KkodCCcLC4lcFJ0JrRwWEhXqyqmJnFEwKGL94yJPXOCpK+lV\nkEWlcpTKwctZ3vAx010mvSbA9zIaZufj+1RJ37HsLcA+BcR7bvLdynayygYgodCbrM5sU+YHPBPU\n/Iyq/ECn9rzKBlMLjap/qJb2pP3A+pfZIvTh/tRx3nVgLf5OIq5KCtyfPePS0hLu3nWuNWtraw2b\nNcCB9LNnz4a23//+9/cE1IUQhwAcAHCbiPZO474Pg+dyYHeAWmuNt956K+QmbZXno5R6z4G61hpX\nr17FtWvXcOrUqfuWPW4Xe3mGtQE6fy+YKQeAKnyMAmkCGAskqknEEAn8h2tnQCQciPf/Lf/oMVet\nc3V1FW+99RaKooBSKgB3BvH7sXCZd8xzgTG89iaUKSEgQD6fSVhnZGBVCqVNkB6SVBBEwc2LZshd\n2hE/R6yvreFqZbj8IUozWCGnrHZtbwA1vBcYbi6IZ/M+5Kxq0juM8uBx5JGchkMfPbXjPh7AmhhP\nAPgNACMAIKLXhRD/iRDiPBHZhw6oA/un0d7rl5ATW4fDYae7AE/u8wTqe1morK6u4uLFi5BSNuwr\neXKPQXjXz7YFhFNpApBvg2MAqKxqTOSpf/tS2ACq21HatNEXy2sMSUgQTKsQU0l+cifP0MsmQ98V\nFaWA94DPffu2Fj4G4ZrSmrEWZWDVu8JQEiZHEhI9OwySmM6xyNivfTLVt5ZZANixZn47W0mW3WR6\nXEt5oqTWYFfp5TapHtcAWOWNaqzjvpN55MVG8IpvvIe0j7yooNM+pNGwUsEkOUySIy02w1Zsu0AT\nXwsA+aTWr+u0v6XDzV5innZev/Irv4LXXnsNALC8vIxXXnkl9Lm0tITPf/7zePXVV8Mx1qvvIR4F\n8DIAKYS4AuBbAD4K4C0iurnllQ9Q7IR4YRnhrVu3dgx496suxk6AutYaly9fxo0bN3Dq1Km55lJt\nFbsB6pubm3jzzTdRVRXOnTvXkHMBtXnDN77f9G+vtJM9l5UIZiLWOgDP8702Akq6ndF/9+Yp3x85\nEJ+5Z8YvPHINm5ubwVaZF9MM4Pn1/aJ/n6f0ZfPaRQgg5BsJTtiMkvitTJ1pgK1AQsEqlywqycCk\nfUirA4MurKkZbyEhq0lTLpPktdY960EW44bExfQGrvq1NeG4Wex2UbFZHsB6deQk1HjD5TVFdTOq\nA0c7pZGTw49C6QI0ONTQvgO17HIn8QAC9VMAFokoXhGV5LdGHiqg/n6Le/fuYXl5GWVZ4uzZszh6\n9GjnF5kn93kxBrtlTVZXV/Hmm28iSRI89dRTU/7yUkp85w0FKQnWilAdOEumGfQYABcmdRp0Bo8S\nDTkKBYDvgHfdPvHtTaPPtkwGcMw+QTSSWsNY0OwfqIE7H8tFOV1lNXKhqaj+PyEI5GJrAFxRFmQ3\nRKIhq+mKiaxlKQpVZ0EnltyUqh9catIOr/YYtJdJP/Tb05FEpcWyl4lLKI37YxlPY0cg6cMKiWwL\nhn3UPxL08HyNtBokFCa9w0jM9LVVvtjweNdJHyQE8qJ2iIm95stsMchjdhob11dw4NTW+SpdQH2v\ndl4vvvgiXnvtNZw/fx5LS0shkfQXf/EX8Td/8zc4e/YslpaW8Oqrr+LOnTt71qh71sUA+DUAvwLn\nvTsC8C/21OH7LGIXr1m678lkgpWVFayuruIjH/kIPvvZz+6Y8EiSBJPJ1taiu43tWPqqqnDp0iXc\nunULH/7wh3eUl8S68nkQOTt5NmwH0ONxEVHEpgNpUrPradIsfmms+52NSox1c3qi3M9KRuSHEfh/\nlj8c5DOQCAz8zx+90lm1E3Cf/3A4RL/f3/fEznbMA6iv3bgKZTUEUO8sCuGAuAUk6eAWRkJCkHW5\nQRGA53m7IXOJQDrgEv/Z6asrwV8PDkGVzV1X0xtAjZxsxaa9xrWmN4Aab8Dkbhcz5BsB0D6h38p0\nyjBgVpQHj4OWTiKJnwM+qoUjMywkovG3gPoDYM34AwD/jRDim3DWjAaoVZsPFVB/t7x3t+uXpSOA\nc33YbjU37y3YnW6/cmW7JEnwzDPPzKxsxxNeDNIJApVhBl1ACoIlgUy1POFJhsm20EkA+1ISEjH9\nwIgTR51UxslkBKhb494qdqRt0gD07YTWNpM+sU0GmJNZ2+Cd71NQ5AwjK0jYqTHEwUCfKNsRaAfc\ndmSK0unTRffirVK1q01Xpdc4xukBEAQyM61Pr2QORRql6kWJqrMXI2XSQ2oK97CwOuwOlEnfWVGm\nA6eD75D1lOkAqkPuVOQHkZWbDS/5Ij/QYO2L/uGa+Y+07OP8EPrFOozKAuAveoeQT6a3TreKNlC/\n34Xzr//6r08d44Ts+PweJS8SDpS/QUR/J4T4sk86OgDgOQDv7G3U789gu904eJdyc3MTZ86cwTPP\nPLPruZ6TVOcZs+ZeLqr0zjvv7HpBweB6v4H65uYmLl68iLIstwTocV//buUZJBFqKqt6XV1WQOa/\nQpV2wD0Ozu9zgL1+BUQA8lzljjG8lIRvXX58Wv9ugecGP8BwOMSlS5cwGrkcGK6bwOx7r9fbN/37\n/eCN27duIbWFr7EhAMgpJzCSCsaDcw6rUkClULp07c3ulQQ27TXsddnS12R9CKObRfEWpj3RSUiY\nbMEVsdsmqv4Sqv5SZ8VsM9gaTI97S+j7ndW1G1dhs1oS1fW587EHoco0Ef21EOIfAPhf4XZGUwA/\nBfBvgIcMqMfBE9K8t8VmefpySefl5WUkSYInnnjC6bp30ec8x7gVa3Lnzp1QenorgM4hhMBEK/QS\nM1V8LAbpAFAZ6Vl21x5woJavs7b+Qk2MTzgUBEA1GPSYeY/b15p4114ICqw+v/JVhgRgE1jf5yxN\nfMy4V8zQ2zqRtR2aFBRsqM7KzH1PFkGyw69xgSUG7YZ6yGUR7CjjNpoSKGFQIgeEt2X0iauzNPOF\n7IMgkXhGXtkqLB60SKF8cmypHFudeWabE16n+kscG9KrNmFkAkkWWrpEJWUrFMkCSEhketypsy98\nQmoX4J+ki+hV037rm/1j6FXTparjMXF/lco7WfU2QB/2jyLXWyfxclRV9SDZeR0G8AqADbjKdceF\nEAeJ6BqAv35PRzbHYCY5lqisr69jeXkZVVVtuUu5k9gPu912n5PJBJcuXcLdu3fx+OOP48knn9w1\n4J6nj3wXUGeAXhQFzp07N5VTMSu6Pndt3PycKL+Ar7it+1mpuhBmngHa/+z07HU/UyYdgvsXkAIw\n1r1aAqQfx+vDn3ONKoBLcfzi4zcwHA6xvr6O69evYzKZQErZAO+Li4tz8djeK1C/fetWwyWMnCrd\nWRJLR1YJsoG4sDIFsUkCGQiy0EnuLHClCrk/DdmL30ltzJuemGH3L50vBqtdEgqCDEw2QDLxLDrv\nxkZ9lAuH6/v5a+IoeweRdrDi5cJhZKPVTvlLV+i8ttTlHKtrGxNcuXIFZVkiSZJGIbv4b3x1dXXX\nQH27gnVd57/4xS/iq1/9Kr7+9a93EjXbBRH9z0KIfwbgUwCuEdGbfO6hAupd3rvzBuo8EcfWP6yh\n6/f7Dfuvnca8GfWugke8kLh48SLyPN9V6em13qcAqkG4isBxntgA0oGmdr0G7TUj0sWKh/Y+2dTZ\nPFqkamt2oLJqpuwlJL02QLjPfOexq2pqMRCz45zIypFIHZJW21p4QwoTm4cdgTjptYtxjyU1nPAq\nMf1+LclgFTlLSqPhwHhIcJU5cho7CUuHK1WperCkGpaTXZKbUXYwAN2uIlBl0ocRCXLTDYYZXLNb\nDSfUTtJFf7xqHJ/qP11AVo3CwmFWTLKDoY+23/1Oo/2dfp/HWQC/Q0TXhBCKiJaFEP9QCFER0a33\nenDzijjnaHNzE6+99hqklEE6dL+xn5WmY+OA06dP4+mnn97zgmKeOVcxUN8rQOcQQsAYwHhwHj9q\ni5Km6tIkSjSq1ZfewttaoLSI9OyOiXdyGPcaE8VJx1fcEmA9iOffpQC++fcn/fv232mPxz/36HVs\nbm7izp07uHz5cnAJaTvQ7HZxtJv/47dvrUL6J0kgWHxuEAkBQU6GKG1tfcse4wyKrVAQELAyhfJg\n36q0wbqHzyhyhQFcwbu2va9OF5C07HN176CzePR9WJUiKYd1tWtuly0EUB7ves4K1pxLUyEdT++A\nri+exNEOkM/x5JNPhp+rqsJwOMRwOMTt27cxmUzwve99D1/72tegtcaHP/xhfOtb38Lzzz+/7d/5\ndgXrZp3/+te/jldffRW/93u/t2X/s0IIIYloBODft889VEA9Dgbq8/KfjfvVWoOIcOPGDVy6dAkH\nDx7cc0lnYOcJSDuNmKHnQh8XL15Er9fbFUBvR6ElEkUNMF6aKLnFCmRJtE3Wtm20AsYmUNJtVRrf\nvu0ewzHR7s+TtzkZ5HP7LtmLtjUbzwz6rMTRsc5bGnozlfAas+0Tk4VxJkEi0z0xxyA/l2VD9x5H\n5T3hefGQCB3AfldCbUVpkNFIGGg0ix9xTMQCQECCyEZL8OIlhYRFJVzyaVwQSsu0kbA6UZ4dp9mT\nbqEWZrrTTNJFDIo1zwzVMU4PYLG4G4o3ubYDpKZwkh/P3jA7z9f0KzdxT7IDEB2rkGHmANws8D8r\nYqA+HA7f73ZeR+A2jq4RhdVVBWfr9dAEEeHWrVtYWVnBZDLBSy+9tO3u325iP4D6eDzGeDzGj370\no07jgL3EPCtNc2XSH//4x5hMJkHispcx/sn3TiFNgaoiCDl9vTEOfHPXlaYgnZTSadOLkpAmwjHr\nxoFwa2sQr6O3XffjALz2YBy+nRQAiUZKS4h2ftNfXHzU90mOrfcA/j8+fhXD4RBvv+2caIwxyPO8\nwb4vLCzclwzp6s0NKGEg/fzF85gVCop0IE1IemZdSJAQ0CpDqgsYmYTdTRKytsVVeYNs2RYoZ4ud\n7XS6AKWb87lJew2r3d04anFU2QBpOXTWvt51bJwdxGDs7Hc3Fx9BXnYD8+Hg+JZ9p2mKpaUlLC0t\nQWuNzc1NvPjii/jd3/1dfOUrX0GapvjGN76BL33pS/iDP/gDnD59emZf2xWsm3X+93//9+/LZpfa\nnppRPJRAfa/euzsJpRSuX7+Od955B4cPH55Lxbj9KDttjMHt27exvLyMfr+Pj370o3sGH4WumXE3\nmRIqI5EqG/TmLGnRNtazU2DFCy2RyBpYGX++qLxERDIAjv3Rm1FZhSr0DaRq68+MGXceY65s0Lvb\nYO3Y1MQT1YmsWzrDmIgRD84zXvZiuYCS63dk+p3VWLuipDSMKdtC0176Ik4OaHvv+EhGw31UyFAB\nSKCd5SRNf+VLeMAua8cajbThJ18Ix56kStUJq6IHBadTL1UPUM56UtkKVqgAwof5UmNrl2MzPzJ1\nfJIuYqF07Eqp+lPa+jLpd7JFU+9J9Ts1+bPiAXMJ+CGA/0oI8XMAvg3gLoBHAPztezqqfYj19XV8\n7GMfw49//OO5gnRgvkB9c3MTy8vLGI/HSJIEL7/88ty00PN6PgyHQ7zxxhvY2NjAE088sWeAziGV\nQFXVc7o2AFlCmrrjUjnXl8mEkOcdu4qawnWAA9qVJigpXOKkRIOBJ6pBuIqmZt36aIj8v9Yt42u5\nL36WcPz7Sx/xbQlwqhOAgH9w6BKGwyHu3LkT9O/tAk7b7cSt3Bi7vCZBAZwLQQABVkhIANbLXLR0\nFaSVreq2RKiSHIIozLF8jkkY3lGUZIIzFvkPS0VzbaPYXdpHWrTqZmQLENY0NPImycPcy4w5CQmT\n5AHY62wQ7mt4JzU/sOudzrWDj7n+8gOdCaVbRTyXnzhxAlmW4Vd/9Vfx8z//8zu79zYF62adX15e\nxvnz52fKZe4nHiqg3vbenSdQN8bg2rVruHnzJo4cObKlP+9uY56MOhFhbW0Nd+7cgVLqvgA6APzr\nHyVTVoxk3O+TSrqtSS2QJ54ViDToxjq6w0THEkkYFgq91Dba8rVla+KM5S8MuDk4mRVwdGIiW0Ue\nWixzYdJQPTWW4LQ18ZYAWBX1k2xZZVV7HbyxCr2km8UwVkFJE5h2B4ybf59tdp4dalLU0hjtgXbb\nBz4u9qRbYFxTAgsZWKVYPtNwdqEMliSyDmcb3hEo0APJvmtDvnJrBOgnYgEDrEPLLDxENFJomSKj\nyZS8plS9oJnnGGXTuR2V6s0E6KPkQCfjv6mWdqVRf1CAOhHdFEJ8G8B/B2fPWAC4DZ949LCEUipo\nuvdDjjSPwnCxx/jZs2dx5MgRfPe7351rwuL9jnM4HOLixYsYj8c4c+YMxuPxrmUu7fif/lUCISzS\npH6fZJvAGwCKwh1jJxgiBFvGcJ3/vy19G2MISSKCvp2Bdd3e9cdgm8hr3wkB4ANNZj0G55y0Gs5T\nLc2soz4gBOE7b52uT3mO5pUzNzEcDrGxsYEbN25gNBrhtddew8LCQoOBv3JXII1kk+6WnFeVAJ68\nIQgYkUAICvOkFSrIDiVMYN1j4FukC1N1MTiYLHF1NXJQIgODHlcorXLvpuUrmFqZADKBLN38qX1d\nj0SPp+ZhnfQ7Cymx4UAcVT69mz/qH8GhSPpS9qYTVjmG/aMYjO+gTPpYv7WKkyem5+l5Wu3uJhic\nf/Ob38T58+eDLe9OQwjRA2CJph9mDxVQj2NeGf1ckOL69et45JFH8Nhjj80t+YRjr77ncbBWnhn0\nwWCAF1544b7HVuiaukhUzIjX+kEA0FbAeiYjTwg2ZPRHEzkJFLpOOgVqcN0G7SxvmWgVNPEShLRD\nPsLBbTmdNEvMVL8ctV7dsfe9pBuIa5KNKqslIfjD8xjjEZWm/koREmQzAL5LdHUAu0KKROrOBFpj\nVe27DtFg5DWpxuRf2JplZza+Sz5TUgYigUyWU/IRC4mSclf4CQUEbJDXNPvIkaD53rRfWAzVoZAA\nG19bih4yTIN1I5LGsRI5ssgHvlL1jtVYLmLBuOSme+oIDpq7U2MDgAm5XYw7+SlYktiOi40n9/X1\n9fe9nRcR/VsA/1YIcc79Ssvv9ZgetLgfML2+vo6LFy/CWotz585NAYF5Oo7tlVGPAXqsQWdHsvsN\nB5jJg2MnYXEsufvZGmq0jaMom0Baihp0C+HlNMKBbikEjG3NUxF5JAWgdQ3Q22NkcC6j1/gc4EA+\n/9w+H8tm4gXG+Tc+1GiPwy67+xcee9v5vxdLWK10IFVY4khwu8kKxudYObKDjxMEpNefUzQ3BoMA\nL40RoADYdauuRLKF7MUol8vU3s2skn7QwAu2J84WQOjY3U76qLzTFwCUyQKyFikySRexUOzMjnH9\nyJmpQncAsDE4AQxONJ4Pw/5RrIsjM+uVxFVJgd0D9e0K1nWd//rXv44jR47g85//PI4ePYrl5Z1P\nx0IIQW61+mkA9+B2TBvx0AH12Hu3LHfntxxHVVW4fPkybt68iUcffTQU0Lhy5cq+JCDt1c83BuiL\ni4v42Mc+hizLGlZwe41/+X2f5R0Sfvz2GgF5So0EHwbpREBpRIPZIAJSVYN3oAb6hZZhgrQkglMM\nUGvcDbvGiBqMWwhkykJ6MB9fx1Ho+s/bGkwlp8YPj8rIwGhvlfBqSWKiZZD75Ek3wOZJdaSzhv5e\nCgvdIalhVn67+3cx8pVNkLQsGktKYa1ELuvvQLvoVGkzGEjkouzUxE/IV0edIcFhq8q+GAWQHs6h\nNwXkLSQmYgGJrFwVWCFRUYZKZFggt71ZYlpbPkEfPexMxjKhPnpiuu3Vmxt47EOz4ToRBd3p+51R\nj4OI5oO63ofxbtnt7iZWV1exvLwMIcTMpFZ+Bs1rrLtl1LsAejyW+92d+OqrCsZY5LkMPulEQFnV\nGvQyAG0BY+rEUmsJWVbPf+zconU9pljvngAoLXWy5PH1UtQe7cawDj5qJx0YB9CkemgamDNol6IJ\n5tvRZuG53aX1A0jEAInUtWwFzZyq+HeCgPXO4AoagghaphDUZNZjsBrLXvh3AAHgl0nf6dz9PbIO\nEKxlBilMq3J1jlSzu5Zn0SPQr5N+qLMxK4pkYcc7mTuJSuYzDQu64n5rYmxXsG7Wea42ffHiRXzh\nC1/Y8f2ieBbA/wc0wDuAhxCoc6RpGrRku4miKHDp0iW88847eOyxx6b8bpMkmVl8Y6+xF9cXTrRa\nXl7GgQMH8LGPfSwksxLRXDXvXc8IbUQA6lIAaVJPGmUlkChqXFdqEViQPPE6twbQd+c4OdWSS6SJ\n+40lOG5BIIMcZlQmjcRU1sorL5UhEigqV7SpggPtEhS08LHspLIKEyshGWAr0+gzjrauvSt4TGOT\nTiWjhvfPbX211jDBSh0Adqjq51n+yqYoSSCRGjpKQJUtlt2x57N3bNhLflab0jP1PRklE1ESHjYj\nO0Am6oRZ9qAvKZtytNGUQFOCBTFESXl42IxknRxqSGGMhQZ7FQcJiU3p5DEb6jAqSjHGQrCh3EvE\nQOZBAuo/KzFPi8J2bAeq29a7cdXmrmDt+7x2XXfKqI9GI1y8eBGj0agToM8rGD/E4DrxEpjJxAF4\n187JWIylBtgtSwsZoVwGxYAD3UwXkCUYIbx2vW7j+iJHAKUC1rpdTRHOe0LJ1Ey9MU3GnsknpRAI\npDbwDsejcXJwv27shKUFjV5SIZEWUtgwB/N8zLufPDcSCSRCN+ZMAUJFmZsrvRGAka4uiITTrZMQ\nkGQbrLskA0Xe3CCSxHDFawFCkTpsIKnWnRuZBMvfmJ1mHXz8exxaZkFqU6QLyCOXmFHeDYiLdIC8\nGmI9P47Fag1FugjrJT0EgZE6iAO2uTvaZf0LAOtia3//NlAfj8e7MvrYrmDdrPPMqp87d66RfLqL\nSOEYdVBrNf3QAfUu792dRFzhbiu/2yRJMBzO9nveS+xGo05EuHnzJlZWVnDw4EF84hOfQL/ftEma\n1+Qcg3El3cSVeeDcrqtQVG5bT8lucBVvVWorvA8uwVh3TWy5FbedVDKAW4JAqmwA7Cxr4VeW00y8\nxy4z6KZD/lKZWrMNoOFWE49DCkKp68G1WXnWvAMM2rvbth1wtE1QEmCsDKx8HAyCC5+0mimNLpcT\nAChtvVDgfrRNWuy56ye2mKyo2WZi3IS8oDqKUUBiaBeQizKU+uatWqAJyuMY2x76HuBXlNbAnAZT\n7H4uZy+Ax7SABbiEp1UcQ4rZ3+0xLcz8rLqCiBos49raGh599NEdX/9B7E+07XbZL3mesdUCgIjw\nzjvvYHl1nOOLAAAgAElEQVR5Gb1eb8fWu/tRF2Or/mKAfvbsWRw7dmxfdx444bMoDPJcBXtGYxzz\nrTWFxQ+RSyrVhoK3ujYETieKhxlkKIYgpctVspoghUtc5V1bgHXufK9mP0nimPuaHa9PslY9Bu7x\nvdt91YCcwu9SOgIokYTFXCORFkra8ByQ8DIX74deExASEgQpLAQIpQflBBHcx+J5y8DZ67IMxVk2\nTrudAY51BuDlMHIq7wdAnaDamjuNTGHg6l7wGLRMkdjK19Ho/turnbkWAovPRgNsqTvKl2bmFg3V\nIQxMrU3fSI8gLzdwK3sMiw6v7inaQD3OXdxp7LRg3XbX7DIWAEwXGcFDCtSBnSeTDodDrKysYGNj\nY0cV7vbTe3eriAH6oUOHOgH6POP//A/1KlpHk1qpm7KWWLfOYTwQD9XjyP3MYJyBc/OV/KtAntbA\nPPRJDnxPKmbK/cIhuj+RCCyIpTrZ1d3De757Bj4w7n6Mo7IuptRIYCURTbQCunL2kkaLoGufYvE9\nAC1M7dueqXaF1PpnZuXbbeIoTRI+j5iNn8XEd7H7liRKkzX6YBcc/lkKi6FZQC5rQB5HQRmMUVPs\ne2UTaM/mpEJDCYPK7zSMba9TmgMC+nISGPsY1Ls+U6SyCmO4R0tTvvD3qgPoJ7NlY6vlQRxKO+e+\n+nNpFUZbX1/HRz/60S2v+SDe3WBb3HlHuy4G0JQTDgaDXSfkz9tud5b0hQH6cDjEuXPn9h2gA8CX\n/xiQCqgqCyGAojBIEhnAOlAz7gGs+6FXltBRjmGmvCSsnT0gl6KWxVgCyDbZeA4G6V2gu32/+FnG\nEpp2CEENgJ4qQp64Oh9K2GBwILybi4U3iyEBCMeqq2j+s+TsFmMmHYRGHwQRdiEJTuYiyJ1TXlJY\niD5SlJ2OKlpm9bPAFo02LGlpA/BK5QFouz5qYqXtosXSGqC7dsVQHcKC9Va6avvvjoGCgsE7g8en\n5vg1s4QPYeTH0cMWaWpu3Fo3nPgegLoYHDmAThb4oQPqHNsB9Y2NDSwvL2MymeDs2bN4/vnndzTJ\n7Vc1u1kTO/u1r6ysYGlpaS52kDsJbdwkxhn1dWXR+lVKJ3MB6olPGxEAOQMsIZyeW4MadliJIt+e\nGqx3vBjg4hdAk8U3fpKeVKJZKCPpWjj4fllWowWyJKoGygsGlqhUScNdJldmihEHHMBmwO1cZ2ZP\nCJxkWpKX3QgLY+ttUAAYVZnfHk2QK+OlLLXHPP/MoD2T2n22keadrSfjNm22vrIqvB9eHHCyLAez\n6z1VNJj3yqSQwqKwWdC/V61rC5s1HkyAS3xlsF5GenYG6XW7pMEqEYkgzWmD/Vkx1I7NOZi6B8XQ\nLGCgZsvgtNZTyUd7TSbdrqIdx9e+9rW5W3g9rLGfdrvxAqC9WxnLCXcT8y5gp5RqyC1HoxGWl5ex\nubn5rgF0DmMsAAldufkgSSUq/zPbNUolQJaglCNgLAFZKqG1hRBuvrYgLz2JJDB+GpO+jfa+64H9\n9oL0GJwT/HHUoJ3bAwzCef50x5QUIZ8qzqNyndTvVXlgLoR7Vinp5u5EEhJpHaj27VVkuag8wSGi\nVyZBTADmDsS3terx7wHEA+G8O5cECQ1LVywkUpSNvgAE61xLKtTU4PsYkQTJDEelesjMOFSttpDo\nG0d0lKrfya6XooeMts+x6wLsQzW7evtEDtCzDrcWagG5GeGeObitxXG88C6KYq7GH/sRkcwlxc8K\nUN+OUY+z9c+ePYvDhw/vapLbL0a93ScR4e2338alS5dw+PBhvPjii+8KQAeA//0v8qnqb5ylz2xE\nmjS163F2fQyo83RaosJti0q4hNKqriiXJtRYDABuMWCpdpkJjHuEi7ltZWqWPk+ml94sqwnMPGvG\nO3YGALfwiOUsiawXFdrfi0E9S29Kch7zUk4z0oYEYCQIaqamXYI8G6+CNKYrSpvUlosz+iptAku1\nU007kbU0CSzJmddvardr00+KAOb5IVDYDNrKUOFVidp7fmIy9FQZ2CRLEiXJKd35xOToqSJo7CtK\nkIpa5rOpB4H9H+oeBluw5+24VzWTRznhejAYoN/vB2lbbM0I7F2jvl1FO47z58/jm9/85gdAfZvY\nT7tdDnYHu379ephr75cM2Q/pi7V2CqDvlFyaV/y3f2AghUBp3Gs7rCHoykIYIEkiq1sAZWV9cqZz\nBWiaDdTMu5CAJULXI9baWuZigCCPgXGMN4N2IWpdOT8X4sJLzJy330Lssy5EDdLThJylb0Jeg96U\nykjv1GJJQkmDyvo8If+78b/HEhe3+yuRSg0igRIplM9DIohQ+C4RGpoSSFgoYaApCRJD1r+7bCsB\njdTNzQTHtLeAvkYK1XrTpeghQRX07hopKpU1CuUVqpYSMigvRB992pzqC6qHvtne93wj2dn8eps+\ntKN2ccSuL+vr6w9SvlGKGUXrHlqg3vbe5WQgKSXOnTuHQ4dmr+S2iv2WvsQA/ciRI+8qQOewlqB5\nRc52K6iBMfvYxqx3nk1bcAGuXWzlKASQJTWYb2vdJ6UIcpVZ4LnSIoBlJQlJa6+Mde+Flt63l6Ud\nAlnSdJ/h0MZNkFsx4w601xNd7NvOWnVta1tJnhPZnYaZe20llKyZeyLRqZEP79coEE3r6HksLrFW\nBSZegELCKUesdY+jtAkSYYPOnZlwl6yqAvsyqnpT11bWbckWJm0Afba+nJiscZyBfuqBfV3xNQ9g\nXNsEGsmWxaZGJgdM3pC0FCYL2npNEklrF2FoFnD0wAEMh0PcunUL47Fru7CwEOxRh8Mher3enif3\n7SrafRB7j/0A6gx+f/zjH+ORRx7BJz/5SeT57iradsW8gXpVVbhx4wZu3769q93feUdVGqSZ+25b\n9j4v/XyhnLsLA/iqtDBKhGewY9JF0JVLxflPFFkz1km9bheX2fmaeW9LWmKCSAhAWpoC7EI0JS5O\n+F3PnXyO5S2Ae2ZI6Zl0D9LblUQlqAX+BbR1NTeIBKwgIALpYYeZ+xEETW4O5b5Z8hKK50UkEdfH\nMKSmEvfbwe5ZnCTK96x8CdYUZWDjK2RTBErcL9+rnfczlouh71LUGGWk6kTrDeHkipkoUFAPuZjg\nXtq0O5z1HrpiVPVwKKvn/b+7rvH0qVbdkAereF0chog6E7UeOqAeR6w17PV6ePrpp++7ut1+MurX\nrl3D5cuXceTIkbk8NO7HIsxankzqCnK8DZlnosF6C+EYd24DuIQerSPwnvCYXGELniBzvysVJwoF\n68ZIVpMoQlFNy2QAoKik38ZkdryZQMrAHXALgS59fayH11bAGtaDbyVnYQG80793yWOA2ot+K0DO\nCasxGz/dj/sQtRHIU9MA4qGNUbC2vk97UVKaxC9epsdqSITKpYl/uLSv1SSxkJQBjNf3TaE6wHVh\nUki2DvPHRlVvZmEovmYWwz/UvfBQ2agWw7GJdn9IXYmwHMePH8fx43UpamstxuNxKBf+7W9/G7/9\n27+N4XCIL37xi/jEJz6BF198EZ/73Odm9hnHdhXtAMe6v/LKK/jqV7+6oz5/1oMTEpMk2bOFbTus\ntXjrrbdw9epVpGmKc+fO4cMf/vBc+gbm94wYj8e4ePEi1tbW0O/38eKLL84NoO/22fBb/8zXRdBe\nN20JaaZQlcbrxiWMsZCJCm2MjZh1nyDqEjnrYndSulTJJHH/122/9DjBFGhKZYypLSDde6qtFZX/\n3Ql1mmBdCL9DK9yCgCUzvIssWyA9FEvyYH4q6dNKP1+7RFFO5A8F3zxpQiSarlyEAOL9mgVCENJo\nXuf2dYEk1UjarygN4F4jQSqqAHxZC08k3K4muHCSq1gtI7F3+z1ZqFDIziCBAZChQEWpq/vhdfJd\nSfs81qE8OKU359g0AyyqaZXHWnUIg2RaprhmlrCkAOzgaxUD9dXV1QcCqAshJIC/mHV+50uZByR4\nYudKYTdu3MBHP/pRfPzjH59LCep5V8mz1uL69esYDofY3NzEJz/5STzzzDP3DdL3yur8zv+dodII\nVeEmE89GRwSlNhT+AdNMurE1cI/nXX52xe21cW2LslnRrmHdaGuZSVkJaFO3bYN2awXKSqDSApOy\nCda7JDXDQmJSyYZkh/X5gAPtw0JhUklURmBSiXA8dqcZlgqVFqiMxERPJ9fwZ1FqhcpIbBZJGG/7\n86uMxLhKIg37dF/chhNY2V4yyG+0Cn2YyFaS21dW1UWfOlwESjP9HvjYsOr+22TWvh0Movm+fCxm\nzcc6b/Qx8tdUVmGs85l9x7FZ9bAWyV3ulVvvREkpMRgMMBgMcPz4cfzyL/8yLly4gBMnTuA3fuM3\ncOLECbz++uvb3nc3wYUyPoidxTylL8YYXLp0Cd/97ndRVRU+/elP45FHHpl7stn9Murj8Rg/+clP\n8MMf/hAnTpzA888/j16vNzeQfj+VTouJriuQRqDd+MmzrDxBELXhdmVpgra9/ZkbQ6Fd13FjnJMM\n/2ys+8cWkNZO90fRsfZ/cZunYGmMlIQsISSKGhIXdn6xVINTQ7LRr/FyFnc/x64zeaOt7MyB1DYJ\n2nO+tqIk/DOQqChBSWlg1znHp4uJLmw+VZnajU012HlX2C6rQT0kDFSjT2bf2VJ3TNP5GmM7bWhR\n0rQmvKDpuXhoFlpttsY8a8blDVXR+9uspvs1xgRzgPvJN3o3g4gsEc0E6g8lo/7Tn/4UUkosLi7i\n2Wef3Rfv3fsNBuiXL1/GsWPHsLCwgKeffnpu/bOucbcRby0yWK+0Z7hLz1abuAhSXYzCGOdrC7jt\nTQ3nr+sKWrg2RVkzOdxXHEXpy0H744Fx73grRVVPuG1NPQdbTNbtphcerqS0165bgTyNXF+CREcE\nX/VJJYKMBkAkaallO5WRwZVGiOldAMAx8q6Sq62v9+4x0kt3ADe2VFnnNhN0/u51XCkoIWfKhNh5\npuFkE4F6awUMCZe8GrT2zvN34hn8WOLD9y2NWyQM0gKWZNC+FyZFZRQWswksyQYwT2Rz+7Q0SWNX\nYFjtzsVoWGUYpE1mfrPMsZg1dw/HOsW9IsczM4omtV0/pJR48cUXdy1b2a6iHbPpH8TOIy5gt1eW\nWmuNK1eu4Pr1643idcD+1MVIkmRPxfbG4zGWl5dx7949nDt3Ds899xyEENjY2NgXF5nY6Wir+M3/\nbQzhCxcBgK5iBxPHcuuKIKTw7Tgh1LPAhmC9dzdJAasJSjkTQ8BdHxYhzLxbCg4vwWM9TEMESS55\nNbZXZA276885xDDLzvMyS2wgRINNd+cotOMdWukBuvRGCFKSl7E4bXqov8HFluCTSXmOta5AX21p\nSz5vh/gywM+RzmBABS07H2MAb8i5bRFEAKsCFPzZY1nmxLoaFSlcvo/xzjGGFIx3qncSyWSKqMlE\nCQPlADqSwLyXlCITVQDucV0NBueZ6P67H5k+BAh5MsGGqS1Oh2YBhhQWkya7fpeOYlzl6Km6PyZz\nODb1bDcZ/nt6wDTqM+P9h2DvM4QQePbZZ6GUwoULF6YSxd7rsNbi2rVruHLlCo4fP45PfepTyLKs\nc5v8fkJKuevJ/X/5sxTWWmRZkymwhsCFzSrrJkBTEvKMtzLrtlVVy2QAB8x58oufC0SESqMB2jni\n/sqqmRCUJn5L04+PE4OKCgBEsG3kPttaeAa6og3cqQahsZTGWoEsrTXuMgKWTkbjPoM08pfn92st\nMDIyAOxeWm8JxzEsVdDwcxsb2VsaizBZdyXIArUMhwiAcp6+zK4L4Rh2p3PnMdTMiRIueZV0LZkx\nEdtdGuduk6tIh25cRdlhlQcHBHfc/Udulj2kqvn3VxiFnte5lz5ZdpA6GU1pFLSVWEhr1rS0SefW\n6naxWXp9Zksa9G9+MsA/fn56u7Vt57XX2K6i3fLyMpaXl3H37l3cvXt3ZrLpBzEde2HU29WlP/vZ\nz06B0/2oi7Fb15cYoJ89ezYA9Li/d8PusSvu3r2LsswaBYqcVEUGkE4tNpu8rqQsDZSS7ncpoJSY\nYr5de8CQRZJISCmgKwupBFj9R6KWnYRiRtYBc6XcOUFONqOk8MdZM+8APRB7ptfvhSUvDMpZIhnm\ncC+RcZ7sAoKcY00i68RWIgEhyblyAdAUA23uR4b8H4JAZV1iPefSGBIuaVTUfuyWZJj3ub/CZq7A\nnWUSRaPyrLwAIW0V0mP2nZ1StGfV2UGrClLH+u9rZPvBYliTQhbl+8Rs/cS6OXNs+6HfAlkYw9As\nNOx2CQJreqmhh29Xw95N3BkvoJ9qfPOnA/zSs93f4QdMoz4z3j8Ido7R3i6dt984F1XqKog0K2KA\nfuLEiQDQ45hn2endTu5aa1j/5Y8rxxlDyNLmmMLWp0GYDGMPWq5SFwNSV1baWS1OCgrJp7wFWrYK\nYBjjgH1bS80sP7PkadqucBq5ypQ1uOfxJqqZjFRWIiQj5TPUFZWu30+ednv+Vrpmd2gGu136fpiN\n79pxZxYdAPKEguSHg91quPhTHNzfqEwgBSFT0w/jUkvHoCftaz1TrlWj38ovNAAPtBMTFgBhTFp1\navkLnUw51rQlNXcn/cYCYFSlHvi77WLWqg+rHIO0wES7/6R+xKRvlr0G67/baNt57VV2tl1Fu89/\n/vMAXAW7tbW1PY/3ZzF2A9TLssSlS5dw+/btzurScbxXdTEAV2RveXkZ6+vrnQCd436kKl3BuxRb\nxfr6Ot544w388/NPQkXzojEWSkmUpXH6clP3CS9PkUqC/AkphJPFGICshCXHppt40vaoWGsLKYRz\nfjF10SQpBch4xEwUMeD+fVgBExI0EcgTwc8lvo3ySa3CaeKVrJNHOZHUkpjanXRzUW2TSyRQGYFE\nNutyJNLlKUkQSLCm3FkxSFGDUtkAvjJYMjbqddikYe/otOlMsqiG/SMfS6RGYVxiaNuSt22Zq0mF\nBFb+PY7KJgFwl1GhujhKm07V07AkUZgMhU9QbdfGiKPN5N8tDqCvZn/H7xU5DuYF1iYDHO+7Qknj\nqglh23/X6+vrOHPmzMw+H5R4KIE6gJCAtF+WXjstER0nLp04cQKf/vSnG57NHLvdjtwudjq587bw\nNy48CcAVpEiUQFHYALirVvW3lIG4qdlofpXCsehALYOZFIQ0Ff4c37cG1Vnq5DVpUgP0MD5/j0QJ\nlJ6t50WEJaCqEJiUJIkSUv2cZC0wKeo+i9LdL+jVXQ6Qv5dPSPKAfFwI5K2FQBO0xwlF9UJFG9Fg\n9es2dVJrqWsXgC7ZinOYccx+Glj/ZsIPg/qFtBsU8Hnu33mwy+CIY6zAINOBHQccMDfkdwk6gP5m\nmTaAdRivFZ1gfVikWMj8hO+1+1woqujQwccxKhMc7NVymWGVB/3/sEpB5MbTn/H+dxptl4D70TRu\nV9GO28yhit3PRDDA3slcXhQFLl26hDt37uAjH/nIlgCd470A6m2A/uyzz25J0LybjPrGxgbeeOMN\nEBGeeuop4DzBVAYVOUcXmTiQ7ciqej6O3V3IEqx0872uTJCwAI7k0bZ2jyGqte5KCRgiqNhxxEth\nuIiRa1gngzovdgRrR8Ax7fyMiN1iAO+NzpKX4PRSF0hqGAz4Z542bt62EFCgsNsavMr9JTyPWgEY\nI5Eq420YBYiLInn5iXN6YWlPLW8RvrM20AbcDmPq7R950cBuXRL1risnreqW7W5lUySytr6tOrTs\nSpgwvsovFgAENxu2022OK63rbNhp2+G2Fn2sc/ST3cnN4hylTBmMdE2+xvN/W8b4oGjUhRCHABwA\ncLvL+eWhA+pt7939rGa3FVA3xuDatWu4evUqPvShD80E6Bw8Gc8LqG83uRtjcPXqVVy7dg2PPvpo\nSNZJM4mqcoDdVZ/zYDKpGWMG4sC0TVaclc/lnflnPhfr1wEH0gGg8IA+1sJz33HlO35AJEntEmAs\nwZTx1uj0TgAXwuDFgpTNRYHrx4/X37vU0/r3UGyprB8Cqbd95N0FN26BSgMVakBuooJPfC8iZwuZ\ndhZrqll4bYBeZIPJ/dybJKGkdVcwA84Pi3jBwEmtXYuFykhoIzDIfaEirZx23jvLLGQapZaBUdK2\ntqJ043UDHJVJYxdiousHAOBYkURRJ2tzb5I13HTivzcOfg+A+/x7qcG9iTs2SLd/IMwTqH8Q+xNb\nAe7JZIKVlRWsrq7i9OnTePLJJ3e827lfBey6+twtQOfYa77RrOgC6pubm3jzzTdRVRWefPJJLC0t\n4df+xzrxWTLrXTkPdWbGGZQzGE8SBWMsSBMSP3GSL3RkTPwMYGmIu9YBeGfHaK2TzEjr2HVm3x2L\nzFVJCeSfN2zjKIVoEEoA+6HXu7MxYOfzU5+P6P6dyHE6FgLWM+xuHmsSKMx260heaK3wDLYIuUA1\nwRXLEOtjQXNOfM4ES9zKKqgWIC5tgqwlfTFWNeZalswIUDgeijFZp1/nftsOM3wtF8MzJIKFb/u+\ncYx0BgFCP6kw1LkrKhjpzXNVYa1sGn3c2HT22b2Be1ivTZqqiGHVjb3aQP0B0qg/CuBlAFIIcQXA\ntwB8FMBbRHTzoQPqcexnkYxZk7sxJjDoJ0+e3Bagt/ucVxWtWUDdWourV6/irbfewqlTp/Dyyy/j\na/8yA5EDmlVZsxtALcdh0M7guiwtlBIhcYeDM/Jl1Jaj9H3HeketHTi3HohXPimJX7ukN4AvrFTW\n1ln83SSqFwdlpJcvyqatJEdV1T68vCXaDpbQMNCPGXkOBu1SAFnarMAa7qUd2Gbrr3bE/vBpJHvR\npraUZIkNg/p4HJWpnWjyKNGVH0qFv7areit7zueRJKfQzlZsWKgoCbd+0IzKZEpyMqn89q6svYFL\nIxtSm8o4l51+5mQ0Srj3yjsH7e3MnUZX+e+3NxZhqdb+d0VcIKNtsfhBvLexFZAdjUZYWVnBvXv3\ncObMGTzzzDO7lg6+G4x6DNDPnDmzY4DOwVLLeUUM1EejES5evIjRaIQnnniikQDNzLTRtsFS80ji\nokec/Kk9w1E7w3idOpGTr7BFoG9vJXzBIgGpBLS2ob0F4B0CnYQGTsduDQXdedMf3YVSjs0XntVn\nS0YRfm59HkFK4155jvZKG2/5WLPqTR93ESwhAUAFwM4kTi1fqT9X4WU7077wDN5jRp3HF9e1UL7a\nKZGAkgYTw3OmK4IXs9pujCJIc3h8qS9E5xYUdd2NwqQhiXOs88YiJK6SHUecT1RGlapzVVdDHXvZ\nIlfYHlVpIyeJY7U8OHWsHZtl2iCmvvnTAT5+dMWNMWLf9qJR3666dNf5nVaknhVE9LoQwgD4NQC/\nAuBzAEYA/gXwEDLqQNMpYN4Z/UD35M4M9VtvvYWTJ0/iM5/5zK6SWPd7ezPWyD/yyCMN54PKJ+/E\nUWny5ZgJWSa9lzpPOjHQdvdIkmYpaXeOV+yEpNW/tYSydJN/VVEDkJP1RYnYZSD0A2SpCFIaoGal\ny6pms9OkltKUVe0cow3BeHlLDNrrRFPHyispUHpJDv+XxOw+6+TZB76dHDouaqY9UQiLiThYGx8z\n8QzMLQHYApAbKwAvHeoC3QAwLt3Tq4ulZ507g+cqWggUWnR6rBdawlqgF8l9tBXQ1unTY9/30gjA\nCCykJiS5TioZFgwcsda9NM4/nu0ts8Q2NJTcRz+z4f31M4vNiQrH9hoPop3Xz2oQUajQORwOt9R2\n7yT2i1E3xgSmf21tbU8AnWPexY2klJhMJrh69So2NjZw7tw5HDt2rHGf/+K/vwkg4GQomcBo68Bv\nyF+yQfIiPQMOuPleCoGq1A3ZixDCMfJSgIRLzASz7B6kA97uEbWmnIMsApDvCmeh6H/27Dn/i60W\nZeuVZTRA092LpS1BO+53Ihlcm0hjLb0chgGsbPVLJGDhxuQqkPqEfUJI3Ix3So2VDfAuRHPHUftr\nBQjG1Dgj5Bp5sJxJDe0BvZM9erbcqlAxlcP4qqiAA+uxc00qTSMhlll4luMAwESn6CXu4cg1P3Ya\nq+M+Bvk0aOcK16HdpIfDvQnGpcSBXktiMxxidXUVw+EQf/7nf44//MM/xJ07d/C9730PUko89thj\n236Xtqsu3XWeY7uK1F3h/dM/B+ANIvo7IcSXiWhDCHEAwHMA3gF+BoD65ubm9hfsMuLJPQbop06d\n2jVA59iPstPGmIYNZJdG/n/4P6jOtJcClbGNP2alRGDChRQBsAMIshMlhU9AjSQqflGrPavNGkJX\nKK0G2pOJQZq6Sams3CTILAmz+kXp2hPVshsG78zaM/MBOCAthJPSCDktpQEcaLfWXZembrEQ/7dZ\nS5gUsTuMgDZuwRGzOHFya54B46Jm3JWq9fHWuuPtYJZdyW6LyUqLoOVPO5JLC04u9dc6TWW9/cpF\no9yCoSm9YfDdZvf5mm6QL6YWXdoKGC3Qz2ywqgSAUdVMMo0dcQC3MInvPSyTsCMwLiUGmXcm8CB+\npzEsdicfi5O4HxaXgIc1fvjDH6IsS5w7dw5Hjx69bxA777oYgNt+v3fvHi5cuICzZ8/uienfryiK\nAmtra7h79y6eeuqpmYscEz0HpBQwlXZuKomCNfAJozYkj1prQJagEulkLqi15gYWZF27GOTX5wHm\n6YV0shppndYboFAUSc74DHmcnEjKwcmiXe+vBvFNTbrqmK8IDpmzwwsRM+V1f1aISN7inGrq9+bH\nE/TetRsXUAP2eiDT4N0BcP7/iMchQgKrlARNolGVeWJclWcG06VxDjGJsB6YN+dKGVxomomvlVUw\nVrYK3TlZTZcu3ZCEEhbrkx4WMveQnOg0mAtwftKoSrFZugfjsEjRX2iC9fVJDzeqQUOHvjrp+c+x\nzoualAJnnj2DwWCA0WiEkydP4sSJE/jN3/xN/OAHP8Af//EfY2FhAX/yJ3+CrWK76tJd5+/cuXM/\nFakPA3gFwAaAKwCOCyEOEtE1AH/NjR56oL5f0peyLLGyshI03nsF6HGf82R2hBC4c+cOVlZWcOzY\nscAS4MEAACAASURBVE6XGQBBckJEDR04g5fGMW1DpTnAT6qtwhKAA9gsoeFJsqxs5AojHVCOGHMG\n3HFRoljG0ta4J4nwYNx6nWIN2ovCTsllitJdnyh3PW89Oo93v2XHvuxJc/K2fnHB/WTpNBsvhUua\ndf2IxsKAXxnUawP0O4xFuGIrAKisbjste2leR+TOlbob8GvjxmtJNBJgK+0YoqKSyFPntMLJsqzB\n72XtYleslafwIOPP4N5IoZdRWMhYC2j/cHF2kQKVd95hdmhSOUDOLP+sHQI+Py5r61DeNdgu1jYl\nHjk8G+jHD/IPgPr7K9hHfHl5GZPJBKdPn8apU6fe62F1RqyVV0rhs5/97PsGoLMTzjvvvINer4eP\nfOQjjSq9cfzn//RtWM+eAwjAnIOshUWb7aZm0ihR0KpLJZ10xRqQB/JSOjcYKQVga2BLliCjZ0wN\nVAnC+63HH6mUjgDiXUDOk3KWi45gYVCuIjKj7tvNgbVLjJe42No1BqhlMDyLKFGTIV3RsG6M5CLx\njBUbEADOTjaYIUBC+buxo4zrQ4YcLWqBcmbTK1INaYoShIlJGmCa3WZ4XCE51dRuM25M0+/LkJyS\nPDKDPtFpw9o3PhdHbA8cx63RAJUWW0oVR0Xz2nGlgmSUNep5nuNTn/oUhBD4yle+MrOvdmxXXbrr\n/E4qUm8RZwH8DhFdE0IoIloWQvxDIURFRLe40UMJ1Dn2w/VFa421tTWsra3h9OnT9w3QOebFqHNV\n1suXL2MwGOCll16aqXv/p//c7wrYenHjJl/PlKtpKy/twXqdqV/7d/OWIDPw7jJqyGocMHbgmuU1\nSsX+68y4OMlNHCVLawSz8dYnnwbVZFhYVJqQJk33GuOlL1K4+xnr2PQ6qdOx8i4p1R2LQTHLZlhC\nI0RsRdmcsMuKAqveZqDdeWbz4cfr34F/y5uj2lqSHxrG7wBY7xmvtWPxpyQlvi/H7DfvPSkF0iS2\n9fKLG1+dtQ2Ux35SzFPbYPMnpWP6F3IK4J6Pxyy5A/vNPkeFwCCnwL5z4iefW8hd+zubaXjIxQuM\ndowKGca2NDBY3XRP5IMDi1urMuyUdElk2mwqF5v5IN4/MR6Pcfr0aaRpisFgdpGT9yqKosDy8jJW\nV1eDVv673/3u+wKka61x6dIl3Lx5E48//jg+85nP4OLFi1vuIlSuKAUUVADfxru3WOMAvOjyQzcE\nsiLIY/h5Yo2FTBQsEYQlEAiA16F7EkRIFXTlLkEVgKlllMpPjGy96LzNHXDn50GaCJ9UWmvRg9wl\nsmHkiKUlccIoy18aUhRy/RLLTaz7nV+ZZbdxQisBEgRDMjDt8QwkW8n/liK3sCgZ1fmuM8Ndg3Qg\nkg9KwLZ2W3nHoDQuAbWkpFHbwlgJA5ccq1Ez/pVRAYiXJkGmdCew5vGlynSy6k323eUvMaM+LFQw\nKdgskpnOZTuJ4UTin/zchhvTHPP83qU4gv+fvTePkuOqz4afqupt9pFGmy3JkmakkWzJtizLsg1f\nDi/ghCUbJzEfBCff4ZDEJiQkYB/W8AEJL7xsNoQ1FoEQloADJHkJ3xsIgiRAHE4kKybYREtPz4w0\nI2n2pdfa7v3+uPW7dau6epmeHs1o1M85Opqu5dat6u7bz/3d5/f8hFJqnHNOD8EGEPC0XNdEvZkR\ndbW6XXd3N3bu3NlUf87lEnXOOSYnJ5HJZNDb24s9e/Z4xYsqf2gdm8mBUfMKUlBbgCCmruvK5cRY\nTAdnIuGUZCwufM0gRdFpcCVbLYIeExF4PaYFNPDqmJ+IKxXwvMkAY0AsHCH3IunkUCP6W06WyTGG\nh6/hkV/b5rJAhhqxpwRX0xU/HuTBTs9F9sPy7oOpVVRD2mov0p5MCItJ1UGnZAYrtartk10kHRs1\nHyTCH5bV2I5PwuMxBJ6xSFoN+sbbjvgxI9lLmFznS3okWV7Il283bR1tSSZ940uW0L2ry8p5JSri\nuEEyXzB960r1+h2p+uQv5Ne/mPevQRH5MBzHKUs+amnU1w40TcPWrVsBAJOTkyuyQtpIXQwgmqCv\nJDlfSp0N13Vx4cIFjI+Pl1lVVrNnfOUjF+QKqQsX3Evqj3kRC9d1oZN0w3N7cWxHRsk5AI3rMgpP\n0hPGOZjrB1oYUVYd0j3GhVBMauEwLt0/EwfQ70ssrkknF8PQZPKoL2vxSH4FJVzYPUt9TcXtaPzV\ntFASLecyiZR+S+CReWpL07iIXGtc6tmNUFJp+F2QCa2KIxjgS3IY1wLBF0MTunjX9dvn0KTtsCoX\nUnOCmBpx1/3IPUH9W1Sh9iZAkszr0o5SrbVRcmJSpqhW4y5YYiK2UEwGtrV78saCbUSSdbWuCCB+\nuyiiXjB1bO2xJUkHhDFAe7uwgxTcZWnf6VrVpSvtr3ZODTwN4A80TbsDwI8AzALYBuCn6kHrkqjT\nm9MMe0bHcTA6OorLly9jx44duPfeezE7O4u5ublmdFWiUekL5xxTU1MYGhpCT08P7rjjDqRSKVy5\ncqVqxb1HPikmbDHdH8WIuIuS0EHSLZI/fdJOI0xMF8UvAEjbLpmp70ljiLA7jtAmhqO8NDhzxmGa\nrvxbtXx0bP9+SX5DxJz07+RMI2RPfhENOp5x4bduWkGy7Ej5D6SFmGmKG4xHROMBsoYUf9P/phUc\n9IXMxr9PcqER8pny96Rkcukuk0xQxNu/JklnVGkLRduLpucJHDEvsx3RZiJUrMmkqL50zPHvJ1/S\nJJEXFWR9Ep+Mew5BDkXjtYAGHhByEyLftiPsJ1MJ/wfHdrSABt6X3FSO9BEKpo72JMPsoo6eztrH\nU3/yBS7ScxQ4jhPI2WhJX9YuViLxk9pdyo+6aZoYHh7G7OzsVSHogK+lr3WdsKtXVDXWSkT9/37D\niJywMMYAVwSQiIxLqBWmFcmLY7uCnHs/DlzztOU64Np+ET3GOQwlEVM4yghiz7jvCEPtapqwadQ0\nETE3lKAS/T6oBJ0mB7SNVjajIHXiXiVoXfPHVCZ9zv3fByLmJLWhCLscO7kyxgaIOxUu8u+btmka\nZMRdFkziQYmPuppJkXLONTiKtEYQf03up/vyzhLtMC3gSCb6r0nSbehc3i9pwB2ZJOwVhFICLpwL\nUu9yUaODxnEyBogbTBoK0MqFar5QsPw3pmAb8nySvhRM8TtStA3kS95EU/MDT3P5IIVVx/NGgi61\nqktX2h+1rR5wzic0TfsRgHdA2DOaAKYAfFs9bl0SdcJyEoVs28aFCxdw5coVSdBpwFspS6+lONRw\nzjEzM4OhoSF0dHTg8OHDgQqstSL0NMDapsjMp8Qezjgsl5WRbkDo2ckaS/esExnzD9A9D11a5jRN\nB4YuNHW27ULTNOl7C/gRD4rQE2IxX4cMECEPevCqy5XC6SW4GuA4QnMvkz4tJqPyjNOkweuH7pF1\nl4Mz8ub1yKkl/Galht6LsLgWl5HzREKHaTIkk0HrMIq2qzaVJF9xHNFH3dBkJF4FkXr1unRfdH4q\n6f9YEIreRyhmlFuXWba4PhF2P6ov/g/3w7TFdRLxoLSpZJVbT5o20J70ZTcARdKDr6H8QKuyGDXR\n1LI1OXkomhrakvV/h2eyS0skjSqQ0SLqaxMrmXOkWnRWgkrQd+/ejf3791clzs2sNK3retXJRDVX\nr6i2ooi6E3q2dE31N1TTNDhMfMn1mA5d0+VvCUljdJ2KGfmWjJx5dovwPNW9lViKxEuJjKF7kXGK\nhuveaicRSfEbEov51sCGIcYZIum6ElkX9wGl/8oz42QWqQW3BaLiXnSdkkTDUXTvfCKPqjxG3C0U\nAX7ws6BehyLvgEhMFTv8/dR3IaXRAE4SIC414fRarAJocqKhIqaQcEOJjhPIklcUvdPAdAaH6VKm\nQ8WfxP+IrAhdtI2y6tWVUDB1JCJWaku2HtCpE0kHgqvDv3x70CxEHc8b8VCvVV260v6obfWCc/4d\nAN/RNG1AvOSZ8DHrkqgvZ3C0bRujo6OYmJioWH56NctOAyJZIZ1Oo62tDbfeeqtc6lFBg2wU/vAj\neRF59gpRxHRhmwX4kW3HG3T0uBEoTgEAthWsNkfE2rJdb2Dj8hhN43BtxQJKkaeQrIb0h9IZhkNG\n9GNxZQnOYjI6rzrUxOK6lND4ER6R5JpI6NKRxraYKCyU0APymnhCSHroh5V07fRbxrnn764JLXoi\nrsOymdSekybfshjinryHBhPye9c0ce/JpC63E0qmmFQkExpMkweIvZDGVJK3cM9NJvh5dxwu+x4z\nfN2j9FO3hH4//JtfKPkk3nH8H6Wi6Se/WspvOZXhJizkyicdwr8+eG5CqfZqO+WkX2z3jylZGmwb\nKJZ0eS/5YrndI12vGj73L514zf/wB/druEDGdQPVHKBYLDa9fYqoV8JSCTrgj+fNyF9S2wtPJjjn\nuHz5MkZGRrB58+a66nZEEfWXPXRW7IvpYN6zUP/XNF2O+bruObvYLrjhjbeur2mn3xLd4NKKERBj\nczg7UUwCxLZYTPdWdHUZDCKSTsScIup+gmiQpGta0IoxcN+aP6ZFQU38VAMTQDC6HhX7kz9rkmOX\nB1FEkSHlHCa20jZKUFUNCgA/Os6YiKDroYi6egxFr0WCbDDZ1WUaHNfwK2h73up0Dtn1qlF/0xHj\nbHgVnPpP9ryMaTAdP9pONTssz92Fti8WdSRiHHGDBxzCAKBQ0uUqa97UMb1ooC3JAySdnmd7kmGx\nEKGbDxWva2Qsr1VdOmp/MypNc86HKu1bl0Q9jHoiG7ZtY2RkBJOTkxUJOmElvXerYW5uDul0GolE\nAgcPHkRnZ2fFYytVs/uDR3OBRB/OuSTFgHB2EVn6XuKnrWaKQyb8yOMZl0Qe8OQvyn1Q+2IC4MlJ\nEoasNgcALmNgjhjUDehwGJNevI7t/w0IYp0Ine/YTN6PYQRlM6bJEI/rgQkCWUaKwkzlRZ6EzIVk\nKBzJpFghoAiH44jIu4Nyf3iyj5SDb9jK0PF18wnPclImwCrSmFRSC5B5zjgsO9iWrOiqWFYmk6Hr\neZKdeDx8LvfkMOKHwvF0/JYtJkLxULGqolmetOo4ADyy7si+CAKvft2sUBCUqr5qmojmtwm3LZQs\nBO4xSh4UxnxWHN/RVr5veo6hu7N6ZCdM1HO5XNXvVQtXHypRX1xcbHr7lUwHTNPEyMgIZmZm6ibo\nhGYT9TC55pxjYmICmUwGGzdurGoaEIZw8/LH6Jc9dBaMM+iaDuaU/2aQCwsFQVzvCyysGhk0L4HU\nha9f12O6JOGO7cCIGZ6NIw+MiZQQShF5IunxmCFIuaEhkdDl2E4Rc3L98i0Y/fb8toPRdMb9MUwl\n7GHyLosOKe340hYlsEDHaf6Yr47Z4Y+KoQW14RK0ChxKBmVyV/CehP48NOFh0VKZSOtNr4+GLmQr\ndP8lW0fM8C2A6VnRaqfjJdlS1J32xaoPsWJMj/tk3bR1xA0XhZIujQfITUw1bwivpJKNMQC85FC0\nrHe9Vple90TdMIwyHaoKy7IwOjqKycnJsqSbSmiG9r1SP6MwPz+PdDoNwzBw4MABdHV1RR4Xbi+K\n+FOmfVgSpBaRYK5M9REaQ4+4E1xPJmIYQv4S86LwoiERUaEoi2P7yUJ+IYzyfkVJaWSxDYOi0FSK\nmsFx/EqpnAvirBmCyFNknrTpjsMkGdZ1DXpIHUGBHtvh4Mx3tSHXGcdhMqHVsTlcxjy3GpEtT9F4\n9XrxuC4KKHlVWl1XJKiSFSTgXY/zMmkM4Ceghqu70j5Ni95n2eIHNR4XP2q2FxlnHpkn7btli0HP\nIveb0EhQMrk8liZBpOWniJZYLYAk9vQcaaJhWn6xJ/q9IElPSplQFEt+noCul9/TYo6jLTQByRd8\nEp7NMwA6OtqAfJGjo01DoeQ/52KJY37RRXdnuSwmLHngnC85AamFlcXVsNtVx16y3iWCPjg4uORV\n2pWqi8E5x/T0NNLpNHp6enDkyBGkUqkltRWWhLquKzzRdZIC+vdqGIZn1ehFzpVEUtdxPXJtKOO/\n+I98013Xl7iIa2tS2kLuMCJZVZcyl3jckMn9sZguCbpacTQcNJDta0G3F7EPcp8Kzr3fN02NfCua\nbx7eFyTt4Y9EONIengA4XCubHOhaMIKt9pFz1R6S+uQRcJLiKEWZoiL9kRMDD6YTtNeNeZIWcrgx\nHb//ti3klETmXe7neFmuBtvx3b5EzpL4u2iKRNOSpYEpEpq8KVzGomqHOK4mVwxoZZUx4FX3ZssP\nDt8vY4HidetldXRdEnV1UKXBPUzUyVd2amoKu3btqougE6qR6kYRNbAvLCwgnU5D0zQMDg6iu7t2\naV1C1PLmQ/9rPpAFrsd0JcO/PJJC+kG5lKn7WepCq+hFgpUEUMdmMqqtDvjUBgMQi3sRanFhUVwj\n8OPg/RjQEp0S5YnFNEnSORfad3k/hhEorGRbwWg83YNtuVIy41tJKkmq3n1ZFpOTEUAUUApX7WRK\npN6yRGSfklFjcc1zxqGogZhQqARb6M0FoecMQucufXJ9xxpNA5LJoK+9HdKxA5DvZ6HIZORJRS7P\nRBTK0EK68PIItmnxgJuNeC/ESoJhBEm17fDA68WcT/TVZF1CvsDR2aHJ1QBK9iqZXJJ49W9C1DZA\nkPVsvnwFIwoLCwvo6OiQBK1Zdl61ykgfP34cADA0NIQPfOADTbnm9YSVJuoqQd+1axf27dvX8KSt\nnhXSpcAwDMzOzuJnP/tZZE7SUqD+Nvziq1VzCd/mljMGTdfheuESnemex7lHwmGAcwYNRqCyL/0t\n9eohRk12ikbc8Il5MhZJ0GXUnCwXyU1MC0pd1MTR4H2W33uU9CWsSw9o1r2xO7i/SpS97IJ+xF09\nnxAyKov4FfavQecbepDci+0UQfeTLcP3qiZhEhzXP8Z0dD+RlPv+8pz7EW9dF3/rmiD14aJ45Oal\nEnBK1FWh5i9ZtoZ8SeQiqZJIWnl1HOA3n1ubpIexnmSM65KoEygKExUtmZ6eXjJBJ6xEhr9K1LPZ\nLNLpNBhj2Lt3L3p6epbVHgD87v+cFf0mFxeRrh0gzOI83XddAYcS4IauG2CMR0qJAu4xHpGmUYik\nLwTLcv1EUAQjOLINj4BTf4CgzEZU+/SjsJSwCkAmxrLQqOgyJiPLAC9bVfBLWIvXcalvF4SXuUGi\nTiQ/kdARVhlxzmFbRKS9/khdPBFwIatRPdjJKz7Ke71QdCWZdwMTBD+aEqVXB1AWzae/jYRyP7aY\nNFEblu1NOqxgxT46NhEX5JwqwQLCzYakLqbFPUtKyPsXkw3xOpePCAHBf1+LJS7lPUUzOmKkolh0\n0dlZeUhbzImo+pUrV5DL5eC6LlzXRWdnJ8bHx8usGpeCWqWnT5w4gfvuuw/9/f14+ctfLl+3UBvS\nvnSFXF8AYHx8HCMjI8sm6IRmBnPm5uYwNTWFQqGAQ4cOLdtLnoj6i3/raWWbHy3nctAXK1WcMZ8p\nOL6OnX4nDIWVqWO51LQbBoyYLgsfGXEDiYSBeDKGRNxALO5r0GMxPag9130SLFYxvba1cpmL+je9\nfdV+qitr1VVZjPd7CV6WMKq2U2ls0irIXVStO1lB0rXD/QtvC0giI/qhkn9fxiLsJCP19YpWnCmR\ncrU+BhWwI4MCyxHvj7D69ScRjGnQdY5ckbTynqzGEsekEkLiSL+XjiuOJ5kLXdNx64+iV8L8/Dz2\n79/f8PlrCeuSqJPuDfCjMKresFmDcTMRi8VQKpXw9NNPw7Zt7N27d1mzwSiNOuccjuMzb5K1qHBd\n5tsralqQJNsqa/eXpDVNRN1V4utHxb0vnu0iFhejrCiA4ScG8hCRVyudUn8MbxkVnq+5sEpUIvYe\nyY7FRBItDVqaJm6Hor2q44xavIk84uWz4VxG5GMx3fNt12GabiD6DnhJna4f3S4WXUHeud83KuBE\nmnkCkX1KUqV2SrbvWBOL6WVknjNfd052YgBQNBlYqFiUZTO4ruZp2PVAVCVfYDLBlVYQiiVfs092\nlSRZUXMUBAH3E6Rsm8O2/X45Dg9UnQXEqgQVJfGJPIfucRrNI/5hT3wVJAsCgNl5t0x/DwBT0w46\nOqJJNw3enHP893//Nzo6OnDq1Cl8+ctfxvDwMI4ePYr9+/fj1a9+tSwNXQu1Sk9nMhlkMhk8+OCD\n6O/vRyZTltjfQgWEx/JmgVZVL126hJ6eHtx5551N+01ohvSFVlR1XcemTZuwdevWphR80nUdf/Du\nrOLWQqt4bmBVjLuA5h2jrpCqVUsBSEtHLeQyIvTmgqTHYgYSqTiSqRjiCQOpthgMXUMsLiLsNEbI\nZFDdTxIVfS7XofvJlsH/o0h6RCwoQMgj8lwratfLaj1FnFsNKnlXnWXKOxhxcoVhUdd4gHCHEda/\nR/UpWjoDaVnpS0S9qt4hPb5pi0kVFV7i3JNHKqYMRVPsa0/5CbCA2O66wAPPaZyYM08aS1hYWGhp\n1K8VaJqGkZERmKaJ3bt3rzmCDgD5fB7pdBr5fB779+8PlKNtFKrry2vePV223yA9tSJrIcjqoOCS\nGHPGJWHWdC0y6UgdvElK4xdT0gPX8osZeZFdMsrStUByK+BpwZnrJ/l4fzB40R0vOqxG3DnjiCcM\nMJfLNtVJByCWUi3L9SQmbmBQVjX7ZC1JGlFD9yuzuq6Q1sTiuuc1T1Fz36bRl84wbx8D5+TFrka5\nWcBqks4VfdOkvaRMhvUSXkk/r040bJuDcxaYUACCtAc8gDVR4VW8L4FDYVoMSYXwu64oSpJI+NF5\ndVVDvTY9S0rMpeXrMEzLPzZqZSWXc9EZ0pbT81zIMnk9Iuu5nIOc4tg1Nesgl7XR2VWeo0ITzb6+\nPvz6r/86Dh8+jPe+97348pe/jHPnzi1J+1urjLTqCnD69Gm84hWvqLvtFgSatZKpyh53796NW265\nBfPz8039XVhO9D9qRXVoaKhpUppXvX5U/s0ZB2dK7QqPuAPB77QwAtDBdCYtGQGRF0QuMIDvux5P\nxGHEdCTbEujsTqKjM4F4zI+Yq5IWIChrAaKj4iRxIVQi6Oo2tc3wdqCcrNPxRFp995fgiWEph3pu\nuP1KCDvLNAoRZS//boQTZqvBDf2cq/erRuIZghOVcLssEOxCYGKRjNNKiL/xVw8H7RWXg7AxwNzc\nXEv6stZBlloTExPo6+vDkSNHmipZoeXD5QzuhUIBQ0NDKBQKGBgYQC6XawpJV/v36v93IrCdBmGX\nCK0XEZfqRF0LOKqIg72EUM4DEXe/TW+QVU6jNiiR1FE0NPSnGs13HNeTy8A/jxMh9a+hRuNpUCSN\nvKysStp5KyiFoT6pHvGVquCJpNRgxVbXFTIYlzGA+cSBec42uqEF/OIZ07yJgk/8HVvYQaqkPhbX\nYVuunFhYjEsyKiPznINb/naVgLuMwzXJsz34eaREWHg8lTPA8dpPhEm48r76DjQiQh+P63LQLxRd\nJOLlEwB1W5jky3YdLuyDUJ44ms+7MgoejsQDYkKRSukoFl20tZVHyy2LBe4pCvMLQfKkJpqTpjGR\nSODQoUNV22kUJIlZqtfu9YyVIOiq7HFubm5V7XYJFLCxLKtsRbVaNdGl4OdfedIn2eHouUbSFyZ/\nJ8RrOh7yPCFJ0aWbi2EY0GM6du/bio7OWMBGUSXkJGUJR8PViLnYHiTu/vbqWvSyfTU+OvV8tKKS\nR8PEXezn5dF2pQ9hNxm17eWgWiQ8/FNO/REadtWO0g/eRLVV63pR22KGmtxbfkAzSTogjAFUor64\nuNgi6msdjuOgp6cH3d3dME2z6bpysvRKJpO1Dw6hWCxiaGgIuVwOAwMD2LRpEzRNw7lz55rWPxkV\n976plBjqOq4s9Qz4xJ3KO6uadLLNAiCdXMQg7hepCJzrnRcg4Lbvua6H3gMG5ke3Yz4xF7IUN7Dc\nSkSXKVFzh3HAcn1XGJeDh6LxsbiI5AcKJFE/40agf9QXVRcv7sFPSiV3GhEJ4YE+xniQJJqm+PFP\nJIxA9dZSyUE8bsj7tUquqKzq3UvAStIRhZViisWkSED1o/PxhPDGFV7ynm5ekcWIvrBycm0J95xk\n0ie9xaI/0VBhexMRea4Xmdc0L4KuR5N10tqblLQrl0E5Uilx3XzeRVen/7fvcU9Rchem6aK9vfJw\ntbDQmCSiGb67QO3S04QTJ060EkmXiaUWElKdvaLykqpFv//i+0l5TZWMqDUWAD95D4DncLUPAOCe\n8uxYGYdjc+RyFt7328HPMf0e5PN57N27Fxs3biy7v+VKaRhj+IVXnpKvKUIOhAh7yASAcwbyUNd0\nDf/Xiw/J770e0o/7lUSjo9hRkpRKhFw9txoxD19DHhNB8OtFlAwGiNaPB/erq9L+wWGiG0XmqyFM\n9OuFlJaGJhWqqpArmvSoc1VUm1So75Oa2BtF0O/acg6dnZ1w3Y6Gc4KisJ6rTK9bot7V1YWOjg5M\nT08jl2vuzA3wB/elEPVSqYRMJoOFhQUMDAzg4MGDK5KYSvjk3++SWfrQfcmAIJr07Y+2SvT+8CrL\nCXDlPDckfSEpjZBCMFmUSL2/SpF6ADWlNKo+nrn+ZCAc5dflj4XvES+184rNpCC13kmeNEN1wOGk\nyfbuS1gs+sm3KqknUARfN4TejpYeLY+A+4mwmiKdoYRUw4t2e5MeTwdP+4nMc4YAaRfPhkliTRr3\ngu1A1zVpEwkIIk3RcQCB7ZwFdfeU1Er6dXGd4KDrgJ6Bf+2SJ9PRdHG8Y3OkUro8NzwBKBTEM8sX\n3LLvQj5vo6MjrhzroFAQk5O2NgOLi3YkeZ+bM6FpQEdHeSGwsQuLAHyXF3VVbDm+u7VKTwPC9YXc\nYFrJpPUjysWrHqcetTZGNeMAGss//69Eyv19vu92KMigNEPji2rxJ49TEu0AYENvDJ/+Tlg+kABw\nBJxx/EdIpeiTrQHxx9PKvpDsjMZcdb+ukOdf/n+eK44Pk9iA7BFlqCQdCT/KqHPD3+lqMpSK1quW\n5AAAIABJREFU14+4TpSjSRRW6ue1kkWjv7/yhcNRbHVbVBssJEyv1nYl1IrcSy16BYJORDzKNSe8\nOiL+Lm/oJbfMIpfLIZ93MT4+jnw+D8YY2tvb0dHRgc7OTnR2diKVSjXEi6KqTDdixLEWsW6Juuq9\nuxJOAUvRIJqmiUwmg7m5OfT39+Pmm29eUYIOAA+8eRyAT4DV5U5NJ+/x4JdJN1SyDXDXkZF4AgcC\n0heNqtQpRNtVojKcByuTAggQeCl58Y4lZwDmpa9zmwdKSpOWkikaeCPuz8rDk4GY50hAEwhHIdjq\nMyHLRtGIElWiyDVHgMzTtV2wMpcZprirxGIiQZRcElyXgTNNRqdVMh+PGUJWA/H5NU1HPnuq3so4\nh2W5Ic92Q0lYNVAs2kgmY2CMo1hwwLjYTu+raYo3kKq+0j05jmdHSTkALkex4EqyribsqjBNF6k2\nQ8haRIuBc0olyL8dB0gmDVgWg2X5GvpCwQmQ8kqgPszMmGX9AMSKACGfdzE9mUdnd+XJtBqdXU4E\nplbp6RMnTuAtb3kLPvCBD2B2dhZf+9rXGrrO9Q5ayaxG1Osl6ADwxR8lASQB3CsdRSIjjCGnjeB8\ns7JsgHzFqxEhcS4HIkq+VwMP+XIDYZ/uyu1FES5Crd+mRuUklbbXEyEPI6oq8VLOr7W/UlQ9jFpR\n9uhzyg+sRr6XQsyXKqVRP4+VPpuBFZIKE60oYq5CyFwS2LhxY0DeyzlHoVBAPp9HNpvF5cuXUSqV\nYBhGgLx3dHTUrLgbJupRlXyvVaxbok64WkUyoqD68u7ZswcHDhyoOQgudVk3Cq985IKfAAqf3Mrl\nU9W8RSkNzUIklwh9VIEkuWQJJor6KElIgIi+h4P1UioDSAmNsPfyz2OuINNqZBxQrBZdHkhsBQDX\ndgN+6XIJVtdgW47Stj8ZCRB6N+gKwxiHzv2+kgTIdYOTG4oOk92kHtNlImwsLiYCVskJvBcUoWcu\nl/7xROZN1z+WiDyRa8d25LHCvYdJzTwLRdf9+w1upyg9oVR0oGsa4gkRoWcuh2uI9mKKhCWbtQPy\nGHK+SaVisG0GTQNKRTcQmctlbXkt03TLzq8EOYlQ+lkoOPJ5W5aLRMLwttsoFGy0t8dRKjlIpYLD\n2eKiWfE6UVhYWMCBAweWdI6KaqWn77vvPszNzTXcdguoGXhRCXql4nV//WRw0hZJxiIIdXiFvhIh\nqiQj10P/l7e1soGbpSBI9pd2LiW315u6Vav9ZmjMGzm2HpIeRvgzsZKxuGZo24H67zN69aN2J2rp\n0DVNQ0dHBzo6OrBlyxa53XEc5PN55HI5TExMIJ/PSwUDEffOzk60t7fL73iYqK90MPRqYt0S9ZWy\n9CJUI+q2bWN4eFi6CtTrNNOMstOveOMoAC+bP0T6IzWORvR2yuzXdE3YHulBq0b6xfElGH5p6UA7\n6kjgPQPuigqghi7u07GDsgc5cWDC1USNyFOSKVUopQmEXA3g3Ld7VGwmCWFdvP+slFUDxqHroSi9\nG7w2ECzEBEMHU/opZTRe5VZ5fYdRLqVwinEYLMeftMSTMdl2LKYHqrhqXNxzImGgVLSh6RoSCUNq\n4QEg6Z1v267nee9F9T2JkOMwJJQVCNtx4TIRlQcUr3ebyeQxQBSPEs/Zv5dSyQnkB8D1vemp4FQ8\n4ZN1mtgQcSdHGzonn7cVhxxxfD5vo1R0AvrXMCav5NDdm5J9isL0ZBGdXcEobDgZfD0VyFhPiLLb\nVVGLoH/l35NKW/551YioETquvFPBl0TslyK5VeUTUX7YaxU1o92hZ0ArCY2Q33qvudTjorCc/kWh\n0nu4lATW1UDlFZHanaL3ermJorFYDD09PQHpCuccpmkil8shl8thZmYGhUIBANDR0QHLstDV1YV8\nPr/kar2VUKuIXdR+Wjk9fvx4ZPCmEax7ok5Lpc1GFFG3bRujo6OYmJhoqJgSVbNrlKjf/4cZeT1y\nCNA9nTlJgYDgTNNhTiACTWDccz3xCKwBI6Ajd103EI1X7RjVtgIRJha0gnRtR+ooOfzEVjUaz1y/\nv750RIlqx/SArIVkNWo/iNCTq4wqa6FIPclXaHJCmnhVWkMe9KrlpLhHDsZceQ0R5YZ81r52PXiN\nKF2+6RFNcoKhY9X/TYWMUgSdVhRM0/E08v4zMk0n5PvuCq16wpcFEdmPe1Ih20s6dt1gFN5xmOev\nL15bVnByFvaYp+JXquxF9Mkn6IxxGSVXzwsnGtHH1jRdOLYr+6+iVLCRai9f7ky1xTA2ModEKo6H\n3ruAx/+4J9LOa7347q5XqERdHW/DBP2JH/vkvBIBq8dDpd6ocrg+WV1ES6vwd9RrBC0Aw3aA9b5W\noT6X5RLpatA0fyWhWUHOtUTIl4pGViqWi0auVw8xV0HPtdluLgRN05BKpZBKpbBp0ya5nTGGfD6P\nTCaDYrGIxx9/HF/4wheQz+fxhje8Abfddhue97znYWBgYEnXq1XErtL+48eP4+tf/zoef/zxJty1\nwLol6oSV8kxXibrjOBgdHcWVK1ewc+fOhqqdAn41u0acZH79D4YAAK6iN6FoeBjhvkkiXyWt3GER\nkUrDJ+zhcyWxpBLMAbtF/1gpbWEcupeEKpuPGcKdgCYf5MOuEDiKxqu6+LCsBQjaPVKCLMlaxGH+\nOWQzSeTX9a7LOA8kj6pkHvAj9LJfHpmnaq6Be4MOh2Q2gWQuTfq+M1eQ3JiugXGURezjSd9NhqQq\nZsmRf8fihnwWtuUGJhe0LSyHKZZs4W6jMA+1MBRBfU2e9erxVCqc+lcPcaFj1T6Wio68hm27SCjy\nFrK0BIB81kI+a/ltlRyUijZSbZU1imGi3oqor12oOUelUgnpdBoTExNl4+3X/iMR0G6rCPtK1yJs\nrEFCtVJETJ0MhCcGS31dre2VQFDP3Ji2ux6sNgkHmnM/q6HaWCoxB4ITweftuYTu7u4m96qOPug6\nurq6kEwmceONN+Lhhx/Gy1/+cjz88MP41V/9Vfz0pz9FJpNZMlGvVcSu0v7PfOYzuP/++5t3g1jH\nRH2l9UmxWAzFYhHDw8O4dOkSduzYgXvuuWdZdkONWnD92u+fDxBl8rilbZxTgqIuybtqw6XrPolV\noZJ3NRrvE2LRVxdBfbKElJ7ogQmE2k/VjYaiw7J9Rwmrw4/EO15dedWdBvAj8eQDHLaXpIg32UuK\ne2Cyr1LGAV82RH/rih+8pmvSFUYllUyJ0MfiRsA/niLz9BzV6HxYZsMs0q4Loq1OPAzF7cbyIuua\npsE2/fujCL4oxqSsbnje8JxxxDxrSgAo2Uxq7AkqCbZNB46tycqyov/BCaBtuaLiqK4FovzhflO7\njsNgK5IdKyRZYVxMOpjLqpLt8HlLQYuoX1twHAczMzOYnZ3F3r17JUH/2n8EJU1hwkGytjBZjII6\nDNZL+sLxjatBssKTgajX1JewQ8lqRHTDWLL2fQ0QcMJqP7vlohFCHob6fvzy7Tk8++yzMIzdy253\nOQhb7W7btg3Pf/7z8fznP7+h9moVsau0P5PJ4MSJExXlMo1g3RL1MJqRpElwXRczMzOYmJjAnj17\nlk3QCY0Q9Zc9dLaMJHPGwcNViQBAZ7KwBefib90wIrXlKolXo/JRUhrd0ANlpcPyF9fri++n7ktA\n6H7V5ycnBwi1EyorL4Lt5dp4aVcWSpyV0hrG4SjkNzKqLhNpuSdl8SupxjxiTdaQYZ08Z1xMDBSH\nGcY5mOVIR5tY3JDPTI3Mq2S5zM4SgG05kjBT3+g4ITsJrjYkEkZA5x5PkDbcT9CUx5ku4krSJ0Wz\ndc2/J9flSKb83AK6P6CcNLuuiPrThIWqw1LyLR0Ttmy0LDcwKSgVbfk8EqkY8llT9kGV3uQWi9B1\nrUz6MjOVx+JsDm0dQd1iuEDGerLzWm8oFot46qmn0NfXhxtuuAE33XST3FdN3gFUJyblFScrHVe5\nb80gkZX6HnaKodfhflZ7XevYZmMtkeooXOtEu140g5CHEX5vf/l2IXNZbm5dM6CO58ux2l0uiJx/\n97vfbZoN77ol6iopJ5nKcq16GGMYGxvDxYsXsWHDBmzevBl79uxZblclllp2+mUPnRUFblyf/FaC\n6jzCFeLMIiYGajS+mpRGOLYYksRyl8sEvbCUhnMOjXnuKQp7JnKqTlAo2k8a+LCWW7WYVF1OAo4u\nSqEnoJzQGwqhFdaJukxy1Q1DWhQCkKSeoupqJJg08rpCuAGUkXn6W4/5UWo6znWZTHiV5BdigkTy\nGzVSr16fx/TApIlcbnRDB3MZSkW/77quwfQSQOk4x9P0Cx27Icl2WGeuvjZLjozUU0JopR9nx2aB\nVRnNE9+q7i30GkCZVt1vR+zPLpRglQRxTwZkMP4zKRVsLM7lAQCptrhMsC1rMxRRZ4yt+o9NC9Fo\nb2/HPffcg3w+j4sXLwb2/fpdvuTpGydFdL2WHSKhmn918Lil9XepyYDVyG1431onwlcb1wvxBlaG\nfNeDSgSd4DhOU4sXNYJGitcdP368bFt/fz/uu+++mkXsovYfP34cGzduxP3334++vj5kMpnl3haA\ndUzUVVACUqNEnTGG8fFxXLhwAVu3bsXdd98tdZLNxFIi6r/8Oz8LkO9w1FpFlIa8EgQBjk6zUstM\nE6Ki8ULOEo5yC3mNWjSJVjkqFTsKW0kSeaftuqaDceYTeteX9Ehpj+JOE0gwVfrNOYcBKNFzT5Pu\nTUQIckKiSGuIQNJ8JkzmxfvhRdx1DZYVlO0QwaQnIB1lSDKjkHvAJ/hqoSiaCFBFVVUuo1pYuoB8\nTvKeHCaTZa2SLSP6jh2Mastk04TvSqPC8RJCXZfDcdxAZN91md9f189DsCzXs5r075sIu205VT+z\nZtGCWbTAGEdbRzCno5ArVTwv0OcmTN5buDoQSdp6TRcvlbQTwuRdRdRHrB4yVMvbuhna9ijJylpD\nPY411xORXgrChY9Wi4QvBS85OC9+s5kWyHVzXXfF8gGXAupDvTLGaq4stYrYVdrf398PABgaGsJD\nDz3U+M0oWPdEnRKQGnF+YYzh8uXLGBkZwZYtW3Ds2DH5w04OLc1EvUT9l17zDABhgafpulcOmlD+\nZWF6ORFWyTuVna9E6PWYHhmND4NIfJi8RyW10heqkj97VBRdFBjynVDovoi4S2kNK5fWqA41nHHf\nFce7N5K9kLzGr8DqE2barusaOGMy6i6TXgFZsEj0WQeYH7EnBxvxPLhMFiXNvKvYNNI9R5F7sqYM\nk3HSvctreBMXksQwzmHA95YnMm0WLWial/jqBDXxrssChF2NXFM/Y3FDRu9lToDXF3ofXdvPY9Cl\nVMaWdpRWyUYiFYdtOXIyQCgVBPlKtSdQzJneM9cDybnZ+TwSqfIiONNXFmCVLMST5YTccRxp4xWV\no9HC2gGNF42M5dXIOxAdoW4kEq+ikeqRS5GzrCVcK/1cCTSDXK9Vgh7+Xtw3OAUgLn87A6vg0sGL\nrQnCPj8/j507dy6rjVpF7Crtp6j6wMBAIPl0OVi3RL2W9241cM4lQe/r68Ndd91VVglvJSqe1iN9\n+cVX/zTYVxZM5OSRFTfKvzhREXgWEUnXNb0iaS4/uILhGdMDCa2AkNSE24nSxetKomigD4iOxKv3\nprZLEiH1vgCFSCp2k2pUHgj6rUP3k17JyUYl86plYzgJFgjq5kkzz1wmveINPRZwp1HJvVr9VX3f\nKKrvb+OAC5k0CgCWF6FmioQlPAGg14ahC89123+PDEOH6UlO4smYZ9EozjNLtjwnnBpBkh7XZYjp\nQq/uMldWpA0mlNryXuj/ZMon2HT9MPKLRfm3bdooFUxwxtHe3V52rKW0oS6VZrNZdHV1RbbfwtrB\nUuWBlRAm7+l0Gj+Zu0W+boS8q6iXfDVC6FtoDtYqQV4tVJJU3X1DBtlsFufO5WCaJhKJRKBa6OXL\nl9HW1uabS7hKwMj7t9LkPRxomZ+fx6233rrsdqsVsau0v1ne6SrWLVFXUa+XOuccExMTyGQy2Lhx\nI+68886KVom6rq9IRN2yyqM/gCAVv/Lbzwa2aUpSZnWEibYfhaeofBQkwQ2BItYkO6H/VScZFYHE\nVoXMc1XWEtLFcy604kz209OtK+S20m2qyZ0BnbwqfYnQyQfIvFtO5tV+EOFULShJdhK+FmMAd32H\nFv/+Fa27Z+PITPE5NQJJoY4k07phiAg9L3/PZCGpeExG6ykCLiU10uJSfd7BpF1HiX5zxmG7DlzV\n1tF0BFln4hyrZCMWN2SknGQzzGUBSY9juzIZVvighyPnJmLxGGzT9/anaHoYpYKIrEdF0dV7UZ8j\nYXp8CkBPQ5rGFlYXK+XmFYvFcM+Nw9i+fXtg++LiIr57Vng2L5e8R6FemY163Hok95oWLDoXfh0+\ntoXmIuqz/bIjFAC5ATfccIPcblkWFhcXMTY2hrm5OcTjccTjcaTTaVlltL29HYlEIjL6Tjls9Hcz\nEI7kr7fxfF0TddV7txpR55xjamoKQ0ND6OnpwZEjR2pWtlqJH4wo6YvruvjFV/9UaKdDnttRWnJy\ndKG/gSgiHzwvOgoPREXiibxL+YlC1kWHKye2hiPo1Iaa1BqOaAfukZXbPEY53hChF3poL9lWGRxU\nki/18owjHN2PIvN0j/IJsaBOT25XovOO60DXdDEZcKOj4q7LpAuMbuhwbKec7Kte7BF2l4CXNOsV\nkgJ8ohq2tKSCRgT6bNFEIBz1d+l8w9ezq0msZtFCLB6T/XVsv33btCsSaqskiHit/YSo4/ILebiO\nW7avsFhAfiGLeDKBeDKOYr4UcH5R81bW28C+3nA17HbVSH0ul0M6nYbjOHjhPlbmIEHSmZUg72GE\niel6JarXy32uFVRLSvZJejkKhQKGhobQ19eHW2+9FYYXUMvn88jlcpifn8fY2FhZ9L2jowNtbW3e\n71iQwC83+h7ON1pvVrvrmqgT4vE4isXyDx7nHNPT0xgaGkJXVxcOHz6Mtra2VeihgPpjQQ4zv/u2\naQDCxETTg7rcKOhqxLpCUihYfUQ6unYfkX8mo/FhqUmUr3pY/65G4aPIe9hiUk1ylZIUIpV0DE1S\nIgg9i7gX1T+eh7TpUWSeIuuqH7sb6iP1z4VKdsWERi1YVCkfgCL14fZoYDPiRtlqAjm8kAuLCsd2\nAsm7rpLM64buUVRYdeSxAMBDhZtc1w6uCnCOmBfBt1wLsbgBQ4+hlCsh0ZaA600IVG07RfXVbUTI\nHduBbdlyshf+HFklS+YexJNxlJTE0exsFqVCEe1dHWXP1TZtxJNxjP5sGIm2FIaHh1EoFOA4Djjn\nyx7YGyk13UJjEJ/V5mphY7EYTNNEoVBAOp1GqVTC3r17Az7JKqrp3isRoGYT+BZaWCpqOQZVI+iW\nZeH8+fMwTROHDh1CR4c/zlLRoa6urrLoezabRTabxfj4OPJ54cZFxJ2i7/F4tPa93uh7lNVui6hf\nI6ABPR6PY3FxUW7nnGNmZgZDQ0Nob2/Hbbfdhvb2cj1rPWimPztVJr106RKGh4fxp58KThqiIt9h\n2UoUkQ9H4ytG4qWNHgu8DqJ2NF5Ia4LnRhHaSpr4SgSWzpXR/LD0SCciHZTLRBV/ivKPj3TMCUmc\n1MFCJczkJR/VlrrdT16NTsr1vdfF6opqVQlAVkmVbXPmJ8wqEzB6toZhgENMhhhnMvpBrjqxeMyb\nFPGK0iXHduDYkBFz1V3HdV04tm/NZZuOnABYRctb3aB9giwXc0W0dbZ5bQuZTDFXQrLdl5kxzuDa\nLhgTfdZ0TV6/VBDkXETJC4hVcG7JL2Tl37m5RWzY5ttrdXR0YHx8HMPDw3jsscdw5swZdHd34/jx\n47jttttwxx131F0huNFS0y3Ujyi73XDe0HLAGMOVK1cwNTWFvXv3oq+vb8njepi8P/300xiyj8nX\nVyP63kILUVgOQeecY2xsDGNjY+jv78eWLVvq/m4kEgn09fUFrA3D0feLFy/CsqxlRd/DVrur6aO+\nElj3RB0IJpPOzs4inU4jlUqVzQqXCiJTzfBe5pxjfn4ek5OTeOR9FoAUyNZPRTmxqyRboeP9KHy1\naLweSgStNxoflRBambzT9sozY3JyqUQaZbXTKv1Qz40q/kQkXyX/mqaXEX8toqIqAwvcrxETxDeq\nKJTfTpC0qw4ylfILHObbO6puN0DQhSd8jLoN8CUv4WqydCjtl3BFH9XCSwSb9PMeKVcnMJSMS8f5\nxN32rmMG2irmitKRxnW9Yy0btmn7g69tS6mNbdqyLUJhsRB4nZ/PwkjEYJVMJFLlJPtKZky219vb\ni3g8jkOHDuEv//Iv8dnPfhYXLlyA4zj4/Oc/j02bNmHv3r1lbUSh0VLTLTQGGs+bQdRN00Qmk8H0\n9DTa2tpw5513Ni3wEovFcN+eKfT09IgJrePIlYBv/ZdIXG6R9xZWArWIuYpqJH1hYQFnz57Fhg0b\ncOzYsaZ4pVeKvpumiVwuVzH63tnZiba2tsjoe6lUkkEoams11RHNxrom6oR4PI58Po+TJ08iHo/j\nlltuQWdn57LbpcjOcon6zMyMnDx84C+qzwKjiJ1KSusl8vVG4mlfxWi84uhCr6P7rRLeyn2KSv4s\nPy4iyZUIvOYnhUZp5amAUtApp1xmA4h7DVdZDSfA+o4vfqSeMVZGpOk6akIsRQuocBSAugg/RdXD\nMp2AJCW0AlEr4Ti8MkSrAOHnBIhJRNiP3YgbsC0bruPKIlgUDbdNW7ZTzBXLrBJLuSISqaS8JmMs\nMGkq5guIe6TMtoT0JZFKopQTRD3WGy9LGp27Mg0jZkhiXlzMQY8ZcG0Hn/xfnXjmmWfQ29sLxhgc\nx8E//dM/4ejRo3jd615X9TlFodFS0y0sHcux21VhWRaGh4cxMzODPXv2YMeOHchkMk3VwhuGAdu2\nYVmW9Jk2DAPT09PYZp1CX18fdu/eLbW1f39aEIsWeW+hESyFnAO1ZS7pdBrFYhEHDx5cVkCzXiST\nSSSTycjoezabxdzcHC5cuADLspBMJiWBLxaLmJycxODgIFzXRTqdxtjYWFPVDquNdU3UdV3HwsIC\n0uk0crkcjh071lQLtuVahS0sLODcuXOIx+N4x0d0ANEOF4RwsSFCveQ2eE7lSDxFv5kS+a4Uja83\nEi8TWz0NeTSqrw74fav8q6VKShiPiM4rri5AOZlXiW2UVaOKSpF6+drTzPPAHKU8Sg8gQPh1w7cx\nVNumewFEJD8syYmS6BCpDxSncDzJS8j7VrW4km47NperBoF7t4PJqmoknMg6ZxwOc+RngiQqFBnn\nnMltVskMrBiQTIbatEpmIJpje+5IesxAfj4L13XFCpflwLUc+T7ohoGiR+if+NQARkdHkU6nkUwm\n8dnPfhYnT57EzMwMDh48iBe/+MVNmXi30Hwsx25XheM4GBkZwcTEBHbt2oV9+/ZB13WUSqWm2u3S\nd+rChQvYtGkTurq6YFkWMpkMuru7I6VVUcSpGnkHWgT+esVSSbmKWjKX8fFxXLx4EXv27MHWrVtX\nleyq0XcVpmlienoaw8PDsn8veclL0N3djfHxcfze7/0epqamsGXLltXodtOxrn+R6EM3ODiIZ599\ntuk+yY0S9Vwuh/Pnz4Mxhn379uHlrz1f13kV5SioTOLluRUi8eGIsTiWBf4PnlM7Ek8gbbz4u/Ek\n19BBSj+j5TEqma8mt6kWmae/STOvFoWKaof6E4hke41FuduEz1NfR0lwOONgritIPGdwbCYnAqpE\nJ/ya2g3r8cskMY5vian2SRwrSBFNIFTYpqVIb3xXGMe2A65Dmq7BNq1AArCma7CKZpmUxyqa0HRB\n1q2i6R2rwyqVEEuKyLqZLyIWj4M5LmxTkHbmiMRX13bERNNx4Xj7/ubxfThz5gw2b96MQ4cOIZcT\nJbA3btyIBx98EHNzc/j0pz+NfD6PF73oRVgKGik13ULjaISoO46DCxcu4PLly9i5cyfuvffewOS1\nWXUxaIXGdV3s3LkTi4uLmJ6eRjqdBmNMLsdPT0+jq6sLnZ2dVZPkqpF3oBV9v16wHGKuopbM5dy5\nc+jp6cFdd921ZgMWZLQxMzODW2+9FT09PXjyySfR3t6O5z73uTh06BCeffZZ/NEf/RG+8pWvrHZ3\nm4K1+U40CZqm4ZZbblkxs/2lEnXVUWDfvn14+UPnAZyreHykRrsCKiWIVj2nyZF4/xyl6I+HWm41\nQHl0PhIhMl9Pwqs4LspLXviO0wpCeBKiWlH6l4/oY1mRnyC5B4SmnrbpCoFVr1U5iZZFkng5EQgl\npoatLlX3GfW6mq4Fo/3wbRjDfdB0DY7jEfZYsP/qCoSq/QeYTGB1bGFR6TJBYjRdAzNFxVaRNBuM\nuktrS9tBLB6DVRQJpLGkIOlE/P1rRn92/r8vHUY6ncbQ0JDMSfmHf/gHvO9978Mb3vAGfOpTn1p2\nxKjRUtMtLA1kB7qUcZcxhosXL2JsbAzbt2/HPffcE6mzXW5dDJWga5qGWCyGQqGA8fFxAKLKYVdX\nFxzHkS4YFy5ckDrczs5OdHV1obu7G52dnVVJUqPkXddaJP5aQbOIOaEaQbdtG+l0GoVCATfffHNT\nZMErhdnZWZw7dw7btm3D0aNHsbCwgNe//vW4dOkS/uZv/gb9/f2r3cUVwbon6iuJen8wTNPE0NAQ\nFhcXMTAwgN/4/WEAtaPo1fTo9aBaBF6210AknvoRFY0X51SX1VRCmMyHfb7FtqWT+QoH+eTSoQqc\nYUeb4Blhch8FckIJb1NJe5TbTRTZV49Vi0yp+4P9rT4RCLvSEHkO94uSpNU+cJuLZB3FZlL1z+eM\ng9v+c1EnQK7lCH24d5OuEqG3TUsSJ1E8SRBy5rpwHUHk1cmJmS9KORFNQip93r7wZztx6tQp7N69\nGwcOHMD4+Dh++7d/Gz09PThx4gQ2b94ced5S0Wip6RaWBrUuBhHcSmCMYXx8HBcuXMBH8Y+VAAAg\nAElEQVS2bdtw9913VyW/jf5WRBF00zRx9uxZmKaJgYGBgPtELBbDhg0bAtZxruvKJLrLly8jm82C\nMYb29nZ0d3fLpf9qybOViFhYOtOKwK8tNJuQh7Fb/3cUi0WcPBmTn6POzk65knPp0iVcuHBBjpFr\nVdNN1pCWZeH2229HKpXC17/+dTz22GN461vfile+8pVrtu/NgBYuvVoD19RXmjEG0zSh6zqefPJJ\n3HvvvU19M0dHR2EYBnbs2BG537ZtZDIZzMzMoL+/H5s3b8aLfuOpyGObiaWQ+YptKJryWmS+Wh8q\nurdUPbf+60WT+erXa+R+RL+CiZWVZTcs8FoczwKva12nmg4/ymmH+hTuZyXUOjZ8f+Wkv3L0MWyf\nqUpmpJWkkqgMKJMDTy5Fkye5OkCThirRc8L//vwhnDlzBolEAvv27YNhGDh+/Di+9KUv4UMf+tB6\niGg3YxC7psZyQIynjuNgbm4OU1NTOHDgQNkxnHNcvnwZw8PD2LJlSyBZsxaefPJJPOc5z6nrWMaE\nLSp9nilxdHh4GIuLi+jv72/I4lFtv1AoIJvNYnFxEdlsFrZto62tTUbeu7q6kEwml3wNNfpesx/X\n3KdkbWKlCXkU1MmbbdtyMkifqWKxiGQyiW3btqG3txednZ11W9JeLdD3eXR0VFpDjo6O4pFHHsGO\nHTvwwQ9+8Fr3S6/rk3HdRNSbaelFiMVikVpJx3EwOjqKK1euYNeuXRgYGMCLXnkKwIjo1wpJcQhL\ndYaJbINX15SrZDeKzC9FVlPmrFJHoiuBRWjPa+nm1fsJV3sNXCtU5TV8H9FSlaA2vJzQ1l4RUB93\nFPFX21Sj/JX3s7raqtYP8ZpWIIKOMmXneRFveh9cx4rMbSAyLvToQkvuqkWtXLouQ9mSQwS+/ZU7\nceHCBfzkJz/B/v37sWHDBjz99NN45JFH8IIXvABPPvnkurLsul4RpVHnnGNiYgKZTAZ9fX246667\nGhrrazlFkBUrraTSBJMsHvfs2YP9+/cvOyCk67qMfJKFHeccpVIJi4uLWFhYwNjYGEqlEpLJpIyW\ndnV1ob29veL1C4UC+mP/Bdd1sW/fPilzqETeW0ms9WE1iHglRK2uxONxbNiwAZ2dnSgUCojH4zh4\n8CB0XUc2m8XMzAxGRkakowpF3unztFLy4WrI5/M4c+YMOjo6cPToUQDARz/6Ufzd3/0dPvKRj+Dn\nfu7nrnqfVgvrmqirIFLdTKIerniq6iF37NiBu+++2yPoY4Hz6rVMbCYa0aNXbS/E4pZL5iv1KzzB\nKJen1K+ZDzvZECoTe9VFpQ7JTQhh/bfq1V5vVL8W8a93f3hftai4+l7SZIUmNHRP6r1EPT9V+kKv\nw0WnAO8ZuX7SbbiNevE3j+/DyZMnsXnzZhw7dgzFYhFvf/vb8Z//+Z/4i7/4Cxw8eLDutlpYm4hy\nfeGcY2pqCkNDQ+jp6cGRI0eQSqUaar9aXQzGRP0Dui4RFwrI7Ny5E8eOHVtRQqNpGtra2tDW1oat\nW7fK7aZpyijpxMQECoUCYrFYIPIej8cxMjKCxcXFyIqrtXTvYVT7qVhvJJ60/WuJjEehlpsLRaZ3\n7doVmEyGJ4NqNdGpqSkUCgU5cVQJ/EolmzLGMDw8jOnpaRw4cAA9PT04efIk3vzmN+OXfumX8OST\nTzaVx10LuC6IOukam2m/BfjLnYwxXLp0CaOjo9i2bRvuuusuvORVpwFcXlo/l0A6VwK1vLbF9VeO\nzNfq01Ij8/55LPLvYBuVJSqVCP5SQWRfEFu3TLZTS7ITJv5L2R/eF35dSTZEiZ5q4Sxd16QTTPi5\nBdpUJgIBcl7N8nIJ5ByIThb99re/jT/5kz/B6173Ojz22GOrEg1qofkIE3WqP9HR0YHDhw8ve7Wk\nUl0MKlakRtsvXbqEsbEx3HjjjU0rBNMoyH9606ZNcptt28hms1hYWMDo6CgKhQKSySQ2btyIfD4v\nbe+q9Xup5J2wFEK7WqSe+qhev1q/r2WSns1mcfbsWXR2duLo0aNVpWCapkV+ntQ8iitXriCdTsN1\nXbS1tUni3tXVhVQqtazVpLm5OZw9e1byqFwuh4cffhjpdBpf/OIXMTg42HDb1zLWNVFvlvduJcRi\nMWSzWfz4xz9GX18fjh49ipc+8J8Appp6HRVrkcxX0kzX1d4SE14bjcyXtVkjAbaRyH2w/ejIfUCi\nEupnPc44zUQ46g0AboXbrHQM82wdgaBUKHhutJvNUp9pGF/82E04deoUdu3ahQMHDuDKlSt47Wtf\ni3g8ju985zvYtm3bstpvYW1icXERi4uLGBsbW3Z1aRVhc4BwNVFd1zE5OYnR0VFs3rx5TVvYGYaB\nQqGAK1euYPv27dixYwc458jlclhcXMSlS5eQy+XAGENHR0cgabUakauVtLpUrDYBXu3rLxfVCLrj\nONLEYv/+/eju7m74OoZhoKenBz09PXIb5xzFYlGu5oyPj6NUKiEejwfIey0LUiA6WfSb3/wm3v/+\n9+Phhx/Gpz/96XWdLFoL6zqZFBClZQFgeHgYbW1tgZK1jYJzjunpaZw/fx62bePYsWOIxWJ48VVI\nFG0UK62Lr339yi4xK3K9BhNGK7a3hATORhJo1wLqsouMkMVUa8fftjxSriKcLBqLxfC5z30On/vc\n5/C+970PL33pS5t2rTWK6zKZlDGGp556Skb3mq1RffbZZ7F9+3Z0d3dLJxdd16FpGmZnZ5HJZNDb\n24s9e/as2aV3kgJlMhls2rQJu3btqkq81cqPRLgcx0F7e7skWt3d3Q0lGTZK3luojVoylytXrmBk\nZAQ33XQTbrzxxqtKcmk1hyLwuVwOnHOZc6G6GEUli46Pj+ORRx7Bxo0b8eijjwYi++sQdb0x656o\nm6YJzrksKXvTTTctq725uTmcP38eqVQKO3fuxPnz5/G2DzaPhKwmVpvMq7jaRDcctVe3Nep8s6Tr\n13B6WUnUiohXPK9Kf5tJzAl/+kYXtm3LKqT5fB49PT1497vfjXvuuQfvfve7V6TU9de//nX09vbi\n9OnTePOb37zk/SuA65ao53I5JJPJJTm01IszZ86gp6cHvb29MoK+sLCAoaEhtLW1ob+/f00nIy8s\nLOD8+fNoa2vDwMBAw1p9ipSS20w2m4Vpmkgmk4HIe1tb24o6zrQQjWokPZfL4ezZs2hvb8fevXvr\ndjxaaagTQiLwpVIJtm0jlUphcnIS27dvx5NPPoknnngCH/7wh/GCF7yg6f24Vsfytblu10QsxXu3\nGhYXF3H+/Hnouo4DBw6go6MDv/CKk03s6epjtWU1Khohrcsh95GR4RrON0tFmAirEwEwvWyCUK0/\nUROLStepF3ROWEMeXj0InLMChDwKXzs+iLNnz+KGG27Apk2bcOrUKXzsYx+T5CqdTuOb3/wmfuM3\nfqOp1z19+jQA4L777kMmk8Hp06cDPui19rfQXBDx0DRNSlKWC3Jx6erqwvDwMDjnSCaTKBaLiMfj\ny5YNrDTy+bysfHrgwIFlF6zRNA3t7e1ob2+X8jHOeSBp9fLly/L5qJH3Wg4hL75lDul0GpZlYd++\nfejq6mqR9zpRS+aSyWQwPz+P/fv3ByQqawGUE9HV1QXGGEZGRjA5OYl9+/ZB0zR885vfxPe//31M\nTU1h165deOKJJ3DgwAHceOONTevDtTyWXzdEvZKVYi3k83kpcRkcHERnZ+e6I+hLQS1iFk7CvJpR\n+maS+5WQr1RLrl3qpKDaMdXkKPVG7qsl8V4tcg6IZNGhoSGk02kcPHgQHR0d+N73vod3vetdeM1r\nXoPXve510DQNIyMjUubWTDzxxBP4+Z//eQBAf38/Tpw4ERi8a+1voXmIsttdju9zuFjRli1b0NXV\nhXQ6DdM0sWXLFti2jTNnzkgtd9hJZTVhWRYymQyy2SwGBgbKnFyaCU3TkEqlkEqlAoXCVIeQ4eHh\nQKKqqlHmnGN0dBRTU1MYGBjApk2b5PvZaNLq9YJaMpfJyUlkMhns3LlTEt+1CjVZ9NixYygUCnjv\ne9+Ln/zkJ/jqV7+KW265Bfl8Hs8880zTJxvX8li+7ok6YanJpMViEUNDQ8jn89i7dy96e3uva4Je\nL+pNwgxXsFwtVJdvLF8dUC0aHaVzr6aDbwYqtRWVWLraUJNF9+/fj6mpKfzhH/4hSqUSvvWtb2H7\n9u3y2JUqHT0/Px8gQDMzM0va30LzQSukjRL1qGqilMyWz+cjSS8VIFpcXJR2kK7rSi03kferoV13\nXRejo6OYnJxsmm97o0gkEujr60NfX5/c5jiOlDeMjY1hdnYWlmWhs7MT27ZtQywWq2iDSWh20uq1\niucPXIHrdka68+TzeZw9exapVAp33nnnms2bAIRu/dy5czJZtK2tDf/4j/+I97znPfj93/99fOQj\nH5ErMR0dHbj77rub3odreSy/roh6PfaMlmVhaGgI8/PzGBgYQF9fX4ugrwDqsUxUsZb080vBUiYC\njbjXNIqoZ75WSDoli05PT+POO+9EPB7HF77wBfz5n/85/uRP/gQve9nLVruLLawCluviFa4mSi4v\n6XQac3Nz2LNnD26++eZI0qsWICJwziV5n5mZwfDwMGzbDpD37u7uphEosgG+ePEitm/fvuK+7Y0i\nFouht7cXjDFcvnwZW7duxa5du6R0ZmJiQtr7hZNWaz2r6yX6/j/6LyObzWJ8PBtw5+nq6kJHRwdm\nZmawuLiIwcFB9Pb2rnZ3K0JNbKVk0StXruB3f/d3kUql8J3vfCdQE6CFaKx7ok4DWa2B3XEcDA8P\nyyjF4OCgR9AzV6mnLVRDs0nktUr8gbVDqJsNtbLo4OAgNm7ciLNnz+KNb3wjbr/9dvzoRz9CV1fX\nVe1Tb28vZmdnAYiIixo5rGd/CyuDsJViNURVEyWd7MTEBHbt2tWQZEDTNHR0dKCjoyNQMIYs6+bm\n5jA6OgrLspBKpSRx7+rqQjKZrPt6YSeXtWwLCfhyUU3TcOjQIbS3twOATEYlqBOd2dnZwLNSVylq\neXOvJ/Lu30tvgIBTMualS5cwMjKCeDwOXdcxPDwckBk1kuC7UqDKou3t7Th69CgMw8BnPvMZfP7z\nn8f73/9+vPjFL76q/bmWx/K1+21vMioN7K7r4sKFC7h06RJuuukm3H333WCMtaLo6xzrlexeq/ja\n8UGcOnUKfX19OHbsGCzLwnvf+15873vfwyc+8QlZQvpq4xWveAVOnToFAMhkMrjvvvsAiIG8t7e3\n4v4WVhb1RNSjqolqmoaxsTGMj49j+/btuPvuu5salVYTMSlSyDlHqVSSBYguXrwYcFGpRkjn5+eR\nTqfR3t6Ow4cPN+zkcjVg2zYymQwWFhawb98+bNiwoerxlSY69KxERFl4cycSibKk1fVG3qtp0Uul\nEtLpNBKJBJ7znOdIa0P1WVGCL1WlXYqPeTNBk+CpqSns378fvb29eOaZZ/Dwww/juc99Lp588kk5\nebuauJbH8nVP1OnLTEmlBMYYxsbGcPHiRVldjnOOF73y1Gp1tYUWrjv8ny/fgXQ6HUgW/cEPfoC3\nv/3teOCBB/CjH/1oVaOHR44cwalTp3DixAn09vbK5KIXvvCFeOqppyrub2FloLp4VSPq4WqimqZh\nYmICo6Oj2Lp161WNSmuahra2NrS1tWHLli0Ayl1UVEJKEpDp6WnpMrZcJ5eVBP2Wjo+PY/fu3Rgc\nHGw4qhv1rAA/aZVyBAqFAgzDWBIhJSK8uLiIc+fOoaOjAwMDA/g/z6yuQ0o1gu66LoaHhzEzM4PB\nwcHA5KfSsyIf82w2iwsXLki3O/Iwp/9XIhl6bm4O586dk9+xUqmEd77znfjxj3+MT33qU7jtttua\nfs16cS2P5eveR50xBtM0oes6/u3f/g3Pec5zcPnyZYyMjGDz5s3YtWsXAOAlrzq9yj1toYXrC1/8\n2E3IZDLYtWsXbrjhBszOzuId73gHZmZm8MlPflJ+N1sow3Xpow4Iwua6LqamppDNZrF3797A/nA1\nUU3TpH5848aN2L1796q7tVRDNpuVSa2pVAqO4yAej8uoez3R5KsFVZKzZcsW7Nq1KzLpcaXgOE6g\nUFOYkHZ3d6Ozs1NOyCzLQjqdRrFYxODgYFUZ3dWIvlcj6ABk0vKNN96IHTt2LCsq7rpuoLBVNpst\nK2y1VEmWCtu2cf78eZimiQMHDqCtrQ0nTpzAu971LvzO7/wOXvva117Vz8Y1hFbBI8An6pqm4Yc/\n/CFisZgcsA3DWNPVRFtoYT0iXFk0Ho/jq1/9Kj72sY/hHe94B+6///41QUTWMK5bom7bNhzHwfz8\nPCYmJnDzzTcDCDq5ULGiubk5ZDIZdHZ2Ys+ePWtaNuI4jrQv3LNnD7Zs2SK/A2o0OZvNBqLJJJ2p\n5V/ebFBdkVQqhb179y7LJrOZoKq1KiFlnszRNE1s374dO3fuXPVKq9VIerFYxJkzZxCPx7Fv374V\ne7ZqPgX9C8uMurq6qn621GTRPXv2YOvWrZicnMTb3vY22LaNj3/84031Ql+HaBF1ALJEbTqdRqFQ\nwB133IHOzk44joNf/M2nV7t7LbRw3eAf//oIxsbGcPnyZZksOjQ0hIcffhh79+7F+9///jVXqGON\n4ron6vl8HsPDwzh06FAZQc9ms1LPOzAwsCp62HoRdnKpN3Kqyhsomkz+5UTeOzo6mk7eS6UShoaG\nUCqVakal1wJmZmZw/vx59PT0oLu7W5J427bR1tYWSFptJJq8VPJeS+ZCk7V9+/atqC9+NZimiVwu\nF5gYkuuRSuBN08SZM2fQ1taGvXv3wjAMfOELX8Djjz+O97znPfiVX/mVVen/NYYWUQcEUb906RKS\nySTS6TS2bt2KDRs2IBaLgXPekry00MJVwP98hCObzSKRSOD8+fMoFou4cOEC/v3f/x0f//jHce+9\n967IdWuVhD5+/DgAYGhoCB/4wAdWpA8rgOuWqDuOA9u2USqV8F//9V+45ZZbkEwmoes6CoUChoaG\nwBjD3r171zSJDDu57N69e9ma+bAUJJfLSYJFZLSzM9qTuxZc15UJguGCRWsRhUIB586dg67r2Ldv\nH9ragoSaEjGJjFI0OZlMlkWTm0Hea8lcpqenkU6nsW3bNtx0001rznbTdd3AZ2tmZga2baOzsxPf\n+ta3sHPnTjzxxBO455578J73vGfFcirW4XjeIuqEc+fOoa+vT2ZG5/N56QZAMpiuri75hWw5vrTQ\nQnNAyaL5fB4HDhxAPB7Hl770JXzlK19BoVCApmlIJBL40z/9U/zCL/xCU699+vRpZDIZ3H///Th+\n/DiOHj0aSBA6ceIE+vv70d/fj5e//OV46KGH1lSmfxVct0R9cXERk5OT6Ovrw8jICBYXF2FZFhhj\n0DQNO3bswPbt29e0Dl11cunv719RSU6YYOVyOQAIkPeurq6K5J1WpEdHR5cU8V8tOI6DkZERzMzM\nNBSVVhN8KZpMLir0vJq5UlEsFnHu3DlomobBwcE1Lc8C/GTRLVu2YOfOnZiamsI73/lOPPPMM0il\nUiiVSujv78ff/u3fNn0it07H87oe0rp3fQHELOu73/0uUqkUNm7ciHPnzuHd73437r33XliWhZGR\nEeTzecRiMXR3d+OvPrqjLGmnRd5baGFp+NLHd+HkyZOysujCwgLe9ra3YWxsDF/84hdlNdFCoVC3\nJ/ZSUKskdCaTQSaTwYMPPoj+/n5kMq2aCWsdFy9exOtf/3pMTU1h27ZtsG0biUQC73vf+9Db24ts\nNouf/OQnsvAQyUC6u7tXnbzn83mk02lwzq+ak4thGOjtLffkJmnDpUuXZEGdMHmnxNaenh4cPXp0\n1Z9fNaha6R07duCuu+5qiEwnk0kkk0ls2rRJblNlRqOjo4GVCjVpdSkrFYwxjI6OYmJiAvv27VtT\nnt1RoGTRUqmEW2+9Fe3t7fjXf/1X/PEf/zF+67d+C3/1V38lVQqTk5MrstpyPY/n1wVR//CHP4yh\noSE88MADSCQS+M3f/E1861vfwqOPPoq2tjbcfvvtOHz4MO644w5s2LAB+XweU1NTLfLeQgsN4H9/\n/hDOnj2LyclJWVn0G9/4Bj784Q/jLW95C171qlcFBvKV0hDXKgn94IMPyr9Pnz6NV7ziFSvSjxaa\nh4MHD+L73/8+/uzP/gzHjx/HC17wAsTjcbzpTW/C9PQ0du/ejSNHjuDw4cPYvn07YrEYpqenkclk\n4DgOOjo6Ag4qV4N8mqaJTCaDXC6HvXv31vQXX2noui4nLwQqqLO4uIixsTHMzMyAc47e3l4kk0lk\ns9kVs/RbLshusbOzc0UmFPF4HBs3bgyMJZS0Staa6mRHlc5E9YV081u3bl2z1WUJUcmiMzMzeOMb\n34iFhQX8/d//PW666SZ5vKZpK1Zp9Hoez68Log4A27Ztw5e//GUMDAzIbZxzzM/P4/Tp0zh16hQe\nffRRnD17Fh0dHbj99ttxxx134PDhw5K8T05OolAoIB6Po6urq0XeW2hBwbe/cicuXrwYqCw6MjKC\nRx55BNu3b8e//Mu/rFqCVDWcPn0aR44cWVO+uS1Ux4te9CK87nWvCxAhxhjS6TROnjyJH/7wh/jo\nRz+K2dlZ9Pf34/Dhwzhy5Ah27twJXdcjyTsR+GYRvbCTy4EDB9asrlvXdaRSKVy6dAnFYhG33XYb\nent7ZeVQsgp0XVda+tHzSiQSq9Jn1W5x//79VzUnwTAM9PT0BJLfabKTzWYxOTmJoaGhgAViKpXC\nxMQEdF1f88WrALHSeebMGaRSKRw9ehSxWAx//dd/jU984hN45zvfiV/7tV9bk5/n9TieXzdEnYob\nqNA0DRs2bMALX/hCvPCFLwTgk/ennnoKp06dwoc+9CE5Wyfyftttt0WS91bkvYXrFWpl0bvuuguc\nc3z0ox/FN77xDTz22GN43vOed9X7VG9J6BMnTlwriUcteDhw4EDZNl3XMTg4iMHBQTzwwAMAROTz\n/PnzOHXqFP75n/8Zjz76KObm5jAwMCADMUTeiYwul7wzxjA+Po6xsTHs2LFjzUdN1YJFu3btChQs\n6uzsDEh0OOeSvM/MzGBkZASWZaGtrS2wUrGSdo2MMVy8eBGXLl1Cf39/wMpyNUGuO11dXdKSkHMu\nHYouXryIZDIJzjl+9rOfBWRGbW1ta+IeAF+WMzk5KSuLptNpvPGNb8SBAwfwgx/8YFXcua7n8fy6\nSCZdLjjnmJubw1NPPYWTJ0/i9OnTOHfuHLq7u3H48GEcPnwYt99+OzZv3iyXD1XyTv/UL2OLvLew\nHhBOFu3o6MCpU6fwpje9CS996Uvx1re+ddU8lmml7MEHH8QHP/hB3HfffThy5IgsGQ2I/BVaMj1x\n4sS1kHyE/7+9c49q+j7/+Psb7pdwRyoXwQABRC6iWNtqt7axtmtrjx48up1269qq5/S37lRnRbdq\nd7StFV1tndoW3Rxz2lHjttqLrcYd11Z7toQoFhUCBEQEQa5JuCQEvr8/0u/HRNCgEBLC8zqHoxKO\nfJLAO+883+d5P5jAw6SjQX9/PzQaDZRKJUpKSqBWq9HZ2YmkpCTWAim8ERB6k4VK8u3MuyOSXBwJ\nz/NoaWlBdXU1IiMj2W6Ru/l/hDxuYQjTaDTC19d3kHkfqRkV0lGcsWDpbmhra2PDlwkJCRCJRIO2\n0ur1evT09LAr9cJjNtbZ+IDFAFdUVLDH12w2Y8eOHTh27Bh27tyJOXPmjOl5rHFTPafUF0fC8zxa\nW1tZ5b2kpARVVVUIDg5mYp+ZmYmIiIghzXtAQAA6Ozuh0+lY3NWCZSpn3y2CGDZ/+2M8qqurMWXK\nFERHR0Ov12PTpk2oqKjAnj17kJKS4uwjorCwkA0WCQI+c+ZMlJSUQKFQYMmSJQgLC0NbWxsOHz48\nHoQdIKM+6vT396O8vBwqlQoqlQpnz56FTqdDcnIyq7ynpqaC53kb8x4QEACxWAyRSIRr164hMDAQ\niYmJLrMA6Fbo9XpoNBr4+PggKSlp1NswhPhDazMqxB9am3dfX99hmXfruMXxkI7S29uLyspK9Pf3\nIyUlZVA85FAIi62ED+tsfOHjbuM17WE9LJqamgp/f3+cOXMG69atw5IlS7B69WqXmE9wQz0noz7W\nCBUK68p7dXU1QkJCbMT+m2++QXJyMhvm8fb2thEvqrwTrszRogyUl5fD09MTUqkU3t7eOHr0KLZs\n2YJXXnkFzz33nMtcxnVTyKiPAWazeZB5NxgMzLzPmDEDHh4euHjxItLT01n1/OaBVVeqqhuNRtbX\nLZVKbQZKx4KbzXtPT89tX//MZjNqamrQ3t6O5ORkpw/i2kNoy2lsbERiYiIiIyNH9P+ZzWab5UMG\ngwE8zw9K6Lnbn7GhhkU7OjqwYcMGNDY2Ys+ePZg6deqI7gNxW8iouwLC5dD//e9/KCoqgkKhYO+w\ns7KykJOTg4yMDISGhrJfSGvxEn4ZybwTzkYYFrXeLFpfX481a9YgJCQEf/jDH0b8wkQMCzLqTsJs\nNuPSpUtQKBTYt28fWltbkZiYiOjoaJY2I5VKMTAwwAzpwMDAoJ73sTbvwtbL5uZmSCQSREZGusyb\naZPJxIyo9ZVnkUgEg8GAmJgYTJ061aX7/IEbGeNC25Oj2nKEeE3r6rv1kK/whsfekK/1sGhycjI8\nPT0hl8vxzjvvYN26dVi2bJnL/Iy4MWTUXYmKigrs3r0bGzZsQEREBJqbm1mlpqSkBLW1tQgPD2eV\nmoyMDISEhNzSvN982ZDMO+FI5HtTUF5ejvDwcCQkJAAAPvzwQxw8eBDbtm1zyCVGe1voBAoKCm57\nuxtCRt3JvPnmm0hLS8OiRYtgNptx8eJFpuelpaXo7u5GSkoKa4OUSqVs+dBYmnfriml0dDQbnHVl\nOjs7UV5ezraEdnV1obu7Gx4eHg5bPDQSjEYjKisr0dfXh5SUFIfFzd4O6yFfwRkYCq4AAB0TSURB\nVLybTCabOQEheYbn+UHDorW1tVi9ejXi4uJQUFDgkCsXpOdDQkZ9PMHzPJqamqBSqVjbTG1tLSIj\nIzFjxgzk5ORg+vTpCA4OZr+IZN4JR7NpVT+MRiOAG0NjgYGB+P3vf4+HH34Yr7322rD6L+8Ue1vo\nBIQJ/xMnToz6GVwYMuouTl9fHy5cuMAGVktLS9HT04PU1FRm3pOTk2E2m5meD7V0aCTmvb29HZWV\nlQgKCoJEInFajOJwEdpyjEYjpFLpoIVQ1ouHdDrdoB5uYfHQWJl367Qcoc3FlSrQ1nMC1o+ZyWSC\nWCzG5cuXkZCQgP/85z/45JNPsGPHDsybN88hZyE9vyW0mXQ8wXEc7rnnHjz55JN48sknAdxY3yxU\nag4dOoS6ujpERUWxnveMjAwEBQVBr9ejoaGBDeyIxWIc2DmFzDtx1wjDoomJiQgKCsLp06exc+dO\nlJeXIzQ0FJWVlfjkk0+wbNmyUf/e9rbQEYQr4+XlxRLBli9fDsDS4lFWVgaVSoV//OMfOHfuHEwm\nE1JTU5meR0ZGoq+vD9euXUNlZaWNeRcMvL2Wiu7ublRVVWFgYADp6ekICAgYi7t81wwMDKCurg7X\nrl27bVvOUIuHrHu46+rqYDAYwHHcIPM+2m0oHR0d0Gg0CAsLw+zZs10yfYbjOPj5+cHPz4/pNc/z\nmD59OsxmMz7++GNs27YNLS0tkEgkKC4uxtSpUxEbGzvqZyE9Hxlk1F0YjuMQHR2NhQsXYuHChQAs\n5r2hoYFV3g8ePIi6ujpMnjyZVWqysrIQEBAwyLwHBQUNMu8tLS342f/VOPmeEq6EMCwqbBb19vbG\nl19+ic2bN+Oll17Ciy++CJ7n2VCaI7C3hQ6wVGlkMpnbZeYS7om3t/egRSwmkwnff/89lEol5HI5\nSktLYTKZMG3aNKbnkyZNgslkQmNjIzQazS3Ne19fH2pqatDR0YGkpCSXXC52M0LcYlRUFHJzc+/Y\n8Hp6eiIkJITF8wGDt4bq9XoAYFtDh/uGZyhMJhMqKythNBrHxZsg4Up9TU0NEhIScM8990Cv1+Ot\nt95CVVUVjhw5AqlUCp1Oh9LSUocNF5Oejwy3MOrCJqqhGG5f1HiB4zjExMQgJiYGTz/9NADLL+PV\nq1ehVCqhUqnw17/+FfX19YiJibEZWPX394fBYEBDQwO6urpgNpvh4+ODP/8hDuHh4VR5n+B89fdZ\nuHLlCs6dO4fk5GSEh4ejsbERa9euhZeXF7788kvcc8897OudHb8oLL8g3IeJpOWAxbzPnDkTM2fO\nZJ8zGo3MvBcXF6O0tBRmsxnTpk1jlXfBvDc0NECn08FkMqG/vx8RERE2iWKuSldXFzQaDTw9PUd9\nS+ettoYK5v3mNzzW5v1WrUY8z6O+vh719fUutWTpdgy1WfTo0aN4++23sXr1arz//vvsPgQFBTms\n7WW4kJ7fmnFv1BUKBVauXInq6upBt6nVagCATCaDVqu97YvAeIbjOMTGxiI2NhaLFi0CcCMmSsh5\n379/PxoaGhAeHo6+vj4EBwfjzTffRGhoKPR6PcrLy20q73/7Y/ygJRVk3t0X+d4UKJVKhIWFITc3\nFxzHYe/evdi/fz/eeust/OQnPxnT89jbQidUXwj3gbTcgo+PD2bNmoVZs2axz/X29uL8+fOsBfL8\n+fMwm82IiIiAVqvFq6++ikcffRRmsxmNjY2sbca6BeRuq8ijidlshlarRUdHB6RSqU0l3JGIRCJ2\nBUJgYGAAXV1d0Ov1aGpqQlVVlc1iK+Fx6+7uRkVFBUJDQ122zcUa682iUqkUoaGhuHLlCtasWYOw\nsDCcPHkSERERY3om0vORMe6Nukwmg0QiGfK2idwXJRKJEB8fj/j4eCxevBiAZVnAe++9h8cffxwe\nHh7Iz89HQ0MD4uLikJ2dzSrvPj4+7LIhmXf35ouDM1BdXQ2NRoNp06YhMDAQFy5cwKpVq3D//ffj\n9OnTTrm8u3TpUqhUlgVgWq2WibiwhU6r1UKr1aKtrQ1tbW1ubdwmCqTlt8bX1xezZ8/G7NmzAViq\nj0uXLoWXlxeef/55/Pe//8W+ffsAAOnp6TZtM729vWhoaIBerwfP8zbGfazMuzBvdfnyZcTFxSE5\nOdnpFWnrQdTo6Gh2TmvzXlZWhv7+fgQHB8PT0xPt7e3Dij50FtabRXNzc8HzPHbv3o2PPvoI27dv\nx8MPP+yUc5Gej4xxb9Rvx3D6oiYS8+bNwy9+8QubrXnCu2+hbebDDz9EU1MT4uLiWC5wRkYGvL29\nodPpUF9fz9ZDi8ViMu/jmL/9MR5KpRJTpkyBVCpFb28vXn/9dZw5cwZ79uxBVlaW086Wk5MDlUoF\nhUKBkJAQJtqPPPIISkpKkJeXB8Dy5rOjo8Np5yTGBtJyW4S9BZmZmexzQipTaWkpu4paVlYGjuMw\nffp0m7YZo9HIzDsAm5730R6+7OzshEajQVBQEGbNmuUSGy5vBcdxCAgIQEdHBzo7O5GSkoJJkyah\np6cHer0ebW1tuHz5MkwmE/z8/Gwq787cRtvX18dmhoQ217Nnz2LNmjWQyWQ4c+aMU7e5kp6PDLeI\nZ5w/f/6QcT4rV67EypUrkZOTA4VCgRMnTtCgwjAYGBhATU0NS5tRq9Vobm7GlClTmHmfPn06vL29\nWeyTYN6tB5yshYHMu+twtCgDFRUV8PDwYJtFT548iY0bN+L555/HSy+95PKXdyc4bhvPSFo+ugj5\n2ufOnWM7Oy5cuACRSISMjAxWeY+Pj2dmdLTMu5AvbjKZhoxbdEV0Oh0qKioQHBwMiURy25713t5e\n6HQ6ll1uXcASHjfrApYjGGpYtKurC2+88QbOnz+PPXv2YNq0aQ77/sSIoXhGe31RxNCIRCIkJiYi\nMTERS5cuBWAx71qtFkqlEt999x127dqFlpYWJCQksCVNcXFx8PLyQmdnJ65cuWJj3g/uShhUdSDz\nPrYMNSza3NyMdevWwWQy4bPPPkNMTIyzj0kQgyAtvzuECvEDDzyABx54AMCN9g7BvBcWFuLChQvw\n8PBAZmYmq7xHRUWhp6cH9fX1MBgMAIZn3ocbt+hKCBXp7u5upKWl2X1TYR19GBUVBcDyuBqNRmbe\nrVtHrc27dWjDSOjp6cGlS5fYsKiXlxeOHTuGTZs24Ve/+hXeffddl1gIRYwctzTqQt/TrfqiiDtH\nJBIhKSkJSUlJ+OlPfwrAIshVVVVQqVT45ptv8N5776G1tRVTp05l5n3KlCnw9PS8pXn39fVFXV0d\nent7kZKSgsUvXHLyPXVPbh4WFYlEKCoqwgcffIBNmzaxBCGCcCVIy0cfjuMQGBiIuXPnYu7cuQAs\nJtNgMODs2bMoKSnBBx98gIsXL8LT0xNZWVms8j6UebceWO3t7UVNTQ2ioqIwe/ZslzeK1r3zCQkJ\nSE1NvWsTzXEcfH194evri0mTJrHPC+Zdr9ejsbGRLSq0nhXw9/cf9vcV3gg1NTWxYdHGxka8+uqr\n8PX1xfHjx9mbB8I9GPetL3K5HMuXL8fevXtZn9PMmTNRUlICwNLzJJFIoNVqsWLFCod8/9tFhgm3\nO+r7uxr9/f2oqqpiG/nUajXa2tqQmJjIxD49PR39/f0oLS2Fv78/vLy8bCo1VHkfPYRhUb1ej9TU\nVAQGBqKiogKrVq1CdnY2Nm/eDLFY7OxjEneGW7a+kJa7FoJ5V6vVrAXy4sWL8Pb2RlZWFqu8x8bG\nQqPRQK/Xw9vb26YFcqy3hd4Jer0eFRUVEIvFkEgkY9o7bzKZmHnX6XTo6emBh4eHTetoQEDAIPMu\nDItGRkYiISEBPM/jT3/6E4qKirBlyxY89thjY3YfiFFhWFo+7o26M7G3FleIFBP6KsPCwibkJHN/\nfz80Gg3reT958iQaGxsxY8YM/OhHP0JWVhbS09PBcRwTLnvDOmTe7fP+W2FoamrClClTEBMTA6PR\niO3bt+Pf//43du3aZRP/NprYMzzC7w0AZsiIO8ItjbozIS0fHjzPQ6fT4ezZs1CpVPjuu+/w7bff\nwtfXFzKZDLm5uZgxYwYmT56Mnp4e6HQ6m8q7K5j3vr4+VFdXw2AwICUlxWUKFX19fez1T6fTobu7\nGx4eHqzi3t7eDrPZjLS0NPj7++P777/H6tWrMXfuXLz++uvw9/d3yLlIzx0K9ag7muFEhuXn5+PE\niRMT+nKth4cH0tLSkJaWhp6eHly/fh2ff/45urq62CR4QUEBdDodkpOTWeV96tSpACxVhLq6Ohvz\nfmj31EExWWTeLfz9/URUVlbi2rVr8PLywtq1a9HU1ISrV69iwYIFkMvlDlkTDQwv73rLli04fPgw\nCgoKKIaLcAlIy4cHx3EIDg7Gj3/8Y8ybNw9Hjx7FW2+9hcWLF+PcuXNQKpV45513UFFRAT8/P2Rn\nZ7OPqKgodHV14cqVK9Dr9RCJRIN63h1p3nmex7Vr11BbW4v4+HikpKS4VO+8l5cXwsLCbNKN+vr6\nUFdXh5qaGvj5+aG+vh4rV65EcHAwrl+/jjfeeAOLFi1yWOIM6blrQEZ9BNiLDMvJyYFEIkFoaCj2\n7t071sdzSV588UWby8bTpk3Dz3/+cwCWZRgVFRVQKpX46quvsGXLFhgMBiQlJbG0GcG8t7e3D4rJ\nmujmXRgW1Wg0bFi0tbUVQUFB6O/vxzPPPIP6+nq88MIL+PWvf40nnnhi1M9gz/DI5XLk5uYCgNts\nlyTGP6Tld46HhwdOnTrFzPVDDz2Ehx56CIDFFHd0dLC2me3bt0Oj0SAgIMCmbUYw73V1dTAYDDbZ\n5qNp3g0GA8rLyxEYGOjyEZECPT09KC8vh7e3N+677z54e3ujs7MTfn5+mDdvHmJjY3H8+HEUFRXh\n2LFjDjkD6blrQEbdgQiDUOvXr8fy5cuZ2E9kbie6np6eSE9PR3p6Op577jkAFvN+6dIlqFQqfPHF\nF3jzzTfR1dUFqVTKxF4ikYDneZuMW39/f4jF4glj3ocaFj106BB27tyJDRs2IC8vb0yqR/YMj1Jp\neezVajUUCgWJOzEuIC0fmlvpOcdxCA0NxSOPPIJHHnkEgMW8t7e3Q61WQ6lUYtu2bdBoNAgMDBxU\neTcYDIPMu1B5DwgIGLZ5N5vNqK6uhk6nQ0pKis1mUlfFOjUnJSUFoaGhaGpqwrp162A2m/Hpp5+y\nBU2OhvTcNSCjPgLsRYYVFhZi/fr1CAkJgUQigVwupx/kO8TT0xMZGRnIyMjAL3/5SwAW8b148SKU\nSiU+++wzvPHGG+jq6kJKSgoz74mJiejv70dbWxtqa2vR19fHVkN/tEcCsVjsFuZ9qM2iVVVVWL16\nNaRSKb755hsEBwc7+5g2hIeHs15fuVxOfY2E0yEtdzwcxyEsLAwymYy1DgkFlpKSEqhUKmzduhWV\nlZUQi8WsBTIrKwuTJk1CV1cXLl++PCzzbp0vLix0c6U2l1thPSwqbKHdv38/CgsLsXnzZixcuNDJ\nJxwM6bnjIaM+AuytxbVGGFIiRo6npycyMzORmZmJF154AYCll+/ChQtQqVT45JNPsGnTJvT09NiY\n98mTJ6O/vx+tra2oqalh5l0sFmPHRn/odDpIpVL2Iu3K5v14cS6uX78OpVKJuLg4SKVS9PX1oaCg\nAF988QXee+893HfffWN+LnuGJzw8nFUiQ0JCoFQqSdgJp0Na7hw4jkN4eDgeffRRPProowAsJru1\ntZWZ988//xxVVVUICQmxqbxHRkYOad59fHxw/fp1iMXicdPmYp3jPn36dAQEBKC8vByrVq1CTk4O\nTp8+7ZSFUaTnrgEZ9RFgby3u2rVrUVBQAIlEgra2NqdEik2UiWwvLy8m4C+++CIAi/iVlZVBpVLh\nX//6F86dOwej0YjU1FRWramqqkJHRwcyMzPh6ekJjUZjU3kPCgqyEXpXMO9HizJQWloKkUiEnJwc\n+Pj44LvvvkN+fj7y8vJw+vRpp7042TM8eXl5kMvl7HNCf+PdotVqodVqceLECeTm5iIkJAQffvgh\nDh8+PLI7QkwoXEHLAdJzwGLeIyIisGDBAixYsACAxby3tLSgpKQESqUSn376KaqrqxEaGsq0PDEx\nEf/85z8xZ84chISEsHQa6+SwO2mbGQt4nkdzczO0Wi3LcTcajdi8eTNOnTqF3bt3O3U4k/TcNaB4\nxnGMvUgxAFiyZAmbyJbJZBN+IttkMqGsrAzHjx/H3r17wfM8Jk2aBIlEwirvKSkp6OvrYzm31m0z\nwoczzLswLNrQ0MCGRdvb27Fx40ZcvXoVe/bscYm+2aHyrm/Oww4LC4NSqRzxGniFQgGZTAa5XI7i\n4mIcPnwYhYWF7p5zTfGMbgjp+Z3B8zy7qnjgwAF89dVXSEtLg7e3N7Kzs5GTk4PMzEyEh4fDYDBA\np9Ohq6sLIpHIRsv9/f2dYt6th0WTk5Ph7e2NU6dO4bXXXsOzzz6Ll19+GZ6ezq+lkp47FMpRd3fy\n8/Mxf/58yGQyKBSKQVUYuVwOrVZLvZRDsGbNGshkMjz22GMwGo34/vvvoVKpUFJSgtLSUpjNZkyb\nNo1Va5KTk2EymVjOrdlsRkBAgE21xpHm/ci+VJSXl7MeWZFIhCNHjmD79u3Iz8/Hz372s3HRg+ko\n8vPzkZub67ZVxpsgo+6GkJ7fHVVVVXj77bexZcsWREREoLm5GSqVCkqlEmq1GjU1NYiIiGDbsrOy\nshAaGmpj3oW8cuued0fpqfWwqFQqRVhYGFpaWvC73/0OnZ2d2LVrF6ZMmeKQ7z1emEB6Tjnq7g5N\nZN8927dvZ3/38fHBrFmzbBYAGY1GnD9/HkqlEh999BFKS0vR39+P9PR0VnkXlgi1tLRAq9Wiv7+f\nVd7//n4ixGLxiM37sUM5qK6uRnl5OdLS0hAYGIja2lr85je/QUxMDE6dOmXzMzBRUSgUWL9+vbOP\nQRB3Den53ZGUlIR9+/axf0dFReGJJ55g8bNCfrqwcK+4uBiXL1/GpEmTbCrvISEh0Ov1qKmpcZh5\n7+zsREVFBSIiIjB79mxwHIeDBw9i165d2LhxIxYvXjyhCy4CpOe2kFF3c2gi++7w8fFBbm4u67nj\neR69vb04f/48VCoVDhw4gLKyMvA8j/T0dFZ5F8z79evXUV1dfdfm3XpYNDY2FsnJyTCbzXj33Xdx\n5MgR7NixAw8++OCYPBauilarZb2SWq2WDf3RzznhrpCe3zkcx2Hy5Ml46qmn8NRTTwGw6HljYyMz\n74cOHUJdXR2ioqJYISY7OxtBQUHQ6/XQarU2m0Lv1LwL21C7urqQnp6OgIAAVFVVYdWqVUhNTcXX\nX3/tculcYw3p+a0hoz6OoYnssYPjOPj5+eHee+/FvffeC8Ai9j09Pazy/pe//AVlZWXgOG5I897c\n3DykeQ8KCrLpRezt7WXDojNmzICPjw9UKhVeffVVPPHEEzhz5ozDNtGNJ4Se3pycHGzdupUNNdHP\nODEeIT0fOziOQ3R0NBYuXMgiD3meR0NDA2ubOXjwIOrq6hAdHY2srCzk5OQgKysLgYGBMBgMNuZd\nMO5isdjGvFsPiwrbUPv6+rB161YcO3YMO3fuxJw5c5z5ULgMpOe3hoz6OGasJ7Jvhb2kAoGCggK3\nulzLcRz8/f0xZ84cJraCeT937hxUKhX+/Oc/o6ysDCKRCNOnT2fVmtjYWPT29g4y7wMDAzAYDEhM\nTMTkyZOh0+nw29/+FhqNBgcOHIBUKnXY/bH3PAq3Ww8VORMScMKdID13LhzHISYmBjExMXj66acB\nWPS8vr6eVd6Liopw9epVREdHs0JMdnY2/P39YTAYcP36dWbe/f0tkb++vr7Izs6Gn58fzpw5g3Xr\n1mHJkiUOT+ciPXcfyKiPY+xFikkkEoSEhEAul6O1tdUhoqpWqwEAMpkMWq0WarV6yCQChUKBEydO\nuJWwD4Vg3u+//37cf//9ACxi393djbNnz6KkpAR79+7FxYsXIRKJkJmZyURcpVLh2WefRXh4ODZv\n3oyvv/4a3d3deOihh7Bx40ZMnjzZYee29zyq1WpIJBJ22f1WzzNBEHcH6bnrwXEc4uLiEBcXh0WL\nFgGwDINeuXKFRUXu378fV69eRWxsLLKzs5GZmQmlUonk5GTMnj0bHR0d7Cqs2WzGK6+8gscffxwe\nHh4OOzfpuXtBRn2cM9Q7YSE2yfp2R71bLS4uxvz58wEAEokECoWCfuFvguM4BAQEYO7cuZg7dy4A\ni3nv6urCmTNnsGXLFmg0GiQkJGDVqlVITk5GdXU1HnjgASxfvhxarRZyuRxXrlzBM88845AzDud5\nzM/Px4kTJ2yqfQRBjB6k566PSCRCfHw84uPjsXjxYgA3klwOHTqENWvWIDY2FidPnsSnn34Kf39/\n+Pj44JVXXkFCQgLUajU2bNiAoqIi+Pn5OeSMpOfuBRl1YkTYSyoALO/eZTLZiDNW3QmO4xAYGIiB\ngQEsW7YMy5cvB8dxMBgM+Pbbb1FVVYWXX34ZAPDggw/iueeec+h57D2POTk5kEgkCA0Nxd69ex16\nFoIgnAPp+d0hmPf29nacOnUKUqkUAwMDqK2txaFDh/DOO+8gPj4eAJiBdiSk5+4FGXXC4QgDUsRg\nHnvsMZt/i8ViPP744046za0R+mTXr1+P5cuXM6EnCGJiQXo+NBzHYdu2bezfIpEIEokEr732mhNP\nNTSk5+MLMurEiLCXVCBUXwjXxt7zWFhYiPXr17OFS3K53O37UwliokF67h6QnrsXY783l3Arli5d\nCq1WC2BwUoHwOblcjsLCQrS1tbEhF8K1sPc8WpOXl8cybgmCcB9Iz90D0nP3gow6MSKEAZWhkgoA\niwgIg09DiQThGth7HteuXYvCwkL2Iu0KcV4EQYwupOfuAem5e8HxPH8nX39HX0wQY4m93NjCwkIA\nQHV1NQ1CEeOZ0dgxTlpOuDSk58QEYFhaThV1wi2wzo0VxN0ahUIBmUyGFStWQKvVQqFQOOOYBEEQ\nhB1IzwniBmTUCbeguLiY9dkJubHWWIu5RCJh/XsEQRCEa0F6ThA3IKM+QZDL5cjPz2d9hWq1Gvn5\n+U4+1ehhLzd2xYoVrA9PrVZj1qxZY3o+R3O7oS65XA6FQoGCgoIxPBFBEI6C9Nx99Zy0nLgZMuoT\nALlcjry8PKjVahbZVFxcjMTERCefbOwRViW707Y9hUKBJUuWDHmbvUvIBEGML0jPb+Buek5aTgwF\nGfUJQF5eHjo6OqDVatlSA6HHz12wlxsroFAo3G7wSCaT3XJZhb1LyARBjC9Iz2/gbnpOWk4MBRn1\nCcLHH3/MYrUA2Ii8OzCc3NjCwkKWHjBRRG44K8EJghhfkJ5PPD0nLZ+4kFGfIFRXVyM3NxeA5dKp\nO1VfAPu5sQqFAvn5+UhMTERoaKhDz2Kvj5D6DAmCGAmk56TnxMThTnPUiXEKx3ESACsBKH/48zDP\n84XOPZX7wXFcDgAJz/NyjuNWAFDxPK8e7u0j+L4neJ6fP8TntwI4wfO8guO4vB++N72iEMQ4hvR8\nbHCGnpOWEzdDFfUJAs/zWp7n83melwMIA/Cxs8/kpiwFIFyf1QK4udRl7/ZRgeM4YSd0MQDhmrgE\ngPtfIyYIN4f0fMxwup6TlhNk1CcAHMdJOI47/MPfZbC866f9z44hBECb1b9vnoKyd/sd80N1ZdYP\nfwqcBAChuvPD894xGtV7giCcB+n5mDKmek5aTgyFp7MPQIwJbQCKrS6XrXT2gYjR44eqmvymz820\n+jtdEicI94H03E0hLSeGgoz6BOCHaovc7hcSo0EHLJeiAUu15ebRfHu3EwRB3BLS8zGF9JxwOtT6\nQhCjy5B9hNRnSBAEMe4gPSecDhl1ghhFbtNHSH2GBEEQ4wjSc8IVoHhGgiAIgiAIgnBBqKJOEARB\nEARBEC4IGXWCIAiCIAiCcEHIqBMEQRAEQRCEC/L/d8ZdEOvFVFwAAAAASUVORK5CYII=\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x7fb800031b38>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "evaluation_points = np.column_stack((field50.x, field50.y))\n",
    "prediction = customModel.predict(evaluation_points)\n",
    "plot_field_and_error(field50.x, field50.y, prediction, field50.A, \"customFunctionLearning_100.pdf\",\n",
    "                     [r\"Custom function after $100$ iterations\", \"Deviation from the numerical solution\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Initialized weights and biases for MLP with 20 parameters.\n",
      "\n",
      "MLP structure:\n",
      "--------------\n",
      "Input features:    2\n",
      "Hidden layers:     1\n",
      "Neurons per layer: 5\n",
      "Number of weights: 20\n",
      "Loaded weights from file ./customFunctionLearning/baselineMLP/best_weights.npy\n",
      "-------------------\n",
      "L2 norm:  0.0004907119808029378\n",
      "Lmax norm:  0.11179031469916989\n"
     ]
    }
   ],
   "source": [
    "baselineMLP = SimpleMLP(name='baselineMLP', number_of_inputs=2, neurons_per_layer=5, hidden_layers=1)\n",
    "customModel = CustomFunctionLearning(name=\"customFunctionLearning\", mlp=baselineMLP, training_data=field50.A, beta=25.0, d_v=0.025)\n",
    "customModel.mlp.read_weights_from_disk(\"./customFunctionLearning/baselineMLP/best_weights.npy\")\n",
    "evaluation_points = np.column_stack((field50.x, field50.y))\n",
    "l2_norm = customModel.l2_norm(evaluation_points, field50.A)\n",
    "lmax_norm = customModel.lmax_norm(evaluation_points, field50.A)\n",
    "print(\"-------------------\")\n",
    "print(\"L2 norm: \", l2_norm)\n",
    "print(\"Lmax norm: \", lmax_norm)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "## Adaptive learning\n",
    "### Custom function after 100 iterations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Initialized weights and biases for MLP with 20 parameters.\n",
      "\n",
      "MLP structure:\n",
      "--------------\n",
      "Input features:    2\n",
      "Hidden layers:     1\n",
      "Neurons per layer: 5\n",
      "Number of weights: 20\n"
     ]
    }
   ],
   "source": [
    "baselineMLP = SimpleMLP(name='baselineMLP', number_of_inputs=2, neurons_per_layer=5, hidden_layers=1)\n",
    "customModel = CustomFunctionLearning(name=\"customFunctionLearningAdaptive\", mlp=baselineMLP, training_data=field50.A, beta=25.0, d_v=0.025)\n",
    "training_points = np.column_stack((field50.x, field50.y))\n",
    "customModel.set_control_points(training_points)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Selecting subset for training: 2 out of 2700 selected.\n",
      "\n",
      "The initial loss is 0.013029879834008608\n",
      "\n",
      "Wrote weights to file ./customFunctionLearningAdaptive/baselineMLP/best_weights\n",
      "The loss after 1 iterations is E=0.013029879258117704\n",
      "Wrote weights to file ./customFunctionLearningAdaptive/baselineMLP/best_weights\n",
      "The loss after 2 iterations is E=0.013029878195884964\n",
      "Wrote weights to file ./customFunctionLearningAdaptive/baselineMLP/best_weights\n",
      "The loss after 3 iterations is E=0.013029876145898855\n",
      "Wrote weights to file ./customFunctionLearningAdaptive/baselineMLP/best_weights\n",
      "The loss after 4 iterations is E=0.013029872059175836\n",
      "Wrote weights to file ./customFunctionLearningAdaptive/baselineMLP/best_weights\n",
      "The loss after 5 iterations is E=0.013029863788949818\n",
      "Wrote weights to file ./customFunctionLearningAdaptive/baselineMLP/best_weights\n",
      "The loss after 6 iterations is E=0.013029847063490656\n",
      "Wrote weights to file ./customFunctionLearningAdaptive/baselineMLP/best_weights\n",
      "The loss after 7 iterations is E=0.01302981356642126\n",
      "Wrote weights to file ./customFunctionLearningAdaptive/baselineMLP/best_weights\n",
      "The loss after 8 iterations is E=0.013029747339001377\n",
      "Wrote weights to file ./customFunctionLearningAdaptive/baselineMLP/best_weights\n",
      "The loss after 9 iterations is E=0.013029618112738952\n",
      "Wrote weights to file ./customFunctionLearningAdaptive/baselineMLP/best_weights\n",
      "The loss after 10 iterations is E=0.013029369114726406\n",
      "Wrote weights to file ./customFunctionLearningAdaptive/baselineMLP/best_weights\n",
      "The loss after 11 iterations is E=0.01302889499907918\n",
      "Wrote weights to file ./customFunctionLearningAdaptive/baselineMLP/best_weights\n",
      "The loss after 12 iterations is E=0.013028002329210005\n",
      "Wrote weights to file ./customFunctionLearningAdaptive/baselineMLP/best_weights\n",
      "The loss after 13 iterations is E=0.013026339631052541\n",
      "Wrote weights to file ./customFunctionLearningAdaptive/baselineMLP/best_weights\n",
      "The loss after 14 iterations is E=0.01302327513920616\n",
      "Wrote weights to file ./customFunctionLearningAdaptive/baselineMLP/best_weights\n",
      "The loss after 15 iterations is E=0.013017686021079255\n",
      "Wrote weights to file ./customFunctionLearningAdaptive/baselineMLP/best_weights\n",
      "The loss after 16 iterations is E=0.013007600412242087\n",
      "Wrote weights to file ./customFunctionLearningAdaptive/baselineMLP/best_weights\n",
      "The loss after 17 iterations is E=0.012989599717845515\n",
      "Wrote weights to file ./customFunctionLearningAdaptive/baselineMLP/best_weights\n",
      "The loss after 18 iterations is E=0.01295784011246147\n",
      "Wrote weights to file ./customFunctionLearningAdaptive/baselineMLP/best_weights\n",
      "The loss after 19 iterations is E=0.012902488213443346\n",
      "Wrote weights to file ./customFunctionLearningAdaptive/baselineMLP/best_weights\n",
      "The loss after 20 iterations is E=0.012807294175231576\n",
      "Wrote weights to file ./customFunctionLearningAdaptive/baselineMLP/best_weights\n",
      "The loss after 21 iterations is E=0.012645976189927195\n",
      "Wrote weights to file ./customFunctionLearningAdaptive/baselineMLP/best_weights\n",
      "The loss after 22 iterations is E=0.012377147437733304\n",
      "Wrote weights to file ./customFunctionLearningAdaptive/baselineMLP/best_weights\n",
      "The loss after 23 iterations is E=0.01193788238251717\n",
      "Wrote weights to file ./customFunctionLearningAdaptive/baselineMLP/best_weights\n",
      "The loss after 24 iterations is E=0.011237145868900138\n",
      "Wrote weights to file ./customFunctionLearningAdaptive/baselineMLP/best_weights\n",
      "The loss after 25 iterations is E=0.010153155817631803\n",
      "Wrote weights to file ./customFunctionLearningAdaptive/baselineMLP/best_weights\n",
      "The loss after 26 iterations is E=0.008545319162306369\n",
      "Using every 900th point for                         training.\n",
      "Selecting subset for training: 3 out of 2700 selected.\n",
      "Wrote weights to file ./customFunctionLearningAdaptive/baselineMLP/best_weights\n",
      "The loss after 27 iterations is E=0.012762093660713196\n",
      "Wrote weights to file ./customFunctionLearningAdaptive/baselineMLP/best_weights\n",
      "The loss after 28 iterations is E=0.0076063152634707745\n",
      "Using every 810th point for                         training.\n",
      "Selecting subset for training: 3 out of 2700 selected.\n",
      "Wrote weights to file ./customFunctionLearningAdaptive/baselineMLP/best_weights\n",
      "The loss after 29 iterations is E=1.3693047857413378e-05\n",
      "Using every 729th point for                         training.\n",
      "Selecting subset for training: 3 out of 2700 selected.\n",
      "Wrote weights to file ./customFunctionLearningAdaptive/baselineMLP/best_weights\n",
      "The loss after 30 iterations is E=0.001633705272756354\n",
      "Using every 656th point for                         training.\n",
      "Selecting subset for training: 4 out of 2700 selected.\n",
      "Wrote weights to file ./customFunctionLearningAdaptive/baselineMLP/best_weights\n",
      "The loss after 31 iterations is E=0.0032067761970187183\n",
      "Using every 590th point for                         training.\n",
      "Selecting subset for training: 4 out of 2700 selected.\n",
      "Wrote weights to file ./customFunctionLearningAdaptive/baselineMLP/best_weights\n",
      "The loss after 32 iterations is E=0.014570641569608259\n",
      "Wrote weights to file ./customFunctionLearningAdaptive/baselineMLP/best_weights\n",
      "The loss after 35 iterations is E=0.01153823054710922\n",
      "Wrote weights to file ./customFunctionLearningAdaptive/baselineMLP/best_weights\n",
      "The loss after 41 iterations is E=0.010318494248457825\n",
      "Wrote weights to file ./customFunctionLearningAdaptive/baselineMLP/best_weights\n",
      "The loss after 42 iterations is E=0.00948899401700605\n",
      "Using every 531th point for                         training.\n",
      "Selecting subset for training: 5 out of 2700 selected.\n",
      "Wrote weights to file ./customFunctionLearningAdaptive/baselineMLP/best_weights\n",
      "The loss after 43 iterations is E=0.011733961801885111\n",
      "Wrote weights to file ./customFunctionLearningAdaptive/baselineMLP/best_weights\n",
      "The loss after 44 iterations is E=0.005805025057471616\n",
      "Using every 477th point for                         training.\n",
      "Selecting subset for training: 5 out of 2700 selected.\n",
      "Wrote weights to file ./customFunctionLearningAdaptive/baselineMLP/best_weights\n",
      "The loss after 45 iterations is E=0.00503952360060454\n",
      "Using every 429th point for                         training.\n",
      "Selecting subset for training: 6 out of 2700 selected.\n",
      "Wrote weights to file ./customFunctionLearningAdaptive/baselineMLP/best_weights\n",
      "The loss after 46 iterations is E=0.0011030722208381148\n",
      "Using every 386th point for                         training.\n",
      "Selecting subset for training: 6 out of 2700 selected.\n",
      "Wrote weights to file ./customFunctionLearningAdaptive/baselineMLP/best_weights\n",
      "The loss after 47 iterations is E=0.0054245991172833415\n",
      "Using every 347th point for                         training.\n",
      "Selecting subset for training: 7 out of 2700 selected.\n",
      "Wrote weights to file ./customFunctionLearningAdaptive/baselineMLP/best_weights\n",
      "The loss after 48 iterations is E=0.0024893151637383776\n",
      "Using every 312th point for                         training.\n",
      "Selecting subset for training: 8 out of 2700 selected.\n",
      "Wrote weights to file ./customFunctionLearningAdaptive/baselineMLP/best_weights\n",
      "The loss after 49 iterations is E=0.012579635332902229\n",
      "Wrote weights to file ./customFunctionLearningAdaptive/baselineMLP/best_weights\n",
      "The loss after 50 iterations is E=0.011957728773048848\n",
      "Wrote weights to file ./customFunctionLearningAdaptive/baselineMLP/best_weights\n",
      "The loss after 51 iterations is E=0.009871989551913825\n",
      "Using every 280th point for                         training.\n",
      "Selecting subset for training: 9 out of 2700 selected.\n",
      "Wrote weights to file ./customFunctionLearningAdaptive/baselineMLP/best_weights\n",
      "The loss after 52 iterations is E=0.008820295113887664\n",
      "Using every 252th point for                         training.\n",
      "Selecting subset for training: 10 out of 2700 selected.\n",
      "Wrote weights to file ./customFunctionLearningAdaptive/baselineMLP/best_weights\n",
      "The loss after 53 iterations is E=0.0047776317703587074\n",
      "Using every 226th point for                         training.\n",
      "Selecting subset for training: 11 out of 2700 selected.\n",
      "Wrote weights to file ./customFunctionLearningAdaptive/baselineMLP/best_weights\n",
      "The loss after 54 iterations is E=0.004305501619282341\n",
      "Using every 203th point for                         training.\n",
      "Selecting subset for training: 13 out of 2700 selected.\n",
      "Wrote weights to file ./customFunctionLearningAdaptive/baselineMLP/best_weights\n",
      "The loss after 55 iterations is E=0.012115339514946049\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wrote weights to file ./customFunctionLearningAdaptive/baselineMLP/best_weights\n",
      "The loss after 58 iterations is E=0.01130656795950899\n",
      "Wrote weights to file ./customFunctionLearningAdaptive/baselineMLP/best_weights\n",
      "The loss after 59 iterations is E=0.01051298618378276\n",
      "Wrote weights to file ./customFunctionLearningAdaptive/baselineMLP/best_weights\n",
      "The loss after 64 iterations is E=0.010216368412766673\n",
      "Wrote weights to file ./customFunctionLearningAdaptive/baselineMLP/best_weights\n",
      "The loss after 65 iterations is E=0.009992729582317277\n",
      "Using every 182th point for                         training.\n",
      "Selecting subset for training: 14 out of 2700 selected.\n",
      "Wrote weights to file ./customFunctionLearningAdaptive/baselineMLP/best_weights\n",
      "The loss after 66 iterations is E=0.004867519492894525\n",
      "Using every 163th point for                         training.\n",
      "Selecting subset for training: 16 out of 2700 selected.\n",
      "Wrote weights to file ./customFunctionLearningAdaptive/baselineMLP/best_weights\n",
      "The loss after 67 iterations is E=0.019175454855759215\n",
      "Wrote weights to file ./customFunctionLearningAdaptive/baselineMLP/best_weights\n",
      "The loss after 68 iterations is E=0.01634618299088192\n",
      "Wrote weights to file ./customFunctionLearningAdaptive/baselineMLP/best_weights\n",
      "The loss after 69 iterations is E=0.015648344968795056\n",
      "Wrote weights to file ./customFunctionLearningAdaptive/baselineMLP/best_weights\n",
      "The loss after 74 iterations is E=0.015640501568640486\n",
      "Wrote weights to file ./customFunctionLearningAdaptive/baselineMLP/best_weights\n",
      "The loss after 75 iterations is E=0.015295641548022064\n",
      "Wrote weights to file ./customFunctionLearningAdaptive/baselineMLP/best_weights\n",
      "The loss after 80 iterations is E=0.014905889079337932\n",
      "Wrote weights to file ./customFunctionLearningAdaptive/baselineMLP/best_weights\n",
      "The loss after 85 iterations is E=0.014667250845666079\n",
      "Wrote weights to file ./customFunctionLearningAdaptive/baselineMLP/best_weights\n",
      "The loss after 86 iterations is E=0.014474822774797466\n",
      "Wrote weights to file ./customFunctionLearningAdaptive/baselineMLP/best_weights\n",
      "The loss after 90 iterations is E=0.01430799951668001\n",
      "Wrote weights to file ./customFunctionLearningAdaptive/baselineMLP/best_weights\n",
      "The loss after 91 iterations is E=0.014081859231027578\n",
      "Wrote weights to file ./customFunctionLearningAdaptive/baselineMLP/best_weights\n",
      "The loss after 95 iterations is E=0.013937554866013386\n",
      "Wrote weights to file ./customFunctionLearningAdaptive/baselineMLP/best_weights\n",
      "The loss after 96 iterations is E=0.013724375674843158\n",
      "Wrote weights to file ./customFunctionLearningAdaptive/baselineMLP/best_weights\n",
      "The loss after 97 iterations is E=0.013642797639785436\n",
      "Wrote weights to file ./customFunctionLearningAdaptive/baselineMLP/best_weights\n",
      "The loss after 100 iterations is E=0.013521837193587129\n",
      "Elapsed time:  0.02 min\n"
     ]
    }
   ],
   "source": [
    "start = time.time()\n",
    "customModel.solve(max_iter=100, learning_rate=0.01, verbose=1, adaptive=True, tolerance=0.01, every=1000)\n",
    "print(\"Elapsed time: {:5.2f} min\".format((time.time() - start) / 60.0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Initialized weights and biases for MLP with 20 parameters.\n",
      "\n",
      "MLP structure:\n",
      "--------------\n",
      "Input features:    2\n",
      "Hidden layers:     1\n",
      "Neurons per layer: 5\n",
      "Number of weights: 20\n",
      "Loaded weights from file ./customFunctionLearningAdaptive/baselineMLP/best_weights.npy\n",
      "-------------------\n",
      "L2 norm:  0.0004156393510664309\n",
      "Lmax norm:  0.10435761221395723\n"
     ]
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAuoAAAD3CAYAAABLsHHKAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x7fa642911128>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "baselineMLP = SimpleMLP(name='baselineMLP', number_of_inputs=2, neurons_per_layer=5, hidden_layers=1)\n",
    "customModel = CustomFunctionLearning(name=\"customFunctionLearningAdaptive\", mlp=baselineMLP, training_data=field50.A, beta=25.0, d_v=0.025)\n",
    "customModel.mlp.read_weights_from_disk(\"./customFunctionLearningAdaptive/baselineMLP/best_weights.npy\")\n",
    "evaluation_points = np.column_stack((field50.x, field50.y))\n",
    "prediction = customModel.predict(evaluation_points)\n",
    "plot_field_and_error(field50.x, field50.y, prediction, field50.A, \"customFunctionBaseline100.pdf\",\n",
    "                     [r\"Custom function after $100$ iterations\", \"Deviation from the numerical solution\"])\n",
    "l2_norm = customModel.l2_norm(evaluation_points, field50.A)\n",
    "lmax_norm = customModel.lmax_norm(evaluation_points, field50.A)\n",
    "print(\"-------------------\")\n",
    "print(\"L2 norm: \", l2_norm)\n",
    "print(\"Lmax norm: \", lmax_norm)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Custom function after 1000 iterations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Initialized weights and biases for MLP with 20 parameters.\n",
      "\n",
      "MLP structure:\n",
      "--------------\n",
      "Input features:    2\n",
      "Hidden layers:     1\n",
      "Neurons per layer: 5\n",
      "Number of weights: 20\n"
     ]
    }
   ],
   "source": [
    "baselineMLP = SimpleMLP(name='baselineMLP1000', number_of_inputs=2, neurons_per_layer=5, hidden_layers=1)\n",
    "customModel = CustomFunctionLearning(name=\"customFunctionLearningAdaptive\", mlp=baselineMLP, training_data=field50.A, beta=25.0, d_v=0.025)\n",
    "training_points = np.column_stack((field50.x, field50.y))\n",
    "customModel.set_control_points(training_points)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Selecting subset for training: 2 out of 2700 selected.\n",
      "\n",
      "The initial loss is 0.004174917303204653\n",
      "\n",
      "Wrote weights to file ./customFunctionLearningAdaptive/baselineMLP1000/best_weights\n",
      "The loss after 1 iterations is E=0.0041749173032041365\n",
      "Using every 900th point for                         training.\n",
      "Selecting subset for training: 3 out of 2700 selected.\n",
      "Wrote weights to file ./customFunctionLearningAdaptive/baselineMLP1000/best_weights\n",
      "The loss after 2 iterations is E=0.13128558100018836\n",
      "Wrote weights to file ./customFunctionLearningAdaptive/baselineMLP1000/best_weights\n",
      "The loss after 3 iterations is E=0.13111885591431688\n",
      "Wrote weights to file ./customFunctionLearningAdaptive/baselineMLP1000/best_weights\n",
      "The loss after 4 iterations is E=0.1308866853520578\n",
      "Wrote weights to file ./customFunctionLearningAdaptive/baselineMLP1000/best_weights\n",
      "The loss after 5 iterations is E=0.13057082515360766\n",
      "Wrote weights to file ./customFunctionLearningAdaptive/baselineMLP1000/best_weights\n",
      "The loss after 6 iterations is E=0.130146910213959\n",
      "Wrote weights to file ./customFunctionLearningAdaptive/baselineMLP1000/best_weights\n",
      "The loss after 7 iterations is E=0.12958363831257894\n",
      "Wrote weights to file ./customFunctionLearningAdaptive/baselineMLP1000/best_weights\n",
      "The loss after 8 iterations is E=0.12884128897332772\n",
      "Wrote weights to file ./customFunctionLearningAdaptive/baselineMLP1000/best_weights\n",
      "The loss after 9 iterations is E=0.12786986573632247\n",
      "Wrote weights to file ./customFunctionLearningAdaptive/baselineMLP1000/best_weights\n",
      "The loss after 10 iterations is E=0.12660697694711856\n",
      "Wrote weights to file ./customFunctionLearningAdaptive/baselineMLP1000/best_weights\n",
      "The loss after 11 iterations is E=0.12497557136489167\n",
      "Wrote weights to file ./customFunctionLearningAdaptive/baselineMLP1000/best_weights\n",
      "The loss after 12 iterations is E=0.12288171532177068\n",
      "Wrote weights to file ./customFunctionLearningAdaptive/baselineMLP1000/best_weights\n",
      "The loss after 13 iterations is E=0.12021272913426778\n",
      "Wrote weights to file ./customFunctionLearningAdaptive/baselineMLP1000/best_weights\n",
      "The loss after 14 iterations is E=0.11683620202805561\n",
      "Wrote weights to file ./customFunctionLearningAdaptive/baselineMLP1000/best_weights\n",
      "The loss after 15 iterations is E=0.11260069225467385\n",
      "Wrote weights to file ./customFunctionLearningAdaptive/baselineMLP1000/best_weights\n",
      "The loss after 16 iterations is E=0.107339304680583\n",
      "Wrote weights to file ./customFunctionLearningAdaptive/baselineMLP1000/best_weights\n",
      "The loss after 17 iterations is E=0.10087781769731359\n",
      "Wrote weights to file ./customFunctionLearningAdaptive/baselineMLP1000/best_weights\n",
      "The loss after 18 iterations is E=0.09304956000494939\n",
      "Wrote weights to file ./customFunctionLearningAdaptive/baselineMLP1000/best_weights\n",
      "The loss after 19 iterations is E=0.08371969790770488\n",
      "Wrote weights to file ./customFunctionLearningAdaptive/baselineMLP1000/best_weights\n",
      "The loss after 20 iterations is E=0.07282180066328324\n",
      "Wrote weights to file ./customFunctionLearningAdaptive/baselineMLP1000/best_weights\n",
      "The loss after 21 iterations is E=0.06040956986459136\n",
      "Wrote weights to file ./customFunctionLearningAdaptive/baselineMLP1000/best_weights\n",
      "The loss after 22 iterations is E=0.04672865105164881\n",
      "Wrote weights to file ./customFunctionLearningAdaptive/baselineMLP1000/best_weights\n",
      "The loss after 23 iterations is E=0.032330596867205165\n",
      "Wrote weights to file ./customFunctionLearningAdaptive/baselineMLP1000/best_weights\n",
      "The loss after 24 iterations is E=0.01834421453324666\n",
      "Wrote weights to file ./customFunctionLearningAdaptive/baselineMLP1000/best_weights\n",
      "The loss after 25 iterations is E=0.00742045156613165\n",
      "Using every 810th point for                         training.\n",
      "Selecting subset for training: 3 out of 2700 selected.\n",
      "Wrote weights to file ./customFunctionLearningAdaptive/baselineMLP1000/best_weights\n",
      "The loss after 26 iterations is E=0.007346235098580041\n",
      "Using every 729th point for                         training.\n",
      "Selecting subset for training: 3 out of 2700 selected.\n",
      "Wrote weights to file ./customFunctionLearningAdaptive/baselineMLP1000/best_weights\n",
      "The loss after 27 iterations is E=0.025224953302598378\n",
      "Wrote weights to file ./customFunctionLearningAdaptive/baselineMLP1000/best_weights\n",
      "The loss after 28 iterations is E=0.0171622314464371\n",
      "Wrote weights to file ./customFunctionLearningAdaptive/baselineMLP1000/best_weights\n",
      "The loss after 29 iterations is E=0.0068846759440709965\n",
      "Using every 656th point for                         training.\n",
      "Selecting subset for training: 4 out of 2700 selected.\n",
      "Wrote weights to file ./customFunctionLearningAdaptive/baselineMLP1000/best_weights\n",
      "The loss after 30 iterations is E=0.0002255532363093353\n",
      "Using every 590th point for                         training.\n",
      "Selecting subset for training: 4 out of 2700 selected.\n",
      "Wrote weights to file ./customFunctionLearningAdaptive/baselineMLP1000/best_weights\n",
      "The loss after 31 iterations is E=0.001364405635943426\n",
      "Using every 531th point for                         training.\n",
      "Selecting subset for training: 5 out of 2700 selected.\n",
      "Wrote weights to file ./customFunctionLearningAdaptive/baselineMLP1000/best_weights\n",
      "The loss after 32 iterations is E=0.0063874992718012885\n",
      "Using every 477th point for                         training.\n",
      "Selecting subset for training: 5 out of 2700 selected.\n",
      "Wrote weights to file ./customFunctionLearningAdaptive/baselineMLP1000/best_weights\n",
      "The loss after 33 iterations is E=0.013791100879687594\n",
      "Wrote weights to file ./customFunctionLearningAdaptive/baselineMLP1000/best_weights\n",
      "The loss after 39 iterations is E=0.012302311603352677\n",
      "Wrote weights to file ./customFunctionLearningAdaptive/baselineMLP1000/best_weights\n",
      "The loss after 40 iterations is E=0.010550247987954798\n",
      "Wrote weights to file ./customFunctionLearningAdaptive/baselineMLP1000/best_weights\n",
      "The loss after 41 iterations is E=0.0090493536835177\n",
      "Using every 429th point for                         training.\n",
      "Selecting subset for training: 6 out of 2700 selected.\n",
      "Wrote weights to file ./customFunctionLearningAdaptive/baselineMLP1000/best_weights\n",
      "The loss after 42 iterations is E=0.0011875465287869006\n",
      "Using every 386th point for                         training.\n",
      "Selecting subset for training: 6 out of 2700 selected.\n",
      "Wrote weights to file ./customFunctionLearningAdaptive/baselineMLP1000/best_weights\n",
      "The loss after 43 iterations is E=0.0030621358412305333\n",
      "Using every 347th point for                         training.\n",
      "Selecting subset for training: 7 out of 2700 selected.\n",
      "Wrote weights to file ./customFunctionLearningAdaptive/baselineMLP1000/best_weights\n",
      "The loss after 44 iterations is E=0.0066291681537097025\n",
      "Using every 312th point for                         training.\n",
      "Selecting subset for training: 8 out of 2700 selected.\n",
      "Wrote weights to file ./customFunctionLearningAdaptive/baselineMLP1000/best_weights\n",
      "The loss after 45 iterations is E=0.012667234639351379\n",
      "Wrote weights to file ./customFunctionLearningAdaptive/baselineMLP1000/best_weights\n",
      "The loss after 46 iterations is E=0.010106620864534406\n",
      "Wrote weights to file ./customFunctionLearningAdaptive/baselineMLP1000/best_weights\n",
      "The loss after 47 iterations is E=0.006804779204756\n",
      "Using every 280th point for                         training.\n",
      "Selecting subset for training: 9 out of 2700 selected.\n",
      "Wrote weights to file ./customFunctionLearningAdaptive/baselineMLP1000/best_weights\n",
      "The loss after 48 iterations is E=0.0023021241386326466\n",
      "Using every 252th point for                         training.\n",
      "Selecting subset for training: 10 out of 2700 selected.\n",
      "Wrote weights to file ./customFunctionLearningAdaptive/baselineMLP1000/best_weights\n",
      "The loss after 49 iterations is E=0.0006815955978066468\n",
      "Using every 226th point for                         training.\n",
      "Selecting subset for training: 11 out of 2700 selected.\n",
      "Wrote weights to file ./customFunctionLearningAdaptive/baselineMLP1000/best_weights\n",
      "The loss after 50 iterations is E=0.003629555982644511\n",
      "Using every 203th point for                         training.\n",
      "Selecting subset for training: 13 out of 2700 selected.\n",
      "Wrote weights to file ./customFunctionLearningAdaptive/baselineMLP1000/best_weights\n",
      "The loss after 51 iterations is E=0.019986310181612735\n",
      "Wrote weights to file ./customFunctionLearningAdaptive/baselineMLP1000/best_weights\n",
      "The loss after 52 iterations is E=0.01921222213897893\n",
      "Wrote weights to file ./customFunctionLearningAdaptive/baselineMLP1000/best_weights\n",
      "The loss after 53 iterations is E=0.016310065083216828\n",
      "Wrote weights to file ./customFunctionLearningAdaptive/baselineMLP1000/best_weights\n",
      "The loss after 54 iterations is E=0.012542271403532005\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wrote weights to file ./customFunctionLearningAdaptive/baselineMLP1000/best_weights\n",
      "The loss after 55 iterations is E=0.009659764738411491\n",
      "Using every 182th point for                         training.\n",
      "Selecting subset for training: 14 out of 2700 selected.\n",
      "Wrote weights to file ./customFunctionLearningAdaptive/baselineMLP1000/best_weights\n",
      "The loss after 56 iterations is E=0.006704000389638443\n",
      "Using every 163th point for                         training.\n",
      "Selecting subset for training: 16 out of 2700 selected.\n",
      "Wrote weights to file ./customFunctionLearningAdaptive/baselineMLP1000/best_weights\n",
      "The loss after 57 iterations is E=0.018300690137653085\n",
      "Wrote weights to file ./customFunctionLearningAdaptive/baselineMLP1000/best_weights\n",
      "The loss after 58 iterations is E=0.017896495842578106\n",
      "Wrote weights to file ./customFunctionLearningAdaptive/baselineMLP1000/best_weights\n",
      "The loss after 59 iterations is E=0.014233421365006077\n",
      "Wrote weights to file ./customFunctionLearningAdaptive/baselineMLP1000/best_weights\n",
      "The loss after 60 iterations is E=0.01261089926611627\n",
      "Wrote weights to file ./customFunctionLearningAdaptive/baselineMLP1000/best_weights\n",
      "The loss after 65 iterations is E=0.011834798933121827\n",
      "Wrote weights to file ./customFunctionLearningAdaptive/baselineMLP1000/best_weights\n",
      "The loss after 66 iterations is E=0.011786230449121565\n",
      "Wrote weights to file ./customFunctionLearningAdaptive/baselineMLP1000/best_weights\n",
      "The loss after 70 iterations is E=0.010883105429815281\n",
      "Wrote weights to file ./customFunctionLearningAdaptive/baselineMLP1000/best_weights\n",
      "The loss after 71 iterations is E=0.010873374535540622\n",
      "Wrote weights to file ./customFunctionLearningAdaptive/baselineMLP1000/best_weights\n",
      "The loss after 75 iterations is E=0.010207282561561176\n",
      "Wrote weights to file ./customFunctionLearningAdaptive/baselineMLP1000/best_weights\n",
      "The loss after 76 iterations is E=0.00994105134065767\n",
      "Using every 146th point for                         training.\n",
      "Selecting subset for training: 18 out of 2700 selected.\n",
      "Wrote weights to file ./customFunctionLearningAdaptive/baselineMLP1000/best_weights\n",
      "The loss after 77 iterations is E=0.020968915218685304\n",
      "Wrote weights to file ./customFunctionLearningAdaptive/baselineMLP1000/best_weights\n",
      "The loss after 78 iterations is E=0.01729806270689001\n",
      "Wrote weights to file ./customFunctionLearningAdaptive/baselineMLP1000/best_weights\n",
      "The loss after 79 iterations is E=0.015420420029249121\n",
      "Wrote weights to file ./customFunctionLearningAdaptive/baselineMLP1000/best_weights\n",
      "The loss after 86 iterations is E=0.014946025443070858\n",
      "Wrote weights to file ./customFunctionLearningAdaptive/baselineMLP1000/best_weights\n",
      "The loss after 87 iterations is E=0.01485926033538125\n",
      "Wrote weights to file ./customFunctionLearningAdaptive/baselineMLP1000/best_weights\n",
      "The loss after 92 iterations is E=0.014358651039063688\n",
      "Wrote weights to file ./customFunctionLearningAdaptive/baselineMLP1000/best_weights\n",
      "The loss after 93 iterations is E=0.014168070252020904\n",
      "Wrote weights to file ./customFunctionLearningAdaptive/baselineMLP1000/best_weights\n",
      "The loss after 98 iterations is E=0.01391631127606832\n",
      "Wrote weights to file ./customFunctionLearningAdaptive/baselineMLP1000/best_weights\n",
      "The loss after 99 iterations is E=0.01354275882434267\n",
      "Wrote weights to file ./customFunctionLearningAdaptive/baselineMLP1000/best_weights\n",
      "The loss after 100 iterations is E=0.013466937452202114\n",
      "Wrote weights to file ./customFunctionLearningAdaptive/baselineMLP1000/best_weights\n",
      "The loss after 104 iterations is E=0.013210790546101941\n",
      "Wrote weights to file ./customFunctionLearningAdaptive/baselineMLP1000/best_weights\n",
      "The loss after 105 iterations is E=0.01293315294379246\n",
      "Wrote weights to file ./customFunctionLearningAdaptive/baselineMLP1000/best_weights\n",
      "The loss after 106 iterations is E=0.01282470219295447\n",
      "Wrote weights to file ./customFunctionLearningAdaptive/baselineMLP1000/best_weights\n",
      "The loss after 109 iterations is E=0.012755248820504841\n",
      "Wrote weights to file ./customFunctionLearningAdaptive/baselineMLP1000/best_weights\n",
      "The loss after 110 iterations is E=0.012565085544970265\n",
      "Wrote weights to file ./customFunctionLearningAdaptive/baselineMLP1000/best_weights\n",
      "The loss after 111 iterations is E=0.012359586141144708\n",
      "Wrote weights to file ./customFunctionLearningAdaptive/baselineMLP1000/best_weights\n",
      "The loss after 112 iterations is E=0.01222356975930483\n",
      "Wrote weights to file ./customFunctionLearningAdaptive/baselineMLP1000/best_weights\n",
      "The loss after 113 iterations is E=0.012173938706752156\n",
      "Wrote weights to file ./customFunctionLearningAdaptive/baselineMLP1000/best_weights\n",
      "The loss after 114 iterations is E=0.012149789159330007\n",
      "Wrote weights to file ./customFunctionLearningAdaptive/baselineMLP1000/best_weights\n",
      "The loss after 115 iterations is E=0.012076162823321782\n",
      "Wrote weights to file ./customFunctionLearningAdaptive/baselineMLP1000/best_weights\n",
      "The loss after 116 iterations is E=0.011937739723647275\n",
      "Wrote weights to file ./customFunctionLearningAdaptive/baselineMLP1000/best_weights\n",
      "The loss after 117 iterations is E=0.011784178229784847\n",
      "Wrote weights to file ./customFunctionLearningAdaptive/baselineMLP1000/best_weights\n",
      "The loss after 118 iterations is E=0.011670486294899645\n",
      "Wrote weights to file ./customFunctionLearningAdaptive/baselineMLP1000/best_weights\n",
      "The loss after 119 iterations is E=0.011605408593463146\n",
      "Wrote weights to file ./customFunctionLearningAdaptive/baselineMLP1000/best_weights\n",
      "The loss after 120 iterations is E=0.011554457113138083\n",
      "Wrote weights to file ./customFunctionLearningAdaptive/baselineMLP1000/best_weights\n",
      "The loss after 121 iterations is E=0.011479409087746357\n",
      "Wrote weights to file ./customFunctionLearningAdaptive/baselineMLP1000/best_weights\n",
      "The loss after 122 iterations is E=0.011371346100823099\n",
      "Wrote weights to file ./customFunctionLearningAdaptive/baselineMLP1000/best_weights\n",
      "The loss after 123 iterations is E=0.011252731904741489\n",
      "Wrote weights to file ./customFunctionLearningAdaptive/baselineMLP1000/best_weights\n",
      "The loss after 124 iterations is E=0.011153026952358154\n",
      "Wrote weights to file ./customFunctionLearningAdaptive/baselineMLP1000/best_weights\n",
      "The loss after 125 iterations is E=0.011081366339604092\n",
      "Wrote weights to file ./customFunctionLearningAdaptive/baselineMLP1000/best_weights\n",
      "The loss after 126 iterations is E=0.011020562223992785\n",
      "Wrote weights to file ./customFunctionLearningAdaptive/baselineMLP1000/best_weights\n",
      "The loss after 127 iterations is E=0.010946881430254983\n",
      "Wrote weights to file ./customFunctionLearningAdaptive/baselineMLP1000/best_weights\n",
      "The loss after 128 iterations is E=0.010854115162654547\n",
      "Wrote weights to file ./customFunctionLearningAdaptive/baselineMLP1000/best_weights\n",
      "The loss after 129 iterations is E=0.01075678138545196\n",
      "Wrote weights to file ./customFunctionLearningAdaptive/baselineMLP1000/best_weights\n",
      "The loss after 130 iterations is E=0.010672160502647982\n",
      "Wrote weights to file ./customFunctionLearningAdaptive/baselineMLP1000/best_weights\n",
      "The loss after 131 iterations is E=0.010603185579352288\n",
      "Wrote weights to file ./customFunctionLearningAdaptive/baselineMLP1000/best_weights\n",
      "The loss after 132 iterations is E=0.010538643852228611\n",
      "Wrote weights to file ./customFunctionLearningAdaptive/baselineMLP1000/best_weights\n",
      "The loss after 133 iterations is E=0.010466442858229062\n",
      "Wrote weights to file ./customFunctionLearningAdaptive/baselineMLP1000/best_weights\n",
      "The loss after 134 iterations is E=0.010384893491189\n",
      "Wrote weights to file ./customFunctionLearningAdaptive/baselineMLP1000/best_weights\n",
      "The loss after 135 iterations is E=0.010302389041867359\n",
      "Wrote weights to file ./customFunctionLearningAdaptive/baselineMLP1000/best_weights\n",
      "The loss after 136 iterations is E=0.010227860606934903\n",
      "Wrote weights to file ./customFunctionLearningAdaptive/baselineMLP1000/best_weights\n",
      "The loss after 137 iterations is E=0.010162118089432336\n",
      "Wrote weights to file ./customFunctionLearningAdaptive/baselineMLP1000/best_weights\n",
      "The loss after 138 iterations is E=0.010098239931454756\n",
      "Wrote weights to file ./customFunctionLearningAdaptive/baselineMLP1000/best_weights\n",
      "The loss after 139 iterations is E=0.01002962834765777\n",
      "Wrote weights to file ./customFunctionLearningAdaptive/baselineMLP1000/best_weights\n",
      "The loss after 140 iterations is E=0.009956589297678085\n",
      "Using every 131th point for                         training.\n",
      "Selecting subset for training: 20 out of 2700 selected.\n",
      "Wrote weights to file ./customFunctionLearningAdaptive/baselineMLP1000/best_weights\n",
      "The loss after 141 iterations is E=0.009406061633168013\n",
      "Using every 117th point for                         training.\n",
      "Selecting subset for training: 23 out of 2700 selected.\n",
      "Wrote weights to file ./customFunctionLearningAdaptive/baselineMLP1000/best_weights\n",
      "The loss after 142 iterations is E=0.007346363307049406\n",
      "Using every 105th point for                         training.\n",
      "Selecting subset for training: 25 out of 2700 selected.\n",
      "Wrote weights to file ./customFunctionLearningAdaptive/baselineMLP1000/best_weights\n",
      "The loss after 143 iterations is E=0.010783792148148393\n",
      "Wrote weights to file ./customFunctionLearningAdaptive/baselineMLP1000/best_weights\n",
      "The loss after 145 iterations is E=0.010382646665978672\n",
      "Wrote weights to file ./customFunctionLearningAdaptive/baselineMLP1000/best_weights\n",
      "The loss after 146 iterations is E=0.009509759869250007\n",
      "Using every 94th point for                         training.\n",
      "Selecting subset for training: 28 out of 2700 selected.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wrote weights to file ./customFunctionLearningAdaptive/baselineMLP1000/best_weights\n",
      "The loss after 147 iterations is E=0.011110437280053153\n",
      "Wrote weights to file ./customFunctionLearningAdaptive/baselineMLP1000/best_weights\n",
      "The loss after 151 iterations is E=0.011044495954371698\n",
      "Wrote weights to file ./customFunctionLearningAdaptive/baselineMLP1000/best_weights\n",
      "The loss after 152 iterations is E=0.010952829052889537\n",
      "Wrote weights to file ./customFunctionLearningAdaptive/baselineMLP1000/best_weights\n",
      "The loss after 156 iterations is E=0.010679552236277838\n",
      "Wrote weights to file ./customFunctionLearningAdaptive/baselineMLP1000/best_weights\n",
      "The loss after 160 iterations is E=0.010613467668347564\n",
      "Wrote weights to file ./customFunctionLearningAdaptive/baselineMLP1000/best_weights\n",
      "The loss after 161 iterations is E=0.010484386149727392\n",
      "Wrote weights to file ./customFunctionLearningAdaptive/baselineMLP1000/best_weights\n",
      "The loss after 165 iterations is E=0.01030688064567326\n",
      "Wrote weights to file ./customFunctionLearningAdaptive/baselineMLP1000/best_weights\n",
      "The loss after 169 iterations is E=0.010191118468972624\n",
      "Wrote weights to file ./customFunctionLearningAdaptive/baselineMLP1000/best_weights\n",
      "The loss after 170 iterations is E=0.010108709258192485\n",
      "Wrote weights to file ./customFunctionLearningAdaptive/baselineMLP1000/best_weights\n",
      "The loss after 173 iterations is E=0.010060447346506907\n",
      "Wrote weights to file ./customFunctionLearningAdaptive/baselineMLP1000/best_weights\n",
      "The loss after 174 iterations is E=0.009956599554499452\n",
      "Using every 84th point for                         training.\n",
      "Selecting subset for training: 32 out of 2700 selected.\n",
      "Wrote weights to file ./customFunctionLearningAdaptive/baselineMLP1000/best_weights\n",
      "The loss after 175 iterations is E=0.006550539290881382\n",
      "Using every 75th point for                         training.\n",
      "Selecting subset for training: 36 out of 2700 selected.\n",
      "Wrote weights to file ./customFunctionLearningAdaptive/baselineMLP1000/best_weights\n",
      "The loss after 176 iterations is E=0.008731667863545912\n",
      "Using every 67th point for                         training.\n",
      "Selecting subset for training: 40 out of 2700 selected.\n",
      "Wrote weights to file ./customFunctionLearningAdaptive/baselineMLP1000/best_weights\n",
      "The loss after 177 iterations is E=0.014519734438705207\n",
      "Wrote weights to file ./customFunctionLearningAdaptive/baselineMLP1000/best_weights\n",
      "The loss after 179 iterations is E=0.014509521963155783\n",
      "Wrote weights to file ./customFunctionLearningAdaptive/baselineMLP1000/best_weights\n",
      "The loss after 180 iterations is E=0.01329083820664322\n",
      "Wrote weights to file ./customFunctionLearningAdaptive/baselineMLP1000/best_weights\n",
      "The loss after 183 iterations is E=0.012673442966533159\n",
      "Wrote weights to file ./customFunctionLearningAdaptive/baselineMLP1000/best_weights\n",
      "The loss after 184 iterations is E=0.011781463810806718\n",
      "Wrote weights to file ./customFunctionLearningAdaptive/baselineMLP1000/best_weights\n",
      "The loss after 186 iterations is E=0.01168786084970721\n",
      "Wrote weights to file ./customFunctionLearningAdaptive/baselineMLP1000/best_weights\n",
      "The loss after 187 iterations is E=0.010863785766104595\n",
      "Wrote weights to file ./customFunctionLearningAdaptive/baselineMLP1000/best_weights\n",
      "The loss after 188 iterations is E=0.010271088651223028\n",
      "Wrote weights to file ./customFunctionLearningAdaptive/baselineMLP1000/best_weights\n",
      "The loss after 189 iterations is E=0.010209785626231656\n",
      "Wrote weights to file ./customFunctionLearningAdaptive/baselineMLP1000/best_weights\n",
      "The loss after 190 iterations is E=0.009991274921725671\n",
      "Using every 60th point for                         training.\n",
      "Selecting subset for training: 45 out of 2700 selected.\n",
      "Wrote weights to file ./customFunctionLearningAdaptive/baselineMLP1000/best_weights\n",
      "The loss after 191 iterations is E=0.016853435117414165\n",
      "Wrote weights to file ./customFunctionLearningAdaptive/baselineMLP1000/best_weights\n",
      "The loss after 194 iterations is E=0.016334951887045437\n",
      "Wrote weights to file ./customFunctionLearningAdaptive/baselineMLP1000/best_weights\n",
      "The loss after 197 iterations is E=0.015832618084400406\n",
      "Wrote weights to file ./customFunctionLearningAdaptive/baselineMLP1000/best_weights\n",
      "The loss after 199 iterations is E=0.015729224401895694\n",
      "Wrote weights to file ./customFunctionLearningAdaptive/baselineMLP1000/best_weights\n",
      "The loss after 200 iterations is E=0.01529307644061142\n",
      "Wrote weights to file ./customFunctionLearningAdaptive/baselineMLP1000/best_weights\n",
      "The loss after 202 iterations is E=0.015105406967066427\n",
      "Wrote weights to file ./customFunctionLearningAdaptive/baselineMLP1000/best_weights\n",
      "The loss after 203 iterations is E=0.01475703777206802\n",
      "Wrote weights to file ./customFunctionLearningAdaptive/baselineMLP1000/best_weights\n",
      "The loss after 205 iterations is E=0.014515981481967356\n",
      "Wrote weights to file ./customFunctionLearningAdaptive/baselineMLP1000/best_weights\n",
      "The loss after 206 iterations is E=0.014212152154237252\n",
      "Wrote weights to file ./customFunctionLearningAdaptive/baselineMLP1000/best_weights\n",
      "The loss after 207 iterations is E=0.014205443416851548\n",
      "Wrote weights to file ./customFunctionLearningAdaptive/baselineMLP1000/best_weights\n",
      "The loss after 208 iterations is E=0.013938354088242655\n",
      "Wrote weights to file ./customFunctionLearningAdaptive/baselineMLP1000/best_weights\n",
      "The loss after 209 iterations is E=0.013685826976427732\n",
      "Wrote weights to file ./customFunctionLearningAdaptive/baselineMLP1000/best_weights\n",
      "The loss after 210 iterations is E=0.013637424798522433\n",
      "Wrote weights to file ./customFunctionLearningAdaptive/baselineMLP1000/best_weights\n",
      "The loss after 211 iterations is E=0.013396838593785624\n",
      "Wrote weights to file ./customFunctionLearningAdaptive/baselineMLP1000/best_weights\n",
      "The loss after 212 iterations is E=0.013176343453037994\n",
      "Wrote weights to file ./customFunctionLearningAdaptive/baselineMLP1000/best_weights\n",
      "The loss after 213 iterations is E=0.013104715223090174\n",
      "Wrote weights to file ./customFunctionLearningAdaptive/baselineMLP1000/best_weights\n",
      "The loss after 214 iterations is E=0.01288404681347599\n",
      "Wrote weights to file ./customFunctionLearningAdaptive/baselineMLP1000/best_weights\n",
      "The loss after 215 iterations is E=0.012698104094236886\n",
      "Wrote weights to file ./customFunctionLearningAdaptive/baselineMLP1000/best_weights\n",
      "The loss after 216 iterations is E=0.012609890345393424\n",
      "Wrote weights to file ./customFunctionLearningAdaptive/baselineMLP1000/best_weights\n",
      "The loss after 217 iterations is E=0.012411258731819946\n",
      "Wrote weights to file ./customFunctionLearningAdaptive/baselineMLP1000/best_weights\n",
      "The loss after 218 iterations is E=0.012249186324411552\n",
      "Wrote weights to file ./customFunctionLearningAdaptive/baselineMLP1000/best_weights\n",
      "The loss after 219 iterations is E=0.012153394858797642\n",
      "Wrote weights to file ./customFunctionLearningAdaptive/baselineMLP1000/best_weights\n",
      "The loss after 220 iterations is E=0.011973447909878672\n",
      "Wrote weights to file ./customFunctionLearningAdaptive/baselineMLP1000/best_weights\n",
      "The loss after 221 iterations is E=0.01183477969235239\n",
      "Wrote weights to file ./customFunctionLearningAdaptive/baselineMLP1000/best_weights\n",
      "The loss after 222 iterations is E=0.011734176803686887\n",
      "Wrote weights to file ./customFunctionLearningAdaptive/baselineMLP1000/best_weights\n",
      "The loss after 223 iterations is E=0.01157328377753252\n",
      "Wrote weights to file ./customFunctionLearningAdaptive/baselineMLP1000/best_weights\n",
      "The loss after 224 iterations is E=0.011451240579838151\n",
      "Wrote weights to file ./customFunctionLearningAdaptive/baselineMLP1000/best_weights\n",
      "The loss after 225 iterations is E=0.011349912683952693\n",
      "Wrote weights to file ./customFunctionLearningAdaptive/baselineMLP1000/best_weights\n",
      "The loss after 226 iterations is E=0.011205863181302143\n",
      "Wrote weights to file ./customFunctionLearningAdaptive/baselineMLP1000/best_weights\n",
      "The loss after 227 iterations is E=0.01109850983204717\n",
      "Wrote weights to file ./customFunctionLearningAdaptive/baselineMLP1000/best_weights\n",
      "The loss after 228 iterations is E=0.010997726903765645\n",
      "Wrote weights to file ./customFunctionLearningAdaptive/baselineMLP1000/best_weights\n",
      "The loss after 229 iterations is E=0.010869484866533842\n",
      "Wrote weights to file ./customFunctionLearningAdaptive/baselineMLP1000/best_weights\n",
      "The loss after 230 iterations is E=0.010772520351735588\n",
      "Wrote weights to file ./customFunctionLearningAdaptive/baselineMLP1000/best_weights\n",
      "The loss after 231 iterations is E=0.010674118518120269\n",
      "Wrote weights to file ./customFunctionLearningAdaptive/baselineMLP1000/best_weights\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The loss after 232 iterations is E=0.010559849670653572\n",
      "Wrote weights to file ./customFunctionLearningAdaptive/baselineMLP1000/best_weights\n",
      "The loss after 233 iterations is E=0.010471193836756662\n",
      "Wrote weights to file ./customFunctionLearningAdaptive/baselineMLP1000/best_weights\n",
      "The loss after 234 iterations is E=0.010376054691257755\n",
      "Wrote weights to file ./customFunctionLearningAdaptive/baselineMLP1000/best_weights\n",
      "The loss after 235 iterations is E=0.010273970495134517\n",
      "Wrote weights to file ./customFunctionLearningAdaptive/baselineMLP1000/best_weights\n",
      "The loss after 236 iterations is E=0.01019125601734064\n",
      "Wrote weights to file ./customFunctionLearningAdaptive/baselineMLP1000/best_weights\n",
      "The loss after 237 iterations is E=0.01010026837327476\n",
      "Wrote weights to file ./customFunctionLearningAdaptive/baselineMLP1000/best_weights\n",
      "The loss after 238 iterations is E=0.010008699706421712\n",
      "Wrote weights to file ./customFunctionLearningAdaptive/baselineMLP1000/best_weights\n",
      "The loss after 239 iterations is E=0.009930560576305161\n",
      "Using every 54th point for                         training.\n",
      "Selecting subset for training: 50 out of 2700 selected.\n",
      "Wrote weights to file ./customFunctionLearningAdaptive/baselineMLP1000/best_weights\n",
      "The loss after 240 iterations is E=0.0031878920939203866\n",
      "Using every 48th point for                         training.\n",
      "Selecting subset for training: 56 out of 2700 selected.\n",
      "Wrote weights to file ./customFunctionLearningAdaptive/baselineMLP1000/best_weights\n",
      "The loss after 241 iterations is E=0.004649085169997612\n",
      "Using every 43th point for                         training.\n",
      "Selecting subset for training: 62 out of 2700 selected.\n",
      "Wrote weights to file ./customFunctionLearningAdaptive/baselineMLP1000/best_weights\n",
      "The loss after 242 iterations is E=0.013427456075627023\n",
      "Wrote weights to file ./customFunctionLearningAdaptive/baselineMLP1000/best_weights\n",
      "The loss after 246 iterations is E=0.012789208019075774\n",
      "Wrote weights to file ./customFunctionLearningAdaptive/baselineMLP1000/best_weights\n",
      "The loss after 249 iterations is E=0.01240769068987741\n",
      "Wrote weights to file ./customFunctionLearningAdaptive/baselineMLP1000/best_weights\n",
      "The loss after 255 iterations is E=0.012357549820216848\n",
      "Wrote weights to file ./customFunctionLearningAdaptive/baselineMLP1000/best_weights\n",
      "The loss after 256 iterations is E=0.011897003857587869\n",
      "Wrote weights to file ./customFunctionLearningAdaptive/baselineMLP1000/best_weights\n",
      "The loss after 259 iterations is E=0.011582394980239337\n",
      "Wrote weights to file ./customFunctionLearningAdaptive/baselineMLP1000/best_weights\n",
      "The loss after 262 iterations is E=0.011421516595594872\n",
      "Wrote weights to file ./customFunctionLearningAdaptive/baselineMLP1000/best_weights\n",
      "The loss after 265 iterations is E=0.0113223564676083\n",
      "Wrote weights to file ./customFunctionLearningAdaptive/baselineMLP1000/best_weights\n",
      "The loss after 266 iterations is E=0.011075759541070209\n",
      "Wrote weights to file ./customFunctionLearningAdaptive/baselineMLP1000/best_weights\n",
      "The loss after 269 iterations is E=0.010862761462915748\n",
      "Wrote weights to file ./customFunctionLearningAdaptive/baselineMLP1000/best_weights\n",
      "The loss after 272 iterations is E=0.010704357427720695\n",
      "Wrote weights to file ./customFunctionLearningAdaptive/baselineMLP1000/best_weights\n",
      "The loss after 273 iterations is E=0.01067501826860887\n",
      "Wrote weights to file ./customFunctionLearningAdaptive/baselineMLP1000/best_weights\n",
      "The loss after 275 iterations is E=0.010553307858532868\n",
      "Wrote weights to file ./customFunctionLearningAdaptive/baselineMLP1000/best_weights\n",
      "The loss after 276 iterations is E=0.01046130979933651\n",
      "Wrote weights to file ./customFunctionLearningAdaptive/baselineMLP1000/best_weights\n",
      "The loss after 278 iterations is E=0.010405770526164952\n",
      "Wrote weights to file ./customFunctionLearningAdaptive/baselineMLP1000/best_weights\n",
      "The loss after 279 iterations is E=0.010283313614802336\n",
      "Wrote weights to file ./customFunctionLearningAdaptive/baselineMLP1000/best_weights\n",
      "The loss after 281 iterations is E=0.01024765343144397\n",
      "Wrote weights to file ./customFunctionLearningAdaptive/baselineMLP1000/best_weights\n",
      "The loss after 282 iterations is E=0.01013057610278627\n",
      "Wrote weights to file ./customFunctionLearningAdaptive/baselineMLP1000/best_weights\n",
      "The loss after 283 iterations is E=0.010111616886083407\n",
      "Wrote weights to file ./customFunctionLearningAdaptive/baselineMLP1000/best_weights\n",
      "The loss after 284 iterations is E=0.010087422257659948\n",
      "Wrote weights to file ./customFunctionLearningAdaptive/baselineMLP1000/best_weights\n",
      "The loss after 285 iterations is E=0.009990318765379183\n",
      "Using every 38th point for                         training.\n",
      "Selecting subset for training: 71 out of 2700 selected.\n",
      "Wrote weights to file ./customFunctionLearningAdaptive/baselineMLP1000/best_weights\n",
      "The loss after 286 iterations is E=0.007753599175194677\n",
      "Using every 34th point for                         training.\n",
      "Selecting subset for training: 79 out of 2700 selected.\n",
      "Wrote weights to file ./customFunctionLearningAdaptive/baselineMLP1000/best_weights\n",
      "The loss after 287 iterations is E=0.007051911895868685\n",
      "Using every 30th point for                         training.\n",
      "Selecting subset for training: 90 out of 2700 selected.\n",
      "Wrote weights to file ./customFunctionLearningAdaptive/baselineMLP1000/best_weights\n",
      "The loss after 288 iterations is E=0.013903440081603534\n",
      "Wrote weights to file ./customFunctionLearningAdaptive/baselineMLP1000/best_weights\n",
      "The loss after 289 iterations is E=0.01385104242692855\n",
      "Wrote weights to file ./customFunctionLearningAdaptive/baselineMLP1000/best_weights\n",
      "The loss after 290 iterations is E=0.013702767351264987\n",
      "Wrote weights to file ./customFunctionLearningAdaptive/baselineMLP1000/best_weights\n",
      "The loss after 291 iterations is E=0.013546736699662497\n",
      "Wrote weights to file ./customFunctionLearningAdaptive/baselineMLP1000/best_weights\n",
      "The loss after 292 iterations is E=0.01347386117212046\n",
      "Wrote weights to file ./customFunctionLearningAdaptive/baselineMLP1000/best_weights\n",
      "The loss after 293 iterations is E=0.013273223546021742\n",
      "Wrote weights to file ./customFunctionLearningAdaptive/baselineMLP1000/best_weights\n",
      "The loss after 294 iterations is E=0.013219844340606995\n",
      "Wrote weights to file ./customFunctionLearningAdaptive/baselineMLP1000/best_weights\n",
      "The loss after 295 iterations is E=0.013041174739751773\n",
      "Wrote weights to file ./customFunctionLearningAdaptive/baselineMLP1000/best_weights\n",
      "The loss after 296 iterations is E=0.012962671070565238\n",
      "Wrote weights to file ./customFunctionLearningAdaptive/baselineMLP1000/best_weights\n",
      "The loss after 297 iterations is E=0.012840568941223119\n",
      "Wrote weights to file ./customFunctionLearningAdaptive/baselineMLP1000/best_weights\n",
      "The loss after 298 iterations is E=0.012728057414463773\n",
      "Wrote weights to file ./customFunctionLearningAdaptive/baselineMLP1000/best_weights\n",
      "The loss after 299 iterations is E=0.012656638475392763\n",
      "Wrote weights to file ./customFunctionLearningAdaptive/baselineMLP1000/best_weights\n",
      "The loss after 300 iterations is E=0.012532218404731121\n",
      "Wrote weights to file ./customFunctionLearningAdaptive/baselineMLP1000/best_weights\n",
      "The loss after 301 iterations is E=0.012482227040133183\n",
      "Wrote weights to file ./customFunctionLearningAdaptive/baselineMLP1000/best_weights\n",
      "The loss after 302 iterations is E=0.012375910926228405\n",
      "Wrote weights to file ./customFunctionLearningAdaptive/baselineMLP1000/best_weights\n",
      "The loss after 303 iterations is E=0.012320625495055615\n",
      "Wrote weights to file ./customFunctionLearningAdaptive/baselineMLP1000/best_weights\n",
      "The loss after 304 iterations is E=0.01224785641521674\n",
      "Wrote weights to file ./customFunctionLearningAdaptive/baselineMLP1000/best_weights\n",
      "The loss after 305 iterations is E=0.012180214982999355\n",
      "Wrote weights to file ./customFunctionLearningAdaptive/baselineMLP1000/best_weights\n",
      "The loss after 306 iterations is E=0.01213525741079704\n",
      "Wrote weights to file ./customFunctionLearningAdaptive/baselineMLP1000/best_weights\n",
      "The loss after 307 iterations is E=0.012064963645470637\n",
      "Wrote weights to file ./customFunctionLearningAdaptive/baselineMLP1000/best_weights\n",
      "The loss after 308 iterations is E=0.01203117675220523\n",
      "Wrote weights to file ./customFunctionLearningAdaptive/baselineMLP1000/best_weights\n",
      "The loss after 309 iterations is E=0.011971559002051466\n",
      "Wrote weights to file ./customFunctionLearningAdaptive/baselineMLP1000/best_weights\n",
      "The loss after 310 iterations is E=0.011935532890321605\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wrote weights to file ./customFunctionLearningAdaptive/baselineMLP1000/best_weights\n",
      "The loss after 311 iterations is E=0.01189233889934809\n",
      "Wrote weights to file ./customFunctionLearningAdaptive/baselineMLP1000/best_weights\n",
      "The loss after 312 iterations is E=0.011850480905659395\n",
      "Wrote weights to file ./customFunctionLearningAdaptive/baselineMLP1000/best_weights\n",
      "The loss after 313 iterations is E=0.011820053783841883\n",
      "Wrote weights to file ./customFunctionLearningAdaptive/baselineMLP1000/best_weights\n",
      "The loss after 314 iterations is E=0.011776850691813591\n",
      "Wrote weights to file ./customFunctionLearningAdaptive/baselineMLP1000/best_weights\n",
      "The loss after 315 iterations is E=0.011751025572985928\n",
      "Wrote weights to file ./customFunctionLearningAdaptive/baselineMLP1000/best_weights\n",
      "The loss after 316 iterations is E=0.011712530902155542\n",
      "Wrote weights to file ./customFunctionLearningAdaptive/baselineMLP1000/best_weights\n",
      "The loss after 317 iterations is E=0.011684889438653177\n",
      "Wrote weights to file ./customFunctionLearningAdaptive/baselineMLP1000/best_weights\n",
      "The loss after 318 iterations is E=0.011653734889950043\n",
      "Wrote weights to file ./customFunctionLearningAdaptive/baselineMLP1000/best_weights\n",
      "The loss after 319 iterations is E=0.01162281830116361\n",
      "Wrote weights to file ./customFunctionLearningAdaptive/baselineMLP1000/best_weights\n",
      "The loss after 320 iterations is E=0.01159723158273554\n",
      "Wrote weights to file ./customFunctionLearningAdaptive/baselineMLP1000/best_weights\n",
      "The loss after 321 iterations is E=0.011565294958087842\n",
      "Wrote weights to file ./customFunctionLearningAdaptive/baselineMLP1000/best_weights\n",
      "The loss after 322 iterations is E=0.011541559398517308\n",
      "Wrote weights to file ./customFunctionLearningAdaptive/baselineMLP1000/best_weights\n",
      "The loss after 323 iterations is E=0.011511565000576094\n",
      "Wrote weights to file ./customFunctionLearningAdaptive/baselineMLP1000/best_weights\n",
      "The loss after 324 iterations is E=0.01148692113555034\n",
      "Wrote weights to file ./customFunctionLearningAdaptive/baselineMLP1000/best_weights\n",
      "The loss after 325 iterations is E=0.011460228345035834\n",
      "Wrote weights to file ./customFunctionLearningAdaptive/baselineMLP1000/best_weights\n",
      "The loss after 326 iterations is E=0.011434057306035922\n",
      "Wrote weights to file ./customFunctionLearningAdaptive/baselineMLP1000/best_weights\n",
      "The loss after 327 iterations is E=0.011410117607379157\n",
      "Wrote weights to file ./customFunctionLearningAdaptive/baselineMLP1000/best_weights\n",
      "The loss after 328 iterations is E=0.011383484938377044\n",
      "Wrote weights to file ./customFunctionLearningAdaptive/baselineMLP1000/best_weights\n",
      "The loss after 329 iterations is E=0.011360819419631006\n",
      "Wrote weights to file ./customFunctionLearningAdaptive/baselineMLP1000/best_weights\n",
      "The loss after 330 iterations is E=0.011335158208727505\n",
      "Wrote weights to file ./customFunctionLearningAdaptive/baselineMLP1000/best_weights\n",
      "The loss after 331 iterations is E=0.011312536595952853\n",
      "Wrote weights to file ./customFunctionLearningAdaptive/baselineMLP1000/best_weights\n",
      "The loss after 332 iterations is E=0.011288678962680647\n",
      "Wrote weights to file ./customFunctionLearningAdaptive/baselineMLP1000/best_weights\n",
      "The loss after 333 iterations is E=0.011265762426264734\n",
      "Wrote weights to file ./customFunctionLearningAdaptive/baselineMLP1000/best_weights\n",
      "The loss after 334 iterations is E=0.011243669613048282\n",
      "Wrote weights to file ./customFunctionLearningAdaptive/baselineMLP1000/best_weights\n",
      "The loss after 335 iterations is E=0.011220846720680073\n",
      "Wrote weights to file ./customFunctionLearningAdaptive/baselineMLP1000/best_weights\n",
      "The loss after 336 iterations is E=0.011199969950055743\n",
      "Wrote weights to file ./customFunctionLearningAdaptive/baselineMLP1000/best_weights\n",
      "The loss after 337 iterations is E=0.01117788994972085\n",
      "Wrote weights to file ./customFunctionLearningAdaptive/baselineMLP1000/best_weights\n",
      "The loss after 338 iterations is E=0.011157677203301629\n",
      "Wrote weights to file ./customFunctionLearningAdaptive/baselineMLP1000/best_weights\n",
      "The loss after 339 iterations is E=0.011136788588269976\n",
      "Wrote weights to file ./customFunctionLearningAdaptive/baselineMLP1000/best_weights\n",
      "The loss after 340 iterations is E=0.01111696392277197\n",
      "Wrote weights to file ./customFunctionLearningAdaptive/baselineMLP1000/best_weights\n",
      "The loss after 341 iterations is E=0.011097368692422076\n",
      "Wrote weights to file ./customFunctionLearningAdaptive/baselineMLP1000/best_weights\n",
      "The loss after 342 iterations is E=0.01107798647665458\n",
      "Wrote weights to file ./customFunctionLearningAdaptive/baselineMLP1000/best_weights\n",
      "The loss after 343 iterations is E=0.011059509466042235\n",
      "Wrote weights to file ./customFunctionLearningAdaptive/baselineMLP1000/best_weights\n",
      "The loss after 344 iterations is E=0.011040777257533222\n",
      "Wrote weights to file ./customFunctionLearningAdaptive/baselineMLP1000/best_weights\n",
      "The loss after 345 iterations is E=0.011023148354328521\n",
      "Wrote weights to file ./customFunctionLearningAdaptive/baselineMLP1000/best_weights\n",
      "The loss after 346 iterations is E=0.011005270586387035\n",
      "Wrote weights to file ./customFunctionLearningAdaptive/baselineMLP1000/best_weights\n",
      "The loss after 347 iterations is E=0.010988289533957274\n",
      "Wrote weights to file ./customFunctionLearningAdaptive/baselineMLP1000/best_weights\n",
      "The loss after 348 iterations is E=0.010971345478335802\n",
      "Wrote weights to file ./customFunctionLearningAdaptive/baselineMLP1000/best_weights\n",
      "The loss after 349 iterations is E=0.010954921634510617\n",
      "Wrote weights to file ./customFunctionLearningAdaptive/baselineMLP1000/best_weights\n",
      "The loss after 350 iterations is E=0.010938868917039034\n",
      "Wrote weights to file ./customFunctionLearningAdaptive/baselineMLP1000/best_weights\n",
      "The loss after 351 iterations is E=0.010923018095473103\n",
      "Wrote weights to file ./customFunctionLearningAdaptive/baselineMLP1000/best_weights\n",
      "The loss after 352 iterations is E=0.010907739894162953\n",
      "Wrote weights to file ./customFunctionLearningAdaptive/baselineMLP1000/best_weights\n",
      "The loss after 353 iterations is E=0.010892506412701194\n",
      "Wrote weights to file ./customFunctionLearningAdaptive/baselineMLP1000/best_weights\n",
      "The loss after 354 iterations is E=0.010877871412811975\n",
      "Wrote weights to file ./customFunctionLearningAdaptive/baselineMLP1000/best_weights\n",
      "The loss after 355 iterations is E=0.01086329226187927\n",
      "Wrote weights to file ./customFunctionLearningAdaptive/baselineMLP1000/best_weights\n",
      "The loss after 356 iterations is E=0.010849206691165394\n",
      "Wrote weights to file ./customFunctionLearningAdaptive/baselineMLP1000/best_weights\n",
      "The loss after 357 iterations is E=0.010835274239759309\n",
      "Wrote weights to file ./customFunctionLearningAdaptive/baselineMLP1000/best_weights\n",
      "The loss after 358 iterations is E=0.010821681414389838\n",
      "Wrote weights to file ./customFunctionLearningAdaptive/baselineMLP1000/best_weights\n",
      "The loss after 359 iterations is E=0.010808353797876383\n",
      "Wrote weights to file ./customFunctionLearningAdaptive/baselineMLP1000/best_weights\n",
      "The loss after 360 iterations is E=0.010795237763630583\n",
      "Wrote weights to file ./customFunctionLearningAdaptive/baselineMLP1000/best_weights\n",
      "The loss after 361 iterations is E=0.010782453428920936\n",
      "Wrote weights to file ./customFunctionLearningAdaptive/baselineMLP1000/best_weights\n",
      "The loss after 362 iterations is E=0.010769807027780496\n",
      "Wrote weights to file ./customFunctionLearningAdaptive/baselineMLP1000/best_weights\n",
      "The loss after 363 iterations is E=0.01075750187682721\n",
      "Wrote weights to file ./customFunctionLearningAdaptive/baselineMLP1000/best_weights\n",
      "The loss after 364 iterations is E=0.010745322169135582\n",
      "Wrote weights to file ./customFunctionLearningAdaptive/baselineMLP1000/best_weights\n",
      "The loss after 365 iterations is E=0.01073344743807405\n",
      "Wrote weights to file ./customFunctionLearningAdaptive/baselineMLP1000/best_weights\n",
      "The loss after 366 iterations is E=0.010721718521252397\n",
      "Wrote weights to file ./customFunctionLearningAdaptive/baselineMLP1000/best_weights\n",
      "The loss after 367 iterations is E=0.010710237028423535\n",
      "Wrote weights to file ./customFunctionLearningAdaptive/baselineMLP1000/best_weights\n",
      "The loss after 368 iterations is E=0.010698935386348344\n",
      "Wrote weights to file ./customFunctionLearningAdaptive/baselineMLP1000/best_weights\n",
      "The loss after 369 iterations is E=0.010687827461045553\n",
      "Wrote weights to file ./customFunctionLearningAdaptive/baselineMLP1000/best_weights\n",
      "The loss after 370 iterations is E=0.010676922981530687\n",
      "Wrote weights to file ./customFunctionLearningAdaptive/baselineMLP1000/best_weights\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The loss after 371 iterations is E=0.010666172470869947\n",
      "Wrote weights to file ./customFunctionLearningAdaptive/baselineMLP1000/best_weights\n",
      "The loss after 372 iterations is E=0.01065563359282325\n",
      "Wrote weights to file ./customFunctionLearningAdaptive/baselineMLP1000/best_weights\n",
      "The loss after 373 iterations is E=0.010645230247994398\n",
      "Wrote weights to file ./customFunctionLearningAdaptive/baselineMLP1000/best_weights\n",
      "The loss after 374 iterations is E=0.010635029880674862\n",
      "Wrote weights to file ./customFunctionLearningAdaptive/baselineMLP1000/best_weights\n",
      "The loss after 375 iterations is E=0.010624959577820715\n",
      "Wrote weights to file ./customFunctionLearningAdaptive/baselineMLP1000/best_weights\n",
      "The loss after 376 iterations is E=0.01061507314693553\n",
      "Wrote weights to file ./customFunctionLearningAdaptive/baselineMLP1000/best_weights\n",
      "The loss after 377 iterations is E=0.010605321330277305\n",
      "Wrote weights to file ./customFunctionLearningAdaptive/baselineMLP1000/best_weights\n",
      "The loss after 378 iterations is E=0.010595731232568979\n",
      "Wrote weights to file ./customFunctionLearningAdaptive/baselineMLP1000/best_weights\n",
      "The loss after 379 iterations is E=0.010586280186073039\n",
      "Wrote weights to file ./customFunctionLearningAdaptive/baselineMLP1000/best_weights\n",
      "The loss after 380 iterations is E=0.01057696970076347\n",
      "Wrote weights to file ./customFunctionLearningAdaptive/baselineMLP1000/best_weights\n",
      "The loss after 381 iterations is E=0.010567801044273477\n",
      "Wrote weights to file ./customFunctionLearningAdaptive/baselineMLP1000/best_weights\n",
      "The loss after 382 iterations is E=0.010558757591805227\n",
      "Wrote weights to file ./customFunctionLearningAdaptive/baselineMLP1000/best_weights\n",
      "The loss after 383 iterations is E=0.010549853383284433\n",
      "Wrote weights to file ./customFunctionLearningAdaptive/baselineMLP1000/best_weights\n",
      "The loss after 384 iterations is E=0.010541063056116823\n",
      "Wrote weights to file ./customFunctionLearningAdaptive/baselineMLP1000/best_weights\n",
      "The loss after 385 iterations is E=0.010532405559030353\n",
      "Wrote weights to file ./customFunctionLearningAdaptive/baselineMLP1000/best_weights\n",
      "The loss after 386 iterations is E=0.010523855934718732\n",
      "Wrote weights to file ./customFunctionLearningAdaptive/baselineMLP1000/best_weights\n",
      "The loss after 387 iterations is E=0.01051542981208231\n",
      "Wrote weights to file ./customFunctionLearningAdaptive/baselineMLP1000/best_weights\n",
      "The loss after 388 iterations is E=0.010507106987891144\n",
      "Wrote weights to file ./customFunctionLearningAdaptive/baselineMLP1000/best_weights\n",
      "The loss after 389 iterations is E=0.01049889702546706\n",
      "Wrote weights to file ./customFunctionLearningAdaptive/baselineMLP1000/best_weights\n",
      "The loss after 390 iterations is E=0.010490787593335426\n",
      "Wrote weights to file ./customFunctionLearningAdaptive/baselineMLP1000/best_weights\n",
      "The loss after 391 iterations is E=0.010482780972599411\n",
      "Wrote weights to file ./customFunctionLearningAdaptive/baselineMLP1000/best_weights\n",
      "The loss after 392 iterations is E=0.010474871181590927\n",
      "Wrote weights to file ./customFunctionLearningAdaptive/baselineMLP1000/best_weights\n",
      "The loss after 393 iterations is E=0.01046705473726642\n",
      "Wrote weights to file ./customFunctionLearningAdaptive/baselineMLP1000/best_weights\n",
      "The loss after 394 iterations is E=0.010459331181907958\n",
      "Wrote weights to file ./customFunctionLearningAdaptive/baselineMLP1000/best_weights\n",
      "The loss after 395 iterations is E=0.010451693444636525\n",
      "Wrote weights to file ./customFunctionLearningAdaptive/baselineMLP1000/best_weights\n",
      "The loss after 396 iterations is E=0.010444143439141209\n",
      "Wrote weights to file ./customFunctionLearningAdaptive/baselineMLP1000/best_weights\n",
      "The loss after 397 iterations is E=0.010436672468370012\n",
      "Wrote weights to file ./customFunctionLearningAdaptive/baselineMLP1000/best_weights\n",
      "The loss after 398 iterations is E=0.010429283597750567\n",
      "Wrote weights to file ./customFunctionLearningAdaptive/baselineMLP1000/best_weights\n",
      "The loss after 399 iterations is E=0.010421968559405656\n",
      "Wrote weights to file ./customFunctionLearningAdaptive/baselineMLP1000/best_weights\n",
      "The loss after 400 iterations is E=0.010414729594315771\n",
      "Wrote weights to file ./customFunctionLearningAdaptive/baselineMLP1000/best_weights\n",
      "The loss after 401 iterations is E=0.01040755945665735\n",
      "Wrote weights to file ./customFunctionLearningAdaptive/baselineMLP1000/best_weights\n",
      "The loss after 402 iterations is E=0.010400459291381626\n",
      "Wrote weights to file ./customFunctionLearningAdaptive/baselineMLP1000/best_weights\n",
      "The loss after 403 iterations is E=0.010393423778622872\n",
      "Wrote weights to file ./customFunctionLearningAdaptive/baselineMLP1000/best_weights\n",
      "The loss after 404 iterations is E=0.01038645249829684\n",
      "Wrote weights to file ./customFunctionLearningAdaptive/baselineMLP1000/best_weights\n",
      "The loss after 405 iterations is E=0.010379541524877996\n",
      "Wrote weights to file ./customFunctionLearningAdaptive/baselineMLP1000/best_weights\n",
      "The loss after 406 iterations is E=0.010372689230010184\n",
      "Wrote weights to file ./customFunctionLearningAdaptive/baselineMLP1000/best_weights\n",
      "The loss after 407 iterations is E=0.010365893245606233\n",
      "Wrote weights to file ./customFunctionLearningAdaptive/baselineMLP1000/best_weights\n",
      "The loss after 408 iterations is E=0.010359151042194727\n",
      "Wrote weights to file ./customFunctionLearningAdaptive/baselineMLP1000/best_weights\n",
      "The loss after 409 iterations is E=0.010352460971182446\n",
      "Wrote weights to file ./customFunctionLearningAdaptive/baselineMLP1000/best_weights\n",
      "The loss after 410 iterations is E=0.010345819976669323\n",
      "Wrote weights to file ./customFunctionLearningAdaptive/baselineMLP1000/best_weights\n",
      "The loss after 411 iterations is E=0.010339227112501551\n",
      "Wrote weights to file ./customFunctionLearningAdaptive/baselineMLP1000/best_weights\n",
      "The loss after 412 iterations is E=0.010332679238073314\n",
      "Wrote weights to file ./customFunctionLearningAdaptive/baselineMLP1000/best_weights\n",
      "The loss after 413 iterations is E=0.010326175476780282\n",
      "Wrote weights to file ./customFunctionLearningAdaptive/baselineMLP1000/best_weights\n",
      "The loss after 414 iterations is E=0.010319712719083034\n",
      "Wrote weights to file ./customFunctionLearningAdaptive/baselineMLP1000/best_weights\n",
      "The loss after 415 iterations is E=0.01031329020304006\n",
      "Wrote weights to file ./customFunctionLearningAdaptive/baselineMLP1000/best_weights\n",
      "The loss after 416 iterations is E=0.010306905171129614\n",
      "Wrote weights to file ./customFunctionLearningAdaptive/baselineMLP1000/best_weights\n",
      "The loss after 417 iterations is E=0.010300556656950677\n",
      "Wrote weights to file ./customFunctionLearningAdaptive/baselineMLP1000/best_weights\n",
      "The loss after 418 iterations is E=0.010294242140846198\n",
      "Wrote weights to file ./customFunctionLearningAdaptive/baselineMLP1000/best_weights\n",
      "The loss after 419 iterations is E=0.010287960558282634\n",
      "Wrote weights to file ./customFunctionLearningAdaptive/baselineMLP1000/best_weights\n",
      "The loss after 420 iterations is E=0.01028170981784426\n",
      "Wrote weights to file ./customFunctionLearningAdaptive/baselineMLP1000/best_weights\n",
      "The loss after 421 iterations is E=0.010275488662560908\n",
      "Wrote weights to file ./customFunctionLearningAdaptive/baselineMLP1000/best_weights\n",
      "The loss after 422 iterations is E=0.01026929521598036\n",
      "Wrote weights to file ./customFunctionLearningAdaptive/baselineMLP1000/best_weights\n",
      "The loss after 423 iterations is E=0.010263128128531973\n",
      "Wrote weights to file ./customFunctionLearningAdaptive/baselineMLP1000/best_weights\n",
      "The loss after 424 iterations is E=0.010256985856490505\n",
      "Wrote weights to file ./customFunctionLearningAdaptive/baselineMLP1000/best_weights\n",
      "The loss after 425 iterations is E=0.010250866974067446\n",
      "Wrote weights to file ./customFunctionLearningAdaptive/baselineMLP1000/best_weights\n",
      "The loss after 426 iterations is E=0.010244770061003548\n",
      "Wrote weights to file ./customFunctionLearningAdaptive/baselineMLP1000/best_weights\n",
      "The loss after 427 iterations is E=0.010238693667110992\n",
      "Wrote weights to file ./customFunctionLearningAdaptive/baselineMLP1000/best_weights\n",
      "The loss after 428 iterations is E=0.010232636582941398\n",
      "Wrote weights to file ./customFunctionLearningAdaptive/baselineMLP1000/best_weights\n",
      "The loss after 429 iterations is E=0.010226597387311277\n",
      "Wrote weights to file ./customFunctionLearningAdaptive/baselineMLP1000/best_weights\n",
      "The loss after 430 iterations is E=0.01022057492170924\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wrote weights to file ./customFunctionLearningAdaptive/baselineMLP1000/best_weights\n",
      "The loss after 431 iterations is E=0.010214567794650199\n",
      "Wrote weights to file ./customFunctionLearningAdaptive/baselineMLP1000/best_weights\n",
      "The loss after 432 iterations is E=0.010208574970524383\n",
      "Wrote weights to file ./customFunctionLearningAdaptive/baselineMLP1000/best_weights\n",
      "The loss after 433 iterations is E=0.010202595145005704\n",
      "Wrote weights to file ./customFunctionLearningAdaptive/baselineMLP1000/best_weights\n",
      "The loss after 434 iterations is E=0.01019662729992452\n",
      "Wrote weights to file ./customFunctionLearningAdaptive/baselineMLP1000/best_weights\n",
      "The loss after 435 iterations is E=0.010190670184462028\n",
      "Wrote weights to file ./customFunctionLearningAdaptive/baselineMLP1000/best_weights\n",
      "The loss after 436 iterations is E=0.010184722855428359\n",
      "Wrote weights to file ./customFunctionLearningAdaptive/baselineMLP1000/best_weights\n",
      "The loss after 437 iterations is E=0.010178784166857227\n",
      "Wrote weights to file ./customFunctionLearningAdaptive/baselineMLP1000/best_weights\n",
      "The loss after 438 iterations is E=0.010172853187657968\n",
      "Wrote weights to file ./customFunctionLearningAdaptive/baselineMLP1000/best_weights\n",
      "The loss after 439 iterations is E=0.01016692882791126\n",
      "Wrote weights to file ./customFunctionLearningAdaptive/baselineMLP1000/best_weights\n",
      "The loss after 440 iterations is E=0.010161010210237697\n",
      "Wrote weights to file ./customFunctionLearningAdaptive/baselineMLP1000/best_weights\n",
      "The loss after 441 iterations is E=0.010155096344523001\n",
      "Wrote weights to file ./customFunctionLearningAdaptive/baselineMLP1000/best_weights\n",
      "The loss after 442 iterations is E=0.010149186372558587\n",
      "Wrote weights to file ./customFunctionLearningAdaptive/baselineMLP1000/best_weights\n",
      "The loss after 443 iterations is E=0.010143279353711\n",
      "Wrote weights to file ./customFunctionLearningAdaptive/baselineMLP1000/best_weights\n",
      "The loss after 444 iterations is E=0.010137374475248045\n",
      "Wrote weights to file ./customFunctionLearningAdaptive/baselineMLP1000/best_weights\n",
      "The loss after 445 iterations is E=0.010131470882766341\n",
      "Wrote weights to file ./customFunctionLearningAdaptive/baselineMLP1000/best_weights\n",
      "The loss after 446 iterations is E=0.010125567791674039\n",
      "Wrote weights to file ./customFunctionLearningAdaptive/baselineMLP1000/best_weights\n",
      "The loss after 447 iterations is E=0.010119664389062043\n",
      "Wrote weights to file ./customFunctionLearningAdaptive/baselineMLP1000/best_weights\n",
      "The loss after 448 iterations is E=0.01011375993227005\n",
      "Wrote weights to file ./customFunctionLearningAdaptive/baselineMLP1000/best_weights\n",
      "The loss after 449 iterations is E=0.010107853680276243\n",
      "Wrote weights to file ./customFunctionLearningAdaptive/baselineMLP1000/best_weights\n",
      "The loss after 450 iterations is E=0.010101944924891349\n",
      "Wrote weights to file ./customFunctionLearningAdaptive/baselineMLP1000/best_weights\n",
      "The loss after 451 iterations is E=0.010096032960261875\n",
      "Wrote weights to file ./customFunctionLearningAdaptive/baselineMLP1000/best_weights\n",
      "The loss after 452 iterations is E=0.010090117117570758\n",
      "Wrote weights to file ./customFunctionLearningAdaptive/baselineMLP1000/best_weights\n",
      "The loss after 453 iterations is E=0.010084196750723952\n",
      "Wrote weights to file ./customFunctionLearningAdaptive/baselineMLP1000/best_weights\n",
      "The loss after 454 iterations is E=0.010078271228243441\n",
      "Wrote weights to file ./customFunctionLearningAdaptive/baselineMLP1000/best_weights\n",
      "The loss after 455 iterations is E=0.010072339934969896\n",
      "Wrote weights to file ./customFunctionLearningAdaptive/baselineMLP1000/best_weights\n",
      "The loss after 456 iterations is E=0.010066402276119865\n",
      "Wrote weights to file ./customFunctionLearningAdaptive/baselineMLP1000/best_weights\n",
      "The loss after 457 iterations is E=0.010060457686872411\n",
      "Wrote weights to file ./customFunctionLearningAdaptive/baselineMLP1000/best_weights\n",
      "The loss after 458 iterations is E=0.010054505609920578\n",
      "Wrote weights to file ./customFunctionLearningAdaptive/baselineMLP1000/best_weights\n",
      "The loss after 459 iterations is E=0.010048545508672882\n",
      "Wrote weights to file ./customFunctionLearningAdaptive/baselineMLP1000/best_weights\n",
      "The loss after 460 iterations is E=0.010042576859665451\n",
      "Wrote weights to file ./customFunctionLearningAdaptive/baselineMLP1000/best_weights\n",
      "The loss after 461 iterations is E=0.010036599169485894\n",
      "Wrote weights to file ./customFunctionLearningAdaptive/baselineMLP1000/best_weights\n",
      "The loss after 462 iterations is E=0.010030611950587319\n",
      "Wrote weights to file ./customFunctionLearningAdaptive/baselineMLP1000/best_weights\n",
      "The loss after 463 iterations is E=0.010024614735932972\n",
      "Wrote weights to file ./customFunctionLearningAdaptive/baselineMLP1000/best_weights\n",
      "The loss after 464 iterations is E=0.010018607068984758\n",
      "Wrote weights to file ./customFunctionLearningAdaptive/baselineMLP1000/best_weights\n",
      "The loss after 465 iterations is E=0.010012588520371579\n",
      "Wrote weights to file ./customFunctionLearningAdaptive/baselineMLP1000/best_weights\n",
      "The loss after 466 iterations is E=0.010006558667053127\n",
      "Wrote weights to file ./customFunctionLearningAdaptive/baselineMLP1000/best_weights\n",
      "The loss after 467 iterations is E=0.010000517104544193\n",
      "Wrote weights to file ./customFunctionLearningAdaptive/baselineMLP1000/best_weights\n",
      "The loss after 468 iterations is E=0.00999446343814891\n",
      "Using every 27th point for                         training.\n",
      "Selecting subset for training: 100 out of 2700 selected.\n",
      "Wrote weights to file ./customFunctionLearningAdaptive/baselineMLP1000/best_weights\n",
      "The loss after 469 iterations is E=0.010731405877577238\n",
      "Wrote weights to file ./customFunctionLearningAdaptive/baselineMLP1000/best_weights\n",
      "The loss after 470 iterations is E=0.01014869238544994\n",
      "Wrote weights to file ./customFunctionLearningAdaptive/baselineMLP1000/best_weights\n",
      "The loss after 472 iterations is E=0.009941377125355633\n",
      "Using every 24th point for                         training.\n",
      "Selecting subset for training: 112 out of 2700 selected.\n",
      "Wrote weights to file ./customFunctionLearningAdaptive/baselineMLP1000/best_weights\n",
      "The loss after 473 iterations is E=0.009921953797963469\n",
      "Using every 21th point for                         training.\n",
      "Selecting subset for training: 128 out of 2700 selected.\n",
      "Wrote weights to file ./customFunctionLearningAdaptive/baselineMLP1000/best_weights\n",
      "The loss after 474 iterations is E=0.01682890672499464\n",
      "Wrote weights to file ./customFunctionLearningAdaptive/baselineMLP1000/best_weights\n",
      "The loss after 475 iterations is E=0.016803697087233428\n",
      "Wrote weights to file ./customFunctionLearningAdaptive/baselineMLP1000/best_weights\n",
      "The loss after 476 iterations is E=0.016771914839281567\n",
      "Wrote weights to file ./customFunctionLearningAdaptive/baselineMLP1000/best_weights\n",
      "The loss after 478 iterations is E=0.016742703674037546\n",
      "Wrote weights to file ./customFunctionLearningAdaptive/baselineMLP1000/best_weights\n",
      "The loss after 479 iterations is E=0.016710479102610653\n",
      "Wrote weights to file ./customFunctionLearningAdaptive/baselineMLP1000/best_weights\n",
      "The loss after 480 iterations is E=0.016701487383854397\n",
      "Wrote weights to file ./customFunctionLearningAdaptive/baselineMLP1000/best_weights\n",
      "The loss after 481 iterations is E=0.016670401180703037\n",
      "Wrote weights to file ./customFunctionLearningAdaptive/baselineMLP1000/best_weights\n",
      "The loss after 482 iterations is E=0.01663802090442331\n",
      "Wrote weights to file ./customFunctionLearningAdaptive/baselineMLP1000/best_weights\n",
      "The loss after 483 iterations is E=0.016623033287842016\n",
      "Wrote weights to file ./customFunctionLearningAdaptive/baselineMLP1000/best_weights\n",
      "The loss after 484 iterations is E=0.01659201869491894\n",
      "Wrote weights to file ./customFunctionLearningAdaptive/baselineMLP1000/best_weights\n",
      "The loss after 485 iterations is E=0.01655999523859558\n",
      "Wrote weights to file ./customFunctionLearningAdaptive/baselineMLP1000/best_weights\n",
      "The loss after 486 iterations is E=0.016541264555154853\n",
      "Wrote weights to file ./customFunctionLearningAdaptive/baselineMLP1000/best_weights\n",
      "The loss after 487 iterations is E=0.016511216882078315\n",
      "Wrote weights to file ./customFunctionLearningAdaptive/baselineMLP1000/best_weights\n",
      "The loss after 488 iterations is E=0.01647973091758194\n",
      "Wrote weights to file ./customFunctionLearningAdaptive/baselineMLP1000/best_weights\n",
      "The loss after 489 iterations is E=0.016458468943240358\n",
      "Wrote weights to file ./customFunctionLearningAdaptive/baselineMLP1000/best_weights\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The loss after 490 iterations is E=0.016429442076709072\n",
      "Wrote weights to file ./customFunctionLearningAdaptive/baselineMLP1000/best_weights\n",
      "The loss after 491 iterations is E=0.01639831838373568\n",
      "Wrote weights to file ./customFunctionLearningAdaptive/baselineMLP1000/best_weights\n",
      "The loss after 492 iterations is E=0.016374992496537153\n",
      "Wrote weights to file ./customFunctionLearningAdaptive/baselineMLP1000/best_weights\n",
      "The loss after 493 iterations is E=0.016346586602173124\n",
      "Wrote weights to file ./customFunctionLearningAdaptive/baselineMLP1000/best_weights\n",
      "The loss after 494 iterations is E=0.0163155365898413\n",
      "Wrote weights to file ./customFunctionLearningAdaptive/baselineMLP1000/best_weights\n",
      "The loss after 495 iterations is E=0.016290309819891567\n",
      "Wrote weights to file ./customFunctionLearningAdaptive/baselineMLP1000/best_weights\n",
      "The loss after 496 iterations is E=0.01626206191247018\n",
      "Wrote weights to file ./customFunctionLearningAdaptive/baselineMLP1000/best_weights\n",
      "The loss after 497 iterations is E=0.016230913009936402\n",
      "Wrote weights to file ./customFunctionLearningAdaptive/baselineMLP1000/best_weights\n",
      "The loss after 498 iterations is E=0.016203963449971766\n",
      "Wrote weights to file ./customFunctionLearningAdaptive/baselineMLP1000/best_weights\n",
      "The loss after 499 iterations is E=0.016175565156320373\n",
      "Wrote weights to file ./customFunctionLearningAdaptive/baselineMLP1000/best_weights\n",
      "The loss after 500 iterations is E=0.01614433444151855\n",
      "Wrote weights to file ./customFunctionLearningAdaptive/baselineMLP1000/best_weights\n",
      "The loss after 501 iterations is E=0.016115963165672646\n",
      "Wrote weights to file ./customFunctionLearningAdaptive/baselineMLP1000/best_weights\n",
      "The loss after 502 iterations is E=0.016087286663139456\n",
      "Wrote weights to file ./customFunctionLearningAdaptive/baselineMLP1000/best_weights\n",
      "The loss after 503 iterations is E=0.016056110213545537\n",
      "Wrote weights to file ./customFunctionLearningAdaptive/baselineMLP1000/best_weights\n",
      "The loss after 504 iterations is E=0.016026691731031766\n",
      "Wrote weights to file ./customFunctionLearningAdaptive/baselineMLP1000/best_weights\n",
      "The loss after 505 iterations is E=0.015997708317599713\n",
      "Wrote weights to file ./customFunctionLearningAdaptive/baselineMLP1000/best_weights\n",
      "The loss after 506 iterations is E=0.01596671889548172\n",
      "Wrote weights to file ./customFunctionLearningAdaptive/baselineMLP1000/best_weights\n",
      "The loss after 507 iterations is E=0.01593660218065761\n",
      "Wrote weights to file ./customFunctionLearningAdaptive/baselineMLP1000/best_weights\n",
      "The loss after 508 iterations is E=0.015907294223121756\n",
      "Wrote weights to file ./customFunctionLearningAdaptive/baselineMLP1000/best_weights\n",
      "The loss after 509 iterations is E=0.015876534143612422\n",
      "Wrote weights to file ./customFunctionLearningAdaptive/baselineMLP1000/best_weights\n",
      "The loss after 510 iterations is E=0.01584597590760863\n",
      "Wrote weights to file ./customFunctionLearningAdaptive/baselineMLP1000/best_weights\n",
      "The loss after 511 iterations is E=0.01581629893706312\n",
      "Wrote weights to file ./customFunctionLearningAdaptive/baselineMLP1000/best_weights\n",
      "The loss after 512 iterations is E=0.015785702686260943\n",
      "Wrote weights to file ./customFunctionLearningAdaptive/baselineMLP1000/best_weights\n",
      "The loss after 513 iterations is E=0.015754857195401522\n",
      "Wrote weights to file ./customFunctionLearningAdaptive/baselineMLP1000/best_weights\n",
      "The loss after 514 iterations is E=0.015724753771281873\n",
      "Wrote weights to file ./customFunctionLearningAdaptive/baselineMLP1000/best_weights\n",
      "The loss after 515 iterations is E=0.015694185255496887\n",
      "Wrote weights to file ./customFunctionLearningAdaptive/baselineMLP1000/best_weights\n",
      "The loss after 516 iterations is E=0.015663131532716163\n",
      "Wrote weights to file ./customFunctionLearningAdaptive/baselineMLP1000/best_weights\n",
      "The loss after 517 iterations is E=0.015632566628396117\n",
      "Wrote weights to file ./customFunctionLearningAdaptive/baselineMLP1000/best_weights\n",
      "The loss after 518 iterations is E=0.015601874934492816\n",
      "Wrote weights to file ./customFunctionLearningAdaptive/baselineMLP1000/best_weights\n",
      "The loss after 519 iterations is E=0.015570650053022734\n",
      "Wrote weights to file ./customFunctionLearningAdaptive/baselineMLP1000/best_weights\n",
      "The loss after 520 iterations is E=0.01553963612563975\n",
      "Wrote weights to file ./customFunctionLearningAdaptive/baselineMLP1000/best_weights\n",
      "The loss after 521 iterations is E=0.015508699959533656\n",
      "Wrote weights to file ./customFunctionLearningAdaptive/baselineMLP1000/best_weights\n",
      "The loss after 522 iterations is E=0.015477318906376717\n",
      "Wrote weights to file ./customFunctionLearningAdaptive/baselineMLP1000/best_weights\n",
      "The loss after 523 iterations is E=0.015445915442860789\n",
      "Wrote weights to file ./customFunctionLearningAdaptive/baselineMLP1000/best_weights\n",
      "The loss after 524 iterations is E=0.015414664295662532\n",
      "Wrote weights to file ./customFunctionLearningAdaptive/baselineMLP1000/best_weights\n",
      "The loss after 525 iterations is E=0.015383125221713178\n",
      "Wrote weights to file ./customFunctionLearningAdaptive/baselineMLP1000/best_weights\n",
      "The loss after 526 iterations is E=0.015351416280061378\n",
      "Wrote weights to file ./customFunctionLearningAdaptive/baselineMLP1000/best_weights\n",
      "The loss after 527 iterations is E=0.01531983106248628\n",
      "Wrote weights to file ./customFunctionLearningAdaptive/baselineMLP1000/best_weights\n",
      "The loss after 528 iterations is E=0.015288115952975268\n",
      "Wrote weights to file ./customFunctionLearningAdaptive/baselineMLP1000/best_weights\n",
      "The loss after 529 iterations is E=0.015256179957713337\n",
      "Wrote weights to file ./customFunctionLearningAdaptive/baselineMLP1000/best_weights\n",
      "The loss after 530 iterations is E=0.015224281367962613\n",
      "Wrote weights to file ./customFunctionLearningAdaptive/baselineMLP1000/best_weights\n",
      "The loss after 531 iterations is E=0.015192361930955888\n",
      "Wrote weights to file ./customFunctionLearningAdaptive/baselineMLP1000/best_weights\n",
      "The loss after 532 iterations is E=0.015160248765816436\n",
      "Wrote weights to file ./customFunctionLearningAdaptive/baselineMLP1000/best_weights\n",
      "The loss after 533 iterations is E=0.015128080371269096\n",
      "Wrote weights to file ./customFunctionLearningAdaptive/baselineMLP1000/best_weights\n",
      "The loss after 534 iterations is E=0.015095931055258947\n",
      "Wrote weights to file ./customFunctionLearningAdaptive/baselineMLP1000/best_weights\n",
      "The loss after 535 iterations is E=0.015063655403331686\n",
      "Wrote weights to file ./customFunctionLearningAdaptive/baselineMLP1000/best_weights\n",
      "The loss after 536 iterations is E=0.015031265798748491\n",
      "Wrote weights to file ./customFunctionLearningAdaptive/baselineMLP1000/best_weights\n",
      "The loss after 537 iterations is E=0.014998877639194674\n",
      "Wrote weights to file ./customFunctionLearningAdaptive/baselineMLP1000/best_weights\n",
      "The loss after 538 iterations is E=0.01496642886759283\n",
      "Wrote weights to file ./customFunctionLearningAdaptive/baselineMLP1000/best_weights\n",
      "The loss after 539 iterations is E=0.014933856638742133\n",
      "Wrote weights to file ./customFunctionLearningAdaptive/baselineMLP1000/best_weights\n",
      "The loss after 540 iterations is E=0.01490124311736477\n",
      "Wrote weights to file ./customFunctionLearningAdaptive/baselineMLP1000/best_weights\n",
      "The loss after 541 iterations is E=0.01486860443326251\n",
      "Wrote weights to file ./customFunctionLearningAdaptive/baselineMLP1000/best_weights\n",
      "The loss after 542 iterations is E=0.014835870152358667\n",
      "Wrote weights to file ./customFunctionLearningAdaptive/baselineMLP1000/best_weights\n",
      "The loss after 543 iterations is E=0.014803060940200959\n",
      "Wrote weights to file ./customFunctionLearningAdaptive/baselineMLP1000/best_weights\n",
      "The loss after 544 iterations is E=0.014770226853511668\n",
      "Wrote weights to file ./customFunctionLearningAdaptive/baselineMLP1000/best_weights\n",
      "The loss after 545 iterations is E=0.014737334939149303\n",
      "Wrote weights to file ./customFunctionLearningAdaptive/baselineMLP1000/best_weights\n",
      "The loss after 546 iterations is E=0.014704361693112173\n",
      "Wrote weights to file ./customFunctionLearningAdaptive/baselineMLP1000/best_weights\n",
      "The loss after 547 iterations is E=0.014671344754855898\n",
      "Wrote weights to file ./customFunctionLearningAdaptive/baselineMLP1000/best_weights\n",
      "The loss after 548 iterations is E=0.014638293456550013\n",
      "Wrote weights to file ./customFunctionLearningAdaptive/baselineMLP1000/best_weights\n",
      "The loss after 549 iterations is E=0.014605177432421733\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wrote weights to file ./customFunctionLearningAdaptive/baselineMLP1000/best_weights\n",
      "The loss after 550 iterations is E=0.014572002249785661\n",
      "Wrote weights to file ./customFunctionLearningAdaptive/baselineMLP1000/best_weights\n",
      "The loss after 551 iterations is E=0.014538794915626851\n",
      "Wrote weights to file ./customFunctionLearningAdaptive/baselineMLP1000/best_weights\n",
      "The loss after 552 iterations is E=0.014505544448753153\n",
      "Wrote weights to file ./customFunctionLearningAdaptive/baselineMLP1000/best_weights\n",
      "The loss after 553 iterations is E=0.01447223525833628\n",
      "Wrote weights to file ./customFunctionLearningAdaptive/baselineMLP1000/best_weights\n",
      "The loss after 554 iterations is E=0.014438885122985188\n",
      "Wrote weights to file ./customFunctionLearningAdaptive/baselineMLP1000/best_weights\n",
      "The loss after 555 iterations is E=0.01440550299648434\n",
      "Wrote weights to file ./customFunctionLearningAdaptive/baselineMLP1000/best_weights\n",
      "The loss after 556 iterations is E=0.01437207485684804\n",
      "Wrote weights to file ./customFunctionLearningAdaptive/baselineMLP1000/best_weights\n",
      "The loss after 557 iterations is E=0.014338600861206967\n",
      "Wrote weights to file ./customFunctionLearningAdaptive/baselineMLP1000/best_weights\n",
      "The loss after 558 iterations is E=0.014305093768690666\n",
      "Wrote weights to file ./customFunctionLearningAdaptive/baselineMLP1000/best_weights\n",
      "The loss after 559 iterations is E=0.01427155338212464\n",
      "Wrote weights to file ./customFunctionLearningAdaptive/baselineMLP1000/best_weights\n",
      "The loss after 560 iterations is E=0.014237972175114992\n",
      "Wrote weights to file ./customFunctionLearningAdaptive/baselineMLP1000/best_weights\n",
      "The loss after 561 iterations is E=0.014204353855348243\n",
      "Wrote weights to file ./customFunctionLearningAdaptive/baselineMLP1000/best_weights\n",
      "The loss after 562 iterations is E=0.014170707165210478\n",
      "Wrote weights to file ./customFunctionLearningAdaptive/baselineMLP1000/best_weights\n",
      "The loss after 563 iterations is E=0.014137029308797417\n",
      "Wrote weights to file ./customFunctionLearningAdaptive/baselineMLP1000/best_weights\n",
      "The loss after 564 iterations is E=0.014103316114557427\n",
      "Wrote weights to file ./customFunctionLearningAdaptive/baselineMLP1000/best_weights\n",
      "The loss after 565 iterations is E=0.014069574187868047\n",
      "Wrote weights to file ./customFunctionLearningAdaptive/baselineMLP1000/best_weights\n",
      "The loss after 566 iterations is E=0.014035807448439713\n",
      "Wrote weights to file ./customFunctionLearningAdaptive/baselineMLP1000/best_weights\n",
      "The loss after 567 iterations is E=0.014002012771564229\n",
      "Wrote weights to file ./customFunctionLearningAdaptive/baselineMLP1000/best_weights\n",
      "The loss after 568 iterations is E=0.013968190680014135\n",
      "Wrote weights to file ./customFunctionLearningAdaptive/baselineMLP1000/best_weights\n",
      "The loss after 569 iterations is E=0.013934345435165208\n",
      "Wrote weights to file ./customFunctionLearningAdaptive/baselineMLP1000/best_weights\n",
      "The loss after 570 iterations is E=0.013900479303744665\n",
      "Wrote weights to file ./customFunctionLearningAdaptive/baselineMLP1000/best_weights\n",
      "The loss after 571 iterations is E=0.013866591123501427\n",
      "Wrote weights to file ./customFunctionLearningAdaptive/baselineMLP1000/best_weights\n",
      "The loss after 572 iterations is E=0.013832681334966465\n",
      "Wrote weights to file ./customFunctionLearningAdaptive/baselineMLP1000/best_weights\n",
      "The loss after 573 iterations is E=0.013798754185591806\n",
      "Wrote weights to file ./customFunctionLearningAdaptive/baselineMLP1000/best_weights\n",
      "The loss after 574 iterations is E=0.013764810935066974\n",
      "Wrote weights to file ./customFunctionLearningAdaptive/baselineMLP1000/best_weights\n",
      "The loss after 575 iterations is E=0.013730850587507332\n",
      "Wrote weights to file ./customFunctionLearningAdaptive/baselineMLP1000/best_weights\n",
      "The loss after 576 iterations is E=0.013696875241934522\n",
      "Wrote weights to file ./customFunctionLearningAdaptive/baselineMLP1000/best_weights\n",
      "The loss after 577 iterations is E=0.013662887364485978\n",
      "Wrote weights to file ./customFunctionLearningAdaptive/baselineMLP1000/best_weights\n",
      "The loss after 578 iterations is E=0.013628887996168735\n",
      "Wrote weights to file ./customFunctionLearningAdaptive/baselineMLP1000/best_weights\n",
      "The loss after 579 iterations is E=0.01359487758758867\n",
      "Wrote weights to file ./customFunctionLearningAdaptive/baselineMLP1000/best_weights\n",
      "The loss after 580 iterations is E=0.013560857204101596\n",
      "Wrote weights to file ./customFunctionLearningAdaptive/baselineMLP1000/best_weights\n",
      "The loss after 581 iterations is E=0.013526829400970195\n",
      "Wrote weights to file ./customFunctionLearningAdaptive/baselineMLP1000/best_weights\n",
      "The loss after 582 iterations is E=0.013492795264759425\n",
      "Wrote weights to file ./customFunctionLearningAdaptive/baselineMLP1000/best_weights\n",
      "The loss after 583 iterations is E=0.013458754982418006\n",
      "Wrote weights to file ./customFunctionLearningAdaptive/baselineMLP1000/best_weights\n",
      "The loss after 584 iterations is E=0.013424710372962575\n",
      "Wrote weights to file ./customFunctionLearningAdaptive/baselineMLP1000/best_weights\n",
      "The loss after 585 iterations is E=0.01339066305989038\n",
      "Wrote weights to file ./customFunctionLearningAdaptive/baselineMLP1000/best_weights\n",
      "The loss after 586 iterations is E=0.013356614172214492\n",
      "Wrote weights to file ./customFunctionLearningAdaptive/baselineMLP1000/best_weights\n",
      "The loss after 587 iterations is E=0.01332256469089773\n",
      "Wrote weights to file ./customFunctionLearningAdaptive/baselineMLP1000/best_weights\n",
      "The loss after 588 iterations is E=0.013288515582262085\n",
      "Wrote weights to file ./customFunctionLearningAdaptive/baselineMLP1000/best_weights\n",
      "The loss after 589 iterations is E=0.013254468709658514\n",
      "Wrote weights to file ./customFunctionLearningAdaptive/baselineMLP1000/best_weights\n",
      "The loss after 590 iterations is E=0.013220425263923611\n",
      "Wrote weights to file ./customFunctionLearningAdaptive/baselineMLP1000/best_weights\n",
      "The loss after 591 iterations is E=0.013186385947684363\n",
      "Wrote weights to file ./customFunctionLearningAdaptive/baselineMLP1000/best_weights\n",
      "The loss after 592 iterations is E=0.013152352215628235\n",
      "Wrote weights to file ./customFunctionLearningAdaptive/baselineMLP1000/best_weights\n",
      "The loss after 593 iterations is E=0.013118325323680192\n",
      "Wrote weights to file ./customFunctionLearningAdaptive/baselineMLP1000/best_weights\n",
      "The loss after 594 iterations is E=0.013084306568472225\n",
      "Wrote weights to file ./customFunctionLearningAdaptive/baselineMLP1000/best_weights\n",
      "The loss after 595 iterations is E=0.013050297067322284\n",
      "Wrote weights to file ./customFunctionLearningAdaptive/baselineMLP1000/best_weights\n",
      "The loss after 596 iterations is E=0.01301629772281924\n",
      "Wrote weights to file ./customFunctionLearningAdaptive/baselineMLP1000/best_weights\n",
      "The loss after 597 iterations is E=0.012982310061646463\n",
      "Wrote weights to file ./customFunctionLearningAdaptive/baselineMLP1000/best_weights\n",
      "The loss after 598 iterations is E=0.012948335278833106\n",
      "Wrote weights to file ./customFunctionLearningAdaptive/baselineMLP1000/best_weights\n",
      "The loss after 599 iterations is E=0.012914374388992253\n",
      "Wrote weights to file ./customFunctionLearningAdaptive/baselineMLP1000/best_weights\n",
      "The loss after 600 iterations is E=0.012880428648292159\n",
      "Wrote weights to file ./customFunctionLearningAdaptive/baselineMLP1000/best_weights\n",
      "The loss after 601 iterations is E=0.01284649912946536\n",
      "Wrote weights to file ./customFunctionLearningAdaptive/baselineMLP1000/best_weights\n",
      "The loss after 602 iterations is E=0.012812587196619608\n",
      "Wrote weights to file ./customFunctionLearningAdaptive/baselineMLP1000/best_weights\n",
      "The loss after 603 iterations is E=0.012778694016347554\n",
      "Wrote weights to file ./customFunctionLearningAdaptive/baselineMLP1000/best_weights\n",
      "The loss after 604 iterations is E=0.012744820585833136\n",
      "Wrote weights to file ./customFunctionLearningAdaptive/baselineMLP1000/best_weights\n",
      "The loss after 605 iterations is E=0.012710968234310746\n",
      "Wrote weights to file ./customFunctionLearningAdaptive/baselineMLP1000/best_weights\n",
      "The loss after 606 iterations is E=0.012677138094781187\n",
      "Wrote weights to file ./customFunctionLearningAdaptive/baselineMLP1000/best_weights\n",
      "The loss after 607 iterations is E=0.012643331395668785\n",
      "Wrote weights to file ./customFunctionLearningAdaptive/baselineMLP1000/best_weights\n",
      "The loss after 608 iterations is E=0.01260954933300197\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wrote weights to file ./customFunctionLearningAdaptive/baselineMLP1000/best_weights\n",
      "The loss after 609 iterations is E=0.012575792955732905\n",
      "Wrote weights to file ./customFunctionLearningAdaptive/baselineMLP1000/best_weights\n",
      "The loss after 610 iterations is E=0.012542063583280855\n",
      "Wrote weights to file ./customFunctionLearningAdaptive/baselineMLP1000/best_weights\n",
      "The loss after 611 iterations is E=0.012508362369365705\n",
      "Wrote weights to file ./customFunctionLearningAdaptive/baselineMLP1000/best_weights\n",
      "The loss after 612 iterations is E=0.012474690482701107\n",
      "Wrote weights to file ./customFunctionLearningAdaptive/baselineMLP1000/best_weights\n",
      "The loss after 613 iterations is E=0.012441049148083824\n",
      "Wrote weights to file ./customFunctionLearningAdaptive/baselineMLP1000/best_weights\n",
      "The loss after 614 iterations is E=0.012407439457698193\n",
      "Wrote weights to file ./customFunctionLearningAdaptive/baselineMLP1000/best_weights\n",
      "The loss after 615 iterations is E=0.012373862695905104\n",
      "Wrote weights to file ./customFunctionLearningAdaptive/baselineMLP1000/best_weights\n",
      "The loss after 616 iterations is E=0.012340320029554758\n",
      "Wrote weights to file ./customFunctionLearningAdaptive/baselineMLP1000/best_weights\n",
      "The loss after 617 iterations is E=0.012306812613320556\n",
      "Wrote weights to file ./customFunctionLearningAdaptive/baselineMLP1000/best_weights\n",
      "The loss after 618 iterations is E=0.012273341682539527\n",
      "Wrote weights to file ./customFunctionLearningAdaptive/baselineMLP1000/best_weights\n",
      "The loss after 619 iterations is E=0.012239908357190434\n",
      "Wrote weights to file ./customFunctionLearningAdaptive/baselineMLP1000/best_weights\n",
      "The loss after 620 iterations is E=0.012206513893279567\n",
      "Wrote weights to file ./customFunctionLearningAdaptive/baselineMLP1000/best_weights\n",
      "The loss after 621 iterations is E=0.012173159467490863\n",
      "Wrote weights to file ./customFunctionLearningAdaptive/baselineMLP1000/best_weights\n",
      "The loss after 622 iterations is E=0.01213984623968013\n",
      "Wrote weights to file ./customFunctionLearningAdaptive/baselineMLP1000/best_weights\n",
      "The loss after 623 iterations is E=0.01210657544398045\n",
      "Wrote weights to file ./customFunctionLearningAdaptive/baselineMLP1000/best_weights\n",
      "The loss after 624 iterations is E=0.012073348219194993\n",
      "Wrote weights to file ./customFunctionLearningAdaptive/baselineMLP1000/best_weights\n",
      "The loss after 625 iterations is E=0.01204016580544388\n",
      "Wrote weights to file ./customFunctionLearningAdaptive/baselineMLP1000/best_weights\n",
      "The loss after 626 iterations is E=0.012007029385411592\n",
      "Wrote weights to file ./customFunctionLearningAdaptive/baselineMLP1000/best_weights\n",
      "The loss after 627 iterations is E=0.011973940130897922\n",
      "Wrote weights to file ./customFunctionLearningAdaptive/baselineMLP1000/best_weights\n",
      "The loss after 628 iterations is E=0.01194089927056147\n",
      "Wrote weights to file ./customFunctionLearningAdaptive/baselineMLP1000/best_weights\n",
      "The loss after 629 iterations is E=0.011907907957561166\n",
      "Wrote weights to file ./customFunctionLearningAdaptive/baselineMLP1000/best_weights\n",
      "The loss after 630 iterations is E=0.011874967424969803\n",
      "Wrote weights to file ./customFunctionLearningAdaptive/baselineMLP1000/best_weights\n",
      "The loss after 631 iterations is E=0.011842078858245337\n",
      "Wrote weights to file ./customFunctionLearningAdaptive/baselineMLP1000/best_weights\n",
      "The loss after 632 iterations is E=0.011809243443042664\n",
      "Wrote weights to file ./customFunctionLearningAdaptive/baselineMLP1000/best_weights\n",
      "The loss after 633 iterations is E=0.011776462401623455\n",
      "Wrote weights to file ./customFunctionLearningAdaptive/baselineMLP1000/best_weights\n",
      "The loss after 634 iterations is E=0.011743736899471873\n",
      "Wrote weights to file ./customFunctionLearningAdaptive/baselineMLP1000/best_weights\n",
      "The loss after 635 iterations is E=0.011711068166602672\n",
      "Wrote weights to file ./customFunctionLearningAdaptive/baselineMLP1000/best_weights\n",
      "The loss after 636 iterations is E=0.01167845738910118\n",
      "Wrote weights to file ./customFunctionLearningAdaptive/baselineMLP1000/best_weights\n",
      "The loss after 637 iterations is E=0.011645905765780375\n",
      "Wrote weights to file ./customFunctionLearningAdaptive/baselineMLP1000/best_weights\n",
      "The loss after 638 iterations is E=0.011613414511724564\n",
      "Wrote weights to file ./customFunctionLearningAdaptive/baselineMLP1000/best_weights\n",
      "The loss after 639 iterations is E=0.011580984803539219\n",
      "Wrote weights to file ./customFunctionLearningAdaptive/baselineMLP1000/best_weights\n",
      "The loss after 640 iterations is E=0.01154861786760253\n",
      "Wrote weights to file ./customFunctionLearningAdaptive/baselineMLP1000/best_weights\n",
      "The loss after 641 iterations is E=0.011516314888915555\n",
      "Wrote weights to file ./customFunctionLearningAdaptive/baselineMLP1000/best_weights\n",
      "The loss after 642 iterations is E=0.01148407707573599\n",
      "Wrote weights to file ./customFunctionLearningAdaptive/baselineMLP1000/best_weights\n",
      "The loss after 643 iterations is E=0.011451905633773069\n",
      "Wrote weights to file ./customFunctionLearningAdaptive/baselineMLP1000/best_weights\n",
      "The loss after 644 iterations is E=0.011419801748789593\n",
      "Wrote weights to file ./customFunctionLearningAdaptive/baselineMLP1000/best_weights\n",
      "The loss after 645 iterations is E=0.011387766639089953\n",
      "Wrote weights to file ./customFunctionLearningAdaptive/baselineMLP1000/best_weights\n",
      "The loss after 646 iterations is E=0.01135580148696277\n",
      "Wrote weights to file ./customFunctionLearningAdaptive/baselineMLP1000/best_weights\n",
      "The loss after 647 iterations is E=0.011323907502844816\n",
      "Wrote weights to file ./customFunctionLearningAdaptive/baselineMLP1000/best_weights\n",
      "The loss after 648 iterations is E=0.01129208587978208\n",
      "Wrote weights to file ./customFunctionLearningAdaptive/baselineMLP1000/best_weights\n",
      "The loss after 649 iterations is E=0.011260337808595325\n",
      "Wrote weights to file ./customFunctionLearningAdaptive/baselineMLP1000/best_weights\n",
      "The loss after 650 iterations is E=0.01122866449283277\n",
      "Wrote weights to file ./customFunctionLearningAdaptive/baselineMLP1000/best_weights\n",
      "The loss after 651 iterations is E=0.011197067110551467\n",
      "Wrote weights to file ./customFunctionLearningAdaptive/baselineMLP1000/best_weights\n",
      "The loss after 652 iterations is E=0.011165546864089044\n",
      "Wrote weights to file ./customFunctionLearningAdaptive/baselineMLP1000/best_weights\n",
      "The loss after 653 iterations is E=0.011134104930909286\n",
      "Wrote weights to file ./customFunctionLearningAdaptive/baselineMLP1000/best_weights\n",
      "The loss after 654 iterations is E=0.011102742499333582\n",
      "Wrote weights to file ./customFunctionLearningAdaptive/baselineMLP1000/best_weights\n",
      "The loss after 655 iterations is E=0.011071460751376876\n",
      "Wrote weights to file ./customFunctionLearningAdaptive/baselineMLP1000/best_weights\n",
      "The loss after 656 iterations is E=0.01104026085837877\n",
      "Wrote weights to file ./customFunctionLearningAdaptive/baselineMLP1000/best_weights\n",
      "The loss after 657 iterations is E=0.011009144002985093\n",
      "Wrote weights to file ./customFunctionLearningAdaptive/baselineMLP1000/best_weights\n",
      "The loss after 658 iterations is E=0.010978111345471761\n",
      "Wrote weights to file ./customFunctionLearningAdaptive/baselineMLP1000/best_weights\n",
      "The loss after 659 iterations is E=0.01094716406003674\n",
      "Wrote weights to file ./customFunctionLearningAdaptive/baselineMLP1000/best_weights\n",
      "The loss after 660 iterations is E=0.01091630330232216\n",
      "Wrote weights to file ./customFunctionLearningAdaptive/baselineMLP1000/best_weights\n",
      "The loss after 661 iterations is E=0.010885530230738565\n",
      "Wrote weights to file ./customFunctionLearningAdaptive/baselineMLP1000/best_weights\n",
      "The loss after 662 iterations is E=0.01085484599792459\n",
      "Wrote weights to file ./customFunctionLearningAdaptive/baselineMLP1000/best_weights\n",
      "The loss after 663 iterations is E=0.010824251745163485\n",
      "Wrote weights to file ./customFunctionLearningAdaptive/baselineMLP1000/best_weights\n",
      "The loss after 664 iterations is E=0.010793748618555046\n",
      "Wrote weights to file ./customFunctionLearningAdaptive/baselineMLP1000/best_weights\n",
      "The loss after 665 iterations is E=0.010763337745253086\n",
      "Wrote weights to file ./customFunctionLearningAdaptive/baselineMLP1000/best_weights\n",
      "The loss after 666 iterations is E=0.010733020258776108\n",
      "Wrote weights to file ./customFunctionLearningAdaptive/baselineMLP1000/best_weights\n",
      "The loss after 667 iterations is E=0.010702797275313106\n",
      "Wrote weights to file ./customFunctionLearningAdaptive/baselineMLP1000/best_weights\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The loss after 668 iterations is E=0.01067266991080956\n",
      "Wrote weights to file ./customFunctionLearningAdaptive/baselineMLP1000/best_weights\n",
      "The loss after 669 iterations is E=0.010642639271152362\n",
      "Wrote weights to file ./customFunctionLearningAdaptive/baselineMLP1000/best_weights\n",
      "The loss after 670 iterations is E=0.010612706452727565\n",
      "Wrote weights to file ./customFunctionLearningAdaptive/baselineMLP1000/best_weights\n",
      "The loss after 671 iterations is E=0.010582872548941106\n",
      "Wrote weights to file ./customFunctionLearningAdaptive/baselineMLP1000/best_weights\n",
      "The loss after 672 iterations is E=0.010553138637072005\n",
      "Wrote weights to file ./customFunctionLearningAdaptive/baselineMLP1000/best_weights\n",
      "The loss after 673 iterations is E=0.010523505794217259\n",
      "Wrote weights to file ./customFunctionLearningAdaptive/baselineMLP1000/best_weights\n",
      "The loss after 674 iterations is E=0.010493975079641003\n",
      "Wrote weights to file ./customFunctionLearningAdaptive/baselineMLP1000/best_weights\n",
      "The loss after 675 iterations is E=0.010464547550305296\n",
      "Wrote weights to file ./customFunctionLearningAdaptive/baselineMLP1000/best_weights\n",
      "The loss after 676 iterations is E=0.010435224247732137\n",
      "Wrote weights to file ./customFunctionLearningAdaptive/baselineMLP1000/best_weights\n",
      "The loss after 677 iterations is E=0.010406006206047317\n",
      "Wrote weights to file ./customFunctionLearningAdaptive/baselineMLP1000/best_weights\n",
      "The loss after 678 iterations is E=0.01037689444795902\n",
      "Wrote weights to file ./customFunctionLearningAdaptive/baselineMLP1000/best_weights\n",
      "The loss after 679 iterations is E=0.01034788998335225\n",
      "Wrote weights to file ./customFunctionLearningAdaptive/baselineMLP1000/best_weights\n",
      "The loss after 680 iterations is E=0.010318993813948199\n",
      "Wrote weights to file ./customFunctionLearningAdaptive/baselineMLP1000/best_weights\n",
      "The loss after 681 iterations is E=0.01029020692478036\n",
      "Wrote weights to file ./customFunctionLearningAdaptive/baselineMLP1000/best_weights\n",
      "The loss after 682 iterations is E=0.010261530293908819\n",
      "Wrote weights to file ./customFunctionLearningAdaptive/baselineMLP1000/best_weights\n",
      "The loss after 683 iterations is E=0.010232964881050579\n",
      "Wrote weights to file ./customFunctionLearningAdaptive/baselineMLP1000/best_weights\n",
      "The loss after 684 iterations is E=0.010204511638003051\n",
      "Wrote weights to file ./customFunctionLearningAdaptive/baselineMLP1000/best_weights\n",
      "The loss after 685 iterations is E=0.010176171498406328\n",
      "Wrote weights to file ./customFunctionLearningAdaptive/baselineMLP1000/best_weights\n",
      "The loss after 686 iterations is E=0.010147945385590482\n",
      "Wrote weights to file ./customFunctionLearningAdaptive/baselineMLP1000/best_weights\n",
      "The loss after 687 iterations is E=0.0101198342059134\n",
      "Wrote weights to file ./customFunctionLearningAdaptive/baselineMLP1000/best_weights\n",
      "The loss after 688 iterations is E=0.010091838852480045\n",
      "Wrote weights to file ./customFunctionLearningAdaptive/baselineMLP1000/best_weights\n",
      "The loss after 689 iterations is E=0.010063960202783027\n",
      "Wrote weights to file ./customFunctionLearningAdaptive/baselineMLP1000/best_weights\n",
      "The loss after 690 iterations is E=0.010036199118270526\n",
      "Wrote weights to file ./customFunctionLearningAdaptive/baselineMLP1000/best_weights\n",
      "The loss after 691 iterations is E=0.010008556445772051\n",
      "Wrote weights to file ./customFunctionLearningAdaptive/baselineMLP1000/best_weights\n",
      "The loss after 692 iterations is E=0.009981033013806245\n",
      "Using every 18th point for                         training.\n",
      "Selecting subset for training: 150 out of 2700 selected.\n",
      "Wrote weights to file ./customFunctionLearningAdaptive/baselineMLP1000/best_weights\n",
      "The loss after 693 iterations is E=0.01008107374076269\n",
      "Wrote weights to file ./customFunctionLearningAdaptive/baselineMLP1000/best_weights\n",
      "The loss after 697 iterations is E=0.007714492777143728\n",
      "Using every 16th point for                         training.\n",
      "Selecting subset for training: 168 out of 2700 selected.\n",
      "Wrote weights to file ./customFunctionLearningAdaptive/baselineMLP1000/best_weights\n",
      "The loss after 698 iterations is E=0.015149453390014311\n",
      "Wrote weights to file ./customFunctionLearningAdaptive/baselineMLP1000/best_weights\n",
      "The loss after 699 iterations is E=0.012919368773066181\n",
      "Wrote weights to file ./customFunctionLearningAdaptive/baselineMLP1000/best_weights\n",
      "The loss after 700 iterations is E=0.009223583412809702\n",
      "Using every 14th point for                         training.\n",
      "Selecting subset for training: 192 out of 2700 selected.\n",
      "Wrote weights to file ./customFunctionLearningAdaptive/baselineMLP1000/best_weights\n",
      "The loss after 701 iterations is E=0.01089428777741314\n",
      "Wrote weights to file ./customFunctionLearningAdaptive/baselineMLP1000/best_weights\n",
      "The loss after 702 iterations is E=0.008389048159641503\n",
      "Using every 12th point for                         training.\n",
      "Selecting subset for training: 225 out of 2700 selected.\n",
      "Wrote weights to file ./customFunctionLearningAdaptive/baselineMLP1000/best_weights\n",
      "The loss after 703 iterations is E=0.011392053351519823\n",
      "Wrote weights to file ./customFunctionLearningAdaptive/baselineMLP1000/best_weights\n",
      "The loss after 716 iterations is E=0.011090483076913153\n",
      "Wrote weights to file ./customFunctionLearningAdaptive/baselineMLP1000/best_weights\n",
      "The loss after 719 iterations is E=0.0109688677725819\n",
      "Wrote weights to file ./customFunctionLearningAdaptive/baselineMLP1000/best_weights\n",
      "The loss after 732 iterations is E=0.010949371999605788\n",
      "Wrote weights to file ./customFunctionLearningAdaptive/baselineMLP1000/best_weights\n",
      "The loss after 735 iterations is E=0.010865413684695903\n",
      "Wrote weights to file ./customFunctionLearningAdaptive/baselineMLP1000/best_weights\n",
      "The loss after 738 iterations is E=0.010809327591086984\n",
      "Wrote weights to file ./customFunctionLearningAdaptive/baselineMLP1000/best_weights\n",
      "The loss after 741 iterations is E=0.010773214099966571\n",
      "Wrote weights to file ./customFunctionLearningAdaptive/baselineMLP1000/best_weights\n",
      "The loss after 744 iterations is E=0.010748789117183593\n",
      "Wrote weights to file ./customFunctionLearningAdaptive/baselineMLP1000/best_weights\n",
      "The loss after 747 iterations is E=0.010729320416218257\n",
      "Wrote weights to file ./customFunctionLearningAdaptive/baselineMLP1000/best_weights\n",
      "The loss after 750 iterations is E=0.01071100474651554\n",
      "Wrote weights to file ./customFunctionLearningAdaptive/baselineMLP1000/best_weights\n",
      "The loss after 753 iterations is E=0.010692655239299458\n",
      "Wrote weights to file ./customFunctionLearningAdaptive/baselineMLP1000/best_weights\n",
      "The loss after 756 iterations is E=0.010674362603370301\n",
      "Wrote weights to file ./customFunctionLearningAdaptive/baselineMLP1000/best_weights\n",
      "The loss after 757 iterations is E=0.010668497341712048\n",
      "Wrote weights to file ./customFunctionLearningAdaptive/baselineMLP1000/best_weights\n",
      "The loss after 759 iterations is E=0.010656259119103603\n",
      "Wrote weights to file ./customFunctionLearningAdaptive/baselineMLP1000/best_weights\n",
      "The loss after 760 iterations is E=0.01064237937408592\n",
      "Wrote weights to file ./customFunctionLearningAdaptive/baselineMLP1000/best_weights\n",
      "The loss after 762 iterations is E=0.010638031032640901\n",
      "Wrote weights to file ./customFunctionLearningAdaptive/baselineMLP1000/best_weights\n",
      "The loss after 763 iterations is E=0.01062051595263414\n",
      "Wrote weights to file ./customFunctionLearningAdaptive/baselineMLP1000/best_weights\n",
      "The loss after 765 iterations is E=0.010619121581099553\n",
      "Wrote weights to file ./customFunctionLearningAdaptive/baselineMLP1000/best_weights\n",
      "The loss after 766 iterations is E=0.01060130116926731\n",
      "Wrote weights to file ./customFunctionLearningAdaptive/baselineMLP1000/best_weights\n",
      "The loss after 768 iterations is E=0.010599171365310455\n",
      "Wrote weights to file ./customFunctionLearningAdaptive/baselineMLP1000/best_weights\n",
      "The loss after 769 iterations is E=0.010583317618107473\n",
      "Wrote weights to file ./customFunctionLearningAdaptive/baselineMLP1000/best_weights\n",
      "The loss after 770 iterations is E=0.010579708128420512\n",
      "Wrote weights to file ./customFunctionLearningAdaptive/baselineMLP1000/best_weights\n",
      "The loss after 771 iterations is E=0.010578278502817982\n",
      "Wrote weights to file ./customFunctionLearningAdaptive/baselineMLP1000/best_weights\n",
      "The loss after 772 iterations is E=0.010565581400280164\n",
      "Wrote weights to file ./customFunctionLearningAdaptive/baselineMLP1000/best_weights\n",
      "The loss after 773 iterations is E=0.010558377021924489\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wrote weights to file ./customFunctionLearningAdaptive/baselineMLP1000/best_weights\n",
      "The loss after 774 iterations is E=0.010556939043287113\n",
      "Wrote weights to file ./customFunctionLearningAdaptive/baselineMLP1000/best_weights\n",
      "The loss after 775 iterations is E=0.010547570528786887\n",
      "Wrote weights to file ./customFunctionLearningAdaptive/baselineMLP1000/best_weights\n",
      "The loss after 776 iterations is E=0.0105385219766031\n",
      "Wrote weights to file ./customFunctionLearningAdaptive/baselineMLP1000/best_weights\n",
      "The loss after 777 iterations is E=0.010535782607585698\n",
      "Wrote weights to file ./customFunctionLearningAdaptive/baselineMLP1000/best_weights\n",
      "The loss after 778 iterations is E=0.01052911533378691\n",
      "Wrote weights to file ./customFunctionLearningAdaptive/baselineMLP1000/best_weights\n",
      "The loss after 779 iterations is E=0.0105197984187246\n",
      "Wrote weights to file ./customFunctionLearningAdaptive/baselineMLP1000/best_weights\n",
      "The loss after 780 iterations is E=0.010515302486004255\n",
      "Wrote weights to file ./customFunctionLearningAdaptive/baselineMLP1000/best_weights\n",
      "The loss after 781 iterations is E=0.010510266208728636\n",
      "Wrote weights to file ./customFunctionLearningAdaptive/baselineMLP1000/best_weights\n",
      "The loss after 782 iterations is E=0.010501744069075402\n",
      "Wrote weights to file ./customFunctionLearningAdaptive/baselineMLP1000/best_weights\n",
      "The loss after 783 iterations is E=0.01049571290657838\n",
      "Wrote weights to file ./customFunctionLearningAdaptive/baselineMLP1000/best_weights\n",
      "The loss after 784 iterations is E=0.010491196482866894\n",
      "Wrote weights to file ./customFunctionLearningAdaptive/baselineMLP1000/best_weights\n",
      "The loss after 785 iterations is E=0.01048389801782671\n",
      "Wrote weights to file ./customFunctionLearningAdaptive/baselineMLP1000/best_weights\n",
      "The loss after 786 iterations is E=0.010476953483165796\n",
      "Wrote weights to file ./customFunctionLearningAdaptive/baselineMLP1000/best_weights\n",
      "The loss after 787 iterations is E=0.010472134020347664\n",
      "Wrote weights to file ./customFunctionLearningAdaptive/baselineMLP1000/best_weights\n",
      "The loss after 788 iterations is E=0.010465938058573066\n",
      "Wrote weights to file ./customFunctionLearningAdaptive/baselineMLP1000/best_weights\n",
      "The loss after 789 iterations is E=0.010458787508676218\n",
      "Wrote weights to file ./customFunctionLearningAdaptive/baselineMLP1000/best_weights\n",
      "The loss after 790 iterations is E=0.010453295985459798\n",
      "Wrote weights to file ./customFunctionLearningAdaptive/baselineMLP1000/best_weights\n",
      "The loss after 791 iterations is E=0.010447755340836475\n",
      "Wrote weights to file ./customFunctionLearningAdaptive/baselineMLP1000/best_weights\n",
      "The loss after 792 iterations is E=0.010440927092497023\n",
      "Wrote weights to file ./customFunctionLearningAdaptive/baselineMLP1000/best_weights\n",
      "The loss after 793 iterations is E=0.010434822973151167\n",
      "Wrote weights to file ./customFunctionLearningAdaptive/baselineMLP1000/best_weights\n",
      "The loss after 794 iterations is E=0.010429432923918172\n",
      "Wrote weights to file ./customFunctionLearningAdaptive/baselineMLP1000/best_weights\n",
      "The loss after 795 iterations is E=0.010423138082157033\n",
      "Wrote weights to file ./customFunctionLearningAdaptive/baselineMLP1000/best_weights\n",
      "The loss after 796 iterations is E=0.010416736395879297\n",
      "Wrote weights to file ./customFunctionLearningAdaptive/baselineMLP1000/best_weights\n",
      "The loss after 797 iterations is E=0.010411149946108143\n",
      "Wrote weights to file ./customFunctionLearningAdaptive/baselineMLP1000/best_weights\n",
      "The loss after 798 iterations is E=0.010405300550189815\n",
      "Wrote weights to file ./customFunctionLearningAdaptive/baselineMLP1000/best_weights\n",
      "The loss after 799 iterations is E=0.010398946298200649\n",
      "Wrote weights to file ./customFunctionLearningAdaptive/baselineMLP1000/best_weights\n",
      "The loss after 800 iterations is E=0.010393066838112242\n",
      "Wrote weights to file ./customFunctionLearningAdaptive/baselineMLP1000/best_weights\n",
      "The loss after 801 iterations is E=0.010387415847190228\n",
      "Wrote weights to file ./customFunctionLearningAdaptive/baselineMLP1000/best_weights\n",
      "The loss after 802 iterations is E=0.010381311043206378\n",
      "Wrote weights to file ./customFunctionLearningAdaptive/baselineMLP1000/best_weights\n",
      "The loss after 803 iterations is E=0.010375248476178161\n",
      "Wrote weights to file ./customFunctionLearningAdaptive/baselineMLP1000/best_weights\n",
      "The loss after 804 iterations is E=0.010369565087963465\n",
      "Wrote weights to file ./customFunctionLearningAdaptive/baselineMLP1000/best_weights\n",
      "The loss after 805 iterations is E=0.010363716142839572\n",
      "Wrote weights to file ./customFunctionLearningAdaptive/baselineMLP1000/best_weights\n",
      "The loss after 806 iterations is E=0.010357655600151354\n",
      "Wrote weights to file ./customFunctionLearningAdaptive/baselineMLP1000/best_weights\n",
      "The loss after 807 iterations is E=0.010351841465009188\n",
      "Wrote weights to file ./customFunctionLearningAdaptive/baselineMLP1000/best_weights\n",
      "The loss after 808 iterations is E=0.010346125187330465\n",
      "Wrote weights to file ./customFunctionLearningAdaptive/baselineMLP1000/best_weights\n",
      "The loss after 809 iterations is E=0.010340195253288428\n",
      "Wrote weights to file ./customFunctionLearningAdaptive/baselineMLP1000/best_weights\n",
      "The loss after 810 iterations is E=0.010334292189917426\n",
      "Wrote weights to file ./customFunctionLearningAdaptive/baselineMLP1000/best_weights\n",
      "The loss after 811 iterations is E=0.010328574973696561\n",
      "Wrote weights to file ./customFunctionLearningAdaptive/baselineMLP1000/best_weights\n",
      "The loss after 812 iterations is E=0.010322789612634612\n",
      "Wrote weights to file ./customFunctionLearningAdaptive/baselineMLP1000/best_weights\n",
      "The loss after 813 iterations is E=0.01031690094812751\n",
      "Wrote weights to file ./customFunctionLearningAdaptive/baselineMLP1000/best_weights\n",
      "The loss after 814 iterations is E=0.01031112650706043\n",
      "Wrote weights to file ./customFunctionLearningAdaptive/baselineMLP1000/best_weights\n",
      "The loss after 815 iterations is E=0.010305417539267338\n",
      "Wrote weights to file ./customFunctionLearningAdaptive/baselineMLP1000/best_weights\n",
      "The loss after 816 iterations is E=0.01029961527815999\n",
      "Wrote weights to file ./customFunctionLearningAdaptive/baselineMLP1000/best_weights\n",
      "The loss after 817 iterations is E=0.010293813034215159\n",
      "Wrote weights to file ./customFunctionLearningAdaptive/baselineMLP1000/best_weights\n",
      "The loss after 818 iterations is E=0.010288107645095433\n",
      "Wrote weights to file ./customFunctionLearningAdaptive/baselineMLP1000/best_weights\n",
      "The loss after 819 iterations is E=0.010282392196603913\n",
      "Wrote weights to file ./customFunctionLearningAdaptive/baselineMLP1000/best_weights\n",
      "The loss after 820 iterations is E=0.01027662258825983\n",
      "Wrote weights to file ./customFunctionLearningAdaptive/baselineMLP1000/best_weights\n",
      "The loss after 821 iterations is E=0.010270898486274327\n",
      "Wrote weights to file ./customFunctionLearningAdaptive/baselineMLP1000/best_weights\n",
      "The loss after 822 iterations is E=0.010265225039272073\n",
      "Wrote weights to file ./customFunctionLearningAdaptive/baselineMLP1000/best_weights\n",
      "The loss after 823 iterations is E=0.01025952013244233\n",
      "Wrote weights to file ./customFunctionLearningAdaptive/baselineMLP1000/best_weights\n",
      "The loss after 824 iterations is E=0.010253803080797088\n",
      "Wrote weights to file ./customFunctionLearningAdaptive/baselineMLP1000/best_weights\n",
      "The loss after 825 iterations is E=0.010248134563524889\n",
      "Wrote weights to file ./customFunctionLearningAdaptive/baselineMLP1000/best_weights\n",
      "The loss after 826 iterations is E=0.010242481956713971\n",
      "Wrote weights to file ./customFunctionLearningAdaptive/baselineMLP1000/best_weights\n",
      "The loss after 827 iterations is E=0.010236804800171334\n",
      "Wrote weights to file ./customFunctionLearningAdaptive/baselineMLP1000/best_weights\n",
      "The loss after 828 iterations is E=0.010231139900341094\n",
      "Wrote weights to file ./customFunctionLearningAdaptive/baselineMLP1000/best_weights\n",
      "The loss after 829 iterations is E=0.010225510181586635\n",
      "Wrote weights to file ./customFunctionLearningAdaptive/baselineMLP1000/best_weights\n",
      "The loss after 830 iterations is E=0.010219880150807421\n",
      "Wrote weights to file ./customFunctionLearningAdaptive/baselineMLP1000/best_weights\n",
      "The loss after 831 iterations is E=0.010214239742519258\n",
      "Wrote weights to file ./customFunctionLearningAdaptive/baselineMLP1000/best_weights\n",
      "The loss after 832 iterations is E=0.010208619602717475\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wrote weights to file ./customFunctionLearningAdaptive/baselineMLP1000/best_weights\n",
      "The loss after 833 iterations is E=0.0102030209130999\n",
      "Wrote weights to file ./customFunctionLearningAdaptive/baselineMLP1000/best_weights\n",
      "The loss after 834 iterations is E=0.010197418584783794\n",
      "Wrote weights to file ./customFunctionLearningAdaptive/baselineMLP1000/best_weights\n",
      "The loss after 835 iterations is E=0.010191816711300603\n",
      "Wrote weights to file ./customFunctionLearningAdaptive/baselineMLP1000/best_weights\n",
      "The loss after 836 iterations is E=0.010186234692278312\n",
      "Wrote weights to file ./customFunctionLearningAdaptive/baselineMLP1000/best_weights\n",
      "The loss after 837 iterations is E=0.0101806652495528\n",
      "Wrote weights to file ./customFunctionLearningAdaptive/baselineMLP1000/best_weights\n",
      "The loss after 838 iterations is E=0.010175093972322437\n",
      "Wrote weights to file ./customFunctionLearningAdaptive/baselineMLP1000/best_weights\n",
      "The loss after 839 iterations is E=0.010169529308623592\n",
      "Wrote weights to file ./customFunctionLearningAdaptive/baselineMLP1000/best_weights\n",
      "The loss after 840 iterations is E=0.010163981178786497\n",
      "Wrote weights to file ./customFunctionLearningAdaptive/baselineMLP1000/best_weights\n",
      "The loss after 841 iterations is E=0.010158441054824018\n",
      "Wrote weights to file ./customFunctionLearningAdaptive/baselineMLP1000/best_weights\n",
      "The loss after 842 iterations is E=0.010152902206098996\n",
      "Wrote weights to file ./customFunctionLearningAdaptive/baselineMLP1000/best_weights\n",
      "The loss after 843 iterations is E=0.010147372290281247\n",
      "Wrote weights to file ./customFunctionLearningAdaptive/baselineMLP1000/best_weights\n",
      "The loss after 844 iterations is E=0.010141855589869141\n",
      "Wrote weights to file ./customFunctionLearningAdaptive/baselineMLP1000/best_weights\n",
      "The loss after 845 iterations is E=0.010136345202465358\n",
      "Wrote weights to file ./customFunctionLearningAdaptive/baselineMLP1000/best_weights\n",
      "The loss after 846 iterations is E=0.010130838382318784\n",
      "Wrote weights to file ./customFunctionLearningAdaptive/baselineMLP1000/best_weights\n",
      "The loss after 847 iterations is E=0.010125340971508164\n",
      "Wrote weights to file ./customFunctionLearningAdaptive/baselineMLP1000/best_weights\n",
      "The loss after 848 iterations is E=0.01011985432938482\n",
      "Wrote weights to file ./customFunctionLearningAdaptive/baselineMLP1000/best_weights\n",
      "The loss after 849 iterations is E=0.010114373493100098\n",
      "Wrote weights to file ./customFunctionLearningAdaptive/baselineMLP1000/best_weights\n",
      "The loss after 850 iterations is E=0.010108897772996112\n",
      "Wrote weights to file ./customFunctionLearningAdaptive/baselineMLP1000/best_weights\n",
      "The loss after 851 iterations is E=0.010103431033441667\n",
      "Wrote weights to file ./customFunctionLearningAdaptive/baselineMLP1000/best_weights\n",
      "The loss after 852 iterations is E=0.010097973444254987\n",
      "Wrote weights to file ./customFunctionLearningAdaptive/baselineMLP1000/best_weights\n",
      "The loss after 853 iterations is E=0.010092521761210223\n",
      "Wrote weights to file ./customFunctionLearningAdaptive/baselineMLP1000/best_weights\n",
      "The loss after 854 iterations is E=0.010087075903353776\n",
      "Wrote weights to file ./customFunctionLearningAdaptive/baselineMLP1000/best_weights\n",
      "The loss after 855 iterations is E=0.010081638430522757\n",
      "Wrote weights to file ./customFunctionLearningAdaptive/baselineMLP1000/best_weights\n",
      "The loss after 856 iterations is E=0.010076209122582905\n",
      "Wrote weights to file ./customFunctionLearningAdaptive/baselineMLP1000/best_weights\n",
      "The loss after 857 iterations is E=0.010070785813198337\n",
      "Wrote weights to file ./customFunctionLearningAdaptive/baselineMLP1000/best_weights\n",
      "The loss after 858 iterations is E=0.01006536868008165\n",
      "Wrote weights to file ./customFunctionLearningAdaptive/baselineMLP1000/best_weights\n",
      "The loss after 859 iterations is E=0.010059959297773051\n",
      "Wrote weights to file ./customFunctionLearningAdaptive/baselineMLP1000/best_weights\n",
      "The loss after 860 iterations is E=0.010054557409967111\n",
      "Wrote weights to file ./customFunctionLearningAdaptive/baselineMLP1000/best_weights\n",
      "The loss after 861 iterations is E=0.010049161641442301\n",
      "Wrote weights to file ./customFunctionLearningAdaptive/baselineMLP1000/best_weights\n",
      "The loss after 862 iterations is E=0.010043772058233372\n",
      "Wrote weights to file ./customFunctionLearningAdaptive/baselineMLP1000/best_weights\n",
      "The loss after 863 iterations is E=0.010038389694875178\n",
      "Wrote weights to file ./customFunctionLearningAdaptive/baselineMLP1000/best_weights\n",
      "The loss after 864 iterations is E=0.010033014370578429\n",
      "Wrote weights to file ./customFunctionLearningAdaptive/baselineMLP1000/best_weights\n",
      "The loss after 865 iterations is E=0.010027645135682885\n",
      "Wrote weights to file ./customFunctionLearningAdaptive/baselineMLP1000/best_weights\n",
      "The loss after 866 iterations is E=0.010022282024455502\n",
      "Wrote weights to file ./customFunctionLearningAdaptive/baselineMLP1000/best_weights\n",
      "The loss after 867 iterations is E=0.010016925656178564\n",
      "Wrote weights to file ./customFunctionLearningAdaptive/baselineMLP1000/best_weights\n",
      "The loss after 868 iterations is E=0.010011575943078076\n",
      "Wrote weights to file ./customFunctionLearningAdaptive/baselineMLP1000/best_weights\n",
      "The loss after 869 iterations is E=0.010006232273656782\n",
      "Wrote weights to file ./customFunctionLearningAdaptive/baselineMLP1000/best_weights\n",
      "The loss after 870 iterations is E=0.010000894553005193\n",
      "Wrote weights to file ./customFunctionLearningAdaptive/baselineMLP1000/best_weights\n",
      "The loss after 871 iterations is E=0.009995563188002765\n",
      "Using every 10th point for                         training.\n",
      "Selecting subset for training: 270 out of 2700 selected.\n",
      "Wrote weights to file ./customFunctionLearningAdaptive/baselineMLP1000/best_weights\n",
      "The loss after 872 iterations is E=0.01106096855453905\n",
      "Wrote weights to file ./customFunctionLearningAdaptive/baselineMLP1000/best_weights\n",
      "The loss after 874 iterations is E=0.010601455272734038\n",
      "Wrote weights to file ./customFunctionLearningAdaptive/baselineMLP1000/best_weights\n",
      "The loss after 875 iterations is E=0.0078265452667871\n",
      "Using every 9th point for                         training.\n",
      "Selecting subset for training: 300 out of 2700 selected.\n",
      "Wrote weights to file ./customFunctionLearningAdaptive/baselineMLP1000/best_weights\n",
      "The loss after 876 iterations is E=0.024396924819272178\n",
      "Wrote weights to file ./customFunctionLearningAdaptive/baselineMLP1000/best_weights\n",
      "The loss after 879 iterations is E=0.01826731656111033\n",
      "Wrote weights to file ./customFunctionLearningAdaptive/baselineMLP1000/best_weights\n",
      "The loss after 882 iterations is E=0.017262090894628046\n",
      "Wrote weights to file ./customFunctionLearningAdaptive/baselineMLP1000/best_weights\n",
      "The loss after 891 iterations is E=0.016229883159613236\n",
      "Wrote weights to file ./customFunctionLearningAdaptive/baselineMLP1000/best_weights\n",
      "The loss after 893 iterations is E=0.01618971747536955\n",
      "Wrote weights to file ./customFunctionLearningAdaptive/baselineMLP1000/best_weights\n",
      "The loss after 915 iterations is E=0.016185178188991782\n",
      "Wrote weights to file ./customFunctionLearningAdaptive/baselineMLP1000/best_weights\n",
      "The loss after 917 iterations is E=0.0160815099845409\n",
      "Wrote weights to file ./customFunctionLearningAdaptive/baselineMLP1000/best_weights\n",
      "The loss after 918 iterations is E=0.01606663617391497\n",
      "Wrote weights to file ./customFunctionLearningAdaptive/baselineMLP1000/best_weights\n",
      "The loss after 919 iterations is E=0.016014606874192365\n",
      "Wrote weights to file ./customFunctionLearningAdaptive/baselineMLP1000/best_weights\n",
      "The loss after 920 iterations is E=0.015974237354094173\n",
      "Wrote weights to file ./customFunctionLearningAdaptive/baselineMLP1000/best_weights\n",
      "The loss after 921 iterations is E=0.01593920090228881\n",
      "Wrote weights to file ./customFunctionLearningAdaptive/baselineMLP1000/best_weights\n",
      "The loss after 922 iterations is E=0.015884786699755078\n",
      "Wrote weights to file ./customFunctionLearningAdaptive/baselineMLP1000/best_weights\n",
      "The loss after 923 iterations is E=0.015884076631483265\n",
      "Wrote weights to file ./customFunctionLearningAdaptive/baselineMLP1000/best_weights\n",
      "The loss after 924 iterations is E=0.01582077921605439\n",
      "Wrote weights to file ./customFunctionLearningAdaptive/baselineMLP1000/best_weights\n",
      "The loss after 926 iterations is E=0.015758673377396355\n",
      "Wrote weights to file ./customFunctionLearningAdaptive/baselineMLP1000/best_weights\n",
      "The loss after 928 iterations is E=0.015712132603416362\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wrote weights to file ./customFunctionLearningAdaptive/baselineMLP1000/best_weights\n",
      "The loss after 930 iterations is E=0.015667339047515157\n",
      "Wrote weights to file ./customFunctionLearningAdaptive/baselineMLP1000/best_weights\n",
      "The loss after 932 iterations is E=0.015632347292667728\n",
      "Wrote weights to file ./customFunctionLearningAdaptive/baselineMLP1000/best_weights\n",
      "The loss after 934 iterations is E=0.015599051886606437\n",
      "Wrote weights to file ./customFunctionLearningAdaptive/baselineMLP1000/best_weights\n",
      "The loss after 936 iterations is E=0.015571955757130732\n",
      "Wrote weights to file ./customFunctionLearningAdaptive/baselineMLP1000/best_weights\n",
      "The loss after 938 iterations is E=0.015546320695133384\n",
      "Wrote weights to file ./customFunctionLearningAdaptive/baselineMLP1000/best_weights\n",
      "The loss after 940 iterations is E=0.015524487138617612\n",
      "Wrote weights to file ./customFunctionLearningAdaptive/baselineMLP1000/best_weights\n",
      "The loss after 942 iterations is E=0.015503763065462167\n",
      "Wrote weights to file ./customFunctionLearningAdaptive/baselineMLP1000/best_weights\n",
      "The loss after 944 iterations is E=0.015485261001243719\n",
      "Wrote weights to file ./customFunctionLearningAdaptive/baselineMLP1000/best_weights\n",
      "The loss after 946 iterations is E=0.015467548571279388\n",
      "Wrote weights to file ./customFunctionLearningAdaptive/baselineMLP1000/best_weights\n",
      "The loss after 948 iterations is E=0.015451086563933417\n",
      "Wrote weights to file ./customFunctionLearningAdaptive/baselineMLP1000/best_weights\n",
      "The loss after 950 iterations is E=0.01543518480972468\n",
      "Wrote weights to file ./customFunctionLearningAdaptive/baselineMLP1000/best_weights\n",
      "The loss after 952 iterations is E=0.015419973642952138\n",
      "Wrote weights to file ./customFunctionLearningAdaptive/baselineMLP1000/best_weights\n",
      "The loss after 954 iterations is E=0.0154051591979963\n",
      "Wrote weights to file ./customFunctionLearningAdaptive/baselineMLP1000/best_weights\n",
      "The loss after 956 iterations is E=0.015390715864507788\n",
      "Wrote weights to file ./customFunctionLearningAdaptive/baselineMLP1000/best_weights\n",
      "The loss after 958 iterations is E=0.015376536899851146\n",
      "Wrote weights to file ./customFunctionLearningAdaptive/baselineMLP1000/best_weights\n",
      "The loss after 960 iterations is E=0.015362541236859623\n",
      "Wrote weights to file ./customFunctionLearningAdaptive/baselineMLP1000/best_weights\n",
      "The loss after 962 iterations is E=0.015348701124901316\n",
      "Wrote weights to file ./customFunctionLearningAdaptive/baselineMLP1000/best_weights\n",
      "The loss after 963 iterations is E=0.015347107894337037\n",
      "Wrote weights to file ./customFunctionLearningAdaptive/baselineMLP1000/best_weights\n",
      "The loss after 964 iterations is E=0.015334937083069\n",
      "Wrote weights to file ./customFunctionLearningAdaptive/baselineMLP1000/best_weights\n",
      "The loss after 965 iterations is E=0.015331888717388045\n",
      "Wrote weights to file ./customFunctionLearningAdaptive/baselineMLP1000/best_weights\n",
      "The loss after 966 iterations is E=0.01532125139189029\n",
      "Wrote weights to file ./customFunctionLearningAdaptive/baselineMLP1000/best_weights\n",
      "The loss after 967 iterations is E=0.015316978498801126\n",
      "Wrote weights to file ./customFunctionLearningAdaptive/baselineMLP1000/best_weights\n",
      "The loss after 968 iterations is E=0.015307590408366642\n",
      "Wrote weights to file ./customFunctionLearningAdaptive/baselineMLP1000/best_weights\n",
      "The loss after 969 iterations is E=0.015302352738657014\n",
      "Wrote weights to file ./customFunctionLearningAdaptive/baselineMLP1000/best_weights\n",
      "The loss after 970 iterations is E=0.015293964770397225\n",
      "Wrote weights to file ./customFunctionLearningAdaptive/baselineMLP1000/best_weights\n",
      "The loss after 971 iterations is E=0.015287967004525385\n",
      "Wrote weights to file ./customFunctionLearningAdaptive/baselineMLP1000/best_weights\n",
      "The loss after 972 iterations is E=0.015280348200811669\n",
      "Wrote weights to file ./customFunctionLearningAdaptive/baselineMLP1000/best_weights\n",
      "The loss after 973 iterations is E=0.015273789722073787\n",
      "Wrote weights to file ./customFunctionLearningAdaptive/baselineMLP1000/best_weights\n",
      "The loss after 974 iterations is E=0.015266749883931934\n",
      "Wrote weights to file ./customFunctionLearningAdaptive/baselineMLP1000/best_weights\n",
      "The loss after 975 iterations is E=0.015259793860037438\n",
      "Wrote weights to file ./customFunctionLearningAdaptive/baselineMLP1000/best_weights\n",
      "The loss after 976 iterations is E=0.015253162198962372\n",
      "Wrote weights to file ./customFunctionLearningAdaptive/baselineMLP1000/best_weights\n",
      "The loss after 977 iterations is E=0.015245948992681217\n",
      "Wrote weights to file ./customFunctionLearningAdaptive/baselineMLP1000/best_weights\n",
      "The loss after 978 iterations is E=0.015239589764760616\n",
      "Wrote weights to file ./customFunctionLearningAdaptive/baselineMLP1000/best_weights\n",
      "The loss after 979 iterations is E=0.015232234636386064\n",
      "Wrote weights to file ./customFunctionLearningAdaptive/baselineMLP1000/best_weights\n",
      "The loss after 980 iterations is E=0.015226034268453264\n",
      "Wrote weights to file ./customFunctionLearningAdaptive/baselineMLP1000/best_weights\n",
      "The loss after 981 iterations is E=0.015218625159985016\n",
      "Wrote weights to file ./customFunctionLearningAdaptive/baselineMLP1000/best_weights\n",
      "The loss after 982 iterations is E=0.015212496660122424\n",
      "Wrote weights to file ./customFunctionLearningAdaptive/baselineMLP1000/best_weights\n",
      "The loss after 983 iterations is E=0.015205103078609518\n",
      "Wrote weights to file ./customFunctionLearningAdaptive/baselineMLP1000/best_weights\n",
      "The loss after 984 iterations is E=0.015198981727892708\n",
      "Wrote weights to file ./customFunctionLearningAdaptive/baselineMLP1000/best_weights\n",
      "The loss after 985 iterations is E=0.015191649109652048\n",
      "Wrote weights to file ./customFunctionLearningAdaptive/baselineMLP1000/best_weights\n",
      "The loss after 986 iterations is E=0.015185488797321595\n",
      "Wrote weights to file ./customFunctionLearningAdaptive/baselineMLP1000/best_weights\n",
      "The loss after 987 iterations is E=0.015178248981811795\n",
      "Wrote weights to file ./customFunctionLearningAdaptive/baselineMLP1000/best_weights\n",
      "The loss after 988 iterations is E=0.015172022455942286\n",
      "Wrote weights to file ./customFunctionLearningAdaptive/baselineMLP1000/best_weights\n",
      "The loss after 989 iterations is E=0.01516488972819189\n",
      "Wrote weights to file ./customFunctionLearningAdaptive/baselineMLP1000/best_weights\n",
      "The loss after 990 iterations is E=0.015158581591905569\n",
      "Wrote weights to file ./customFunctionLearningAdaptive/baselineMLP1000/best_weights\n",
      "The loss after 991 iterations is E=0.015151561063754005\n",
      "Wrote weights to file ./customFunctionLearningAdaptive/baselineMLP1000/best_weights\n",
      "The loss after 992 iterations is E=0.015145169024208451\n",
      "Wrote weights to file ./customFunctionLearningAdaptive/baselineMLP1000/best_weights\n",
      "The loss after 993 iterations is E=0.015138255122151124\n",
      "Wrote weights to file ./customFunctionLearningAdaptive/baselineMLP1000/best_weights\n",
      "The loss after 994 iterations is E=0.015131783251692463\n",
      "Wrote weights to file ./customFunctionLearningAdaptive/baselineMLP1000/best_weights\n",
      "The loss after 995 iterations is E=0.015124965578747077\n",
      "Wrote weights to file ./customFunctionLearningAdaptive/baselineMLP1000/best_weights\n",
      "The loss after 996 iterations is E=0.015118424845362306\n",
      "Wrote weights to file ./customFunctionLearningAdaptive/baselineMLP1000/best_weights\n",
      "The loss after 997 iterations is E=0.015111688059709569\n",
      "Wrote weights to file ./customFunctionLearningAdaptive/baselineMLP1000/best_weights\n",
      "The loss after 998 iterations is E=0.01510509160702486\n",
      "Wrote weights to file ./customFunctionLearningAdaptive/baselineMLP1000/best_weights\n",
      "The loss after 999 iterations is E=0.015098419442220849\n",
      "Wrote weights to file ./customFunctionLearningAdaptive/baselineMLP1000/best_weights\n",
      "The loss after 1000 iterations is E=0.015091782198887344\n",
      "Elapsed time:  2.62 min\n"
     ]
    }
   ],
   "source": [
    "start = time.time()\n",
    "customModel.solve(max_iter=1000, learning_rate=0.01, verbose=1, adaptive=True, tolerance=0.01, every=1000)\n",
    "print(\"Elapsed time: {:5.2f} min\".format((time.time() - start) / 60.0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Initialized weights and biases for MLP with 20 parameters.\n",
      "\n",
      "MLP structure:\n",
      "--------------\n",
      "Input features:    2\n",
      "Hidden layers:     1\n",
      "Neurons per layer: 5\n",
      "Number of weights: 20\n",
      "Loaded weights from file ./customFunctionLearningAdaptive/baselineMLP1000/best_weights.npy\n",
      "-------------------\n",
      "L2 norm:  0.00013313799350845017\n",
      "Lmax norm:  0.05359531848654081\n"
     ]
    }
   ],
   "source": [
    "baselineMLP = SimpleMLP(name='baselineMLP1000', number_of_inputs=2, neurons_per_layer=5, hidden_layers=1)\n",
    "customModel = CustomFunctionLearning(name=\"customFunctionLearningAdaptive\", mlp=baselineMLP, training_data=field50.A, beta=25.0, d_v=0.025)\n",
    "customModel.mlp.read_weights_from_disk(\"./customFunctionLearningAdaptive/baselineMLP1000/best_weights.npy\")\n",
    "evaluation_points = np.column_stack((field50.x, field50.y))\n",
    "l2_norm = customModel.l2_norm(evaluation_points, field50.A)\n",
    "lmax_norm = customModel.lmax_norm(evaluation_points, field50.A)\n",
    "print(\"-------------------\")\n",
    "print(\"L2 norm: \", l2_norm)\n",
    "print(\"Lmax norm: \", lmax_norm)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Non-adaptive after 1000 iterations (for comparison)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Initialized weights and biases for MLP with 20 parameters.\n",
      "\n",
      "MLP structure:\n",
      "--------------\n",
      "Input features:    2\n",
      "Hidden layers:     1\n",
      "Neurons per layer: 5\n",
      "Number of weights: 20\n"
     ]
    }
   ],
   "source": [
    "baselineMLP = SimpleMLP(name='baselineMLP1000', number_of_inputs=2, neurons_per_layer=5, hidden_layers=1)\n",
    "customModel = CustomFunctionLearning(name=\"customFunctionLearning\", mlp=baselineMLP, training_data=field50.A, beta=25.0, d_v=0.025)\n",
    "training_points = np.column_stack((field50.x, field50.y))\n",
    "customModel.set_control_points(training_points)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "The initial loss is 106.69989434100859\n",
      "\n",
      "Wrote weights to file ./customFunctionLearning/baselineMLP1000/best_weights\n",
      "The loss after 1 iterations is E=104.14263177946617\n",
      "Wrote weights to file ./customFunctionLearning/baselineMLP1000/best_weights\n",
      "The loss after 2 iterations is E=101.4669958326886\n",
      "Wrote weights to file ./customFunctionLearning/baselineMLP1000/best_weights\n",
      "The loss after 3 iterations is E=98.66338353437207\n",
      "Wrote weights to file ./customFunctionLearning/baselineMLP1000/best_weights\n",
      "The loss after 4 iterations is E=95.7212175517689\n",
      "Wrote weights to file ./customFunctionLearning/baselineMLP1000/best_weights\n",
      "The loss after 5 iterations is E=92.62885012542628\n",
      "Wrote weights to file ./customFunctionLearning/baselineMLP1000/best_weights\n",
      "The loss after 6 iterations is E=89.37346485455804\n",
      "Wrote weights to file ./customFunctionLearning/baselineMLP1000/best_weights\n",
      "The loss after 7 iterations is E=85.94098163872204\n",
      "Wrote weights to file ./customFunctionLearning/baselineMLP1000/best_weights\n",
      "The loss after 8 iterations is E=82.31597401890099\n",
      "Wrote weights to file ./customFunctionLearning/baselineMLP1000/best_weights\n",
      "The loss after 9 iterations is E=78.48161458060717\n",
      "Wrote weights to file ./customFunctionLearning/baselineMLP1000/best_weights\n",
      "The loss after 10 iterations is E=74.41967461932666\n",
      "Wrote weights to file ./customFunctionLearning/baselineMLP1000/best_weights\n",
      "The loss after 11 iterations is E=70.11062172830356\n",
      "Wrote weights to file ./customFunctionLearning/baselineMLP1000/best_weights\n",
      "The loss after 12 iterations is E=65.53388827500139\n",
      "Wrote weights to file ./customFunctionLearning/baselineMLP1000/best_weights\n",
      "The loss after 13 iterations is E=60.66843375265886\n",
      "Wrote weights to file ./customFunctionLearning/baselineMLP1000/best_weights\n",
      "The loss after 14 iterations is E=55.49381116123431\n",
      "Wrote weights to file ./customFunctionLearning/baselineMLP1000/best_weights\n",
      "The loss after 15 iterations is E=49.992103396156395\n",
      "Wrote weights to file ./customFunctionLearning/baselineMLP1000/best_weights\n",
      "The loss after 16 iterations is E=44.15138300088996\n",
      "Wrote weights to file ./customFunctionLearning/baselineMLP1000/best_weights\n",
      "The loss after 17 iterations is E=37.971899451872424\n",
      "Wrote weights to file ./customFunctionLearning/baselineMLP1000/best_weights\n",
      "The loss after 18 iterations is E=31.477306639500352\n",
      "Wrote weights to file ./customFunctionLearning/baselineMLP1000/best_weights\n",
      "The loss after 19 iterations is E=24.735622719706416\n",
      "Wrote weights to file ./customFunctionLearning/baselineMLP1000/best_weights\n",
      "The loss after 20 iterations is E=17.9002181220176\n",
      "Wrote weights to file ./customFunctionLearning/baselineMLP1000/best_weights\n",
      "The loss after 21 iterations is E=11.296493671494169\n",
      "Wrote weights to file ./customFunctionLearning/baselineMLP1000/best_weights\n",
      "The loss after 22 iterations is E=5.634784127796669\n",
      "Wrote weights to file ./customFunctionLearning/baselineMLP1000/best_weights\n",
      "The loss after 23 iterations is E=2.689639587814777\n",
      "Wrote weights to file ./customFunctionLearning/baselineMLP1000/best_weights\n",
      "The loss after 29 iterations is E=2.491412933888189\n",
      "Wrote weights to file ./customFunctionLearning/baselineMLP1000/best_weights\n",
      "The loss after 43 iterations is E=2.4641036193878154\n",
      "Wrote weights to file ./customFunctionLearning/baselineMLP1000/best_weights\n",
      "The loss after 44 iterations is E=2.1704337835855885\n",
      "Wrote weights to file ./customFunctionLearning/baselineMLP1000/best_weights\n",
      "The loss after 53 iterations is E=2.125789934478305\n",
      "Wrote weights to file ./customFunctionLearning/baselineMLP1000/best_weights\n",
      "The loss after 54 iterations is E=2.0328283100868467\n",
      "Wrote weights to file ./customFunctionLearning/baselineMLP1000/best_weights\n",
      "The loss after 64 iterations is E=2.02853700835548\n",
      "Wrote weights to file ./customFunctionLearning/baselineMLP1000/best_weights\n",
      "The loss after 65 iterations is E=1.9634580018945718\n",
      "Wrote weights to file ./customFunctionLearning/baselineMLP1000/best_weights\n",
      "The loss after 66 iterations is E=1.9374435096159088\n",
      "Wrote weights to file ./customFunctionLearning/baselineMLP1000/best_weights\n",
      "The loss after 74 iterations is E=1.9226582495414892\n",
      "Wrote weights to file ./customFunctionLearning/baselineMLP1000/best_weights\n",
      "The loss after 75 iterations is E=1.8914285082070768\n",
      "Wrote weights to file ./customFunctionLearning/baselineMLP1000/best_weights\n",
      "The loss after 76 iterations is E=1.8760080024708865\n",
      "Wrote weights to file ./customFunctionLearning/baselineMLP1000/best_weights\n",
      "The loss after 77 iterations is E=1.8747789432457556\n",
      "Wrote weights to file ./customFunctionLearning/baselineMLP1000/best_weights\n",
      "The loss after 83 iterations is E=1.8681855438009038\n",
      "Wrote weights to file ./customFunctionLearning/baselineMLP1000/best_weights\n",
      "The loss after 84 iterations is E=1.8504656941342037\n",
      "Wrote weights to file ./customFunctionLearning/baselineMLP1000/best_weights\n",
      "The loss after 85 iterations is E=1.834330172357951\n",
      "Wrote weights to file ./customFunctionLearning/baselineMLP1000/best_weights\n",
      "The loss after 86 iterations is E=1.8225115136730767\n",
      "Wrote weights to file ./customFunctionLearning/baselineMLP1000/best_weights\n",
      "The loss after 87 iterations is E=1.8160160492366513\n",
      "Wrote weights to file ./customFunctionLearning/baselineMLP1000/best_weights\n",
      "The loss after 88 iterations is E=1.8139747565539472\n",
      "Wrote weights to file ./customFunctionLearning/baselineMLP1000/best_weights\n",
      "The loss after 90 iterations is E=1.813888025611446\n",
      "Wrote weights to file ./customFunctionLearning/baselineMLP1000/best_weights\n",
      "The loss after 91 iterations is E=1.8111628133452475\n",
      "Wrote weights to file ./customFunctionLearning/baselineMLP1000/best_weights\n",
      "The loss after 92 iterations is E=1.8052288829066478\n",
      "Wrote weights to file ./customFunctionLearning/baselineMLP1000/best_weights\n",
      "The loss after 93 iterations is E=1.7967018463346984\n",
      "Wrote weights to file ./customFunctionLearning/baselineMLP1000/best_weights\n",
      "The loss after 94 iterations is E=1.787080501549547\n",
      "Wrote weights to file ./customFunctionLearning/baselineMLP1000/best_weights\n",
      "The loss after 95 iterations is E=1.7780121704455474\n",
      "Wrote weights to file ./customFunctionLearning/baselineMLP1000/best_weights\n",
      "The loss after 96 iterations is E=1.7706485156084912\n",
      "Wrote weights to file ./customFunctionLearning/baselineMLP1000/best_weights\n",
      "The loss after 97 iterations is E=1.765325540513375\n",
      "Wrote weights to file ./customFunctionLearning/baselineMLP1000/best_weights\n",
      "The loss after 98 iterations is E=1.761612906379481\n",
      "Wrote weights to file ./customFunctionLearning/baselineMLP1000/best_weights\n",
      "The loss after 99 iterations is E=1.7586247158955375\n",
      "Wrote weights to file ./customFunctionLearning/baselineMLP1000/best_weights\n",
      "The loss after 100 iterations is E=1.7554192286748267\n",
      "Wrote weights to file ./customFunctionLearning/baselineMLP1000/best_weights\n",
      "The loss after 101 iterations is E=1.7513316225137725\n",
      "Wrote weights to file ./customFunctionLearning/baselineMLP1000/best_weights\n",
      "The loss after 102 iterations is E=1.7461460477320627\n",
      "Wrote weights to file ./customFunctionLearning/baselineMLP1000/best_weights\n",
      "The loss after 103 iterations is E=1.7400853558959515\n",
      "Wrote weights to file ./customFunctionLearning/baselineMLP1000/best_weights\n",
      "The loss after 104 iterations is E=1.73365641303012\n",
      "Wrote weights to file ./customFunctionLearning/baselineMLP1000/best_weights\n",
      "The loss after 105 iterations is E=1.7274253871558234\n",
      "Wrote weights to file ./customFunctionLearning/baselineMLP1000/best_weights\n",
      "The loss after 106 iterations is E=1.7218075916243694\n",
      "Wrote weights to file ./customFunctionLearning/baselineMLP1000/best_weights\n",
      "The loss after 107 iterations is E=1.7169412308944623\n",
      "Wrote weights to file ./customFunctionLearning/baselineMLP1000/best_weights\n",
      "The loss after 108 iterations is E=1.7126788800985835\n",
      "Wrote weights to file ./customFunctionLearning/baselineMLP1000/best_weights\n",
      "The loss after 109 iterations is E=1.708685571456603\n",
      "Wrote weights to file ./customFunctionLearning/baselineMLP1000/best_weights\n",
      "The loss after 110 iterations is E=1.7045936434664883\n",
      "Wrote weights to file ./customFunctionLearning/baselineMLP1000/best_weights\n",
      "The loss after 111 iterations is E=1.7001473483614031\n",
      "Wrote weights to file ./customFunctionLearning/baselineMLP1000/best_weights\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The loss after 112 iterations is E=1.6952817025735079\n",
      "Wrote weights to file ./customFunctionLearning/baselineMLP1000/best_weights\n",
      "The loss after 113 iterations is E=1.690113948938757\n",
      "Wrote weights to file ./customFunctionLearning/baselineMLP1000/best_weights\n",
      "The loss after 114 iterations is E=1.6848651512869104\n",
      "Wrote weights to file ./customFunctionLearning/baselineMLP1000/best_weights\n",
      "The loss after 115 iterations is E=1.6797553702128551\n",
      "Wrote weights to file ./customFunctionLearning/baselineMLP1000/best_weights\n",
      "The loss after 116 iterations is E=1.6749188501797583\n",
      "Wrote weights to file ./customFunctionLearning/baselineMLP1000/best_weights\n",
      "The loss after 117 iterations is E=1.6703686031922023\n",
      "Wrote weights to file ./customFunctionLearning/baselineMLP1000/best_weights\n",
      "The loss after 118 iterations is E=1.6660142821278336\n",
      "Wrote weights to file ./customFunctionLearning/baselineMLP1000/best_weights\n",
      "The loss after 119 iterations is E=1.6617158232807976\n",
      "Wrote weights to file ./customFunctionLearning/baselineMLP1000/best_weights\n",
      "The loss after 120 iterations is E=1.6573454997381398\n",
      "Wrote weights to file ./customFunctionLearning/baselineMLP1000/best_weights\n",
      "The loss after 121 iterations is E=1.6528335980233986\n",
      "Wrote weights to file ./customFunctionLearning/baselineMLP1000/best_weights\n",
      "The loss after 122 iterations is E=1.6481839208795372\n",
      "Wrote weights to file ./customFunctionLearning/baselineMLP1000/best_weights\n",
      "The loss after 123 iterations is E=1.6434589278591625\n",
      "Wrote weights to file ./customFunctionLearning/baselineMLP1000/best_weights\n",
      "The loss after 124 iterations is E=1.6387453787059778\n",
      "Wrote weights to file ./customFunctionLearning/baselineMLP1000/best_weights\n",
      "The loss after 125 iterations is E=1.634116623016592\n",
      "Wrote weights to file ./customFunctionLearning/baselineMLP1000/best_weights\n",
      "The loss after 126 iterations is E=1.629606345317553\n",
      "Wrote weights to file ./customFunctionLearning/baselineMLP1000/best_weights\n",
      "The loss after 127 iterations is E=1.6252020171955124\n",
      "Wrote weights to file ./customFunctionLearning/baselineMLP1000/best_weights\n",
      "The loss after 128 iterations is E=1.620857530811548\n",
      "Wrote weights to file ./customFunctionLearning/baselineMLP1000/best_weights\n",
      "The loss after 129 iterations is E=1.6165170481292657\n",
      "Wrote weights to file ./customFunctionLearning/baselineMLP1000/best_weights\n",
      "The loss after 130 iterations is E=1.6121387832852294\n",
      "Wrote weights to file ./customFunctionLearning/baselineMLP1000/best_weights\n",
      "The loss after 131 iterations is E=1.607709143039156\n",
      "Wrote weights to file ./customFunctionLearning/baselineMLP1000/best_weights\n",
      "The loss after 132 iterations is E=1.6032430748980457\n",
      "Wrote weights to file ./customFunctionLearning/baselineMLP1000/best_weights\n",
      "The loss after 133 iterations is E=1.5987728203392015\n",
      "Wrote weights to file ./customFunctionLearning/baselineMLP1000/best_weights\n",
      "The loss after 134 iterations is E=1.5943316838073758\n",
      "Wrote weights to file ./customFunctionLearning/baselineMLP1000/best_weights\n",
      "The loss after 135 iterations is E=1.5899403224751871\n",
      "Wrote weights to file ./customFunctionLearning/baselineMLP1000/best_weights\n",
      "The loss after 136 iterations is E=1.5856006685549142\n",
      "Wrote weights to file ./customFunctionLearning/baselineMLP1000/best_weights\n",
      "The loss after 137 iterations is E=1.581298535832705\n",
      "Wrote weights to file ./customFunctionLearning/baselineMLP1000/best_weights\n",
      "The loss after 138 iterations is E=1.5770122464077068\n",
      "Wrote weights to file ./customFunctionLearning/baselineMLP1000/best_weights\n",
      "The loss after 139 iterations is E=1.5727226914929093\n",
      "Wrote weights to file ./customFunctionLearning/baselineMLP1000/best_weights\n",
      "The loss after 140 iterations is E=1.5684205229961443\n",
      "Wrote weights to file ./customFunctionLearning/baselineMLP1000/best_weights\n",
      "The loss after 141 iterations is E=1.5641081104060235\n",
      "Wrote weights to file ./customFunctionLearning/baselineMLP1000/best_weights\n",
      "The loss after 142 iterations is E=1.559796412326137\n",
      "Wrote weights to file ./customFunctionLearning/baselineMLP1000/best_weights\n",
      "The loss after 143 iterations is E=1.5554989183551726\n",
      "Wrote weights to file ./customFunctionLearning/baselineMLP1000/best_weights\n",
      "The loss after 144 iterations is E=1.5512256229210317\n",
      "Wrote weights to file ./customFunctionLearning/baselineMLP1000/best_weights\n",
      "The loss after 145 iterations is E=1.5469794817348996\n",
      "Wrote weights to file ./customFunctionLearning/baselineMLP1000/best_weights\n",
      "The loss after 146 iterations is E=1.542756384347821\n",
      "Wrote weights to file ./customFunctionLearning/baselineMLP1000/best_weights\n",
      "The loss after 147 iterations is E=1.5385480610866413\n",
      "Wrote weights to file ./customFunctionLearning/baselineMLP1000/best_weights\n",
      "The loss after 148 iterations is E=1.5343462201941465\n",
      "Wrote weights to file ./customFunctionLearning/baselineMLP1000/best_weights\n",
      "The loss after 149 iterations is E=1.530145976007477\n",
      "Wrote weights to file ./customFunctionLearning/baselineMLP1000/best_weights\n",
      "The loss after 150 iterations is E=1.5259472487826118\n",
      "Wrote weights to file ./customFunctionLearning/baselineMLP1000/best_weights\n",
      "The loss after 151 iterations is E=1.5217538966883304\n",
      "Wrote weights to file ./customFunctionLearning/baselineMLP1000/best_weights\n",
      "The loss after 152 iterations is E=1.5175713439216525\n",
      "Wrote weights to file ./customFunctionLearning/baselineMLP1000/best_weights\n",
      "The loss after 153 iterations is E=1.5134039754044055\n",
      "Wrote weights to file ./customFunctionLearning/baselineMLP1000/best_weights\n",
      "The loss after 154 iterations is E=1.5092534436154437\n",
      "Wrote weights to file ./customFunctionLearning/baselineMLP1000/best_weights\n",
      "The loss after 155 iterations is E=1.5051184409280802\n",
      "Wrote weights to file ./customFunctionLearning/baselineMLP1000/best_weights\n",
      "The loss after 156 iterations is E=1.5009957690632372\n",
      "Wrote weights to file ./customFunctionLearning/baselineMLP1000/best_weights\n",
      "The loss after 157 iterations is E=1.4968820175547912\n",
      "Wrote weights to file ./customFunctionLearning/baselineMLP1000/best_weights\n",
      "The loss after 158 iterations is E=1.4927750291113575\n",
      "Wrote weights to file ./customFunctionLearning/baselineMLP1000/best_weights\n",
      "The loss after 159 iterations is E=1.4886745676272626\n",
      "Wrote weights to file ./customFunctionLearning/baselineMLP1000/best_weights\n",
      "The loss after 160 iterations is E=1.484582046896405\n",
      "Wrote weights to file ./customFunctionLearning/baselineMLP1000/best_weights\n",
      "The loss after 161 iterations is E=1.4804996023575285\n",
      "Wrote weights to file ./customFunctionLearning/baselineMLP1000/best_weights\n",
      "The loss after 162 iterations is E=1.4764290189077596\n",
      "Wrote weights to file ./customFunctionLearning/baselineMLP1000/best_weights\n",
      "The loss after 163 iterations is E=1.4723710005353796\n",
      "Wrote weights to file ./customFunctionLearning/baselineMLP1000/best_weights\n",
      "The loss after 164 iterations is E=1.4683250370512861\n",
      "Wrote weights to file ./customFunctionLearning/baselineMLP1000/best_weights\n",
      "The loss after 165 iterations is E=1.464289821631766\n",
      "Wrote weights to file ./customFunctionLearning/baselineMLP1000/best_weights\n",
      "The loss after 166 iterations is E=1.4602639418097536\n",
      "Wrote weights to file ./customFunctionLearning/baselineMLP1000/best_weights\n",
      "The loss after 167 iterations is E=1.4562464948656835\n",
      "Wrote weights to file ./customFunctionLearning/baselineMLP1000/best_weights\n",
      "The loss after 168 iterations is E=1.4522373716413013\n",
      "Wrote weights to file ./customFunctionLearning/baselineMLP1000/best_weights\n",
      "The loss after 169 iterations is E=1.4482371426095728\n",
      "Wrote weights to file ./customFunctionLearning/baselineMLP1000/best_weights\n",
      "The loss after 170 iterations is E=1.4442466666119855\n",
      "Wrote weights to file ./customFunctionLearning/baselineMLP1000/best_weights\n",
      "The loss after 171 iterations is E=1.4402666438689635\n",
      "Wrote weights to file ./customFunctionLearning/baselineMLP1000/best_weights\n",
      "The loss after 172 iterations is E=1.4362973203719112\n",
      "Wrote weights to file ./customFunctionLearning/baselineMLP1000/best_weights\n",
      "The loss after 173 iterations is E=1.4323384470842109\n",
      "Wrote weights to file ./customFunctionLearning/baselineMLP1000/best_weights\n",
      "The loss after 174 iterations is E=1.4283894661889291\n",
      "Wrote weights to file ./customFunctionLearning/baselineMLP1000/best_weights\n",
      "The loss after 175 iterations is E=1.424449800990707\n",
      "Wrote weights to file ./customFunctionLearning/baselineMLP1000/best_weights\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The loss after 176 iterations is E=1.4205191021143728\n",
      "Wrote weights to file ./customFunctionLearning/baselineMLP1000/best_weights\n",
      "The loss after 177 iterations is E=1.416597348286397\n",
      "Wrote weights to file ./customFunctionLearning/baselineMLP1000/best_weights\n",
      "The loss after 178 iterations is E=1.4126847827418234\n",
      "Wrote weights to file ./customFunctionLearning/baselineMLP1000/best_weights\n",
      "The loss after 179 iterations is E=1.4087817429555127\n",
      "Wrote weights to file ./customFunctionLearning/baselineMLP1000/best_weights\n",
      "The loss after 180 iterations is E=1.4048884789365375\n",
      "Wrote weights to file ./customFunctionLearning/baselineMLP1000/best_weights\n",
      "The loss after 181 iterations is E=1.4010050437520918\n",
      "Wrote weights to file ./customFunctionLearning/baselineMLP1000/best_weights\n",
      "The loss after 182 iterations is E=1.3971312928339612\n",
      "Wrote weights to file ./customFunctionLearning/baselineMLP1000/best_weights\n",
      "The loss after 183 iterations is E=1.393266973167305\n",
      "Wrote weights to file ./customFunctionLearning/baselineMLP1000/best_weights\n",
      "The loss after 184 iterations is E=1.3894118460903668\n",
      "Wrote weights to file ./customFunctionLearning/baselineMLP1000/best_weights\n",
      "The loss after 185 iterations is E=1.3855657820105325\n",
      "Wrote weights to file ./customFunctionLearning/baselineMLP1000/best_weights\n",
      "The loss after 186 iterations is E=1.3817287889155088\n",
      "Wrote weights to file ./customFunctionLearning/baselineMLP1000/best_weights\n",
      "The loss after 187 iterations is E=1.3779009734386494\n",
      "Wrote weights to file ./customFunctionLearning/baselineMLP1000/best_weights\n",
      "The loss after 188 iterations is E=1.3740824642747134\n",
      "Wrote weights to file ./customFunctionLearning/baselineMLP1000/best_weights\n",
      "The loss after 189 iterations is E=1.3702733397109543\n",
      "Wrote weights to file ./customFunctionLearning/baselineMLP1000/best_weights\n",
      "The loss after 190 iterations is E=1.366473591744146\n",
      "Wrote weights to file ./customFunctionLearning/baselineMLP1000/best_weights\n",
      "The loss after 191 iterations is E=1.3626831366744492\n",
      "Wrote weights to file ./customFunctionLearning/baselineMLP1000/best_weights\n",
      "The loss after 192 iterations is E=1.3589018589203383\n",
      "Wrote weights to file ./customFunctionLearning/baselineMLP1000/best_weights\n",
      "The loss after 193 iterations is E=1.35512966184913\n",
      "Wrote weights to file ./customFunctionLearning/baselineMLP1000/best_weights\n",
      "The loss after 194 iterations is E=1.3513665008491456\n",
      "Wrote weights to file ./customFunctionLearning/baselineMLP1000/best_weights\n",
      "The loss after 195 iterations is E=1.347612386498423\n",
      "Wrote weights to file ./customFunctionLearning/baselineMLP1000/best_weights\n",
      "The loss after 196 iterations is E=1.3438673617116899\n",
      "Wrote weights to file ./customFunctionLearning/baselineMLP1000/best_weights\n",
      "The loss after 197 iterations is E=1.340131468154566\n",
      "Wrote weights to file ./customFunctionLearning/baselineMLP1000/best_weights\n",
      "The loss after 198 iterations is E=1.3364047195088644\n",
      "Wrote weights to file ./customFunctionLearning/baselineMLP1000/best_weights\n",
      "The loss after 199 iterations is E=1.3326870928672625\n",
      "Wrote weights to file ./customFunctionLearning/baselineMLP1000/best_weights\n",
      "The loss after 200 iterations is E=1.3289785390598339\n",
      "Wrote weights to file ./customFunctionLearning/baselineMLP1000/best_weights\n",
      "The loss after 201 iterations is E=1.325279003691172\n",
      "Wrote weights to file ./customFunctionLearning/baselineMLP1000/best_weights\n",
      "The loss after 202 iterations is E=1.321588447080447\n",
      "Wrote weights to file ./customFunctionLearning/baselineMLP1000/best_weights\n",
      "The loss after 203 iterations is E=1.3179068538968868\n",
      "Wrote weights to file ./customFunctionLearning/baselineMLP1000/best_weights\n",
      "The loss after 204 iterations is E=1.3142342297746334\n",
      "Wrote weights to file ./customFunctionLearning/baselineMLP1000/best_weights\n",
      "The loss after 205 iterations is E=1.3105705888291952\n",
      "Wrote weights to file ./customFunctionLearning/baselineMLP1000/best_weights\n",
      "The loss after 206 iterations is E=1.3069159396121708\n",
      "Wrote weights to file ./customFunctionLearning/baselineMLP1000/best_weights\n",
      "The loss after 207 iterations is E=1.303270276441351\n",
      "Wrote weights to file ./customFunctionLearning/baselineMLP1000/best_weights\n",
      "The loss after 208 iterations is E=1.2996335792513622\n",
      "Wrote weights to file ./customFunctionLearning/baselineMLP1000/best_weights\n",
      "The loss after 209 iterations is E=1.2960058204898797\n",
      "Wrote weights to file ./customFunctionLearning/baselineMLP1000/best_weights\n",
      "The loss after 210 iterations is E=1.292386974485955\n",
      "Wrote weights to file ./customFunctionLearning/baselineMLP1000/best_weights\n",
      "The loss after 211 iterations is E=1.2887770243524925\n",
      "Wrote weights to file ./customFunctionLearning/baselineMLP1000/best_weights\n",
      "The loss after 212 iterations is E=1.2851759635498994\n",
      "Wrote weights to file ./customFunctionLearning/baselineMLP1000/best_weights\n",
      "The loss after 213 iterations is E=1.2815837923155458\n",
      "Wrote weights to file ./customFunctionLearning/baselineMLP1000/best_weights\n",
      "The loss after 214 iterations is E=1.2780005115991646\n",
      "Wrote weights to file ./customFunctionLearning/baselineMLP1000/best_weights\n",
      "The loss after 215 iterations is E=1.2744261178774694\n",
      "Wrote weights to file ./customFunctionLearning/baselineMLP1000/best_weights\n",
      "The loss after 216 iterations is E=1.2708606011962795\n",
      "Wrote weights to file ./customFunctionLearning/baselineMLP1000/best_weights\n",
      "The loss after 217 iterations is E=1.267303946807767\n",
      "Wrote weights to file ./customFunctionLearning/baselineMLP1000/best_weights\n",
      "The loss after 218 iterations is E=1.2637561389598277\n",
      "Wrote weights to file ./customFunctionLearning/baselineMLP1000/best_weights\n",
      "The loss after 219 iterations is E=1.2602171646040743\n",
      "Wrote weights to file ./customFunctionLearning/baselineMLP1000/best_weights\n",
      "The loss after 220 iterations is E=1.2566870152247664\n",
      "Wrote weights to file ./customFunctionLearning/baselineMLP1000/best_weights\n",
      "The loss after 221 iterations is E=1.2531656862273053\n",
      "Wrote weights to file ./customFunctionLearning/baselineMLP1000/best_weights\n",
      "The loss after 222 iterations is E=1.2496531746174075\n",
      "Wrote weights to file ./customFunctionLearning/baselineMLP1000/best_weights\n",
      "The loss after 223 iterations is E=1.2461494764089185\n",
      "Wrote weights to file ./customFunctionLearning/baselineMLP1000/best_weights\n",
      "The loss after 224 iterations is E=1.2426545850748003\n",
      "Wrote weights to file ./customFunctionLearning/baselineMLP1000/best_weights\n",
      "The loss after 225 iterations is E=1.239168491608952\n",
      "Wrote weights to file ./customFunctionLearning/baselineMLP1000/best_weights\n",
      "The loss after 226 iterations is E=1.2356911858725312\n",
      "Wrote weights to file ./customFunctionLearning/baselineMLP1000/best_weights\n",
      "The loss after 227 iterations is E=1.232222658323977\n",
      "Wrote weights to file ./customFunctionLearning/baselineMLP1000/best_weights\n",
      "The loss after 228 iterations is E=1.2287629012041914\n",
      "Wrote weights to file ./customFunctionLearning/baselineMLP1000/best_weights\n",
      "The loss after 229 iterations is E=1.2253119086846633\n",
      "Wrote weights to file ./customFunctionLearning/baselineMLP1000/best_weights\n",
      "The loss after 230 iterations is E=1.221869676089243\n",
      "Wrote weights to file ./customFunctionLearning/baselineMLP1000/best_weights\n",
      "The loss after 231 iterations is E=1.2184361987407517\n",
      "Wrote weights to file ./customFunctionLearning/baselineMLP1000/best_weights\n",
      "The loss after 232 iterations is E=1.2150114710718514\n",
      "Wrote weights to file ./customFunctionLearning/baselineMLP1000/best_weights\n",
      "The loss after 233 iterations is E=1.2115954863931762\n",
      "Wrote weights to file ./customFunctionLearning/baselineMLP1000/best_weights\n",
      "The loss after 234 iterations is E=1.2081882373125523\n",
      "Wrote weights to file ./customFunctionLearning/baselineMLP1000/best_weights\n",
      "The loss after 235 iterations is E=1.2047897164739778\n",
      "Wrote weights to file ./customFunctionLearning/baselineMLP1000/best_weights\n",
      "The loss after 236 iterations is E=1.2013999171836032\n",
      "Wrote weights to file ./customFunctionLearning/baselineMLP1000/best_weights\n",
      "The loss after 237 iterations is E=1.198018833624082\n",
      "Wrote weights to file ./customFunctionLearning/baselineMLP1000/best_weights\n",
      "The loss after 238 iterations is E=1.194646460619323\n",
      "Wrote weights to file ./customFunctionLearning/baselineMLP1000/best_weights\n",
      "The loss after 239 iterations is E=1.1912827931456553\n",
      "Wrote weights to file ./customFunctionLearning/baselineMLP1000/best_weights\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The loss after 240 iterations is E=1.1879278258786106\n",
      "Wrote weights to file ./customFunctionLearning/baselineMLP1000/best_weights\n",
      "The loss after 241 iterations is E=1.1845815529952657\n",
      "Wrote weights to file ./customFunctionLearning/baselineMLP1000/best_weights\n",
      "The loss after 242 iterations is E=1.1812439682834621\n",
      "Wrote weights to file ./customFunctionLearning/baselineMLP1000/best_weights\n",
      "The loss after 243 iterations is E=1.1779150654445734\n",
      "Wrote weights to file ./customFunctionLearning/baselineMLP1000/best_weights\n",
      "The loss after 244 iterations is E=1.174594838398596\n",
      "Wrote weights to file ./customFunctionLearning/baselineMLP1000/best_weights\n",
      "The loss after 245 iterations is E=1.1712832814334952\n",
      "Wrote weights to file ./customFunctionLearning/baselineMLP1000/best_weights\n",
      "The loss after 246 iterations is E=1.1679803891487317\n",
      "Wrote weights to file ./customFunctionLearning/baselineMLP1000/best_weights\n",
      "The loss after 247 iterations is E=1.1646861562567672\n",
      "Wrote weights to file ./customFunctionLearning/baselineMLP1000/best_weights\n",
      "The loss after 248 iterations is E=1.1614005773676452\n",
      "Wrote weights to file ./customFunctionLearning/baselineMLP1000/best_weights\n",
      "The loss after 249 iterations is E=1.1581236468676228\n",
      "Wrote weights to file ./customFunctionLearning/baselineMLP1000/best_weights\n",
      "The loss after 250 iterations is E=1.1548553589344799\n",
      "Wrote weights to file ./customFunctionLearning/baselineMLP1000/best_weights\n",
      "The loss after 251 iterations is E=1.1515957076538246\n",
      "Wrote weights to file ./customFunctionLearning/baselineMLP1000/best_weights\n",
      "The loss after 252 iterations is E=1.1483446871552108\n",
      "Wrote weights to file ./customFunctionLearning/baselineMLP1000/best_weights\n",
      "The loss after 253 iterations is E=1.1451022916911486\n",
      "Wrote weights to file ./customFunctionLearning/baselineMLP1000/best_weights\n",
      "The loss after 254 iterations is E=1.1418685156257244\n",
      "Wrote weights to file ./customFunctionLearning/baselineMLP1000/best_weights\n",
      "The loss after 255 iterations is E=1.1386433533527613\n",
      "Wrote weights to file ./customFunctionLearning/baselineMLP1000/best_weights\n",
      "The loss after 256 iterations is E=1.1354267991966753\n",
      "Wrote weights to file ./customFunctionLearning/baselineMLP1000/best_weights\n",
      "The loss after 257 iterations is E=1.132218847349028\n",
      "Wrote weights to file ./customFunctionLearning/baselineMLP1000/best_weights\n",
      "The loss after 258 iterations is E=1.1290194918659946\n",
      "Wrote weights to file ./customFunctionLearning/baselineMLP1000/best_weights\n",
      "The loss after 259 iterations is E=1.1258287267156541\n",
      "Wrote weights to file ./customFunctionLearning/baselineMLP1000/best_weights\n",
      "The loss after 260 iterations is E=1.1226465458402899\n",
      "Wrote weights to file ./customFunctionLearning/baselineMLP1000/best_weights\n",
      "The loss after 261 iterations is E=1.1194729431971233\n",
      "Wrote weights to file ./customFunctionLearning/baselineMLP1000/best_weights\n",
      "The loss after 262 iterations is E=1.116307912758805\n",
      "Wrote weights to file ./customFunctionLearning/baselineMLP1000/best_weights\n",
      "The loss after 263 iterations is E=1.1131514484793006\n",
      "Wrote weights to file ./customFunctionLearning/baselineMLP1000/best_weights\n",
      "The loss after 264 iterations is E=1.110003544247916\n",
      "Wrote weights to file ./customFunctionLearning/baselineMLP1000/best_weights\n",
      "The loss after 265 iterations is E=1.1068641938560977\n",
      "Wrote weights to file ./customFunctionLearning/baselineMLP1000/best_weights\n",
      "The loss after 266 iterations is E=1.1037333909904774\n",
      "Wrote weights to file ./customFunctionLearning/baselineMLP1000/best_weights\n",
      "The loss after 267 iterations is E=1.100611129249155\n",
      "Wrote weights to file ./customFunctionLearning/baselineMLP1000/best_weights\n",
      "The loss after 268 iterations is E=1.0974974021665869\n",
      "Wrote weights to file ./customFunctionLearning/baselineMLP1000/best_weights\n",
      "The loss after 269 iterations is E=1.0943922032305324\n",
      "Wrote weights to file ./customFunctionLearning/baselineMLP1000/best_weights\n",
      "The loss after 270 iterations is E=1.091295525881949\n",
      "Wrote weights to file ./customFunctionLearning/baselineMLP1000/best_weights\n",
      "The loss after 271 iterations is E=1.088207363499614\n",
      "Wrote weights to file ./customFunctionLearning/baselineMLP1000/best_weights\n",
      "The loss after 272 iterations is E=1.085127709379386\n",
      "Wrote weights to file ./customFunctionLearning/baselineMLP1000/best_weights\n",
      "The loss after 273 iterations is E=1.0820565567192146\n",
      "Wrote weights to file ./customFunctionLearning/baselineMLP1000/best_weights\n",
      "The loss after 274 iterations is E=1.0789938986161718\n",
      "Wrote weights to file ./customFunctionLearning/baselineMLP1000/best_weights\n",
      "The loss after 275 iterations is E=1.0759397280743555\n",
      "Wrote weights to file ./customFunctionLearning/baselineMLP1000/best_weights\n",
      "The loss after 276 iterations is E=1.072894038016835\n",
      "Wrote weights to file ./customFunctionLearning/baselineMLP1000/best_weights\n",
      "The loss after 277 iterations is E=1.0698568212939914\n",
      "Wrote weights to file ./customFunctionLearning/baselineMLP1000/best_weights\n",
      "The loss after 278 iterations is E=1.0668280706837816\n",
      "Wrote weights to file ./customFunctionLearning/baselineMLP1000/best_weights\n",
      "The loss after 279 iterations is E=1.0638077788845182\n",
      "Wrote weights to file ./customFunctionLearning/baselineMLP1000/best_weights\n",
      "The loss after 280 iterations is E=1.0607959385046777\n",
      "Wrote weights to file ./customFunctionLearning/baselineMLP1000/best_weights\n",
      "The loss after 281 iterations is E=1.0577925420548346\n",
      "Wrote weights to file ./customFunctionLearning/baselineMLP1000/best_weights\n",
      "The loss after 282 iterations is E=1.0547975819448179\n",
      "Wrote weights to file ./customFunctionLearning/baselineMLP1000/best_weights\n",
      "The loss after 283 iterations is E=1.0518110504856426\n",
      "Wrote weights to file ./customFunctionLearning/baselineMLP1000/best_weights\n",
      "The loss after 284 iterations is E=1.0488329398934235\n",
      "Wrote weights to file ./customFunctionLearning/baselineMLP1000/best_weights\n",
      "The loss after 285 iterations is E=1.045863242291922\n",
      "Wrote weights to file ./customFunctionLearning/baselineMLP1000/best_weights\n",
      "The loss after 286 iterations is E=1.0429019497118455\n",
      "Wrote weights to file ./customFunctionLearning/baselineMLP1000/best_weights\n",
      "The loss after 287 iterations is E=1.0399490540872736\n",
      "Wrote weights to file ./customFunctionLearning/baselineMLP1000/best_weights\n",
      "The loss after 288 iterations is E=1.0370045472512897\n",
      "Wrote weights to file ./customFunctionLearning/baselineMLP1000/best_weights\n",
      "The loss after 289 iterations is E=1.034068420933007\n",
      "Wrote weights to file ./customFunctionLearning/baselineMLP1000/best_weights\n",
      "The loss after 290 iterations is E=1.0311406667571608\n",
      "Wrote weights to file ./customFunctionLearning/baselineMLP1000/best_weights\n",
      "The loss after 291 iterations is E=1.0282212762459166\n",
      "Wrote weights to file ./customFunctionLearning/baselineMLP1000/best_weights\n",
      "The loss after 292 iterations is E=1.0253102408213264\n",
      "Wrote weights to file ./customFunctionLearning/baselineMLP1000/best_weights\n",
      "The loss after 293 iterations is E=1.022407551806887\n",
      "Wrote weights to file ./customFunctionLearning/baselineMLP1000/best_weights\n",
      "The loss after 294 iterations is E=1.019513200427313\n",
      "Wrote weights to file ./customFunctionLearning/baselineMLP1000/best_weights\n",
      "The loss after 295 iterations is E=1.0166271778067761\n",
      "Wrote weights to file ./customFunctionLearning/baselineMLP1000/best_weights\n",
      "The loss after 296 iterations is E=1.0137494749665863\n",
      "Wrote weights to file ./customFunctionLearning/baselineMLP1000/best_weights\n",
      "The loss after 297 iterations is E=1.0108800828234012\n",
      "Wrote weights to file ./customFunctionLearning/baselineMLP1000/best_weights\n",
      "The loss after 298 iterations is E=1.0080189921885572\n",
      "Wrote weights to file ./customFunctionLearning/baselineMLP1000/best_weights\n",
      "The loss after 299 iterations is E=1.0051661937683163\n",
      "Wrote weights to file ./customFunctionLearning/baselineMLP1000/best_weights\n",
      "The loss after 300 iterations is E=1.0023216781644897\n",
      "Wrote weights to file ./customFunctionLearning/baselineMLP1000/best_weights\n",
      "The loss after 301 iterations is E=0.9994854358747667\n",
      "Wrote weights to file ./customFunctionLearning/baselineMLP1000/best_weights\n",
      "The loss after 302 iterations is E=0.9966574572924864\n",
      "Wrote weights to file ./customFunctionLearning/baselineMLP1000/best_weights\n",
      "The loss after 303 iterations is E=0.9938377327059837\n",
      "Wrote weights to file ./customFunctionLearning/baselineMLP1000/best_weights\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The loss after 304 iterations is E=0.9910262522980386\n",
      "Wrote weights to file ./customFunctionLearning/baselineMLP1000/best_weights\n",
      "The loss after 305 iterations is E=0.9882230061457313\n",
      "Wrote weights to file ./customFunctionLearning/baselineMLP1000/best_weights\n",
      "The loss after 306 iterations is E=0.9854279842209329\n",
      "Wrote weights to file ./customFunctionLearning/baselineMLP1000/best_weights\n",
      "The loss after 307 iterations is E=0.9826411763911672\n",
      "Wrote weights to file ./customFunctionLearning/baselineMLP1000/best_weights\n",
      "The loss after 308 iterations is E=0.9798625724204992\n",
      "Wrote weights to file ./customFunctionLearning/baselineMLP1000/best_weights\n",
      "The loss after 309 iterations is E=0.9770921619701292\n",
      "Wrote weights to file ./customFunctionLearning/baselineMLP1000/best_weights\n",
      "The loss after 310 iterations is E=0.9743299345985754\n",
      "Wrote weights to file ./customFunctionLearning/baselineMLP1000/best_weights\n",
      "The loss after 311 iterations is E=0.9715758797615303\n",
      "Wrote weights to file ./customFunctionLearning/baselineMLP1000/best_weights\n",
      "The loss after 312 iterations is E=0.9688299868116741\n",
      "Wrote weights to file ./customFunctionLearning/baselineMLP1000/best_weights\n",
      "The loss after 313 iterations is E=0.9660922449986179\n",
      "Wrote weights to file ./customFunctionLearning/baselineMLP1000/best_weights\n",
      "The loss after 314 iterations is E=0.9633626434691499\n",
      "Wrote weights to file ./customFunctionLearning/baselineMLP1000/best_weights\n",
      "The loss after 315 iterations is E=0.960641171267603\n",
      "Wrote weights to file ./customFunctionLearning/baselineMLP1000/best_weights\n",
      "The loss after 316 iterations is E=0.9579278173363319\n",
      "Wrote weights to file ./customFunctionLearning/baselineMLP1000/best_weights\n",
      "The loss after 317 iterations is E=0.9552225705161613\n",
      "Wrote weights to file ./customFunctionLearning/baselineMLP1000/best_weights\n",
      "The loss after 318 iterations is E=0.9525254195467614\n",
      "Wrote weights to file ./customFunctionLearning/baselineMLP1000/best_weights\n",
      "The loss after 319 iterations is E=0.9498363530670516\n",
      "Wrote weights to file ./customFunctionLearning/baselineMLP1000/best_weights\n",
      "The loss after 320 iterations is E=0.947155359615716\n",
      "Wrote weights to file ./customFunctionLearning/baselineMLP1000/best_weights\n",
      "The loss after 321 iterations is E=0.9444824276318428\n",
      "Wrote weights to file ./customFunctionLearning/baselineMLP1000/best_weights\n",
      "The loss after 322 iterations is E=0.9418175454557186\n",
      "Wrote weights to file ./customFunctionLearning/baselineMLP1000/best_weights\n",
      "The loss after 323 iterations is E=0.9391607013296887\n",
      "Wrote weights to file ./customFunctionLearning/baselineMLP1000/best_weights\n",
      "The loss after 324 iterations is E=0.9365118833989149\n",
      "Wrote weights to file ./customFunctionLearning/baselineMLP1000/best_weights\n",
      "The loss after 325 iterations is E=0.933871079712085\n",
      "Wrote weights to file ./customFunctionLearning/baselineMLP1000/best_weights\n",
      "The loss after 326 iterations is E=0.9312382782220262\n",
      "Wrote weights to file ./customFunctionLearning/baselineMLP1000/best_weights\n",
      "The loss after 327 iterations is E=0.9286134667862121\n",
      "Wrote weights to file ./customFunctionLearning/baselineMLP1000/best_weights\n",
      "The loss after 328 iterations is E=0.9259966331673101\n",
      "Wrote weights to file ./customFunctionLearning/baselineMLP1000/best_weights\n",
      "The loss after 329 iterations is E=0.9233877650337914\n",
      "Wrote weights to file ./customFunctionLearning/baselineMLP1000/best_weights\n",
      "The loss after 330 iterations is E=0.9207868499605789\n",
      "Wrote weights to file ./customFunctionLearning/baselineMLP1000/best_weights\n",
      "The loss after 331 iterations is E=0.9181938754297454\n",
      "Wrote weights to file ./customFunctionLearning/baselineMLP1000/best_weights\n",
      "The loss after 332 iterations is E=0.9156088288312583\n",
      "Wrote weights to file ./customFunctionLearning/baselineMLP1000/best_weights\n",
      "The loss after 333 iterations is E=0.9130316974637416\n",
      "Wrote weights to file ./customFunctionLearning/baselineMLP1000/best_weights\n",
      "The loss after 334 iterations is E=0.9104624685352467\n",
      "Wrote weights to file ./customFunctionLearning/baselineMLP1000/best_weights\n",
      "The loss after 335 iterations is E=0.9079011291641041\n",
      "Wrote weights to file ./customFunctionLearning/baselineMLP1000/best_weights\n",
      "The loss after 336 iterations is E=0.9053476663797835\n",
      "Wrote weights to file ./customFunctionLearning/baselineMLP1000/best_weights\n",
      "The loss after 337 iterations is E=0.9028020671238275\n",
      "Wrote weights to file ./customFunctionLearning/baselineMLP1000/best_weights\n",
      "The loss after 338 iterations is E=0.9002643182507905\n",
      "Wrote weights to file ./customFunctionLearning/baselineMLP1000/best_weights\n",
      "The loss after 339 iterations is E=0.897734406529171\n",
      "Wrote weights to file ./customFunctionLearning/baselineMLP1000/best_weights\n",
      "The loss after 340 iterations is E=0.8952123186423402\n",
      "Wrote weights to file ./customFunctionLearning/baselineMLP1000/best_weights\n",
      "The loss after 341 iterations is E=0.892698041189393\n",
      "Wrote weights to file ./customFunctionLearning/baselineMLP1000/best_weights\n",
      "The loss after 342 iterations is E=0.890191560686017\n",
      "Wrote weights to file ./customFunctionLearning/baselineMLP1000/best_weights\n",
      "The loss after 343 iterations is E=0.8876928635653258\n",
      "Wrote weights to file ./customFunctionLearning/baselineMLP1000/best_weights\n",
      "The loss after 344 iterations is E=0.8852019361787195\n",
      "Wrote weights to file ./customFunctionLearning/baselineMLP1000/best_weights\n",
      "The loss after 345 iterations is E=0.8827187647967515\n",
      "Wrote weights to file ./customFunctionLearning/baselineMLP1000/best_weights\n",
      "The loss after 346 iterations is E=0.8802433356100176\n",
      "Wrote weights to file ./customFunctionLearning/baselineMLP1000/best_weights\n",
      "The loss after 347 iterations is E=0.8777756347300805\n",
      "Wrote weights to file ./customFunctionLearning/baselineMLP1000/best_weights\n",
      "The loss after 348 iterations is E=0.8753156481903812\n",
      "Wrote weights to file ./customFunctionLearning/baselineMLP1000/best_weights\n",
      "The loss after 349 iterations is E=0.8728633619472177\n",
      "Wrote weights to file ./customFunctionLearning/baselineMLP1000/best_weights\n",
      "The loss after 350 iterations is E=0.8704187618807143\n",
      "Wrote weights to file ./customFunctionLearning/baselineMLP1000/best_weights\n",
      "The loss after 351 iterations is E=0.8679818337957965\n",
      "Wrote weights to file ./customFunctionLearning/baselineMLP1000/best_weights\n",
      "The loss after 352 iterations is E=0.865552563423244\n",
      "Wrote weights to file ./customFunctionLearning/baselineMLP1000/best_weights\n",
      "The loss after 353 iterations is E=0.8631309364206616\n",
      "Wrote weights to file ./customFunctionLearning/baselineMLP1000/best_weights\n",
      "The loss after 354 iterations is E=0.8607169383735357\n",
      "Wrote weights to file ./customFunctionLearning/baselineMLP1000/best_weights\n",
      "The loss after 355 iterations is E=0.8583105547962092\n",
      "Wrote weights to file ./customFunctionLearning/baselineMLP1000/best_weights\n",
      "The loss after 356 iterations is E=0.8559117711328836\n",
      "Wrote weights to file ./customFunctionLearning/baselineMLP1000/best_weights\n",
      "The loss after 357 iterations is E=0.8535205727585958\n",
      "Wrote weights to file ./customFunctionLearning/baselineMLP1000/best_weights\n",
      "The loss after 358 iterations is E=0.8511369449801884\n",
      "Wrote weights to file ./customFunctionLearning/baselineMLP1000/best_weights\n",
      "The loss after 359 iterations is E=0.8487608730372903\n",
      "Wrote weights to file ./customFunctionLearning/baselineMLP1000/best_weights\n",
      "The loss after 360 iterations is E=0.846392342103268\n",
      "Wrote weights to file ./customFunctionLearning/baselineMLP1000/best_weights\n",
      "The loss after 361 iterations is E=0.8440313372862408\n",
      "Wrote weights to file ./customFunctionLearning/baselineMLP1000/best_weights\n",
      "The loss after 362 iterations is E=0.8416778436300423\n",
      "Wrote weights to file ./customFunctionLearning/baselineMLP1000/best_weights\n",
      "The loss after 363 iterations is E=0.8393318461152487\n",
      "Wrote weights to file ./customFunctionLearning/baselineMLP1000/best_weights\n",
      "The loss after 364 iterations is E=0.8369933296601724\n",
      "Wrote weights to file ./customFunctionLearning/baselineMLP1000/best_weights\n",
      "The loss after 365 iterations is E=0.8346622791219023\n",
      "Wrote weights to file ./customFunctionLearning/baselineMLP1000/best_weights\n",
      "The loss after 366 iterations is E=0.8323386792973061\n",
      "Wrote weights to file ./customFunctionLearning/baselineMLP1000/best_weights\n",
      "The loss after 367 iterations is E=0.8300225149240976\n",
      "Wrote weights to file ./customFunctionLearning/baselineMLP1000/best_weights\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The loss after 368 iterations is E=0.8277137706818319\n",
      "Wrote weights to file ./customFunctionLearning/baselineMLP1000/best_weights\n",
      "The loss after 369 iterations is E=0.8254124311929723\n",
      "Wrote weights to file ./customFunctionLearning/baselineMLP1000/best_weights\n",
      "The loss after 370 iterations is E=0.8231184810238779\n",
      "Wrote weights to file ./customFunctionLearning/baselineMLP1000/best_weights\n",
      "The loss after 371 iterations is E=0.8208319046858332\n",
      "Wrote weights to file ./customFunctionLearning/baselineMLP1000/best_weights\n",
      "The loss after 372 iterations is E=0.8185526866360501\n",
      "Wrote weights to file ./customFunctionLearning/baselineMLP1000/best_weights\n",
      "The loss after 373 iterations is E=0.8162808112786649\n",
      "Wrote weights to file ./customFunctionLearning/baselineMLP1000/best_weights\n",
      "The loss after 374 iterations is E=0.8140162629657407\n",
      "Wrote weights to file ./customFunctionLearning/baselineMLP1000/best_weights\n",
      "The loss after 375 iterations is E=0.8117590259982489\n",
      "Wrote weights to file ./customFunctionLearning/baselineMLP1000/best_weights\n",
      "The loss after 376 iterations is E=0.8095090846270735\n",
      "Wrote weights to file ./customFunctionLearning/baselineMLP1000/best_weights\n",
      "The loss after 377 iterations is E=0.8072664230540054\n",
      "Wrote weights to file ./customFunctionLearning/baselineMLP1000/best_weights\n",
      "The loss after 378 iterations is E=0.8050310254327485\n",
      "Wrote weights to file ./customFunctionLearning/baselineMLP1000/best_weights\n",
      "The loss after 379 iterations is E=0.8028028758699067\n",
      "Wrote weights to file ./customFunctionLearning/baselineMLP1000/best_weights\n",
      "The loss after 380 iterations is E=0.8005819584260027\n",
      "Wrote weights to file ./customFunctionLearning/baselineMLP1000/best_weights\n",
      "The loss after 381 iterations is E=0.7983682571164713\n",
      "Wrote weights to file ./customFunctionLearning/baselineMLP1000/best_weights\n",
      "The loss after 382 iterations is E=0.7961617559126691\n",
      "Wrote weights to file ./customFunctionLearning/baselineMLP1000/best_weights\n",
      "The loss after 383 iterations is E=0.7939624387428632\n",
      "Wrote weights to file ./customFunctionLearning/baselineMLP1000/best_weights\n",
      "The loss after 384 iterations is E=0.7917702894932372\n",
      "Wrote weights to file ./customFunctionLearning/baselineMLP1000/best_weights\n",
      "The loss after 385 iterations is E=0.789585292008856\n",
      "Wrote weights to file ./customFunctionLearning/baselineMLP1000/best_weights\n",
      "The loss after 386 iterations is E=0.7874074300946591\n",
      "Wrote weights to file ./customFunctionLearning/baselineMLP1000/best_weights\n",
      "The loss after 387 iterations is E=0.7852366875164299\n",
      "Wrote weights to file ./customFunctionLearning/baselineMLP1000/best_weights\n",
      "The loss after 388 iterations is E=0.783073048001759\n",
      "Wrote weights to file ./customFunctionLearning/baselineMLP1000/best_weights\n",
      "The loss after 389 iterations is E=0.7809164952409993\n",
      "Wrote weights to file ./customFunctionLearning/baselineMLP1000/best_weights\n",
      "The loss after 390 iterations is E=0.7787670128882291\n",
      "Wrote weights to file ./customFunctionLearning/baselineMLP1000/best_weights\n",
      "The loss after 391 iterations is E=0.7766245845621987\n",
      "Wrote weights to file ./customFunctionLearning/baselineMLP1000/best_weights\n",
      "The loss after 392 iterations is E=0.7744891938472911\n",
      "Wrote weights to file ./customFunctionLearning/baselineMLP1000/best_weights\n",
      "The loss after 393 iterations is E=0.7723608242944558\n",
      "Wrote weights to file ./customFunctionLearning/baselineMLP1000/best_weights\n",
      "The loss after 394 iterations is E=0.7702394594221729\n",
      "Wrote weights to file ./customFunctionLearning/baselineMLP1000/best_weights\n",
      "The loss after 395 iterations is E=0.7681250827173802\n",
      "Wrote weights to file ./customFunctionLearning/baselineMLP1000/best_weights\n",
      "The loss after 396 iterations is E=0.7660176776364205\n",
      "Wrote weights to file ./customFunctionLearning/baselineMLP1000/best_weights\n",
      "The loss after 397 iterations is E=0.7639172276059745\n",
      "Wrote weights to file ./customFunctionLearning/baselineMLP1000/best_weights\n",
      "The loss after 398 iterations is E=0.7618237160239888\n",
      "Wrote weights to file ./customFunctionLearning/baselineMLP1000/best_weights\n",
      "The loss after 399 iterations is E=0.7597371262606\n",
      "Wrote weights to file ./customFunctionLearning/baselineMLP1000/best_weights\n",
      "The loss after 400 iterations is E=0.7576574416590491\n",
      "Wrote weights to file ./customFunctionLearning/baselineMLP1000/best_weights\n",
      "The loss after 401 iterations is E=0.755584645536582\n",
      "Wrote weights to file ./customFunctionLearning/baselineMLP1000/best_weights\n",
      "The loss after 402 iterations is E=0.7535187211853656\n",
      "Wrote weights to file ./customFunctionLearning/baselineMLP1000/best_weights\n",
      "The loss after 403 iterations is E=0.7514596518733561\n",
      "Wrote weights to file ./customFunctionLearning/baselineMLP1000/best_weights\n",
      "The loss after 404 iterations is E=0.7494074208452217\n",
      "Wrote weights to file ./customFunctionLearning/baselineMLP1000/best_weights\n",
      "The loss after 405 iterations is E=0.7473620113231897\n",
      "Wrote weights to file ./customFunctionLearning/baselineMLP1000/best_weights\n",
      "The loss after 406 iterations is E=0.7453234065079501\n",
      "Wrote weights to file ./customFunctionLearning/baselineMLP1000/best_weights\n",
      "The loss after 407 iterations is E=0.7432915895795171\n",
      "Wrote weights to file ./customFunctionLearning/baselineMLP1000/best_weights\n",
      "The loss after 408 iterations is E=0.7412665436980954\n",
      "Wrote weights to file ./customFunctionLearning/baselineMLP1000/best_weights\n",
      "The loss after 409 iterations is E=0.7392482520049446\n",
      "Wrote weights to file ./customFunctionLearning/baselineMLP1000/best_weights\n",
      "The loss after 410 iterations is E=0.7372366976232408\n",
      "Wrote weights to file ./customFunctionLearning/baselineMLP1000/best_weights\n",
      "The loss after 411 iterations is E=0.7352318636589202\n",
      "Wrote weights to file ./customFunctionLearning/baselineMLP1000/best_weights\n",
      "The loss after 412 iterations is E=0.7332337332015322\n",
      "Wrote weights to file ./customFunctionLearning/baselineMLP1000/best_weights\n",
      "The loss after 413 iterations is E=0.7312422893250689\n",
      "Wrote weights to file ./customFunctionLearning/baselineMLP1000/best_weights\n",
      "The loss after 414 iterations is E=0.7292575150888005\n",
      "Wrote weights to file ./customFunctionLearning/baselineMLP1000/best_weights\n",
      "The loss after 415 iterations is E=0.7272793935381033\n",
      "Wrote weights to file ./customFunctionLearning/baselineMLP1000/best_weights\n",
      "The loss after 416 iterations is E=0.7253079077052638\n",
      "Wrote weights to file ./customFunctionLearning/baselineMLP1000/best_weights\n",
      "The loss after 417 iterations is E=0.723343040610297\n",
      "Wrote weights to file ./customFunctionLearning/baselineMLP1000/best_weights\n",
      "The loss after 418 iterations is E=0.7213847752617444\n",
      "Wrote weights to file ./customFunctionLearning/baselineMLP1000/best_weights\n",
      "The loss after 419 iterations is E=0.7194330946574726\n",
      "Wrote weights to file ./customFunctionLearning/baselineMLP1000/best_weights\n",
      "The loss after 420 iterations is E=0.7174879817854574\n",
      "Wrote weights to file ./customFunctionLearning/baselineMLP1000/best_weights\n",
      "The loss after 421 iterations is E=0.7155494196245682\n",
      "Wrote weights to file ./customFunctionLearning/baselineMLP1000/best_weights\n",
      "The loss after 422 iterations is E=0.7136173911453483\n",
      "Wrote weights to file ./customFunctionLearning/baselineMLP1000/best_weights\n",
      "The loss after 423 iterations is E=0.711691879310777\n",
      "Wrote weights to file ./customFunctionLearning/baselineMLP1000/best_weights\n",
      "The loss after 424 iterations is E=0.7097728670770409\n",
      "Wrote weights to file ./customFunctionLearning/baselineMLP1000/best_weights\n",
      "The loss after 425 iterations is E=0.7078603373942924\n",
      "Wrote weights to file ./customFunctionLearning/baselineMLP1000/best_weights\n",
      "The loss after 426 iterations is E=0.7059542732073864\n",
      "Wrote weights to file ./customFunctionLearning/baselineMLP1000/best_weights\n",
      "The loss after 427 iterations is E=0.7040546574566403\n",
      "Wrote weights to file ./customFunctionLearning/baselineMLP1000/best_weights\n",
      "The loss after 428 iterations is E=0.7021614730785612\n",
      "Wrote weights to file ./customFunctionLearning/baselineMLP1000/best_weights\n",
      "The loss after 429 iterations is E=0.7002747030065724\n",
      "Wrote weights to file ./customFunctionLearning/baselineMLP1000/best_weights\n",
      "The loss after 430 iterations is E=0.6983943301717422\n",
      "Wrote weights to file ./customFunctionLearning/baselineMLP1000/best_weights\n",
      "The loss after 431 iterations is E=0.6965203375034953\n",
      "Wrote weights to file ./customFunctionLearning/baselineMLP1000/best_weights\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The loss after 432 iterations is E=0.6946527079303063\n",
      "Wrote weights to file ./customFunctionLearning/baselineMLP1000/best_weights\n",
      "The loss after 433 iterations is E=0.6927914243804242\n",
      "Wrote weights to file ./customFunctionLearning/baselineMLP1000/best_weights\n",
      "The loss after 434 iterations is E=0.6909364697825453\n",
      "Wrote weights to file ./customFunctionLearning/baselineMLP1000/best_weights\n",
      "The loss after 435 iterations is E=0.6890878270665056\n",
      "Wrote weights to file ./customFunctionLearning/baselineMLP1000/best_weights\n",
      "The loss after 436 iterations is E=0.6872454791639544\n",
      "Wrote weights to file ./customFunctionLearning/baselineMLP1000/best_weights\n",
      "The loss after 437 iterations is E=0.6854094090090413\n",
      "Wrote weights to file ./customFunctionLearning/baselineMLP1000/best_weights\n",
      "The loss after 438 iterations is E=0.6835795995390656\n",
      "Wrote weights to file ./customFunctionLearning/baselineMLP1000/best_weights\n",
      "The loss after 439 iterations is E=0.6817560336951385\n",
      "Wrote weights to file ./customFunctionLearning/baselineMLP1000/best_weights\n",
      "The loss after 440 iterations is E=0.6799386944228424\n",
      "Wrote weights to file ./customFunctionLearning/baselineMLP1000/best_weights\n",
      "The loss after 441 iterations is E=0.6781275646728732\n",
      "Wrote weights to file ./customFunctionLearning/baselineMLP1000/best_weights\n",
      "The loss after 442 iterations is E=0.676322627401662\n",
      "Wrote weights to file ./customFunctionLearning/baselineMLP1000/best_weights\n",
      "The loss after 443 iterations is E=0.6745238655720324\n",
      "Wrote weights to file ./customFunctionLearning/baselineMLP1000/best_weights\n",
      "The loss after 444 iterations is E=0.672731262153797\n",
      "Wrote weights to file ./customFunctionLearning/baselineMLP1000/best_weights\n",
      "The loss after 445 iterations is E=0.670944800124391\n",
      "Wrote weights to file ./customFunctionLearning/baselineMLP1000/best_weights\n",
      "The loss after 446 iterations is E=0.6691644624694723\n",
      "Wrote weights to file ./customFunctionLearning/baselineMLP1000/best_weights\n",
      "The loss after 447 iterations is E=0.6673902321835283\n",
      "Wrote weights to file ./customFunctionLearning/baselineMLP1000/best_weights\n",
      "The loss after 448 iterations is E=0.6656220922704542\n",
      "Wrote weights to file ./customFunctionLearning/baselineMLP1000/best_weights\n",
      "The loss after 449 iterations is E=0.6638600257441692\n",
      "Wrote weights to file ./customFunctionLearning/baselineMLP1000/best_weights\n",
      "The loss after 450 iterations is E=0.6621040156291687\n",
      "Wrote weights to file ./customFunctionLearning/baselineMLP1000/best_weights\n",
      "The loss after 451 iterations is E=0.6603540449611156\n",
      "Wrote weights to file ./customFunctionLearning/baselineMLP1000/best_weights\n",
      "The loss after 452 iterations is E=0.6586100967874028\n",
      "Wrote weights to file ./customFunctionLearning/baselineMLP1000/best_weights\n",
      "The loss after 453 iterations is E=0.6568721541677032\n",
      "Wrote weights to file ./customFunctionLearning/baselineMLP1000/best_weights\n",
      "The loss after 454 iterations is E=0.6551402001745475\n",
      "Wrote weights to file ./customFunctionLearning/baselineMLP1000/best_weights\n",
      "The loss after 455 iterations is E=0.6534142178938381\n",
      "Wrote weights to file ./customFunctionLearning/baselineMLP1000/best_weights\n",
      "The loss after 456 iterations is E=0.6516941904254198\n",
      "Wrote weights to file ./customFunctionLearning/baselineMLP1000/best_weights\n",
      "The loss after 457 iterations is E=0.6499801008835876\n",
      "Wrote weights to file ./customFunctionLearning/baselineMLP1000/best_weights\n",
      "The loss after 458 iterations is E=0.6482719323976274\n",
      "Wrote weights to file ./customFunctionLearning/baselineMLP1000/best_weights\n",
      "The loss after 459 iterations is E=0.6465696681123279\n",
      "Wrote weights to file ./customFunctionLearning/baselineMLP1000/best_weights\n",
      "The loss after 460 iterations is E=0.6448732911884895\n",
      "Wrote weights to file ./customFunctionLearning/baselineMLP1000/best_weights\n",
      "The loss after 461 iterations is E=0.643182784803443\n",
      "Wrote weights to file ./customFunctionLearning/baselineMLP1000/best_weights\n",
      "The loss after 462 iterations is E=0.6414981321515275\n",
      "Wrote weights to file ./customFunctionLearning/baselineMLP1000/best_weights\n",
      "The loss after 463 iterations is E=0.6398193164446027\n",
      "Wrote weights to file ./customFunctionLearning/baselineMLP1000/best_weights\n",
      "The loss after 464 iterations is E=0.6381463209125117\n",
      "Wrote weights to file ./customFunctionLearning/baselineMLP1000/best_weights\n",
      "The loss after 465 iterations is E=0.6364791288035795\n",
      "Wrote weights to file ./customFunctionLearning/baselineMLP1000/best_weights\n",
      "The loss after 466 iterations is E=0.6348177233850734\n",
      "Wrote weights to file ./customFunctionLearning/baselineMLP1000/best_weights\n",
      "The loss after 467 iterations is E=0.6331620879436708\n",
      "Wrote weights to file ./customFunctionLearning/baselineMLP1000/best_weights\n",
      "The loss after 468 iterations is E=0.6315122057859149\n",
      "Wrote weights to file ./customFunctionLearning/baselineMLP1000/best_weights\n",
      "The loss after 469 iterations is E=0.6298680602386706\n",
      "Wrote weights to file ./customFunctionLearning/baselineMLP1000/best_weights\n",
      "The loss after 470 iterations is E=0.62822963464957\n",
      "Wrote weights to file ./customFunctionLearning/baselineMLP1000/best_weights\n",
      "The loss after 471 iterations is E=0.6265969123874474\n",
      "Wrote weights to file ./customFunctionLearning/baselineMLP1000/best_weights\n",
      "The loss after 472 iterations is E=0.6249698768427723\n",
      "Wrote weights to file ./customFunctionLearning/baselineMLP1000/best_weights\n",
      "The loss after 473 iterations is E=0.6233485114280822\n",
      "Wrote weights to file ./customFunctionLearning/baselineMLP1000/best_weights\n",
      "The loss after 474 iterations is E=0.6217327995783845\n",
      "Wrote weights to file ./customFunctionLearning/baselineMLP1000/best_weights\n",
      "The loss after 475 iterations is E=0.6201227247515974\n",
      "Wrote weights to file ./customFunctionLearning/baselineMLP1000/best_weights\n",
      "The loss after 476 iterations is E=0.6185182704289264\n",
      "Wrote weights to file ./customFunctionLearning/baselineMLP1000/best_weights\n",
      "The loss after 477 iterations is E=0.6169194201152804\n",
      "Wrote weights to file ./customFunctionLearning/baselineMLP1000/best_weights\n",
      "The loss after 478 iterations is E=0.6153261573396694\n",
      "Wrote weights to file ./customFunctionLearning/baselineMLP1000/best_weights\n",
      "The loss after 479 iterations is E=0.6137384656555839\n",
      "Wrote weights to file ./customFunctionLearning/baselineMLP1000/best_weights\n",
      "The loss after 480 iterations is E=0.6121563286413788\n",
      "Wrote weights to file ./customFunctionLearning/baselineMLP1000/best_weights\n",
      "The loss after 481 iterations is E=0.6105797299006457\n",
      "Wrote weights to file ./customFunctionLearning/baselineMLP1000/best_weights\n",
      "The loss after 482 iterations is E=0.6090086530625973\n",
      "Wrote weights to file ./customFunctionLearning/baselineMLP1000/best_weights\n",
      "The loss after 483 iterations is E=0.6074430817824058\n",
      "Wrote weights to file ./customFunctionLearning/baselineMLP1000/best_weights\n",
      "The loss after 484 iterations is E=0.6058829997415814\n",
      "Wrote weights to file ./customFunctionLearning/baselineMLP1000/best_weights\n",
      "The loss after 485 iterations is E=0.6043283906483191\n",
      "Wrote weights to file ./customFunctionLearning/baselineMLP1000/best_weights\n",
      "The loss after 486 iterations is E=0.6027792382378332\n",
      "Wrote weights to file ./customFunctionLearning/baselineMLP1000/best_weights\n",
      "The loss after 487 iterations is E=0.601235526272706\n",
      "Wrote weights to file ./customFunctionLearning/baselineMLP1000/best_weights\n",
      "The loss after 488 iterations is E=0.599697238543222\n",
      "Wrote weights to file ./customFunctionLearning/baselineMLP1000/best_weights\n",
      "The loss after 489 iterations is E=0.5981643588676918\n",
      "Wrote weights to file ./customFunctionLearning/baselineMLP1000/best_weights\n",
      "The loss after 490 iterations is E=0.5966368710927746\n",
      "Wrote weights to file ./customFunctionLearning/baselineMLP1000/best_weights\n",
      "The loss after 491 iterations is E=0.595114759093793\n",
      "Wrote weights to file ./customFunctionLearning/baselineMLP1000/best_weights\n",
      "The loss after 492 iterations is E=0.5935980067750437\n",
      "Wrote weights to file ./customFunctionLearning/baselineMLP1000/best_weights\n",
      "The loss after 493 iterations is E=0.5920865980701016\n",
      "Wrote weights to file ./customFunctionLearning/baselineMLP1000/best_weights\n",
      "The loss after 494 iterations is E=0.59058051694212\n",
      "Wrote weights to file ./customFunctionLearning/baselineMLP1000/best_weights\n",
      "The loss after 495 iterations is E=0.5890797473841239\n",
      "Wrote weights to file ./customFunctionLearning/baselineMLP1000/best_weights\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The loss after 496 iterations is E=0.5875842734192984\n",
      "Wrote weights to file ./customFunctionLearning/baselineMLP1000/best_weights\n",
      "The loss after 497 iterations is E=0.5860940791012632\n",
      "Wrote weights to file ./customFunctionLearning/baselineMLP1000/best_weights\n",
      "The loss after 498 iterations is E=0.584609148514354\n",
      "Wrote weights to file ./customFunctionLearning/baselineMLP1000/best_weights\n",
      "The loss after 499 iterations is E=0.5831294657739019\n",
      "Wrote weights to file ./customFunctionLearning/baselineMLP1000/best_weights\n",
      "The loss after 500 iterations is E=0.5816550150264819\n",
      "Wrote weights to file ./customFunctionLearning/baselineMLP1000/best_weights\n",
      "The loss after 501 iterations is E=0.5801857804501912\n",
      "Wrote weights to file ./customFunctionLearning/baselineMLP1000/best_weights\n",
      "The loss after 502 iterations is E=0.5787217462548865\n",
      "Wrote weights to file ./customFunctionLearning/baselineMLP1000/best_weights\n",
      "The loss after 503 iterations is E=0.5772628966824498\n",
      "Wrote weights to file ./customFunctionLearning/baselineMLP1000/best_weights\n",
      "The loss after 504 iterations is E=0.5758092160070164\n",
      "Wrote weights to file ./customFunctionLearning/baselineMLP1000/best_weights\n",
      "The loss after 505 iterations is E=0.5743606885352284\n",
      "Wrote weights to file ./customFunctionLearning/baselineMLP1000/best_weights\n",
      "The loss after 506 iterations is E=0.5729172986064572\n",
      "Wrote weights to file ./customFunctionLearning/baselineMLP1000/best_weights\n",
      "The loss after 507 iterations is E=0.5714790305930444\n",
      "Wrote weights to file ./customFunctionLearning/baselineMLP1000/best_weights\n",
      "The loss after 508 iterations is E=0.570045868900512\n",
      "Wrote weights to file ./customFunctionLearning/baselineMLP1000/best_weights\n",
      "The loss after 509 iterations is E=0.5686177979677878\n",
      "Wrote weights to file ./customFunctionLearning/baselineMLP1000/best_weights\n",
      "The loss after 510 iterations is E=0.5671948022674215\n",
      "Wrote weights to file ./customFunctionLearning/baselineMLP1000/best_weights\n",
      "The loss after 511 iterations is E=0.565776866305786\n",
      "Wrote weights to file ./customFunctionLearning/baselineMLP1000/best_weights\n",
      "The loss after 512 iterations is E=0.5643639746232901\n",
      "Wrote weights to file ./customFunctionLearning/baselineMLP1000/best_weights\n",
      "The loss after 513 iterations is E=0.5629561117945644\n",
      "Wrote weights to file ./customFunctionLearning/baselineMLP1000/best_weights\n",
      "The loss after 514 iterations is E=0.5615532624286704\n",
      "Wrote weights to file ./customFunctionLearning/baselineMLP1000/best_weights\n",
      "The loss after 515 iterations is E=0.5601554111692797\n",
      "Wrote weights to file ./customFunctionLearning/baselineMLP1000/best_weights\n",
      "The loss after 516 iterations is E=0.5587625426948556\n",
      "Wrote weights to file ./customFunctionLearning/baselineMLP1000/best_weights\n",
      "The loss after 517 iterations is E=0.5573746417188451\n",
      "Wrote weights to file ./customFunctionLearning/baselineMLP1000/best_weights\n",
      "The loss after 518 iterations is E=0.5559916929898427\n",
      "Wrote weights to file ./customFunctionLearning/baselineMLP1000/best_weights\n",
      "The loss after 519 iterations is E=0.5546136812917652\n",
      "Wrote weights to file ./customFunctionLearning/baselineMLP1000/best_weights\n",
      "The loss after 520 iterations is E=0.5532405914440124\n",
      "Wrote weights to file ./customFunctionLearning/baselineMLP1000/best_weights\n",
      "The loss after 521 iterations is E=0.5518724083016376\n",
      "Wrote weights to file ./customFunctionLearning/baselineMLP1000/best_weights\n",
      "The loss after 522 iterations is E=0.5505091167554969\n",
      "Wrote weights to file ./customFunctionLearning/baselineMLP1000/best_weights\n",
      "The loss after 523 iterations is E=0.5491507017324042\n",
      "Wrote weights to file ./customFunctionLearning/baselineMLP1000/best_weights\n",
      "The loss after 524 iterations is E=0.5477971481952744\n",
      "Wrote weights to file ./customFunctionLearning/baselineMLP1000/best_weights\n",
      "The loss after 525 iterations is E=0.5464484411432741\n",
      "Wrote weights to file ./customFunctionLearning/baselineMLP1000/best_weights\n",
      "The loss after 526 iterations is E=0.5451045656119587\n",
      "Wrote weights to file ./customFunctionLearning/baselineMLP1000/best_weights\n",
      "The loss after 527 iterations is E=0.5437655066733958\n",
      "Wrote weights to file ./customFunctionLearning/baselineMLP1000/best_weights\n",
      "The loss after 528 iterations is E=0.5424312494363153\n",
      "Wrote weights to file ./customFunctionLearning/baselineMLP1000/best_weights\n",
      "The loss after 529 iterations is E=0.541101779046222\n",
      "Wrote weights to file ./customFunctionLearning/baselineMLP1000/best_weights\n",
      "The loss after 530 iterations is E=0.5397770806855184\n",
      "Wrote weights to file ./customFunctionLearning/baselineMLP1000/best_weights\n",
      "The loss after 531 iterations is E=0.53845713957363\n",
      "Wrote weights to file ./customFunctionLearning/baselineMLP1000/best_weights\n",
      "The loss after 532 iterations is E=0.5371419409671077\n",
      "Wrote weights to file ./customFunctionLearning/baselineMLP1000/best_weights\n",
      "The loss after 533 iterations is E=0.5358314701597497\n",
      "Wrote weights to file ./customFunctionLearning/baselineMLP1000/best_weights\n",
      "The loss after 534 iterations is E=0.5345257124827025\n",
      "Wrote weights to file ./customFunctionLearning/baselineMLP1000/best_weights\n",
      "The loss after 535 iterations is E=0.5332246533045558\n",
      "Wrote weights to file ./customFunctionLearning/baselineMLP1000/best_weights\n",
      "The loss after 536 iterations is E=0.5319282780314523\n",
      "Wrote weights to file ./customFunctionLearning/baselineMLP1000/best_weights\n",
      "The loss after 537 iterations is E=0.5306365721071759\n",
      "Wrote weights to file ./customFunctionLearning/baselineMLP1000/best_weights\n",
      "The loss after 538 iterations is E=0.5293495210132345\n",
      "Wrote weights to file ./customFunctionLearning/baselineMLP1000/best_weights\n",
      "The loss after 539 iterations is E=0.528067110268966\n",
      "Wrote weights to file ./customFunctionLearning/baselineMLP1000/best_weights\n",
      "The loss after 540 iterations is E=0.526789325431596\n",
      "Wrote weights to file ./customFunctionLearning/baselineMLP1000/best_weights\n",
      "The loss after 541 iterations is E=0.525516152096338\n",
      "Wrote weights to file ./customFunctionLearning/baselineMLP1000/best_weights\n",
      "The loss after 542 iterations is E=0.5242475758964592\n",
      "Wrote weights to file ./customFunctionLearning/baselineMLP1000/best_weights\n",
      "The loss after 543 iterations is E=0.5229835825033464\n",
      "Wrote weights to file ./customFunctionLearning/baselineMLP1000/best_weights\n",
      "The loss after 544 iterations is E=0.521724157626586\n",
      "Wrote weights to file ./customFunctionLearning/baselineMLP1000/best_weights\n",
      "The loss after 545 iterations is E=0.5204692870140215\n",
      "Wrote weights to file ./customFunctionLearning/baselineMLP1000/best_weights\n",
      "The loss after 546 iterations is E=0.5192189564518164\n",
      "Wrote weights to file ./customFunctionLearning/baselineMLP1000/best_weights\n",
      "The loss after 547 iterations is E=0.5179731517645056\n",
      "Wrote weights to file ./customFunctionLearning/baselineMLP1000/best_weights\n",
      "The loss after 548 iterations is E=0.5167318588150611\n",
      "Wrote weights to file ./customFunctionLearning/baselineMLP1000/best_weights\n",
      "The loss after 549 iterations is E=0.5154950635049372\n",
      "Wrote weights to file ./customFunctionLearning/baselineMLP1000/best_weights\n",
      "The loss after 550 iterations is E=0.514262751774106\n",
      "Wrote weights to file ./customFunctionLearning/baselineMLP1000/best_weights\n",
      "The loss after 551 iterations is E=0.5130349096011226\n",
      "Wrote weights to file ./customFunctionLearning/baselineMLP1000/best_weights\n",
      "The loss after 552 iterations is E=0.511811523003148\n",
      "Wrote weights to file ./customFunctionLearning/baselineMLP1000/best_weights\n",
      "The loss after 553 iterations is E=0.5105925780359979\n",
      "Wrote weights to file ./customFunctionLearning/baselineMLP1000/best_weights\n",
      "The loss after 554 iterations is E=0.5093780607941689\n",
      "Wrote weights to file ./customFunctionLearning/baselineMLP1000/best_weights\n",
      "The loss after 555 iterations is E=0.5081679574108736\n",
      "Wrote weights to file ./customFunctionLearning/baselineMLP1000/best_weights\n",
      "The loss after 556 iterations is E=0.5069622540580697\n",
      "Wrote weights to file ./customFunctionLearning/baselineMLP1000/best_weights\n",
      "The loss after 557 iterations is E=0.5057609369464818\n",
      "Wrote weights to file ./customFunctionLearning/baselineMLP1000/best_weights\n",
      "The loss after 558 iterations is E=0.5045639923256191\n",
      "Wrote weights to file ./customFunctionLearning/baselineMLP1000/best_weights\n",
      "The loss after 559 iterations is E=0.5033714064838084\n",
      "Wrote weights to file ./customFunctionLearning/baselineMLP1000/best_weights\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The loss after 560 iterations is E=0.5021831657481918\n",
      "Wrote weights to file ./customFunctionLearning/baselineMLP1000/best_weights\n",
      "The loss after 561 iterations is E=0.5009992564847553\n",
      "Wrote weights to file ./customFunctionLearning/baselineMLP1000/best_weights\n",
      "The loss after 562 iterations is E=0.49981966509832354\n",
      "Wrote weights to file ./customFunctionLearning/baselineMLP1000/best_weights\n",
      "The loss after 563 iterations is E=0.49864437803258477\n",
      "Wrote weights to file ./customFunctionLearning/baselineMLP1000/best_weights\n",
      "The loss after 564 iterations is E=0.4974733817700735\n",
      "Wrote weights to file ./customFunctionLearning/baselineMLP1000/best_weights\n",
      "The loss after 565 iterations is E=0.49630666283219493\n",
      "Wrote weights to file ./customFunctionLearning/baselineMLP1000/best_weights\n",
      "The loss after 566 iterations is E=0.49514420777920226\n",
      "Wrote weights to file ./customFunctionLearning/baselineMLP1000/best_weights\n",
      "The loss after 567 iterations is E=0.49398600321021296\n",
      "Wrote weights to file ./customFunctionLearning/baselineMLP1000/best_weights\n",
      "The loss after 568 iterations is E=0.49283203576317997\n",
      "Wrote weights to file ./customFunctionLearning/baselineMLP1000/best_weights\n",
      "The loss after 569 iterations is E=0.49168229211490383\n",
      "Wrote weights to file ./customFunctionLearning/baselineMLP1000/best_weights\n",
      "The loss after 570 iterations is E=0.4905367589810071\n",
      "Wrote weights to file ./customFunctionLearning/baselineMLP1000/best_weights\n",
      "The loss after 571 iterations is E=0.4893954231159247\n",
      "Wrote weights to file ./customFunctionLearning/baselineMLP1000/best_weights\n",
      "The loss after 572 iterations is E=0.48825827131287863\n",
      "Wrote weights to file ./customFunctionLearning/baselineMLP1000/best_weights\n",
      "The loss after 573 iterations is E=0.48712529040387975\n",
      "Wrote weights to file ./customFunctionLearning/baselineMLP1000/best_weights\n",
      "The loss after 574 iterations is E=0.48599646725967904\n",
      "Wrote weights to file ./customFunctionLearning/baselineMLP1000/best_weights\n",
      "The loss after 575 iterations is E=0.48487178878976067\n",
      "Wrote weights to file ./customFunctionLearning/baselineMLP1000/best_weights\n",
      "The loss after 576 iterations is E=0.4837512419423072\n",
      "Wrote weights to file ./customFunctionLearning/baselineMLP1000/best_weights\n",
      "The loss after 577 iterations is E=0.4826348137041755\n",
      "Wrote weights to file ./customFunctionLearning/baselineMLP1000/best_weights\n",
      "The loss after 578 iterations is E=0.48152249110086415\n",
      "Wrote weights to file ./customFunctionLearning/baselineMLP1000/best_weights\n",
      "The loss after 579 iterations is E=0.4804142611964765\n",
      "Wrote weights to file ./customFunctionLearning/baselineMLP1000/best_weights\n",
      "The loss after 580 iterations is E=0.4793101110936842\n",
      "Wrote weights to file ./customFunctionLearning/baselineMLP1000/best_weights\n",
      "The loss after 581 iterations is E=0.47821002793369255\n",
      "Wrote weights to file ./customFunctionLearning/baselineMLP1000/best_weights\n",
      "The loss after 582 iterations is E=0.4771139988961997\n",
      "Wrote weights to file ./customFunctionLearning/baselineMLP1000/best_weights\n",
      "The loss after 583 iterations is E=0.47602201119935333\n",
      "Wrote weights to file ./customFunctionLearning/baselineMLP1000/best_weights\n",
      "The loss after 584 iterations is E=0.4749340520997002\n",
      "Wrote weights to file ./customFunctionLearning/baselineMLP1000/best_weights\n",
      "The loss after 585 iterations is E=0.47385010889214957\n",
      "Wrote weights to file ./customFunctionLearning/baselineMLP1000/best_weights\n",
      "The loss after 586 iterations is E=0.4727701689099134\n",
      "Wrote weights to file ./customFunctionLearning/baselineMLP1000/best_weights\n",
      "The loss after 587 iterations is E=0.47169421952446566\n",
      "Wrote weights to file ./customFunctionLearning/baselineMLP1000/best_weights\n",
      "The loss after 588 iterations is E=0.4706222481454842\n",
      "Wrote weights to file ./customFunctionLearning/baselineMLP1000/best_weights\n",
      "The loss after 589 iterations is E=0.46955424222078784\n",
      "Wrote weights to file ./customFunctionLearning/baselineMLP1000/best_weights\n",
      "The loss after 590 iterations is E=0.46849018923630537\n",
      "Wrote weights to file ./customFunctionLearning/baselineMLP1000/best_weights\n",
      "The loss after 591 iterations is E=0.4674300767159786\n",
      "Wrote weights to file ./customFunctionLearning/baselineMLP1000/best_weights\n",
      "The loss after 592 iterations is E=0.4663738922217339\n",
      "Wrote weights to file ./customFunctionLearning/baselineMLP1000/best_weights\n",
      "The loss after 593 iterations is E=0.4653216233534052\n",
      "Wrote weights to file ./customFunctionLearning/baselineMLP1000/best_weights\n",
      "The loss after 594 iterations is E=0.46427325774867406\n",
      "Wrote weights to file ./customFunctionLearning/baselineMLP1000/best_weights\n",
      "The loss after 595 iterations is E=0.46322878308299725\n",
      "Wrote weights to file ./customFunctionLearning/baselineMLP1000/best_weights\n",
      "The loss after 596 iterations is E=0.46218818706955167\n",
      "Wrote weights to file ./customFunctionLearning/baselineMLP1000/best_weights\n",
      "The loss after 597 iterations is E=0.4611514574591476\n",
      "Wrote weights to file ./customFunctionLearning/baselineMLP1000/best_weights\n",
      "The loss after 598 iterations is E=0.46011858204017425\n",
      "Wrote weights to file ./customFunctionLearning/baselineMLP1000/best_weights\n",
      "The loss after 599 iterations is E=0.459089548638516\n",
      "Wrote weights to file ./customFunctionLearning/baselineMLP1000/best_weights\n",
      "The loss after 600 iterations is E=0.4580643451174758\n",
      "Wrote weights to file ./customFunctionLearning/baselineMLP1000/best_weights\n",
      "The loss after 601 iterations is E=0.45704295937771844\n",
      "Wrote weights to file ./customFunctionLearning/baselineMLP1000/best_weights\n",
      "The loss after 602 iterations is E=0.45602537935716536\n",
      "Wrote weights to file ./customFunctionLearning/baselineMLP1000/best_weights\n",
      "The loss after 603 iterations is E=0.4550115930309376\n",
      "Wrote weights to file ./customFunctionLearning/baselineMLP1000/best_weights\n",
      "The loss after 604 iterations is E=0.4540015884112615\n",
      "Wrote weights to file ./customFunctionLearning/baselineMLP1000/best_weights\n",
      "The loss after 605 iterations is E=0.45299535354739195\n",
      "Wrote weights to file ./customFunctionLearning/baselineMLP1000/best_weights\n",
      "The loss after 606 iterations is E=0.45199287652553316\n",
      "Wrote weights to file ./customFunctionLearning/baselineMLP1000/best_weights\n",
      "The loss after 607 iterations is E=0.45099414546874383\n",
      "Wrote weights to file ./customFunctionLearning/baselineMLP1000/best_weights\n",
      "The loss after 608 iterations is E=0.4499991485368548\n",
      "Wrote weights to file ./customFunctionLearning/baselineMLP1000/best_weights\n",
      "The loss after 609 iterations is E=0.4490078739263902\n",
      "Wrote weights to file ./customFunctionLearning/baselineMLP1000/best_weights\n",
      "The loss after 610 iterations is E=0.44802030987046176\n",
      "Wrote weights to file ./customFunctionLearning/baselineMLP1000/best_weights\n",
      "The loss after 611 iterations is E=0.4470364446386896\n",
      "Wrote weights to file ./customFunctionLearning/baselineMLP1000/best_weights\n",
      "The loss after 612 iterations is E=0.4460562665371082\n",
      "Wrote weights to file ./customFunctionLearning/baselineMLP1000/best_weights\n",
      "The loss after 613 iterations is E=0.445079763908075\n",
      "Wrote weights to file ./customFunctionLearning/baselineMLP1000/best_weights\n",
      "The loss after 614 iterations is E=0.4441069251301674\n",
      "Wrote weights to file ./customFunctionLearning/baselineMLP1000/best_weights\n",
      "The loss after 615 iterations is E=0.44313773861810035\n",
      "Wrote weights to file ./customFunctionLearning/baselineMLP1000/best_weights\n",
      "The loss after 616 iterations is E=0.44217219282262327\n",
      "Wrote weights to file ./customFunctionLearning/baselineMLP1000/best_weights\n",
      "The loss after 617 iterations is E=0.4412102762304162\n",
      "Wrote weights to file ./customFunctionLearning/baselineMLP1000/best_weights\n",
      "The loss after 618 iterations is E=0.4402519773640054\n",
      "Wrote weights to file ./customFunctionLearning/baselineMLP1000/best_weights\n",
      "The loss after 619 iterations is E=0.4392972847816503\n",
      "Wrote weights to file ./customFunctionLearning/baselineMLP1000/best_weights\n",
      "The loss after 620 iterations is E=0.4383461870772435\n",
      "Wrote weights to file ./customFunctionLearning/baselineMLP1000/best_weights\n",
      "The loss after 621 iterations is E=0.43739867288022083\n",
      "Wrote weights to file ./customFunctionLearning/baselineMLP1000/best_weights\n",
      "The loss after 622 iterations is E=0.4364547308554445\n",
      "Wrote weights to file ./customFunctionLearning/baselineMLP1000/best_weights\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The loss after 623 iterations is E=0.43551434970310426\n",
      "Wrote weights to file ./customFunctionLearning/baselineMLP1000/best_weights\n",
      "The loss after 624 iterations is E=0.4345775181586131\n",
      "Wrote weights to file ./customFunctionLearning/baselineMLP1000/best_weights\n",
      "The loss after 625 iterations is E=0.4336442249924992\n",
      "Wrote weights to file ./customFunctionLearning/baselineMLP1000/best_weights\n",
      "The loss after 626 iterations is E=0.4327144590103014\n",
      "Wrote weights to file ./customFunctionLearning/baselineMLP1000/best_weights\n",
      "The loss after 627 iterations is E=0.43178820905245824\n",
      "Wrote weights to file ./customFunctionLearning/baselineMLP1000/best_weights\n",
      "The loss after 628 iterations is E=0.4308654639942029\n",
      "Wrote weights to file ./customFunctionLearning/baselineMLP1000/best_weights\n",
      "The loss after 629 iterations is E=0.4299462127454451\n",
      "Wrote weights to file ./customFunctionLearning/baselineMLP1000/best_weights\n",
      "The loss after 630 iterations is E=0.4290304442506769\n",
      "Wrote weights to file ./customFunctionLearning/baselineMLP1000/best_weights\n",
      "The loss after 631 iterations is E=0.4281181474888433\n",
      "Wrote weights to file ./customFunctionLearning/baselineMLP1000/best_weights\n",
      "The loss after 632 iterations is E=0.42720931147324415\n",
      "Wrote weights to file ./customFunctionLearning/baselineMLP1000/best_weights\n",
      "The loss after 633 iterations is E=0.4263039252514103\n",
      "Wrote weights to file ./customFunctionLearning/baselineMLP1000/best_weights\n",
      "The loss after 634 iterations is E=0.4254019779049987\n",
      "Wrote weights to file ./customFunctionLearning/baselineMLP1000/best_weights\n",
      "The loss after 635 iterations is E=0.4245034585496696\n",
      "Wrote weights to file ./customFunctionLearning/baselineMLP1000/best_weights\n",
      "The loss after 636 iterations is E=0.42360835633498384\n",
      "Wrote weights to file ./customFunctionLearning/baselineMLP1000/best_weights\n",
      "The loss after 637 iterations is E=0.4227166604442755\n",
      "Wrote weights to file ./customFunctionLearning/baselineMLP1000/best_weights\n",
      "The loss after 638 iterations is E=0.42182836009453795\n",
      "Wrote weights to file ./customFunctionLearning/baselineMLP1000/best_weights\n",
      "The loss after 639 iterations is E=0.42094344453631455\n",
      "Wrote weights to file ./customFunctionLearning/baselineMLP1000/best_weights\n",
      "The loss after 640 iterations is E=0.42006190305357194\n",
      "Wrote weights to file ./customFunctionLearning/baselineMLP1000/best_weights\n",
      "The loss after 641 iterations is E=0.4191837249635829\n",
      "Wrote weights to file ./customFunctionLearning/baselineMLP1000/best_weights\n",
      "The loss after 642 iterations is E=0.41830889961681805\n",
      "Wrote weights to file ./customFunctionLearning/baselineMLP1000/best_weights\n",
      "The loss after 643 iterations is E=0.4174374163968076\n",
      "Wrote weights to file ./customFunctionLearning/baselineMLP1000/best_weights\n",
      "The loss after 644 iterations is E=0.41656926472004324\n",
      "Wrote weights to file ./customFunctionLearning/baselineMLP1000/best_weights\n",
      "The loss after 645 iterations is E=0.41570443403584034\n",
      "Wrote weights to file ./customFunctionLearning/baselineMLP1000/best_weights\n",
      "The loss after 646 iterations is E=0.4148429138262249\n",
      "Wrote weights to file ./customFunctionLearning/baselineMLP1000/best_weights\n",
      "The loss after 647 iterations is E=0.41398469360581597\n",
      "Wrote weights to file ./customFunctionLearning/baselineMLP1000/best_weights\n",
      "The loss after 648 iterations is E=0.41312976292169185\n",
      "Wrote weights to file ./customFunctionLearning/baselineMLP1000/best_weights\n",
      "The loss after 649 iterations is E=0.4122781113532819\n",
      "Wrote weights to file ./customFunctionLearning/baselineMLP1000/best_weights\n",
      "The loss after 650 iterations is E=0.4114297285122334\n",
      "Wrote weights to file ./customFunctionLearning/baselineMLP1000/best_weights\n",
      "The loss after 651 iterations is E=0.4105846040422945\n",
      "Wrote weights to file ./customFunctionLearning/baselineMLP1000/best_weights\n",
      "The loss after 652 iterations is E=0.4097427276191913\n",
      "Wrote weights to file ./customFunctionLearning/baselineMLP1000/best_weights\n",
      "The loss after 653 iterations is E=0.408904088950495\n",
      "Wrote weights to file ./customFunctionLearning/baselineMLP1000/best_weights\n",
      "The loss after 654 iterations is E=0.4080686777755131\n",
      "Wrote weights to file ./customFunctionLearning/baselineMLP1000/best_weights\n",
      "The loss after 655 iterations is E=0.40723648386515204\n",
      "Wrote weights to file ./customFunctionLearning/baselineMLP1000/best_weights\n",
      "The loss after 656 iterations is E=0.4064074970218008\n",
      "Wrote weights to file ./customFunctionLearning/baselineMLP1000/best_weights\n",
      "The loss after 657 iterations is E=0.4055817070791977\n",
      "Wrote weights to file ./customFunctionLearning/baselineMLP1000/best_weights\n",
      "The loss after 658 iterations is E=0.40475910390231373\n",
      "Wrote weights to file ./customFunctionLearning/baselineMLP1000/best_weights\n",
      "The loss after 659 iterations is E=0.40393967738722436\n",
      "Wrote weights to file ./customFunctionLearning/baselineMLP1000/best_weights\n",
      "The loss after 660 iterations is E=0.4031234174609805\n",
      "Wrote weights to file ./customFunctionLearning/baselineMLP1000/best_weights\n",
      "The loss after 661 iterations is E=0.4023103140814885\n",
      "Wrote weights to file ./customFunctionLearning/baselineMLP1000/best_weights\n",
      "The loss after 662 iterations is E=0.40150035723737837\n",
      "Wrote weights to file ./customFunctionLearning/baselineMLP1000/best_weights\n",
      "The loss after 663 iterations is E=0.4006935369478804\n",
      "Wrote weights to file ./customFunctionLearning/baselineMLP1000/best_weights\n",
      "The loss after 664 iterations is E=0.39988984326269933\n",
      "Wrote weights to file ./customFunctionLearning/baselineMLP1000/best_weights\n",
      "The loss after 665 iterations is E=0.39908926626188695\n",
      "Wrote weights to file ./customFunctionLearning/baselineMLP1000/best_weights\n",
      "The loss after 666 iterations is E=0.39829179605571546\n",
      "Wrote weights to file ./customFunctionLearning/baselineMLP1000/best_weights\n",
      "The loss after 667 iterations is E=0.3974974227845526\n",
      "Wrote weights to file ./customFunctionLearning/baselineMLP1000/best_weights\n",
      "The loss after 668 iterations is E=0.39670613661872905\n",
      "Wrote weights to file ./customFunctionLearning/baselineMLP1000/best_weights\n",
      "The loss after 669 iterations is E=0.39591792775842066\n",
      "Wrote weights to file ./customFunctionLearning/baselineMLP1000/best_weights\n",
      "The loss after 670 iterations is E=0.3951327864335125\n",
      "Wrote weights to file ./customFunctionLearning/baselineMLP1000/best_weights\n",
      "The loss after 671 iterations is E=0.3943507029034754\n",
      "Wrote weights to file ./customFunctionLearning/baselineMLP1000/best_weights\n",
      "The loss after 672 iterations is E=0.39357166745724176\n",
      "Wrote weights to file ./customFunctionLearning/baselineMLP1000/best_weights\n",
      "The loss after 673 iterations is E=0.39279567041307545\n",
      "Wrote weights to file ./customFunctionLearning/baselineMLP1000/best_weights\n",
      "The loss after 674 iterations is E=0.39202270211844564\n",
      "Wrote weights to file ./customFunctionLearning/baselineMLP1000/best_weights\n",
      "The loss after 675 iterations is E=0.3912527529498938\n",
      "Wrote weights to file ./customFunctionLearning/baselineMLP1000/best_weights\n",
      "The loss after 676 iterations is E=0.3904858133129187\n",
      "Wrote weights to file ./customFunctionLearning/baselineMLP1000/best_weights\n",
      "The loss after 677 iterations is E=0.38972187364183963\n",
      "Wrote weights to file ./customFunctionLearning/baselineMLP1000/best_weights\n",
      "The loss after 678 iterations is E=0.38896092439967556\n",
      "Wrote weights to file ./customFunctionLearning/baselineMLP1000/best_weights\n",
      "The loss after 679 iterations is E=0.3882029560780079\n",
      "Wrote weights to file ./customFunctionLearning/baselineMLP1000/best_weights\n",
      "The loss after 680 iterations is E=0.3874479591968687\n",
      "Wrote weights to file ./customFunctionLearning/baselineMLP1000/best_weights\n",
      "The loss after 681 iterations is E=0.38669592430460453\n",
      "Wrote weights to file ./customFunctionLearning/baselineMLP1000/best_weights\n",
      "The loss after 682 iterations is E=0.3859468419777467\n",
      "Wrote weights to file ./customFunctionLearning/baselineMLP1000/best_weights\n",
      "The loss after 683 iterations is E=0.3852007028208956\n",
      "Wrote weights to file ./customFunctionLearning/baselineMLP1000/best_weights\n",
      "The loss after 684 iterations is E=0.38445749746658414\n",
      "Wrote weights to file ./customFunctionLearning/baselineMLP1000/best_weights\n",
      "The loss after 685 iterations is E=0.3837172165751609\n",
      "Wrote weights to file ./customFunctionLearning/baselineMLP1000/best_weights\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The loss after 686 iterations is E=0.3829798508346496\n",
      "Wrote weights to file ./customFunctionLearning/baselineMLP1000/best_weights\n",
      "The loss after 687 iterations is E=0.38224539096064486\n",
      "Wrote weights to file ./customFunctionLearning/baselineMLP1000/best_weights\n",
      "The loss after 688 iterations is E=0.38151382769616093\n",
      "Wrote weights to file ./customFunctionLearning/baselineMLP1000/best_weights\n",
      "The loss after 689 iterations is E=0.38078515181153283\n",
      "Wrote weights to file ./customFunctionLearning/baselineMLP1000/best_weights\n",
      "The loss after 690 iterations is E=0.3800593541042634\n",
      "Wrote weights to file ./customFunctionLearning/baselineMLP1000/best_weights\n",
      "The loss after 691 iterations is E=0.3793364253989241\n",
      "Wrote weights to file ./customFunctionLearning/baselineMLP1000/best_weights\n",
      "The loss after 692 iterations is E=0.3786163565470125\n",
      "Wrote weights to file ./customFunctionLearning/baselineMLP1000/best_weights\n",
      "The loss after 693 iterations is E=0.37789913842683237\n",
      "Wrote weights to file ./customFunctionLearning/baselineMLP1000/best_weights\n",
      "The loss after 694 iterations is E=0.3771847619433775\n",
      "Wrote weights to file ./customFunctionLearning/baselineMLP1000/best_weights\n",
      "The loss after 695 iterations is E=0.3764732180281906\n",
      "Wrote weights to file ./customFunctionLearning/baselineMLP1000/best_weights\n",
      "The loss after 696 iterations is E=0.3757644976392598\n",
      "Wrote weights to file ./customFunctionLearning/baselineMLP1000/best_weights\n",
      "The loss after 697 iterations is E=0.3750585917608763\n",
      "Wrote weights to file ./customFunctionLearning/baselineMLP1000/best_weights\n",
      "The loss after 698 iterations is E=0.37435549140352387\n",
      "Wrote weights to file ./customFunctionLearning/baselineMLP1000/best_weights\n",
      "The loss after 699 iterations is E=0.37365518760375044\n",
      "Wrote weights to file ./customFunctionLearning/baselineMLP1000/best_weights\n",
      "The loss after 700 iterations is E=0.37295767142404634\n",
      "Wrote weights to file ./customFunctionLearning/baselineMLP1000/best_weights\n",
      "The loss after 701 iterations is E=0.37226293395272286\n",
      "Wrote weights to file ./customFunctionLearning/baselineMLP1000/best_weights\n",
      "The loss after 702 iterations is E=0.3715709663037907\n",
      "Wrote weights to file ./customFunctionLearning/baselineMLP1000/best_weights\n",
      "The loss after 703 iterations is E=0.37088175961683284\n",
      "Wrote weights to file ./customFunctionLearning/baselineMLP1000/best_weights\n",
      "The loss after 704 iterations is E=0.37019530505689224\n",
      "Wrote weights to file ./customFunctionLearning/baselineMLP1000/best_weights\n",
      "The loss after 705 iterations is E=0.36951159381434745\n",
      "Wrote weights to file ./customFunctionLearning/baselineMLP1000/best_weights\n",
      "The loss after 706 iterations is E=0.3688306171047872\n",
      "Wrote weights to file ./customFunctionLearning/baselineMLP1000/best_weights\n",
      "The loss after 707 iterations is E=0.36815236616889246\n",
      "Wrote weights to file ./customFunctionLearning/baselineMLP1000/best_weights\n",
      "The loss after 708 iterations is E=0.36747683227232397\n",
      "Wrote weights to file ./customFunctionLearning/baselineMLP1000/best_weights\n",
      "The loss after 709 iterations is E=0.36680400670559604\n",
      "Wrote weights to file ./customFunctionLearning/baselineMLP1000/best_weights\n",
      "The loss after 710 iterations is E=0.366133880783956\n",
      "Wrote weights to file ./customFunctionLearning/baselineMLP1000/best_weights\n",
      "The loss after 711 iterations is E=0.36546644584727034\n",
      "Wrote weights to file ./customFunctionLearning/baselineMLP1000/best_weights\n",
      "The loss after 712 iterations is E=0.36480169325990297\n",
      "Wrote weights to file ./customFunctionLearning/baselineMLP1000/best_weights\n",
      "The loss after 713 iterations is E=0.3641396144106005\n",
      "Wrote weights to file ./customFunctionLearning/baselineMLP1000/best_weights\n",
      "The loss after 714 iterations is E=0.363480200712376\n",
      "Wrote weights to file ./customFunctionLearning/baselineMLP1000/best_weights\n",
      "The loss after 715 iterations is E=0.3628234436023859\n",
      "Wrote weights to file ./customFunctionLearning/baselineMLP1000/best_weights\n",
      "The loss after 716 iterations is E=0.3621693345418219\n",
      "Wrote weights to file ./customFunctionLearning/baselineMLP1000/best_weights\n",
      "The loss after 717 iterations is E=0.3615178650157868\n",
      "Wrote weights to file ./customFunctionLearning/baselineMLP1000/best_weights\n",
      "The loss after 718 iterations is E=0.3608690265331884\n",
      "Wrote weights to file ./customFunctionLearning/baselineMLP1000/best_weights\n",
      "The loss after 719 iterations is E=0.36022281062661493\n",
      "Wrote weights to file ./customFunctionLearning/baselineMLP1000/best_weights\n",
      "The loss after 720 iterations is E=0.35957920885222544\n",
      "Wrote weights to file ./customFunctionLearning/baselineMLP1000/best_weights\n",
      "The loss after 721 iterations is E=0.3589382127896398\n",
      "Wrote weights to file ./customFunctionLearning/baselineMLP1000/best_weights\n",
      "The loss after 722 iterations is E=0.35829981404181765\n",
      "Wrote weights to file ./customFunctionLearning/baselineMLP1000/best_weights\n",
      "The loss after 723 iterations is E=0.3576640042349509\n",
      "Wrote weights to file ./customFunctionLearning/baselineMLP1000/best_weights\n",
      "The loss after 724 iterations is E=0.3570307750183495\n",
      "Wrote weights to file ./customFunctionLearning/baselineMLP1000/best_weights\n",
      "The loss after 725 iterations is E=0.35640011806432415\n",
      "Wrote weights to file ./customFunctionLearning/baselineMLP1000/best_weights\n",
      "The loss after 726 iterations is E=0.3557720250680889\n",
      "Wrote weights to file ./customFunctionLearning/baselineMLP1000/best_weights\n",
      "The loss after 727 iterations is E=0.3551464877476366\n",
      "Wrote weights to file ./customFunctionLearning/baselineMLP1000/best_weights\n",
      "The loss after 728 iterations is E=0.35452349784363535\n",
      "Wrote weights to file ./customFunctionLearning/baselineMLP1000/best_weights\n",
      "The loss after 729 iterations is E=0.353903047119315\n",
      "Wrote weights to file ./customFunctionLearning/baselineMLP1000/best_weights\n",
      "The loss after 730 iterations is E=0.353285127360363\n",
      "Wrote weights to file ./customFunctionLearning/baselineMLP1000/best_weights\n",
      "The loss after 731 iterations is E=0.35266973037481036\n",
      "Wrote weights to file ./customFunctionLearning/baselineMLP1000/best_weights\n",
      "The loss after 732 iterations is E=0.35205684799292936\n",
      "Wrote weights to file ./customFunctionLearning/baselineMLP1000/best_weights\n",
      "The loss after 733 iterations is E=0.35144647206711926\n",
      "Wrote weights to file ./customFunctionLearning/baselineMLP1000/best_weights\n",
      "The loss after 734 iterations is E=0.3508385944718052\n",
      "Wrote weights to file ./customFunctionLearning/baselineMLP1000/best_weights\n",
      "The loss after 735 iterations is E=0.35023320710332917\n",
      "Wrote weights to file ./customFunctionLearning/baselineMLP1000/best_weights\n",
      "The loss after 736 iterations is E=0.34963030187984423\n",
      "Wrote weights to file ./customFunctionLearning/baselineMLP1000/best_weights\n",
      "The loss after 737 iterations is E=0.3490298707412109\n",
      "Wrote weights to file ./customFunctionLearning/baselineMLP1000/best_weights\n",
      "The loss after 738 iterations is E=0.34843190564888943\n",
      "Wrote weights to file ./customFunctionLearning/baselineMLP1000/best_weights\n",
      "The loss after 739 iterations is E=0.3478363985858377\n",
      "Wrote weights to file ./customFunctionLearning/baselineMLP1000/best_weights\n",
      "The loss after 740 iterations is E=0.3472433415564136\n",
      "Wrote weights to file ./customFunctionLearning/baselineMLP1000/best_weights\n",
      "The loss after 741 iterations is E=0.34665272658625784\n",
      "Wrote weights to file ./customFunctionLearning/baselineMLP1000/best_weights\n",
      "The loss after 742 iterations is E=0.3460645457222066\n",
      "Wrote weights to file ./customFunctionLearning/baselineMLP1000/best_weights\n",
      "The loss after 743 iterations is E=0.345478791032182\n",
      "Wrote weights to file ./customFunctionLearning/baselineMLP1000/best_weights\n",
      "The loss after 744 iterations is E=0.3448954546050922\n",
      "Wrote weights to file ./customFunctionLearning/baselineMLP1000/best_weights\n",
      "The loss after 745 iterations is E=0.3443145285507338\n",
      "Wrote weights to file ./customFunctionLearning/baselineMLP1000/best_weights\n",
      "The loss after 746 iterations is E=0.34373600499968776\n",
      "Wrote weights to file ./customFunctionLearning/baselineMLP1000/best_weights\n",
      "The loss after 747 iterations is E=0.3431598761032256\n",
      "Wrote weights to file ./customFunctionLearning/baselineMLP1000/best_weights\n",
      "The loss after 748 iterations is E=0.3425861340332061\n",
      "Wrote weights to file ./customFunctionLearning/baselineMLP1000/best_weights\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The loss after 749 iterations is E=0.3420147709819797\n",
      "Wrote weights to file ./customFunctionLearning/baselineMLP1000/best_weights\n",
      "The loss after 750 iterations is E=0.3414457791622907\n",
      "Wrote weights to file ./customFunctionLearning/baselineMLP1000/best_weights\n",
      "The loss after 751 iterations is E=0.3408791508071815\n",
      "Wrote weights to file ./customFunctionLearning/baselineMLP1000/best_weights\n",
      "The loss after 752 iterations is E=0.3403148781698983\n",
      "Wrote weights to file ./customFunctionLearning/baselineMLP1000/best_weights\n",
      "The loss after 753 iterations is E=0.33975295352378965\n",
      "Wrote weights to file ./customFunctionLearning/baselineMLP1000/best_weights\n",
      "The loss after 754 iterations is E=0.3391933691622185\n",
      "Wrote weights to file ./customFunctionLearning/baselineMLP1000/best_weights\n",
      "The loss after 755 iterations is E=0.33863611739846705\n",
      "Wrote weights to file ./customFunctionLearning/baselineMLP1000/best_weights\n",
      "The loss after 756 iterations is E=0.3380811905656403\n",
      "Wrote weights to file ./customFunctionLearning/baselineMLP1000/best_weights\n",
      "The loss after 757 iterations is E=0.3375285810165744\n",
      "Wrote weights to file ./customFunctionLearning/baselineMLP1000/best_weights\n",
      "The loss after 758 iterations is E=0.33697828112374534\n",
      "Wrote weights to file ./customFunctionLearning/baselineMLP1000/best_weights\n",
      "The loss after 759 iterations is E=0.3364302832791834\n",
      "Wrote weights to file ./customFunctionLearning/baselineMLP1000/best_weights\n",
      "The loss after 760 iterations is E=0.33588457989436815\n",
      "Wrote weights to file ./customFunctionLearning/baselineMLP1000/best_weights\n",
      "The loss after 761 iterations is E=0.33534116340015263\n",
      "Wrote weights to file ./customFunctionLearning/baselineMLP1000/best_weights\n",
      "The loss after 762 iterations is E=0.3348000262466665\n",
      "Wrote weights to file ./customFunctionLearning/baselineMLP1000/best_weights\n",
      "The loss after 763 iterations is E=0.3342611609032309\n",
      "Wrote weights to file ./customFunctionLearning/baselineMLP1000/best_weights\n",
      "The loss after 764 iterations is E=0.333724559858267\n",
      "Wrote weights to file ./customFunctionLearning/baselineMLP1000/best_weights\n",
      "The loss after 765 iterations is E=0.3331902156192117\n",
      "Wrote weights to file ./customFunctionLearning/baselineMLP1000/best_weights\n",
      "The loss after 766 iterations is E=0.33265812071243084\n",
      "Wrote weights to file ./customFunctionLearning/baselineMLP1000/best_weights\n",
      "The loss after 767 iterations is E=0.33212826768312564\n",
      "Wrote weights to file ./customFunctionLearning/baselineMLP1000/best_weights\n",
      "The loss after 768 iterations is E=0.3316006490952642\n",
      "Wrote weights to file ./customFunctionLearning/baselineMLP1000/best_weights\n",
      "The loss after 769 iterations is E=0.3310752575314771\n",
      "Wrote weights to file ./customFunctionLearning/baselineMLP1000/best_weights\n",
      "The loss after 770 iterations is E=0.3305520855929899\n",
      "Wrote weights to file ./customFunctionLearning/baselineMLP1000/best_weights\n",
      "The loss after 771 iterations is E=0.3300311258995264\n",
      "Wrote weights to file ./customFunctionLearning/baselineMLP1000/best_weights\n",
      "The loss after 772 iterations is E=0.32951237108923614\n",
      "Wrote weights to file ./customFunctionLearning/baselineMLP1000/best_weights\n",
      "The loss after 773 iterations is E=0.32899581381860843\n",
      "Wrote weights to file ./customFunctionLearning/baselineMLP1000/best_weights\n",
      "The loss after 774 iterations is E=0.3284814467623903\n",
      "Wrote weights to file ./customFunctionLearning/baselineMLP1000/best_weights\n",
      "The loss after 775 iterations is E=0.32796926261350406\n",
      "Wrote weights to file ./customFunctionLearning/baselineMLP1000/best_weights\n",
      "The loss after 776 iterations is E=0.32745925408297566\n",
      "Wrote weights to file ./customFunctionLearning/baselineMLP1000/best_weights\n",
      "The loss after 777 iterations is E=0.32695141389984683\n",
      "Wrote weights to file ./customFunctionLearning/baselineMLP1000/best_weights\n",
      "The loss after 778 iterations is E=0.3264457348111001\n",
      "Wrote weights to file ./customFunctionLearning/baselineMLP1000/best_weights\n",
      "The loss after 779 iterations is E=0.3259422095815783\n",
      "Wrote weights to file ./customFunctionLearning/baselineMLP1000/best_weights\n",
      "The loss after 780 iterations is E=0.32544083099391197\n",
      "Wrote weights to file ./customFunctionLearning/baselineMLP1000/best_weights\n",
      "The loss after 781 iterations is E=0.3249415918484388\n",
      "Wrote weights to file ./customFunctionLearning/baselineMLP1000/best_weights\n",
      "The loss after 782 iterations is E=0.3244444849631322\n",
      "Wrote weights to file ./customFunctionLearning/baselineMLP1000/best_weights\n",
      "The loss after 783 iterations is E=0.32394950317351745\n",
      "Wrote weights to file ./customFunctionLearning/baselineMLP1000/best_weights\n",
      "The loss after 784 iterations is E=0.32345663933260566\n",
      "Wrote weights to file ./customFunctionLearning/baselineMLP1000/best_weights\n",
      "The loss after 785 iterations is E=0.3229658863108186\n",
      "Wrote weights to file ./customFunctionLearning/baselineMLP1000/best_weights\n",
      "The loss after 786 iterations is E=0.32247723699591274\n",
      "Wrote weights to file ./customFunctionLearning/baselineMLP1000/best_weights\n",
      "The loss after 787 iterations is E=0.3219906842929082\n",
      "Wrote weights to file ./customFunctionLearning/baselineMLP1000/best_weights\n",
      "The loss after 788 iterations is E=0.32150622112401567\n",
      "Wrote weights to file ./customFunctionLearning/baselineMLP1000/best_weights\n",
      "The loss after 789 iterations is E=0.3210238404285682\n",
      "Wrote weights to file ./customFunctionLearning/baselineMLP1000/best_weights\n",
      "The loss after 790 iterations is E=0.3205435351629477\n",
      "Wrote weights to file ./customFunctionLearning/baselineMLP1000/best_weights\n",
      "The loss after 791 iterations is E=0.3200652983005153\n",
      "Wrote weights to file ./customFunctionLearning/baselineMLP1000/best_weights\n",
      "The loss after 792 iterations is E=0.3195891228315471\n",
      "Wrote weights to file ./customFunctionLearning/baselineMLP1000/best_weights\n",
      "The loss after 793 iterations is E=0.3191150017631556\n",
      "Wrote weights to file ./customFunctionLearning/baselineMLP1000/best_weights\n",
      "The loss after 794 iterations is E=0.31864292811923395\n",
      "Wrote weights to file ./customFunctionLearning/baselineMLP1000/best_weights\n",
      "The loss after 795 iterations is E=0.3181728949403819\n",
      "Wrote weights to file ./customFunctionLearning/baselineMLP1000/best_weights\n",
      "The loss after 796 iterations is E=0.31770489528383755\n",
      "Wrote weights to file ./customFunctionLearning/baselineMLP1000/best_weights\n",
      "The loss after 797 iterations is E=0.31723892222341443\n",
      "Wrote weights to file ./customFunctionLearning/baselineMLP1000/best_weights\n",
      "The loss after 798 iterations is E=0.3167749688494424\n",
      "Wrote weights to file ./customFunctionLearning/baselineMLP1000/best_weights\n",
      "The loss after 799 iterations is E=0.31631302826868973\n",
      "Wrote weights to file ./customFunctionLearning/baselineMLP1000/best_weights\n",
      "The loss after 800 iterations is E=0.31585309360431013\n",
      "Wrote weights to file ./customFunctionLearning/baselineMLP1000/best_weights\n",
      "The loss after 801 iterations is E=0.31539515799577605\n",
      "Wrote weights to file ./customFunctionLearning/baselineMLP1000/best_weights\n",
      "The loss after 802 iterations is E=0.3149392145988161\n",
      "Wrote weights to file ./customFunctionLearning/baselineMLP1000/best_weights\n",
      "The loss after 803 iterations is E=0.31448525658535376\n",
      "Wrote weights to file ./customFunctionLearning/baselineMLP1000/best_weights\n",
      "The loss after 804 iterations is E=0.3140332771434432\n",
      "Wrote weights to file ./customFunctionLearning/baselineMLP1000/best_weights\n",
      "The loss after 805 iterations is E=0.3135832694772136\n",
      "Wrote weights to file ./customFunctionLearning/baselineMLP1000/best_weights\n",
      "The loss after 806 iterations is E=0.3131352268068056\n",
      "Wrote weights to file ./customFunctionLearning/baselineMLP1000/best_weights\n",
      "The loss after 807 iterations is E=0.3126891423683166\n",
      "Wrote weights to file ./customFunctionLearning/baselineMLP1000/best_weights\n",
      "The loss after 808 iterations is E=0.31224500941373085\n",
      "Wrote weights to file ./customFunctionLearning/baselineMLP1000/best_weights\n",
      "The loss after 809 iterations is E=0.31180282121087965\n",
      "Wrote weights to file ./customFunctionLearning/baselineMLP1000/best_weights\n",
      "The loss after 810 iterations is E=0.3113625710433654\n",
      "Wrote weights to file ./customFunctionLearning/baselineMLP1000/best_weights\n",
      "The loss after 811 iterations is E=0.31092425221051856\n",
      "Wrote weights to file ./customFunctionLearning/baselineMLP1000/best_weights\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The loss after 812 iterations is E=0.31048785802733275\n",
      "Wrote weights to file ./customFunctionLearning/baselineMLP1000/best_weights\n",
      "The loss after 813 iterations is E=0.31005338182441355\n",
      "Wrote weights to file ./customFunctionLearning/baselineMLP1000/best_weights\n",
      "The loss after 814 iterations is E=0.3096208169479248\n",
      "Wrote weights to file ./customFunctionLearning/baselineMLP1000/best_weights\n",
      "The loss after 815 iterations is E=0.3091901567595295\n",
      "Wrote weights to file ./customFunctionLearning/baselineMLP1000/best_weights\n",
      "The loss after 816 iterations is E=0.30876139463633984\n",
      "Wrote weights to file ./customFunctionLearning/baselineMLP1000/best_weights\n",
      "The loss after 817 iterations is E=0.3083345239708646\n",
      "Wrote weights to file ./customFunctionLearning/baselineMLP1000/best_weights\n",
      "The loss after 818 iterations is E=0.3079095381709568\n",
      "Wrote weights to file ./customFunctionLearning/baselineMLP1000/best_weights\n",
      "The loss after 819 iterations is E=0.3074864306597566\n",
      "Wrote weights to file ./customFunctionLearning/baselineMLP1000/best_weights\n",
      "The loss after 820 iterations is E=0.3070651948756491\n",
      "Wrote weights to file ./customFunctionLearning/baselineMLP1000/best_weights\n",
      "The loss after 821 iterations is E=0.30664582427220394\n",
      "Wrote weights to file ./customFunctionLearning/baselineMLP1000/best_weights\n",
      "The loss after 822 iterations is E=0.3062283123181369\n",
      "Wrote weights to file ./customFunctionLearning/baselineMLP1000/best_weights\n",
      "The loss after 823 iterations is E=0.3058126524972478\n",
      "Wrote weights to file ./customFunctionLearning/baselineMLP1000/best_weights\n",
      "The loss after 824 iterations is E=0.3053988383083839\n",
      "Wrote weights to file ./customFunctionLearning/baselineMLP1000/best_weights\n",
      "The loss after 825 iterations is E=0.30498686326538144\n",
      "Wrote weights to file ./customFunctionLearning/baselineMLP1000/best_weights\n",
      "The loss after 826 iterations is E=0.30457672089702464\n",
      "Wrote weights to file ./customFunctionLearning/baselineMLP1000/best_weights\n",
      "The loss after 827 iterations is E=0.30416840474699736\n",
      "Wrote weights to file ./customFunctionLearning/baselineMLP1000/best_weights\n",
      "The loss after 828 iterations is E=0.3037619083738335\n",
      "Wrote weights to file ./customFunctionLearning/baselineMLP1000/best_weights\n",
      "The loss after 829 iterations is E=0.3033572253508768\n",
      "Wrote weights to file ./customFunctionLearning/baselineMLP1000/best_weights\n",
      "The loss after 830 iterations is E=0.30295434926623316\n",
      "Wrote weights to file ./customFunctionLearning/baselineMLP1000/best_weights\n",
      "The loss after 831 iterations is E=0.30255327372272195\n",
      "Wrote weights to file ./customFunctionLearning/baselineMLP1000/best_weights\n",
      "The loss after 832 iterations is E=0.3021539923378357\n",
      "Wrote weights to file ./customFunctionLearning/baselineMLP1000/best_weights\n",
      "The loss after 833 iterations is E=0.3017564987437013\n",
      "Wrote weights to file ./customFunctionLearning/baselineMLP1000/best_weights\n",
      "The loss after 834 iterations is E=0.3013607865870272\n",
      "Wrote weights to file ./customFunctionLearning/baselineMLP1000/best_weights\n",
      "The loss after 835 iterations is E=0.3009668495290662\n",
      "Wrote weights to file ./customFunctionLearning/baselineMLP1000/best_weights\n",
      "The loss after 836 iterations is E=0.30057468124557696\n",
      "Wrote weights to file ./customFunctionLearning/baselineMLP1000/best_weights\n",
      "The loss after 837 iterations is E=0.3001842754267743\n",
      "Wrote weights to file ./customFunctionLearning/baselineMLP1000/best_weights\n",
      "The loss after 838 iterations is E=0.2997956257772996\n",
      "Wrote weights to file ./customFunctionLearning/baselineMLP1000/best_weights\n",
      "The loss after 839 iterations is E=0.29940872601616514\n",
      "Wrote weights to file ./customFunctionLearning/baselineMLP1000/best_weights\n",
      "The loss after 840 iterations is E=0.2990235698767342\n",
      "Wrote weights to file ./customFunctionLearning/baselineMLP1000/best_weights\n",
      "The loss after 841 iterations is E=0.29864015110666214\n",
      "Wrote weights to file ./customFunctionLearning/baselineMLP1000/best_weights\n",
      "The loss after 842 iterations is E=0.2982584634678714\n",
      "Wrote weights to file ./customFunctionLearning/baselineMLP1000/best_weights\n",
      "The loss after 843 iterations is E=0.29787850073651156\n",
      "Wrote weights to file ./customFunctionLearning/baselineMLP1000/best_weights\n",
      "The loss after 844 iterations is E=0.29750025670291325\n",
      "Wrote weights to file ./customFunctionLearning/baselineMLP1000/best_weights\n",
      "The loss after 845 iterations is E=0.29712372517156066\n",
      "Wrote weights to file ./customFunctionLearning/baselineMLP1000/best_weights\n",
      "The loss after 846 iterations is E=0.2967488999610525\n",
      "Wrote weights to file ./customFunctionLearning/baselineMLP1000/best_weights\n",
      "The loss after 847 iterations is E=0.29637577490406664\n",
      "Wrote weights to file ./customFunctionLearning/baselineMLP1000/best_weights\n",
      "The loss after 848 iterations is E=0.29600434384731455\n",
      "Wrote weights to file ./customFunctionLearning/baselineMLP1000/best_weights\n",
      "The loss after 849 iterations is E=0.2956346006515262\n",
      "Wrote weights to file ./customFunctionLearning/baselineMLP1000/best_weights\n",
      "The loss after 850 iterations is E=0.2952665391913965\n",
      "Wrote weights to file ./customFunctionLearning/baselineMLP1000/best_weights\n",
      "The loss after 851 iterations is E=0.2949001533555643\n",
      "Wrote weights to file ./customFunctionLearning/baselineMLP1000/best_weights\n",
      "The loss after 852 iterations is E=0.29453543704656904\n",
      "Wrote weights to file ./customFunctionLearning/baselineMLP1000/best_weights\n",
      "The loss after 853 iterations is E=0.2941723841808275\n",
      "Wrote weights to file ./customFunctionLearning/baselineMLP1000/best_weights\n",
      "The loss after 854 iterations is E=0.2938109886885925\n",
      "Wrote weights to file ./customFunctionLearning/baselineMLP1000/best_weights\n",
      "The loss after 855 iterations is E=0.2934512445139256\n",
      "Wrote weights to file ./customFunctionLearning/baselineMLP1000/best_weights\n",
      "The loss after 856 iterations is E=0.29309314561467037\n",
      "Wrote weights to file ./customFunctionLearning/baselineMLP1000/best_weights\n",
      "The loss after 857 iterations is E=0.2927366859624055\n",
      "Wrote weights to file ./customFunctionLearning/baselineMLP1000/best_weights\n",
      "The loss after 858 iterations is E=0.2923818595424319\n",
      "Wrote weights to file ./customFunctionLearning/baselineMLP1000/best_weights\n",
      "The loss after 859 iterations is E=0.2920286603537336\n",
      "Wrote weights to file ./customFunctionLearning/baselineMLP1000/best_weights\n",
      "The loss after 860 iterations is E=0.29167708240894874\n",
      "Wrote weights to file ./customFunctionLearning/baselineMLP1000/best_weights\n",
      "The loss after 861 iterations is E=0.2913271197343415\n",
      "Wrote weights to file ./customFunctionLearning/baselineMLP1000/best_weights\n",
      "The loss after 862 iterations is E=0.2909787663697732\n",
      "Wrote weights to file ./customFunctionLearning/baselineMLP1000/best_weights\n",
      "The loss after 863 iterations is E=0.29063201636867464\n",
      "Wrote weights to file ./customFunctionLearning/baselineMLP1000/best_weights\n",
      "The loss after 864 iterations is E=0.2902868637980153\n",
      "Wrote weights to file ./customFunctionLearning/baselineMLP1000/best_weights\n",
      "The loss after 865 iterations is E=0.28994330273828095\n",
      "Wrote weights to file ./customFunctionLearning/baselineMLP1000/best_weights\n",
      "The loss after 866 iterations is E=0.2896013272834398\n",
      "Wrote weights to file ./customFunctionLearning/baselineMLP1000/best_weights\n",
      "The loss after 867 iterations is E=0.28926093154092436\n",
      "Wrote weights to file ./customFunctionLearning/baselineMLP1000/best_weights\n",
      "The loss after 868 iterations is E=0.28892210963159737\n",
      "Wrote weights to file ./customFunctionLearning/baselineMLP1000/best_weights\n",
      "The loss after 869 iterations is E=0.2885848556897296\n",
      "Wrote weights to file ./customFunctionLearning/baselineMLP1000/best_weights\n",
      "The loss after 870 iterations is E=0.2882491638629787\n",
      "Wrote weights to file ./customFunctionLearning/baselineMLP1000/best_weights\n",
      "The loss after 871 iterations is E=0.2879150283123549\n",
      "Wrote weights to file ./customFunctionLearning/baselineMLP1000/best_weights\n",
      "The loss after 872 iterations is E=0.2875824432122028\n",
      "Wrote weights to file ./customFunctionLearning/baselineMLP1000/best_weights\n",
      "The loss after 873 iterations is E=0.2872514027501778\n",
      "Wrote weights to file ./customFunctionLearning/baselineMLP1000/best_weights\n",
      "The loss after 874 iterations is E=0.2869219011272198\n",
      "Wrote weights to file ./customFunctionLearning/baselineMLP1000/best_weights\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The loss after 875 iterations is E=0.2865939325575308\n",
      "Wrote weights to file ./customFunctionLearning/baselineMLP1000/best_weights\n",
      "The loss after 876 iterations is E=0.2862674912685511\n",
      "Wrote weights to file ./customFunctionLearning/baselineMLP1000/best_weights\n",
      "The loss after 877 iterations is E=0.2859425715009399\n",
      "Wrote weights to file ./customFunctionLearning/baselineMLP1000/best_weights\n",
      "The loss after 878 iterations is E=0.28561916750855\n",
      "Wrote weights to file ./customFunctionLearning/baselineMLP1000/best_weights\n",
      "The loss after 879 iterations is E=0.2852972735584053\n",
      "Wrote weights to file ./customFunctionLearning/baselineMLP1000/best_weights\n",
      "The loss after 880 iterations is E=0.2849768839306825\n",
      "Wrote weights to file ./customFunctionLearning/baselineMLP1000/best_weights\n",
      "The loss after 881 iterations is E=0.2846579929186865\n",
      "Wrote weights to file ./customFunctionLearning/baselineMLP1000/best_weights\n",
      "The loss after 882 iterations is E=0.2843405948288365\n",
      "Wrote weights to file ./customFunctionLearning/baselineMLP1000/best_weights\n",
      "The loss after 883 iterations is E=0.2840246839806389\n",
      "Wrote weights to file ./customFunctionLearning/baselineMLP1000/best_weights\n",
      "The loss after 884 iterations is E=0.28371025470666617\n",
      "Wrote weights to file ./customFunctionLearning/baselineMLP1000/best_weights\n",
      "The loss after 885 iterations is E=0.2833973013525473\n",
      "Wrote weights to file ./customFunctionLearning/baselineMLP1000/best_weights\n",
      "The loss after 886 iterations is E=0.2830858182769338\n",
      "Wrote weights to file ./customFunctionLearning/baselineMLP1000/best_weights\n",
      "The loss after 887 iterations is E=0.2827757998514946\n",
      "Wrote weights to file ./customFunctionLearning/baselineMLP1000/best_weights\n",
      "The loss after 888 iterations is E=0.2824672404608908\n",
      "Wrote weights to file ./customFunctionLearning/baselineMLP1000/best_weights\n",
      "The loss after 889 iterations is E=0.2821601345027565\n",
      "Wrote weights to file ./customFunctionLearning/baselineMLP1000/best_weights\n",
      "The loss after 890 iterations is E=0.2818544763876827\n",
      "Wrote weights to file ./customFunctionLearning/baselineMLP1000/best_weights\n",
      "The loss after 891 iterations is E=0.28155026053920124\n",
      "Wrote weights to file ./customFunctionLearning/baselineMLP1000/best_weights\n",
      "The loss after 892 iterations is E=0.2812474813937657\n",
      "Wrote weights to file ./customFunctionLearning/baselineMLP1000/best_weights\n",
      "The loss after 893 iterations is E=0.28094613340073343\n",
      "Wrote weights to file ./customFunctionLearning/baselineMLP1000/best_weights\n",
      "The loss after 894 iterations is E=0.2806462110223519\n",
      "Wrote weights to file ./customFunctionLearning/baselineMLP1000/best_weights\n",
      "The loss after 895 iterations is E=0.2803477087337384\n",
      "Wrote weights to file ./customFunctionLearning/baselineMLP1000/best_weights\n",
      "The loss after 896 iterations is E=0.28005062102286704\n",
      "Wrote weights to file ./customFunctionLearning/baselineMLP1000/best_weights\n",
      "The loss after 897 iterations is E=0.27975494239055354\n",
      "Wrote weights to file ./customFunctionLearning/baselineMLP1000/best_weights\n",
      "The loss after 898 iterations is E=0.2794606673504346\n",
      "Wrote weights to file ./customFunctionLearning/baselineMLP1000/best_weights\n",
      "The loss after 899 iterations is E=0.27916779042896267\n",
      "Wrote weights to file ./customFunctionLearning/baselineMLP1000/best_weights\n",
      "The loss after 900 iterations is E=0.27887630616537745\n",
      "Wrote weights to file ./customFunctionLearning/baselineMLP1000/best_weights\n",
      "The loss after 901 iterations is E=0.27858620911170523\n",
      "Wrote weights to file ./customFunctionLearning/baselineMLP1000/best_weights\n",
      "The loss after 902 iterations is E=0.27829749383273306\n",
      "Wrote weights to file ./customFunctionLearning/baselineMLP1000/best_weights\n",
      "The loss after 903 iterations is E=0.2780101549060018\n",
      "Wrote weights to file ./customFunctionLearning/baselineMLP1000/best_weights\n",
      "The loss after 904 iterations is E=0.27772418692178963\n",
      "Wrote weights to file ./customFunctionLearning/baselineMLP1000/best_weights\n",
      "The loss after 905 iterations is E=0.27743958448310113\n",
      "Wrote weights to file ./customFunctionLearning/baselineMLP1000/best_weights\n",
      "The loss after 906 iterations is E=0.27715634220564883\n",
      "Wrote weights to file ./customFunctionLearning/baselineMLP1000/best_weights\n",
      "The loss after 907 iterations is E=0.27687445471784383\n",
      "Wrote weights to file ./customFunctionLearning/baselineMLP1000/best_weights\n",
      "The loss after 908 iterations is E=0.27659391666078426\n",
      "Wrote weights to file ./customFunctionLearning/baselineMLP1000/best_weights\n",
      "The loss after 909 iterations is E=0.27631472268823926\n",
      "Wrote weights to file ./customFunctionLearning/baselineMLP1000/best_weights\n",
      "The loss after 910 iterations is E=0.2760368674666381\n",
      "Wrote weights to file ./customFunctionLearning/baselineMLP1000/best_weights\n",
      "The loss after 911 iterations is E=0.2757603456750604\n",
      "Wrote weights to file ./customFunctionLearning/baselineMLP1000/best_weights\n",
      "The loss after 912 iterations is E=0.27548515200522006\n",
      "Wrote weights to file ./customFunctionLearning/baselineMLP1000/best_weights\n",
      "The loss after 913 iterations is E=0.2752112811614543\n",
      "Wrote weights to file ./customFunctionLearning/baselineMLP1000/best_weights\n",
      "The loss after 914 iterations is E=0.27493872786072054\n",
      "Wrote weights to file ./customFunctionLearning/baselineMLP1000/best_weights\n",
      "The loss after 915 iterations is E=0.2746674868325717\n",
      "Wrote weights to file ./customFunctionLearning/baselineMLP1000/best_weights\n",
      "The loss after 916 iterations is E=0.2743975528191534\n",
      "Wrote weights to file ./customFunctionLearning/baselineMLP1000/best_weights\n",
      "The loss after 917 iterations is E=0.27412892057519767\n",
      "Wrote weights to file ./customFunctionLearning/baselineMLP1000/best_weights\n",
      "The loss after 918 iterations is E=0.2738615848679975\n",
      "Wrote weights to file ./customFunctionLearning/baselineMLP1000/best_weights\n",
      "The loss after 919 iterations is E=0.27359554047741413\n",
      "Wrote weights to file ./customFunctionLearning/baselineMLP1000/best_weights\n",
      "The loss after 920 iterations is E=0.2733307821958535\n",
      "Wrote weights to file ./customFunctionLearning/baselineMLP1000/best_weights\n",
      "The loss after 921 iterations is E=0.27306730482826247\n",
      "Wrote weights to file ./customFunctionLearning/baselineMLP1000/best_weights\n",
      "The loss after 922 iterations is E=0.2728051031921198\n",
      "Wrote weights to file ./customFunctionLearning/baselineMLP1000/best_weights\n",
      "The loss after 923 iterations is E=0.2725441721174223\n",
      "Wrote weights to file ./customFunctionLearning/baselineMLP1000/best_weights\n",
      "The loss after 924 iterations is E=0.2722845064466808\n",
      "Wrote weights to file ./customFunctionLearning/baselineMLP1000/best_weights\n",
      "The loss after 925 iterations is E=0.2720261010349062\n",
      "Wrote weights to file ./customFunctionLearning/baselineMLP1000/best_weights\n",
      "The loss after 926 iterations is E=0.2717689507496005\n",
      "Wrote weights to file ./customFunctionLearning/baselineMLP1000/best_weights\n",
      "The loss after 927 iterations is E=0.27151305047075647\n",
      "Wrote weights to file ./customFunctionLearning/baselineMLP1000/best_weights\n",
      "The loss after 928 iterations is E=0.27125839509083605\n",
      "Wrote weights to file ./customFunctionLearning/baselineMLP1000/best_weights\n",
      "The loss after 929 iterations is E=0.27100497951477104\n",
      "Wrote weights to file ./customFunctionLearning/baselineMLP1000/best_weights\n",
      "The loss after 930 iterations is E=0.2707527986599523\n",
      "Wrote weights to file ./customFunctionLearning/baselineMLP1000/best_weights\n",
      "The loss after 931 iterations is E=0.2705018474562203\n",
      "Wrote weights to file ./customFunctionLearning/baselineMLP1000/best_weights\n",
      "The loss after 932 iterations is E=0.2702521208458592\n",
      "Wrote weights to file ./customFunctionLearning/baselineMLP1000/best_weights\n",
      "The loss after 933 iterations is E=0.270003613783586\n",
      "Wrote weights to file ./customFunctionLearning/baselineMLP1000/best_weights\n",
      "The loss after 934 iterations is E=0.26975632123654514\n",
      "Wrote weights to file ./customFunctionLearning/baselineMLP1000/best_weights\n",
      "The loss after 935 iterations is E=0.2695102381843041\n",
      "Wrote weights to file ./customFunctionLearning/baselineMLP1000/best_weights\n",
      "The loss after 936 iterations is E=0.26926535961883813\n",
      "Wrote weights to file ./customFunctionLearning/baselineMLP1000/best_weights\n",
      "The loss after 937 iterations is E=0.26902168054452735\n",
      "Wrote weights to file ./customFunctionLearning/baselineMLP1000/best_weights\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The loss after 938 iterations is E=0.2687791959781516\n",
      "Wrote weights to file ./customFunctionLearning/baselineMLP1000/best_weights\n",
      "The loss after 939 iterations is E=0.26853790094888\n",
      "Wrote weights to file ./customFunctionLearning/baselineMLP1000/best_weights\n",
      "The loss after 940 iterations is E=0.26829779049826463\n",
      "Wrote weights to file ./customFunctionLearning/baselineMLP1000/best_weights\n",
      "The loss after 941 iterations is E=0.2680588596802354\n",
      "Wrote weights to file ./customFunctionLearning/baselineMLP1000/best_weights\n",
      "The loss after 942 iterations is E=0.2678211035610912\n",
      "Wrote weights to file ./customFunctionLearning/baselineMLP1000/best_weights\n",
      "The loss after 943 iterations is E=0.2675845172194944\n",
      "Wrote weights to file ./customFunctionLearning/baselineMLP1000/best_weights\n",
      "The loss after 944 iterations is E=0.2673490957464652\n",
      "Wrote weights to file ./customFunctionLearning/baselineMLP1000/best_weights\n",
      "The loss after 945 iterations is E=0.2671148342453735\n",
      "Wrote weights to file ./customFunctionLearning/baselineMLP1000/best_weights\n",
      "The loss after 946 iterations is E=0.26688172783193537\n",
      "Wrote weights to file ./customFunctionLearning/baselineMLP1000/best_weights\n",
      "The loss after 947 iterations is E=0.2666497716341994\n",
      "Wrote weights to file ./customFunctionLearning/baselineMLP1000/best_weights\n",
      "The loss after 948 iterations is E=0.26641896079255517\n",
      "Wrote weights to file ./customFunctionLearning/baselineMLP1000/best_weights\n",
      "The loss after 949 iterations is E=0.2661892904597093\n",
      "Wrote weights to file ./customFunctionLearning/baselineMLP1000/best_weights\n",
      "The loss after 950 iterations is E=0.26596075580069656\n",
      "Wrote weights to file ./customFunctionLearning/baselineMLP1000/best_weights\n",
      "The loss after 951 iterations is E=0.2657333519928612\n",
      "Wrote weights to file ./customFunctionLearning/baselineMLP1000/best_weights\n",
      "The loss after 952 iterations is E=0.2655070742258558\n",
      "Wrote weights to file ./customFunctionLearning/baselineMLP1000/best_weights\n",
      "The loss after 953 iterations is E=0.2652819177016386\n",
      "Wrote weights to file ./customFunctionLearning/baselineMLP1000/best_weights\n",
      "The loss after 954 iterations is E=0.2650578776344652\n",
      "Wrote weights to file ./customFunctionLearning/baselineMLP1000/best_weights\n",
      "The loss after 955 iterations is E=0.26483494925088225\n",
      "Wrote weights to file ./customFunctionLearning/baselineMLP1000/best_weights\n",
      "The loss after 956 iterations is E=0.2646131277897216\n",
      "Wrote weights to file ./customFunctionLearning/baselineMLP1000/best_weights\n",
      "The loss after 957 iterations is E=0.2643924085020987\n",
      "Wrote weights to file ./customFunctionLearning/baselineMLP1000/best_weights\n",
      "The loss after 958 iterations is E=0.26417278665140337\n",
      "Wrote weights to file ./customFunctionLearning/baselineMLP1000/best_weights\n",
      "The loss after 959 iterations is E=0.2639542575132991\n",
      "Wrote weights to file ./customFunctionLearning/baselineMLP1000/best_weights\n",
      "The loss after 960 iterations is E=0.2637368163757106\n",
      "Wrote weights to file ./customFunctionLearning/baselineMLP1000/best_weights\n",
      "The loss after 961 iterations is E=0.26352045853882694\n",
      "Wrote weights to file ./customFunctionLearning/baselineMLP1000/best_weights\n",
      "The loss after 962 iterations is E=0.26330517931508624\n",
      "Wrote weights to file ./customFunctionLearning/baselineMLP1000/best_weights\n",
      "The loss after 963 iterations is E=0.263090974029186\n",
      "Wrote weights to file ./customFunctionLearning/baselineMLP1000/best_weights\n",
      "The loss after 964 iterations is E=0.26287783801806325\n",
      "Wrote weights to file ./customFunctionLearning/baselineMLP1000/best_weights\n",
      "The loss after 965 iterations is E=0.26266576663089575\n",
      "Wrote weights to file ./customFunctionLearning/baselineMLP1000/best_weights\n",
      "The loss after 966 iterations is E=0.2624547552290984\n",
      "Wrote weights to file ./customFunctionLearning/baselineMLP1000/best_weights\n",
      "The loss after 967 iterations is E=0.2622447991863164\n",
      "Wrote weights to file ./customFunctionLearning/baselineMLP1000/best_weights\n",
      "The loss after 968 iterations is E=0.26203589388841936\n",
      "Wrote weights to file ./customFunctionLearning/baselineMLP1000/best_weights\n",
      "The loss after 969 iterations is E=0.26182803473349986\n",
      "Wrote weights to file ./customFunctionLearning/baselineMLP1000/best_weights\n",
      "The loss after 970 iterations is E=0.2616212171318651\n",
      "Wrote weights to file ./customFunctionLearning/baselineMLP1000/best_weights\n",
      "The loss after 971 iterations is E=0.261415436506036\n",
      "Wrote weights to file ./customFunctionLearning/baselineMLP1000/best_weights\n",
      "The loss after 972 iterations is E=0.26121068829073746\n",
      "Wrote weights to file ./customFunctionLearning/baselineMLP1000/best_weights\n",
      "The loss after 973 iterations is E=0.2610069679328991\n",
      "Wrote weights to file ./customFunctionLearning/baselineMLP1000/best_weights\n",
      "The loss after 974 iterations is E=0.260804270891645\n",
      "Wrote weights to file ./customFunctionLearning/baselineMLP1000/best_weights\n",
      "The loss after 975 iterations is E=0.2606025926382973\n",
      "Wrote weights to file ./customFunctionLearning/baselineMLP1000/best_weights\n",
      "The loss after 976 iterations is E=0.2604019286563606\n",
      "Wrote weights to file ./customFunctionLearning/baselineMLP1000/best_weights\n",
      "The loss after 977 iterations is E=0.26020227444152483\n",
      "Wrote weights to file ./customFunctionLearning/baselineMLP1000/best_weights\n",
      "The loss after 978 iterations is E=0.26000362550166045\n",
      "Wrote weights to file ./customFunctionLearning/baselineMLP1000/best_weights\n",
      "The loss after 979 iterations is E=0.259805977356811\n",
      "Wrote weights to file ./customFunctionLearning/baselineMLP1000/best_weights\n",
      "The loss after 980 iterations is E=0.25960932553918964\n",
      "Wrote weights to file ./customFunctionLearning/baselineMLP1000/best_weights\n",
      "The loss after 981 iterations is E=0.2594136655931723\n",
      "Wrote weights to file ./customFunctionLearning/baselineMLP1000/best_weights\n",
      "The loss after 982 iterations is E=0.2592189930753005\n",
      "Wrote weights to file ./customFunctionLearning/baselineMLP1000/best_weights\n",
      "The loss after 983 iterations is E=0.2590253035542654\n",
      "Wrote weights to file ./customFunctionLearning/baselineMLP1000/best_weights\n",
      "The loss after 984 iterations is E=0.25883259261091246\n",
      "Wrote weights to file ./customFunctionLearning/baselineMLP1000/best_weights\n",
      "The loss after 985 iterations is E=0.2586408558382345\n",
      "Wrote weights to file ./customFunctionLearning/baselineMLP1000/best_weights\n",
      "The loss after 986 iterations is E=0.2584500888413633\n",
      "Wrote weights to file ./customFunctionLearning/baselineMLP1000/best_weights\n",
      "The loss after 987 iterations is E=0.25826028723756855\n",
      "Wrote weights to file ./customFunctionLearning/baselineMLP1000/best_weights\n",
      "The loss after 988 iterations is E=0.25807144665625315\n",
      "Wrote weights to file ./customFunctionLearning/baselineMLP1000/best_weights\n",
      "The loss after 989 iterations is E=0.25788356273894714\n",
      "Wrote weights to file ./customFunctionLearning/baselineMLP1000/best_weights\n",
      "The loss after 990 iterations is E=0.2576966311393036\n",
      "Wrote weights to file ./customFunctionLearning/baselineMLP1000/best_weights\n",
      "The loss after 991 iterations is E=0.257510647523091\n",
      "Wrote weights to file ./customFunctionLearning/baselineMLP1000/best_weights\n",
      "The loss after 992 iterations is E=0.25732560756819506\n",
      "Wrote weights to file ./customFunctionLearning/baselineMLP1000/best_weights\n",
      "The loss after 993 iterations is E=0.2571415069646079\n",
      "Wrote weights to file ./customFunctionLearning/baselineMLP1000/best_weights\n",
      "The loss after 994 iterations is E=0.25695834141442464\n",
      "Wrote weights to file ./customFunctionLearning/baselineMLP1000/best_weights\n",
      "The loss after 995 iterations is E=0.25677610663184014\n",
      "Wrote weights to file ./customFunctionLearning/baselineMLP1000/best_weights\n",
      "The loss after 996 iterations is E=0.25659479834314175\n",
      "Wrote weights to file ./customFunctionLearning/baselineMLP1000/best_weights\n",
      "The loss after 997 iterations is E=0.25641441228670825\n",
      "Wrote weights to file ./customFunctionLearning/baselineMLP1000/best_weights\n",
      "The loss after 998 iterations is E=0.2562349442130001\n",
      "Wrote weights to file ./customFunctionLearning/baselineMLP1000/best_weights\n",
      "The loss after 999 iterations is E=0.2560563898845549\n",
      "Wrote weights to file ./customFunctionLearning/baselineMLP1000/best_weights\n",
      "The loss after 1000 iterations is E=0.25587874507598646\n",
      "Elapsed time: 53.79 min\n"
     ]
    }
   ],
   "source": [
    "start = time.time()\n",
    "customModel.solve(max_iter=1000, learning_rate=0.01, verbose=1, adaptive=False, tolerance=0.01)\n",
    "print(\"Elapsed time: {:5.2f} min\".format((time.time() - start) / 60.0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Initialized weights and biases for MLP with 20 parameters.\n",
      "\n",
      "MLP structure:\n",
      "--------------\n",
      "Input features:    2\n",
      "Hidden layers:     1\n",
      "Neurons per layer: 5\n",
      "Number of weights: 20\n",
      "Loaded weights from file ./customFunctionLearning/baselineMLP1000/best_weights.npy\n",
      "-------------------\n",
      "L2 norm:  0.00018734984662657056\n",
      "Lmax norm:  0.08510684116093437\n"
     ]
    }
   ],
   "source": [
    "baselineMLP = SimpleMLP(name='baselineMLP1000', number_of_inputs=2, neurons_per_layer=5, hidden_layers=1)\n",
    "customModel = CustomFunctionLearning(name=\"customFunctionLearning\", mlp=baselineMLP, training_data=field50.A, beta=25.0, d_v=0.025)\n",
    "customModel.mlp.read_weights_from_disk(\"./customFunctionLearning/baselineMLP1000/best_weights.npy\")\n",
    "evaluation_points = np.column_stack((field50.x, field50.y))\n",
    "l2_norm = customModel.l2_norm(evaluation_points, field50.A)\n",
    "lmax_norm = customModel.lmax_norm(evaluation_points, field50.A)\n",
    "print(\"-------------------\")\n",
    "print(\"L2 norm: \", l2_norm)\n",
    "print(\"Lmax norm: \", lmax_norm)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Final baseline model evaluation\n",
    "### Custom function learning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Initialized weights and biases for MLP with 20 parameters.\n",
      "\n",
      "MLP structure:\n",
      "--------------\n",
      "Input features:    2\n",
      "Hidden layers:     1\n",
      "Neurons per layer: 5\n",
      "Number of weights: 20\n",
      "Loaded weights from file ./customFunctionLearningAdaptive/baselineMLP1000/best_weights.npy\n"
     ]
    }
   ],
   "source": [
    "baselineMLP = SimpleMLP(name='baselineMLPFinal', number_of_inputs=2, neurons_per_layer=5, hidden_layers=1)\n",
    "customModel = CustomFunctionLearning(name=\"customFunctionLearningAdaptive\", mlp=baselineMLP, training_data=field50.A, beta=25.0, d_v=0.025)\n",
    "customModel.mlp.read_weights_from_disk(\"./customFunctionLearningAdaptive/baselineMLP1000/best_weights.npy\")\n",
    "training_points = np.column_stack((field50.x, field50.y))\n",
    "customModel.set_control_points(training_points)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Selecting subset for training: 2 out of 2700 selected.\n",
      "\n",
      "The initial loss is 0.00011192855542033302\n",
      "\n",
      "Wrote weights to file ./customFunctionLearningAdaptive/baselineMLPFinal/best_weights\n",
      "The loss after 1 iterations is E=4.0502535880974184e-05\n",
      "Using every 900th point for                         training.\n",
      "Selecting subset for training: 3 out of 2700 selected.\n",
      "Wrote weights to file ./customFunctionLearningAdaptive/baselineMLPFinal/best_weights\n",
      "The loss after 2 iterations is E=0.00035376955638237114\n",
      "Using every 810th point for                         training.\n",
      "Selecting subset for training: 3 out of 2700 selected.\n",
      "Wrote weights to file ./customFunctionLearningAdaptive/baselineMLPFinal/best_weights\n",
      "The loss after 3 iterations is E=0.0001875523753889355\n",
      "Using every 729th point for                         training.\n",
      "Selecting subset for training: 3 out of 2700 selected.\n",
      "Wrote weights to file ./customFunctionLearningAdaptive/baselineMLPFinal/best_weights\n",
      "The loss after 4 iterations is E=0.0013327612254574919\n",
      "Using every 656th point for                         training.\n",
      "Selecting subset for training: 4 out of 2700 selected.\n",
      "Wrote weights to file ./customFunctionLearningAdaptive/baselineMLPFinal/best_weights\n",
      "The loss after 5 iterations is E=0.0013575396386508029\n",
      "Using every 590th point for                         training.\n",
      "Selecting subset for training: 4 out of 2700 selected.\n",
      "Wrote weights to file ./customFunctionLearningAdaptive/baselineMLPFinal/best_weights\n",
      "The loss after 6 iterations is E=0.0013005428287472668\n",
      "Using every 531th point for                         training.\n",
      "Selecting subset for training: 5 out of 2700 selected.\n",
      "Wrote weights to file ./customFunctionLearningAdaptive/baselineMLPFinal/best_weights\n",
      "The loss after 7 iterations is E=0.00028508897716635637\n",
      "Using every 477th point for                         training.\n",
      "Selecting subset for training: 5 out of 2700 selected.\n",
      "Wrote weights to file ./customFunctionLearningAdaptive/baselineMLPFinal/best_weights\n",
      "The loss after 8 iterations is E=0.00019926440341583166\n",
      "Using every 429th point for                         training.\n",
      "Selecting subset for training: 6 out of 2700 selected.\n",
      "Wrote weights to file ./customFunctionLearningAdaptive/baselineMLPFinal/best_weights\n",
      "The loss after 9 iterations is E=0.0005568499369670333\n",
      "Using every 386th point for                         training.\n",
      "Selecting subset for training: 6 out of 2700 selected.\n",
      "Wrote weights to file ./customFunctionLearningAdaptive/baselineMLPFinal/best_weights\n",
      "The loss after 10 iterations is E=0.000379418101268899\n",
      "Using every 347th point for                         training.\n",
      "Selecting subset for training: 7 out of 2700 selected.\n",
      "Wrote weights to file ./customFunctionLearningAdaptive/baselineMLPFinal/best_weights\n",
      "The loss after 11 iterations is E=0.00024855996861335253\n",
      "Using every 312th point for                         training.\n",
      "Selecting subset for training: 8 out of 2700 selected.\n",
      "Wrote weights to file ./customFunctionLearningAdaptive/baselineMLPFinal/best_weights\n",
      "The loss after 12 iterations is E=0.0005209001103396825\n",
      "Using every 280th point for                         training.\n",
      "Selecting subset for training: 9 out of 2700 selected.\n",
      "Wrote weights to file ./customFunctionLearningAdaptive/baselineMLPFinal/best_weights\n",
      "The loss after 13 iterations is E=0.00033830949082266513\n",
      "Using every 252th point for                         training.\n",
      "Selecting subset for training: 10 out of 2700 selected.\n",
      "Wrote weights to file ./customFunctionLearningAdaptive/baselineMLPFinal/best_weights\n",
      "The loss after 14 iterations is E=0.0011566398037139132\n",
      "Using every 226th point for                         training.\n",
      "Selecting subset for training: 11 out of 2700 selected.\n",
      "Wrote weights to file ./customFunctionLearningAdaptive/baselineMLPFinal/best_weights\n",
      "The loss after 15 iterations is E=0.0002861181349639069\n",
      "Using every 203th point for                         training.\n",
      "Selecting subset for training: 13 out of 2700 selected.\n",
      "Wrote weights to file ./customFunctionLearningAdaptive/baselineMLPFinal/best_weights\n",
      "The loss after 16 iterations is E=0.00038202576283597644\n",
      "Using every 182th point for                         training.\n",
      "Selecting subset for training: 14 out of 2700 selected.\n",
      "Wrote weights to file ./customFunctionLearningAdaptive/baselineMLPFinal/best_weights\n",
      "The loss after 17 iterations is E=0.00048612177860405363\n",
      "Using every 163th point for                         training.\n",
      "Selecting subset for training: 16 out of 2700 selected.\n",
      "Wrote weights to file ./customFunctionLearningAdaptive/baselineMLPFinal/best_weights\n",
      "The loss after 18 iterations is E=0.0008228254117566936\n",
      "Using every 146th point for                         training.\n",
      "Selecting subset for training: 18 out of 2700 selected.\n",
      "Wrote weights to file ./customFunctionLearningAdaptive/baselineMLPFinal/best_weights\n",
      "The loss after 19 iterations is E=0.00041287494919594296\n",
      "Using every 131th point for                         training.\n",
      "Selecting subset for training: 20 out of 2700 selected.\n",
      "Wrote weights to file ./customFunctionLearningAdaptive/baselineMLPFinal/best_weights\n",
      "The loss after 20 iterations is E=0.003886578005426084\n",
      "Using every 117th point for                         training.\n",
      "Selecting subset for training: 23 out of 2700 selected.\n",
      "Wrote weights to file ./customFunctionLearningAdaptive/baselineMLPFinal/best_weights\n",
      "The loss after 21 iterations is E=0.0028206208463974584\n",
      "Using every 105th point for                         training.\n",
      "Selecting subset for training: 25 out of 2700 selected.\n",
      "Wrote weights to file ./customFunctionLearningAdaptive/baselineMLPFinal/best_weights\n",
      "The loss after 22 iterations is E=0.003082512478035418\n",
      "Using every 94th point for                         training.\n",
      "Selecting subset for training: 28 out of 2700 selected.\n",
      "Wrote weights to file ./customFunctionLearningAdaptive/baselineMLPFinal/best_weights\n",
      "The loss after 23 iterations is E=0.001854528654091975\n",
      "Using every 84th point for                         training.\n",
      "Selecting subset for training: 32 out of 2700 selected.\n",
      "Wrote weights to file ./customFunctionLearningAdaptive/baselineMLPFinal/best_weights\n",
      "The loss after 24 iterations is E=0.0015309515770217089\n",
      "Using every 75th point for                         training.\n",
      "Selecting subset for training: 36 out of 2700 selected.\n",
      "Wrote weights to file ./customFunctionLearningAdaptive/baselineMLPFinal/best_weights\n",
      "The loss after 25 iterations is E=0.001782193654373594\n",
      "Using every 67th point for                         training.\n",
      "Selecting subset for training: 40 out of 2700 selected.\n",
      "Wrote weights to file ./customFunctionLearningAdaptive/baselineMLPFinal/best_weights\n",
      "The loss after 26 iterations is E=0.004084286010164037\n",
      "Using every 60th point for                         training.\n",
      "Selecting subset for training: 45 out of 2700 selected.\n",
      "Wrote weights to file ./customFunctionLearningAdaptive/baselineMLPFinal/best_weights\n",
      "The loss after 27 iterations is E=0.0034637412838703844\n",
      "Using every 54th point for                         training.\n",
      "Selecting subset for training: 50 out of 2700 selected.\n",
      "Wrote weights to file ./customFunctionLearningAdaptive/baselineMLPFinal/best_weights\n",
      "The loss after 28 iterations is E=0.001868895618536641\n",
      "Using every 48th point for                         training.\n",
      "Selecting subset for training: 56 out of 2700 selected.\n",
      "Wrote weights to file ./customFunctionLearningAdaptive/baselineMLPFinal/best_weights\n",
      "The loss after 29 iterations is E=0.004088589769072151\n",
      "Using every 43th point for                         training.\n",
      "Selecting subset for training: 62 out of 2700 selected.\n",
      "Wrote weights to file ./customFunctionLearningAdaptive/baselineMLPFinal/best_weights\n",
      "The loss after 30 iterations is E=0.0029761911469195104\n",
      "Using every 38th point for                         training.\n",
      "Selecting subset for training: 71 out of 2700 selected.\n",
      "Wrote weights to file ./customFunctionLearningAdaptive/baselineMLPFinal/best_weights\n",
      "The loss after 31 iterations is E=0.00403508133544717\n",
      "Using every 34th point for                         training.\n",
      "Selecting subset for training: 79 out of 2700 selected.\n",
      "Wrote weights to file ./customFunctionLearningAdaptive/baselineMLPFinal/best_weights\n",
      "The loss after 32 iterations is E=0.002114134687086638\n",
      "Using every 30th point for                         training.\n",
      "Selecting subset for training: 90 out of 2700 selected.\n",
      "Wrote weights to file ./customFunctionLearningAdaptive/baselineMLPFinal/best_weights\n",
      "The loss after 33 iterations is E=0.005899388590173796\n",
      "Using every 27th point for                         training.\n",
      "Selecting subset for training: 100 out of 2700 selected.\n",
      "Wrote weights to file ./customFunctionLearningAdaptive/baselineMLPFinal/best_weights\n",
      "The loss after 34 iterations is E=0.005608453062128499\n",
      "Using every 24th point for                         training.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Selecting subset for training: 112 out of 2700 selected.\n",
      "Wrote weights to file ./customFunctionLearningAdaptive/baselineMLPFinal/best_weights\n",
      "The loss after 35 iterations is E=0.004506808445765395\n",
      "Using every 21th point for                         training.\n",
      "Selecting subset for training: 128 out of 2700 selected.\n",
      "Wrote weights to file ./customFunctionLearningAdaptive/baselineMLPFinal/best_weights\n",
      "The loss after 36 iterations is E=0.012052790630535245\n",
      "Wrote weights to file ./customFunctionLearningAdaptive/baselineMLPFinal/best_weights\n",
      "The loss after 37 iterations is E=0.01002920534529858\n",
      "Wrote weights to file ./customFunctionLearningAdaptive/baselineMLPFinal/best_weights\n",
      "The loss after 39 iterations is E=0.009879327247823063\n",
      "Using every 18th point for                         training.\n",
      "Selecting subset for training: 150 out of 2700 selected.\n",
      "Wrote weights to file ./customFunctionLearningAdaptive/baselineMLPFinal/best_weights\n",
      "The loss after 40 iterations is E=0.011125917626903898\n",
      "Wrote weights to file ./customFunctionLearningAdaptive/baselineMLPFinal/best_weights\n",
      "The loss after 41 iterations is E=0.009865304604514106\n",
      "Using every 16th point for                         training.\n",
      "Selecting subset for training: 168 out of 2700 selected.\n",
      "Wrote weights to file ./customFunctionLearningAdaptive/baselineMLPFinal/best_weights\n",
      "The loss after 42 iterations is E=0.008098688965584792\n",
      "Using every 14th point for                         training.\n",
      "Selecting subset for training: 192 out of 2700 selected.\n",
      "Wrote weights to file ./customFunctionLearningAdaptive/baselineMLPFinal/best_weights\n",
      "The loss after 43 iterations is E=0.008094214069701617\n",
      "Using every 12th point for                         training.\n",
      "Selecting subset for training: 225 out of 2700 selected.\n",
      "Wrote weights to file ./customFunctionLearningAdaptive/baselineMLPFinal/best_weights\n",
      "The loss after 44 iterations is E=0.010304913537333885\n",
      "Wrote weights to file ./customFunctionLearningAdaptive/baselineMLPFinal/best_weights\n",
      "The loss after 46 iterations is E=0.009821473386036764\n",
      "Using every 10th point for                         training.\n",
      "Selecting subset for training: 270 out of 2700 selected.\n",
      "Wrote weights to file ./customFunctionLearningAdaptive/baselineMLPFinal/best_weights\n",
      "The loss after 47 iterations is E=0.011793229768907555\n",
      "Wrote weights to file ./customFunctionLearningAdaptive/baselineMLPFinal/best_weights\n",
      "The loss after 49 iterations is E=0.011285316857127794\n",
      "Wrote weights to file ./customFunctionLearningAdaptive/baselineMLPFinal/best_weights\n",
      "The loss after 52 iterations is E=0.010820112454979098\n",
      "Wrote weights to file ./customFunctionLearningAdaptive/baselineMLPFinal/best_weights\n",
      "The loss after 54 iterations is E=0.010808325678278791\n",
      "Wrote weights to file ./customFunctionLearningAdaptive/baselineMLPFinal/best_weights\n",
      "The loss after 55 iterations is E=0.010766345532865701\n",
      "Wrote weights to file ./customFunctionLearningAdaptive/baselineMLPFinal/best_weights\n",
      "The loss after 57 iterations is E=0.010569420168177098\n",
      "Wrote weights to file ./customFunctionLearningAdaptive/baselineMLPFinal/best_weights\n",
      "The loss after 65 iterations is E=0.010523173069494742\n",
      "Wrote weights to file ./customFunctionLearningAdaptive/baselineMLPFinal/best_weights\n",
      "The loss after 68 iterations is E=0.010462515122424316\n",
      "Wrote weights to file ./customFunctionLearningAdaptive/baselineMLPFinal/best_weights\n",
      "The loss after 70 iterations is E=0.010389662799330448\n",
      "Wrote weights to file ./customFunctionLearningAdaptive/baselineMLPFinal/best_weights\n",
      "The loss after 73 iterations is E=0.010298223135846371\n",
      "Wrote weights to file ./customFunctionLearningAdaptive/baselineMLPFinal/best_weights\n",
      "The loss after 76 iterations is E=0.0102460578287899\n",
      "Wrote weights to file ./customFunctionLearningAdaptive/baselineMLPFinal/best_weights\n",
      "The loss after 79 iterations is E=0.01021640943272329\n",
      "Wrote weights to file ./customFunctionLearningAdaptive/baselineMLPFinal/best_weights\n",
      "The loss after 82 iterations is E=0.010192150664114856\n",
      "Wrote weights to file ./customFunctionLearningAdaptive/baselineMLPFinal/best_weights\n",
      "The loss after 85 iterations is E=0.010164995807271988\n",
      "Wrote weights to file ./customFunctionLearningAdaptive/baselineMLPFinal/best_weights\n",
      "The loss after 86 iterations is E=0.010161000417562347\n",
      "Wrote weights to file ./customFunctionLearningAdaptive/baselineMLPFinal/best_weights\n",
      "The loss after 88 iterations is E=0.010136188362404631\n",
      "Wrote weights to file ./customFunctionLearningAdaptive/baselineMLPFinal/best_weights\n",
      "The loss after 89 iterations is E=0.010120159702699064\n",
      "Wrote weights to file ./customFunctionLearningAdaptive/baselineMLPFinal/best_weights\n",
      "The loss after 91 iterations is E=0.010109337305096446\n",
      "Wrote weights to file ./customFunctionLearningAdaptive/baselineMLPFinal/best_weights\n",
      "The loss after 92 iterations is E=0.010088194197921233\n",
      "Wrote weights to file ./customFunctionLearningAdaptive/baselineMLPFinal/best_weights\n",
      "The loss after 93 iterations is E=0.010086646543396275\n",
      "Wrote weights to file ./customFunctionLearningAdaptive/baselineMLPFinal/best_weights\n",
      "The loss after 94 iterations is E=0.010083931461242795\n",
      "Wrote weights to file ./customFunctionLearningAdaptive/baselineMLPFinal/best_weights\n",
      "The loss after 95 iterations is E=0.010067278977296123\n",
      "Wrote weights to file ./customFunctionLearningAdaptive/baselineMLPFinal/best_weights\n",
      "The loss after 96 iterations is E=0.01005677286600521\n",
      "Wrote weights to file ./customFunctionLearningAdaptive/baselineMLPFinal/best_weights\n",
      "The loss after 98 iterations is E=0.010050338944335706\n",
      "Wrote weights to file ./customFunctionLearningAdaptive/baselineMLPFinal/best_weights\n",
      "The loss after 99 iterations is E=0.010036762811242822\n",
      "Wrote weights to file ./customFunctionLearningAdaptive/baselineMLPFinal/best_weights\n",
      "The loss after 100 iterations is E=0.010029806032438105\n",
      "Wrote weights to file ./customFunctionLearningAdaptive/baselineMLPFinal/best_weights\n",
      "The loss after 101 iterations is E=0.010027830731548226\n",
      "Wrote weights to file ./customFunctionLearningAdaptive/baselineMLPFinal/best_weights\n",
      "The loss after 102 iterations is E=0.010020200453875937\n",
      "Wrote weights to file ./customFunctionLearningAdaptive/baselineMLPFinal/best_weights\n",
      "The loss after 103 iterations is E=0.010008628683656868\n",
      "Wrote weights to file ./customFunctionLearningAdaptive/baselineMLPFinal/best_weights\n",
      "The loss after 104 iterations is E=0.010001284184302482\n",
      "Wrote weights to file ./customFunctionLearningAdaptive/baselineMLPFinal/best_weights\n",
      "The loss after 105 iterations is E=0.009997369927015595\n",
      "Using every 9th point for                         training.\n",
      "Selecting subset for training: 300 out of 2700 selected.\n",
      "Wrote weights to file ./customFunctionLearningAdaptive/baselineMLPFinal/best_weights\n",
      "The loss after 106 iterations is E=0.015837604589278603\n",
      "Wrote weights to file ./customFunctionLearningAdaptive/baselineMLPFinal/best_weights\n",
      "The loss after 107 iterations is E=0.015703145599266832\n",
      "Wrote weights to file ./customFunctionLearningAdaptive/baselineMLPFinal/best_weights\n",
      "The loss after 108 iterations is E=0.015536308327211252\n",
      "Wrote weights to file ./customFunctionLearningAdaptive/baselineMLPFinal/best_weights\n",
      "The loss after 109 iterations is E=0.01538409443154906\n",
      "Wrote weights to file ./customFunctionLearningAdaptive/baselineMLPFinal/best_weights\n",
      "The loss after 110 iterations is E=0.01524005384146847\n",
      "Wrote weights to file ./customFunctionLearningAdaptive/baselineMLPFinal/best_weights\n",
      "The loss after 111 iterations is E=0.015093574451399147\n",
      "Wrote weights to file ./customFunctionLearningAdaptive/baselineMLPFinal/best_weights\n",
      "The loss after 112 iterations is E=0.014967015474176433\n",
      "Wrote weights to file ./customFunctionLearningAdaptive/baselineMLPFinal/best_weights\n",
      "The loss after 113 iterations is E=0.01488717315383555\n",
      "Wrote weights to file ./customFunctionLearningAdaptive/baselineMLPFinal/best_weights\n",
      "The loss after 114 iterations is E=0.014850870331776584\n",
      "Wrote weights to file ./customFunctionLearningAdaptive/baselineMLPFinal/best_weights\n",
      "The loss after 115 iterations is E=0.014837856066693585\n",
      "Wrote weights to file ./customFunctionLearningAdaptive/baselineMLPFinal/best_weights\n",
      "The loss after 127 iterations is E=0.014817716476021215\n",
      "Wrote weights to file ./customFunctionLearningAdaptive/baselineMLPFinal/best_weights\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The loss after 128 iterations is E=0.014786327951429576\n",
      "Wrote weights to file ./customFunctionLearningAdaptive/baselineMLPFinal/best_weights\n",
      "The loss after 129 iterations is E=0.014758283781214528\n",
      "Wrote weights to file ./customFunctionLearningAdaptive/baselineMLPFinal/best_weights\n",
      "The loss after 130 iterations is E=0.014736042963030703\n",
      "Wrote weights to file ./customFunctionLearningAdaptive/baselineMLPFinal/best_weights\n",
      "The loss after 131 iterations is E=0.01472078831269797\n",
      "Wrote weights to file ./customFunctionLearningAdaptive/baselineMLPFinal/best_weights\n",
      "The loss after 132 iterations is E=0.014712340199590824\n",
      "Wrote weights to file ./customFunctionLearningAdaptive/baselineMLPFinal/best_weights\n",
      "The loss after 133 iterations is E=0.014709222229762288\n",
      "Wrote weights to file ./customFunctionLearningAdaptive/baselineMLPFinal/best_weights\n",
      "The loss after 137 iterations is E=0.014708463855551425\n",
      "Wrote weights to file ./customFunctionLearningAdaptive/baselineMLPFinal/best_weights\n",
      "The loss after 138 iterations is E=0.014703863278725873\n",
      "Wrote weights to file ./customFunctionLearningAdaptive/baselineMLPFinal/best_weights\n",
      "The loss after 139 iterations is E=0.014696627022479642\n",
      "Wrote weights to file ./customFunctionLearningAdaptive/baselineMLPFinal/best_weights\n",
      "The loss after 140 iterations is E=0.014687051926910182\n",
      "Wrote weights to file ./customFunctionLearningAdaptive/baselineMLPFinal/best_weights\n",
      "The loss after 141 iterations is E=0.01467579662556036\n",
      "Wrote weights to file ./customFunctionLearningAdaptive/baselineMLPFinal/best_weights\n",
      "The loss after 142 iterations is E=0.014663552019039142\n",
      "Wrote weights to file ./customFunctionLearningAdaptive/baselineMLPFinal/best_weights\n",
      "The loss after 143 iterations is E=0.014651096349259029\n",
      "Wrote weights to file ./customFunctionLearningAdaptive/baselineMLPFinal/best_weights\n",
      "The loss after 144 iterations is E=0.014639096183354369\n",
      "Wrote weights to file ./customFunctionLearningAdaptive/baselineMLPFinal/best_weights\n",
      "The loss after 145 iterations is E=0.014628158546293384\n",
      "Wrote weights to file ./customFunctionLearningAdaptive/baselineMLPFinal/best_weights\n",
      "The loss after 146 iterations is E=0.01461874365398135\n",
      "Wrote weights to file ./customFunctionLearningAdaptive/baselineMLPFinal/best_weights\n",
      "The loss after 147 iterations is E=0.014611341234666213\n",
      "Wrote weights to file ./customFunctionLearningAdaptive/baselineMLPFinal/best_weights\n",
      "The loss after 148 iterations is E=0.014606512073978273\n",
      "Wrote weights to file ./customFunctionLearningAdaptive/baselineMLPFinal/best_weights\n",
      "The loss after 149 iterations is E=0.014605591994761602\n",
      "Wrote weights to file ./customFunctionLearningAdaptive/baselineMLPFinal/best_weights\n",
      "The loss after 160 iterations is E=0.01451977187547644\n",
      "Wrote weights to file ./customFunctionLearningAdaptive/baselineMLPFinal/best_weights\n",
      "The loss after 164 iterations is E=0.014500065903677428\n",
      "Wrote weights to file ./customFunctionLearningAdaptive/baselineMLPFinal/best_weights\n",
      "The loss after 168 iterations is E=0.014471191706564421\n",
      "Wrote weights to file ./customFunctionLearningAdaptive/baselineMLPFinal/best_weights\n",
      "The loss after 172 iterations is E=0.014448748057087863\n",
      "Wrote weights to file ./customFunctionLearningAdaptive/baselineMLPFinal/best_weights\n",
      "The loss after 176 iterations is E=0.014435534942228236\n",
      "Wrote weights to file ./customFunctionLearningAdaptive/baselineMLPFinal/best_weights\n",
      "The loss after 180 iterations is E=0.014420990277971269\n",
      "Wrote weights to file ./customFunctionLearningAdaptive/baselineMLPFinal/best_weights\n",
      "The loss after 183 iterations is E=0.014394053276698673\n",
      "Wrote weights to file ./customFunctionLearningAdaptive/baselineMLPFinal/best_weights\n",
      "The loss after 187 iterations is E=0.014366238750465428\n",
      "Wrote weights to file ./customFunctionLearningAdaptive/baselineMLPFinal/best_weights\n",
      "The loss after 188 iterations is E=0.014365568280237322\n",
      "Wrote weights to file ./customFunctionLearningAdaptive/baselineMLPFinal/best_weights\n",
      "The loss after 191 iterations is E=0.014342555316373882\n",
      "Wrote weights to file ./customFunctionLearningAdaptive/baselineMLPFinal/best_weights\n",
      "The loss after 192 iterations is E=0.014333984921846926\n",
      "Wrote weights to file ./customFunctionLearningAdaptive/baselineMLPFinal/best_weights\n",
      "The loss after 195 iterations is E=0.014321671779840544\n",
      "Wrote weights to file ./customFunctionLearningAdaptive/baselineMLPFinal/best_weights\n",
      "The loss after 196 iterations is E=0.014305239051904294\n",
      "Wrote weights to file ./customFunctionLearningAdaptive/baselineMLPFinal/best_weights\n",
      "The loss after 199 iterations is E=0.01430152710600878\n",
      "Wrote weights to file ./customFunctionLearningAdaptive/baselineMLPFinal/best_weights\n",
      "The loss after 200 iterations is E=0.014281158898091622\n",
      "Wrote weights to file ./customFunctionLearningAdaptive/baselineMLPFinal/best_weights\n",
      "The loss after 201 iterations is E=0.014281092899357459\n",
      "Wrote weights to file ./customFunctionLearningAdaptive/baselineMLPFinal/best_weights\n",
      "The loss after 203 iterations is E=0.014279094232645497\n",
      "Wrote weights to file ./customFunctionLearningAdaptive/baselineMLPFinal/best_weights\n",
      "The loss after 204 iterations is E=0.014261434058449204\n",
      "Wrote weights to file ./customFunctionLearningAdaptive/baselineMLPFinal/best_weights\n",
      "The loss after 205 iterations is E=0.014251268915351787\n",
      "Wrote weights to file ./customFunctionLearningAdaptive/baselineMLPFinal/best_weights\n",
      "The loss after 208 iterations is E=0.014241926074458518\n",
      "Wrote weights to file ./customFunctionLearningAdaptive/baselineMLPFinal/best_weights\n",
      "The loss after 209 iterations is E=0.014228876636123173\n",
      "Wrote weights to file ./customFunctionLearningAdaptive/baselineMLPFinal/best_weights\n",
      "The loss after 210 iterations is E=0.014221424156558981\n",
      "Wrote weights to file ./customFunctionLearningAdaptive/baselineMLPFinal/best_weights\n",
      "The loss after 211 iterations is E=0.014219733974367493\n",
      "Wrote weights to file ./customFunctionLearningAdaptive/baselineMLPFinal/best_weights\n",
      "The loss after 212 iterations is E=0.014216924517253725\n",
      "Wrote weights to file ./customFunctionLearningAdaptive/baselineMLPFinal/best_weights\n",
      "The loss after 213 iterations is E=0.014208802758288465\n",
      "Wrote weights to file ./customFunctionLearningAdaptive/baselineMLPFinal/best_weights\n",
      "The loss after 214 iterations is E=0.01419847157891222\n",
      "Wrote weights to file ./customFunctionLearningAdaptive/baselineMLPFinal/best_weights\n",
      "The loss after 215 iterations is E=0.014190815261089515\n",
      "Wrote weights to file ./customFunctionLearningAdaptive/baselineMLPFinal/best_weights\n",
      "The loss after 216 iterations is E=0.014186775367229105\n",
      "Wrote weights to file ./customFunctionLearningAdaptive/baselineMLPFinal/best_weights\n",
      "The loss after 217 iterations is E=0.014183175131630435\n",
      "Wrote weights to file ./customFunctionLearningAdaptive/baselineMLPFinal/best_weights\n",
      "The loss after 218 iterations is E=0.014177011430388499\n",
      "Wrote weights to file ./customFunctionLearningAdaptive/baselineMLPFinal/best_weights\n",
      "The loss after 219 iterations is E=0.014168658700621123\n",
      "Wrote weights to file ./customFunctionLearningAdaptive/baselineMLPFinal/best_weights\n",
      "The loss after 220 iterations is E=0.014160540758426005\n",
      "Wrote weights to file ./customFunctionLearningAdaptive/baselineMLPFinal/best_weights\n",
      "The loss after 221 iterations is E=0.014154379502698631\n",
      "Wrote weights to file ./customFunctionLearningAdaptive/baselineMLPFinal/best_weights\n",
      "The loss after 222 iterations is E=0.014149661038703526\n",
      "Wrote weights to file ./customFunctionLearningAdaptive/baselineMLPFinal/best_weights\n",
      "The loss after 223 iterations is E=0.014144654049937078\n",
      "Wrote weights to file ./customFunctionLearningAdaptive/baselineMLPFinal/best_weights\n",
      "The loss after 224 iterations is E=0.014138293866974612\n",
      "Wrote weights to file ./customFunctionLearningAdaptive/baselineMLPFinal/best_weights\n",
      "The loss after 225 iterations is E=0.014130872701003633\n",
      "Wrote weights to file ./customFunctionLearningAdaptive/baselineMLPFinal/best_weights\n",
      "The loss after 226 iterations is E=0.014123534821424859\n",
      "Wrote weights to file ./customFunctionLearningAdaptive/baselineMLPFinal/best_weights\n",
      "The loss after 227 iterations is E=0.014117076571586003\n",
      "Wrote weights to file ./customFunctionLearningAdaptive/baselineMLPFinal/best_weights\n",
      "The loss after 228 iterations is E=0.014111440191455911\n",
      "Wrote weights to file ./customFunctionLearningAdaptive/baselineMLPFinal/best_weights\n",
      "The loss after 229 iterations is E=0.014105970661007822\n",
      "Wrote weights to file ./customFunctionLearningAdaptive/baselineMLPFinal/best_weights\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The loss after 230 iterations is E=0.014100033231116672\n",
      "Wrote weights to file ./customFunctionLearningAdaptive/baselineMLPFinal/best_weights\n",
      "The loss after 231 iterations is E=0.014093473819956145\n",
      "Wrote weights to file ./customFunctionLearningAdaptive/baselineMLPFinal/best_weights\n",
      "The loss after 232 iterations is E=0.014086556361412834\n",
      "Wrote weights to file ./customFunctionLearningAdaptive/baselineMLPFinal/best_weights\n",
      "The loss after 233 iterations is E=0.014079717671117523\n",
      "Wrote weights to file ./customFunctionLearningAdaptive/baselineMLPFinal/best_weights\n",
      "The loss after 234 iterations is E=0.014073229245240765\n",
      "Wrote weights to file ./customFunctionLearningAdaptive/baselineMLPFinal/best_weights\n",
      "The loss after 235 iterations is E=0.014067097213589425\n",
      "Wrote weights to file ./customFunctionLearningAdaptive/baselineMLPFinal/best_weights\n",
      "The loss after 236 iterations is E=0.01406113151337171\n",
      "Wrote weights to file ./customFunctionLearningAdaptive/baselineMLPFinal/best_weights\n",
      "The loss after 237 iterations is E=0.01405509982402142\n",
      "Wrote weights to file ./customFunctionLearningAdaptive/baselineMLPFinal/best_weights\n",
      "The loss after 238 iterations is E=0.01404886180475122\n",
      "Wrote weights to file ./customFunctionLearningAdaptive/baselineMLPFinal/best_weights\n",
      "The loss after 239 iterations is E=0.01404239334338022\n",
      "Wrote weights to file ./customFunctionLearningAdaptive/baselineMLPFinal/best_weights\n",
      "The loss after 240 iterations is E=0.014035778112353242\n",
      "Wrote weights to file ./customFunctionLearningAdaptive/baselineMLPFinal/best_weights\n",
      "The loss after 241 iterations is E=0.014029121610133276\n",
      "Wrote weights to file ./customFunctionLearningAdaptive/baselineMLPFinal/best_weights\n",
      "The loss after 242 iterations is E=0.014022516002859102\n",
      "Wrote weights to file ./customFunctionLearningAdaptive/baselineMLPFinal/best_weights\n",
      "The loss after 243 iterations is E=0.014016003875218487\n",
      "Wrote weights to file ./customFunctionLearningAdaptive/baselineMLPFinal/best_weights\n",
      "The loss after 244 iterations is E=0.014009584431248372\n",
      "Wrote weights to file ./customFunctionLearningAdaptive/baselineMLPFinal/best_weights\n",
      "The loss after 245 iterations is E=0.014003228679439936\n",
      "Wrote weights to file ./customFunctionLearningAdaptive/baselineMLPFinal/best_weights\n",
      "The loss after 246 iterations is E=0.01399689872133827\n",
      "Wrote weights to file ./customFunctionLearningAdaptive/baselineMLPFinal/best_weights\n",
      "The loss after 247 iterations is E=0.013990562689904354\n",
      "Wrote weights to file ./customFunctionLearningAdaptive/baselineMLPFinal/best_weights\n",
      "The loss after 248 iterations is E=0.013984198907643443\n",
      "Wrote weights to file ./customFunctionLearningAdaptive/baselineMLPFinal/best_weights\n",
      "The loss after 249 iterations is E=0.013977801282270818\n",
      "Wrote weights to file ./customFunctionLearningAdaptive/baselineMLPFinal/best_weights\n",
      "The loss after 250 iterations is E=0.013971368715919858\n",
      "Wrote weights to file ./customFunctionLearningAdaptive/baselineMLPFinal/best_weights\n",
      "The loss after 251 iterations is E=0.013964912367449286\n",
      "Wrote weights to file ./customFunctionLearningAdaptive/baselineMLPFinal/best_weights\n",
      "The loss after 252 iterations is E=0.013958439351329132\n",
      "Wrote weights to file ./customFunctionLearningAdaptive/baselineMLPFinal/best_weights\n",
      "The loss after 253 iterations is E=0.013951967405613813\n",
      "Wrote weights to file ./customFunctionLearningAdaptive/baselineMLPFinal/best_weights\n",
      "The loss after 254 iterations is E=0.013945506963622312\n",
      "Wrote weights to file ./customFunctionLearningAdaptive/baselineMLPFinal/best_weights\n",
      "The loss after 255 iterations is E=0.013939084236204635\n",
      "Wrote weights to file ./customFunctionLearningAdaptive/baselineMLPFinal/best_weights\n",
      "The loss after 256 iterations is E=0.013932720636631023\n",
      "Wrote weights to file ./customFunctionLearningAdaptive/baselineMLPFinal/best_weights\n",
      "The loss after 257 iterations is E=0.01392647046962196\n",
      "Wrote weights to file ./customFunctionLearningAdaptive/baselineMLPFinal/best_weights\n",
      "The loss after 258 iterations is E=0.013920393484869842\n",
      "Wrote weights to file ./customFunctionLearningAdaptive/baselineMLPFinal/best_weights\n",
      "The loss after 259 iterations is E=0.013914633012603764\n",
      "Wrote weights to file ./customFunctionLearningAdaptive/baselineMLPFinal/best_weights\n",
      "The loss after 260 iterations is E=0.013909370701101924\n",
      "Wrote weights to file ./customFunctionLearningAdaptive/baselineMLPFinal/best_weights\n",
      "The loss after 261 iterations is E=0.013905038744233846\n",
      "Wrote weights to file ./customFunctionLearningAdaptive/baselineMLPFinal/best_weights\n",
      "The loss after 262 iterations is E=0.01390221365991798\n",
      "Wrote weights to file ./customFunctionLearningAdaptive/baselineMLPFinal/best_weights\n",
      "The loss after 276 iterations is E=0.013840486274556182\n",
      "Wrote weights to file ./customFunctionLearningAdaptive/baselineMLPFinal/best_weights\n",
      "The loss after 281 iterations is E=0.013790021635055714\n",
      "Wrote weights to file ./customFunctionLearningAdaptive/baselineMLPFinal/best_weights\n",
      "The loss after 285 iterations is E=0.01378587848224854\n",
      "Wrote weights to file ./customFunctionLearningAdaptive/baselineMLPFinal/best_weights\n",
      "The loss after 290 iterations is E=0.013718511406915468\n",
      "Wrote weights to file ./customFunctionLearningAdaptive/baselineMLPFinal/best_weights\n",
      "The loss after 295 iterations is E=0.013694279826431307\n",
      "Wrote weights to file ./customFunctionLearningAdaptive/baselineMLPFinal/best_weights\n",
      "The loss after 299 iterations is E=0.013674148418836189\n",
      "Wrote weights to file ./customFunctionLearningAdaptive/baselineMLPFinal/best_weights\n",
      "The loss after 300 iterations is E=0.013671458521392621\n",
      "Wrote weights to file ./customFunctionLearningAdaptive/baselineMLPFinal/best_weights\n",
      "The loss after 304 iterations is E=0.013637459963024868\n",
      "Wrote weights to file ./customFunctionLearningAdaptive/baselineMLPFinal/best_weights\n",
      "The loss after 308 iterations is E=0.01363538021176021\n",
      "Wrote weights to file ./customFunctionLearningAdaptive/baselineMLPFinal/best_weights\n",
      "The loss after 309 iterations is E=0.01360656895764794\n",
      "Wrote weights to file ./customFunctionLearningAdaptive/baselineMLPFinal/best_weights\n",
      "The loss after 310 iterations is E=0.013605095760060446\n",
      "Wrote weights to file ./customFunctionLearningAdaptive/baselineMLPFinal/best_weights\n",
      "The loss after 313 iterations is E=0.013598848756747552\n",
      "Wrote weights to file ./customFunctionLearningAdaptive/baselineMLPFinal/best_weights\n",
      "The loss after 314 iterations is E=0.013577369847885637\n",
      "Wrote weights to file ./customFunctionLearningAdaptive/baselineMLPFinal/best_weights\n",
      "The loss after 315 iterations is E=0.013570083169491931\n",
      "Wrote weights to file ./customFunctionLearningAdaptive/baselineMLPFinal/best_weights\n",
      "The loss after 318 iterations is E=0.01356524804827742\n",
      "Wrote weights to file ./customFunctionLearningAdaptive/baselineMLPFinal/best_weights\n",
      "The loss after 319 iterations is E=0.013549238659540495\n",
      "Wrote weights to file ./customFunctionLearningAdaptive/baselineMLPFinal/best_weights\n",
      "The loss after 320 iterations is E=0.013537951766633327\n",
      "Wrote weights to file ./customFunctionLearningAdaptive/baselineMLPFinal/best_weights\n",
      "The loss after 321 iterations is E=0.013534903095426765\n",
      "Wrote weights to file ./customFunctionLearningAdaptive/baselineMLPFinal/best_weights\n",
      "The loss after 322 iterations is E=0.01353473741720428\n",
      "Wrote weights to file ./customFunctionLearningAdaptive/baselineMLPFinal/best_weights\n",
      "The loss after 323 iterations is E=0.013530586413829274\n",
      "Wrote weights to file ./customFunctionLearningAdaptive/baselineMLPFinal/best_weights\n",
      "The loss after 324 iterations is E=0.01352060143659822\n",
      "Wrote weights to file ./customFunctionLearningAdaptive/baselineMLPFinal/best_weights\n",
      "The loss after 325 iterations is E=0.013509127244513218\n",
      "Wrote weights to file ./customFunctionLearningAdaptive/baselineMLPFinal/best_weights\n",
      "The loss after 326 iterations is E=0.013500779434762657\n",
      "Wrote weights to file ./customFunctionLearningAdaptive/baselineMLPFinal/best_weights\n",
      "The loss after 327 iterations is E=0.013496509200440047\n",
      "Wrote weights to file ./customFunctionLearningAdaptive/baselineMLPFinal/best_weights\n",
      "The loss after 328 iterations is E=0.013493513105348139\n",
      "Wrote weights to file ./customFunctionLearningAdaptive/baselineMLPFinal/best_weights\n",
      "The loss after 329 iterations is E=0.013488448445754975\n",
      "Wrote weights to file ./customFunctionLearningAdaptive/baselineMLPFinal/best_weights\n",
      "The loss after 330 iterations is E=0.013480548771450199\n",
      "Wrote weights to file ./customFunctionLearningAdaptive/baselineMLPFinal/best_weights\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The loss after 331 iterations is E=0.013471391213222973\n",
      "Wrote weights to file ./customFunctionLearningAdaptive/baselineMLPFinal/best_weights\n",
      "The loss after 332 iterations is E=0.013463330501304325\n",
      "Wrote weights to file ./customFunctionLearningAdaptive/baselineMLPFinal/best_weights\n",
      "The loss after 333 iterations is E=0.013457321392123047\n",
      "Wrote weights to file ./customFunctionLearningAdaptive/baselineMLPFinal/best_weights\n",
      "The loss after 334 iterations is E=0.013452585047476356\n",
      "Wrote weights to file ./customFunctionLearningAdaptive/baselineMLPFinal/best_weights\n",
      "The loss after 335 iterations is E=0.013447602727665159\n",
      "Wrote weights to file ./customFunctionLearningAdaptive/baselineMLPFinal/best_weights\n",
      "The loss after 336 iterations is E=0.013441327203464706\n",
      "Wrote weights to file ./customFunctionLearningAdaptive/baselineMLPFinal/best_weights\n",
      "The loss after 337 iterations is E=0.013433895300859363\n",
      "Wrote weights to file ./customFunctionLearningAdaptive/baselineMLPFinal/best_weights\n",
      "The loss after 338 iterations is E=0.013426124816949067\n",
      "Wrote weights to file ./customFunctionLearningAdaptive/baselineMLPFinal/best_weights\n",
      "The loss after 339 iterations is E=0.013418912035730791\n",
      "Wrote weights to file ./customFunctionLearningAdaptive/baselineMLPFinal/best_weights\n",
      "The loss after 340 iterations is E=0.013412584406800094\n",
      "Wrote weights to file ./customFunctionLearningAdaptive/baselineMLPFinal/best_weights\n",
      "The loss after 341 iterations is E=0.013406864189036732\n",
      "Wrote weights to file ./customFunctionLearningAdaptive/baselineMLPFinal/best_weights\n",
      "The loss after 342 iterations is E=0.013401186607656107\n",
      "Wrote weights to file ./customFunctionLearningAdaptive/baselineMLPFinal/best_weights\n",
      "The loss after 343 iterations is E=0.01339507802297969\n",
      "Wrote weights to file ./customFunctionLearningAdaptive/baselineMLPFinal/best_weights\n",
      "The loss after 344 iterations is E=0.01338842734168035\n",
      "Wrote weights to file ./customFunctionLearningAdaptive/baselineMLPFinal/best_weights\n",
      "The loss after 345 iterations is E=0.013381399518888906\n",
      "Wrote weights to file ./customFunctionLearningAdaptive/baselineMLPFinal/best_weights\n",
      "The loss after 346 iterations is E=0.013374328779760572\n",
      "Wrote weights to file ./customFunctionLearningAdaptive/baselineMLPFinal/best_weights\n",
      "The loss after 347 iterations is E=0.013367471895238021\n",
      "Wrote weights to file ./customFunctionLearningAdaptive/baselineMLPFinal/best_weights\n",
      "The loss after 348 iterations is E=0.013360930421642537\n",
      "Wrote weights to file ./customFunctionLearningAdaptive/baselineMLPFinal/best_weights\n",
      "The loss after 349 iterations is E=0.013354640327955533\n",
      "Wrote weights to file ./customFunctionLearningAdaptive/baselineMLPFinal/best_weights\n",
      "The loss after 350 iterations is E=0.01334844685003059\n",
      "Wrote weights to file ./customFunctionLearningAdaptive/baselineMLPFinal/best_weights\n",
      "The loss after 351 iterations is E=0.01334219676163772\n",
      "Wrote weights to file ./customFunctionLearningAdaptive/baselineMLPFinal/best_weights\n",
      "The loss after 352 iterations is E=0.013335787387549994\n",
      "Wrote weights to file ./customFunctionLearningAdaptive/baselineMLPFinal/best_weights\n",
      "The loss after 353 iterations is E=0.013329204574648434\n",
      "Wrote weights to file ./customFunctionLearningAdaptive/baselineMLPFinal/best_weights\n",
      "The loss after 354 iterations is E=0.013322479884145061\n",
      "Wrote weights to file ./customFunctionLearningAdaptive/baselineMLPFinal/best_weights\n",
      "The loss after 355 iterations is E=0.013315687508996865\n",
      "Wrote weights to file ./customFunctionLearningAdaptive/baselineMLPFinal/best_weights\n",
      "The loss after 356 iterations is E=0.013308889134471605\n",
      "Wrote weights to file ./customFunctionLearningAdaptive/baselineMLPFinal/best_weights\n",
      "The loss after 357 iterations is E=0.013302133427775253\n",
      "Wrote weights to file ./customFunctionLearningAdaptive/baselineMLPFinal/best_weights\n",
      "The loss after 358 iterations is E=0.013295439090115853\n",
      "Wrote weights to file ./customFunctionLearningAdaptive/baselineMLPFinal/best_weights\n",
      "The loss after 359 iterations is E=0.01328880448160559\n",
      "Wrote weights to file ./customFunctionLearningAdaptive/baselineMLPFinal/best_weights\n",
      "The loss after 360 iterations is E=0.01328221383232589\n",
      "Wrote weights to file ./customFunctionLearningAdaptive/baselineMLPFinal/best_weights\n",
      "The loss after 361 iterations is E=0.013275646431071892\n",
      "Wrote weights to file ./customFunctionLearningAdaptive/baselineMLPFinal/best_weights\n",
      "The loss after 362 iterations is E=0.013269083915145136\n",
      "Wrote weights to file ./customFunctionLearningAdaptive/baselineMLPFinal/best_weights\n",
      "The loss after 363 iterations is E=0.013262511588169058\n",
      "Wrote weights to file ./customFunctionLearningAdaptive/baselineMLPFinal/best_weights\n",
      "The loss after 364 iterations is E=0.013255923399375553\n",
      "Wrote weights to file ./customFunctionLearningAdaptive/baselineMLPFinal/best_weights\n",
      "The loss after 365 iterations is E=0.013249315103305914\n",
      "Wrote weights to file ./customFunctionLearningAdaptive/baselineMLPFinal/best_weights\n",
      "The loss after 366 iterations is E=0.0132426917952934\n",
      "Wrote weights to file ./customFunctionLearningAdaptive/baselineMLPFinal/best_weights\n",
      "The loss after 367 iterations is E=0.013236055362138994\n",
      "Wrote weights to file ./customFunctionLearningAdaptive/baselineMLPFinal/best_weights\n",
      "The loss after 368 iterations is E=0.013229417872904176\n",
      "Wrote weights to file ./customFunctionLearningAdaptive/baselineMLPFinal/best_weights\n",
      "The loss after 369 iterations is E=0.013222784698645336\n",
      "Wrote weights to file ./customFunctionLearningAdaptive/baselineMLPFinal/best_weights\n",
      "The loss after 370 iterations is E=0.013216175501987833\n",
      "Wrote weights to file ./customFunctionLearningAdaptive/baselineMLPFinal/best_weights\n",
      "The loss after 371 iterations is E=0.013209602027906251\n",
      "Wrote weights to file ./customFunctionLearningAdaptive/baselineMLPFinal/best_weights\n",
      "The loss after 372 iterations is E=0.013203101134164764\n",
      "Wrote weights to file ./customFunctionLearningAdaptive/baselineMLPFinal/best_weights\n",
      "The loss after 373 iterations is E=0.013196702863530614\n",
      "Wrote weights to file ./customFunctionLearningAdaptive/baselineMLPFinal/best_weights\n",
      "The loss after 374 iterations is E=0.01319048993059102\n",
      "Wrote weights to file ./customFunctionLearningAdaptive/baselineMLPFinal/best_weights\n",
      "The loss after 375 iterations is E=0.013184543925718523\n",
      "Wrote weights to file ./customFunctionLearningAdaptive/baselineMLPFinal/best_weights\n",
      "The loss after 376 iterations is E=0.013179075780862508\n",
      "Wrote weights to file ./customFunctionLearningAdaptive/baselineMLPFinal/best_weights\n",
      "The loss after 377 iterations is E=0.013174312495324003\n",
      "Wrote weights to file ./customFunctionLearningAdaptive/baselineMLPFinal/best_weights\n",
      "The loss after 378 iterations is E=0.013170845173291135\n",
      "Wrote weights to file ./customFunctionLearningAdaptive/baselineMLPFinal/best_weights\n",
      "The loss after 379 iterations is E=0.013169316466807366\n",
      "Wrote weights to file ./customFunctionLearningAdaptive/baselineMLPFinal/best_weights\n",
      "The loss after 393 iterations is E=0.013116721096031826\n",
      "Wrote weights to file ./customFunctionLearningAdaptive/baselineMLPFinal/best_weights\n",
      "The loss after 399 iterations is E=0.013022536250599908\n",
      "Wrote weights to file ./customFunctionLearningAdaptive/baselineMLPFinal/best_weights\n",
      "The loss after 404 iterations is E=0.013001281169055164\n",
      "Wrote weights to file ./customFunctionLearningAdaptive/baselineMLPFinal/best_weights\n",
      "The loss after 409 iterations is E=0.012977583817875046\n",
      "Wrote weights to file ./customFunctionLearningAdaptive/baselineMLPFinal/best_weights\n",
      "The loss after 410 iterations is E=0.012973560791926023\n",
      "Wrote weights to file ./customFunctionLearningAdaptive/baselineMLPFinal/best_weights\n",
      "The loss after 414 iterations is E=0.012952189710295872\n",
      "Wrote weights to file ./customFunctionLearningAdaptive/baselineMLPFinal/best_weights\n",
      "The loss after 415 iterations is E=0.012926120416589625\n",
      "Wrote weights to file ./customFunctionLearningAdaptive/baselineMLPFinal/best_weights\n",
      "The loss after 420 iterations is E=0.012891630068778725\n",
      "Wrote weights to file ./customFunctionLearningAdaptive/baselineMLPFinal/best_weights\n",
      "The loss after 425 iterations is E=0.012865537753066442\n",
      "Wrote weights to file ./customFunctionLearningAdaptive/baselineMLPFinal/best_weights\n",
      "The loss after 426 iterations is E=0.012855353579606918\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wrote weights to file ./customFunctionLearningAdaptive/baselineMLPFinal/best_weights\n",
      "The loss after 430 iterations is E=0.012841928658126032\n",
      "Wrote weights to file ./customFunctionLearningAdaptive/baselineMLPFinal/best_weights\n",
      "The loss after 431 iterations is E=0.01282326591298625\n",
      "Wrote weights to file ./customFunctionLearningAdaptive/baselineMLPFinal/best_weights\n",
      "The loss after 432 iterations is E=0.01281761889799889\n",
      "Wrote weights to file ./customFunctionLearningAdaptive/baselineMLPFinal/best_weights\n",
      "The loss after 435 iterations is E=0.012812786920364277\n",
      "Wrote weights to file ./customFunctionLearningAdaptive/baselineMLPFinal/best_weights\n",
      "The loss after 436 iterations is E=0.012798051373764269\n",
      "Wrote weights to file ./customFunctionLearningAdaptive/baselineMLPFinal/best_weights\n",
      "The loss after 437 iterations is E=0.012784697505226863\n",
      "Wrote weights to file ./customFunctionLearningAdaptive/baselineMLPFinal/best_weights\n",
      "The loss after 438 iterations is E=0.012777821624228311\n",
      "Wrote weights to file ./customFunctionLearningAdaptive/baselineMLPFinal/best_weights\n",
      "The loss after 439 iterations is E=0.012776003113703681\n",
      "Wrote weights to file ./customFunctionLearningAdaptive/baselineMLPFinal/best_weights\n",
      "The loss after 440 iterations is E=0.012773972799983902\n",
      "Wrote weights to file ./customFunctionLearningAdaptive/baselineMLPFinal/best_weights\n",
      "The loss after 441 iterations is E=0.012767954335097108\n",
      "Wrote weights to file ./customFunctionLearningAdaptive/baselineMLPFinal/best_weights\n",
      "The loss after 442 iterations is E=0.012757836497606662\n",
      "Wrote weights to file ./customFunctionLearningAdaptive/baselineMLPFinal/best_weights\n",
      "The loss after 443 iterations is E=0.012747008172087529\n",
      "Wrote weights to file ./customFunctionLearningAdaptive/baselineMLPFinal/best_weights\n",
      "The loss after 444 iterations is E=0.012738508472793009\n",
      "Wrote weights to file ./customFunctionLearningAdaptive/baselineMLPFinal/best_weights\n",
      "The loss after 445 iterations is E=0.012733116148573418\n",
      "Wrote weights to file ./customFunctionLearningAdaptive/baselineMLPFinal/best_weights\n",
      "The loss after 446 iterations is E=0.012729217761592929\n",
      "Wrote weights to file ./customFunctionLearningAdaptive/baselineMLPFinal/best_weights\n",
      "The loss after 447 iterations is E=0.012724470390634481\n",
      "Wrote weights to file ./customFunctionLearningAdaptive/baselineMLPFinal/best_weights\n",
      "The loss after 448 iterations is E=0.012717719574982024\n",
      "Wrote weights to file ./customFunctionLearningAdaptive/baselineMLPFinal/best_weights\n",
      "The loss after 449 iterations is E=0.012709283469556947\n",
      "Wrote weights to file ./customFunctionLearningAdaptive/baselineMLPFinal/best_weights\n",
      "The loss after 450 iterations is E=0.012700642049231145\n",
      "Wrote weights to file ./customFunctionLearningAdaptive/baselineMLPFinal/best_weights\n",
      "The loss after 451 iterations is E=0.012693016646955418\n",
      "Wrote weights to file ./customFunctionLearningAdaptive/baselineMLPFinal/best_weights\n",
      "The loss after 452 iterations is E=0.012686770387216469\n",
      "Wrote weights to file ./customFunctionLearningAdaptive/baselineMLPFinal/best_weights\n",
      "The loss after 453 iterations is E=0.012681356272836638\n",
      "Wrote weights to file ./customFunctionLearningAdaptive/baselineMLPFinal/best_weights\n",
      "The loss after 454 iterations is E=0.01267585413000419\n",
      "Wrote weights to file ./customFunctionLearningAdaptive/baselineMLPFinal/best_weights\n",
      "The loss after 455 iterations is E=0.012669629715719275\n",
      "Wrote weights to file ./customFunctionLearningAdaptive/baselineMLPFinal/best_weights\n",
      "The loss after 456 iterations is E=0.012662542979745598\n",
      "Wrote weights to file ./customFunctionLearningAdaptive/baselineMLPFinal/best_weights\n",
      "The loss after 457 iterations is E=0.012655000721118595\n",
      "Wrote weights to file ./customFunctionLearningAdaptive/baselineMLPFinal/best_weights\n",
      "The loss after 458 iterations is E=0.012647511990232422\n",
      "Wrote weights to file ./customFunctionLearningAdaptive/baselineMLPFinal/best_weights\n",
      "The loss after 459 iterations is E=0.012640469169345766\n",
      "Wrote weights to file ./customFunctionLearningAdaptive/baselineMLPFinal/best_weights\n",
      "The loss after 460 iterations is E=0.01263395016433496\n",
      "Wrote weights to file ./customFunctionLearningAdaptive/baselineMLPFinal/best_weights\n",
      "The loss after 461 iterations is E=0.012627772616867627\n",
      "Wrote weights to file ./customFunctionLearningAdaptive/baselineMLPFinal/best_weights\n",
      "The loss after 462 iterations is E=0.012621646190052693\n",
      "Wrote weights to file ./customFunctionLearningAdaptive/baselineMLPFinal/best_weights\n",
      "The loss after 463 iterations is E=0.012615316845489022\n",
      "Wrote weights to file ./customFunctionLearningAdaptive/baselineMLPFinal/best_weights\n",
      "The loss after 464 iterations is E=0.012608689136148045\n",
      "Wrote weights to file ./customFunctionLearningAdaptive/baselineMLPFinal/best_weights\n",
      "The loss after 465 iterations is E=0.012601787099241401\n",
      "Wrote weights to file ./customFunctionLearningAdaptive/baselineMLPFinal/best_weights\n",
      "The loss after 466 iterations is E=0.012594752226002887\n",
      "Wrote weights to file ./customFunctionLearningAdaptive/baselineMLPFinal/best_weights\n",
      "The loss after 467 iterations is E=0.012587722229168824\n",
      "Wrote weights to file ./customFunctionLearningAdaptive/baselineMLPFinal/best_weights\n",
      "The loss after 468 iterations is E=0.012580807949994985\n",
      "Wrote weights to file ./customFunctionLearningAdaptive/baselineMLPFinal/best_weights\n",
      "The loss after 469 iterations is E=0.012574048247020651\n",
      "Wrote weights to file ./customFunctionLearningAdaptive/baselineMLPFinal/best_weights\n",
      "The loss after 470 iterations is E=0.012567424424366385\n",
      "Wrote weights to file ./customFunctionLearningAdaptive/baselineMLPFinal/best_weights\n",
      "The loss after 471 iterations is E=0.012560881305271596\n",
      "Wrote weights to file ./customFunctionLearningAdaptive/baselineMLPFinal/best_weights\n",
      "The loss after 472 iterations is E=0.01255435309360046\n",
      "Wrote weights to file ./customFunctionLearningAdaptive/baselineMLPFinal/best_weights\n",
      "The loss after 473 iterations is E=0.012547789499652486\n",
      "Wrote weights to file ./customFunctionLearningAdaptive/baselineMLPFinal/best_weights\n",
      "The loss after 474 iterations is E=0.012541156249326477\n",
      "Wrote weights to file ./customFunctionLearningAdaptive/baselineMLPFinal/best_weights\n",
      "The loss after 475 iterations is E=0.012534450334613938\n",
      "Wrote weights to file ./customFunctionLearningAdaptive/baselineMLPFinal/best_weights\n",
      "The loss after 476 iterations is E=0.012527675822606532\n",
      "Wrote weights to file ./customFunctionLearningAdaptive/baselineMLPFinal/best_weights\n",
      "The loss after 477 iterations is E=0.012520856427302373\n",
      "Wrote weights to file ./customFunctionLearningAdaptive/baselineMLPFinal/best_weights\n",
      "The loss after 478 iterations is E=0.012514008168476112\n",
      "Wrote weights to file ./customFunctionLearningAdaptive/baselineMLPFinal/best_weights\n",
      "The loss after 479 iterations is E=0.012507152430236196\n",
      "Wrote weights to file ./customFunctionLearningAdaptive/baselineMLPFinal/best_weights\n",
      "The loss after 480 iterations is E=0.012500298766904611\n",
      "Wrote weights to file ./customFunctionLearningAdaptive/baselineMLPFinal/best_weights\n",
      "The loss after 481 iterations is E=0.012493455969714353\n",
      "Wrote weights to file ./customFunctionLearningAdaptive/baselineMLPFinal/best_weights\n",
      "The loss after 482 iterations is E=0.012486624758565452\n",
      "Wrote weights to file ./customFunctionLearningAdaptive/baselineMLPFinal/best_weights\n",
      "The loss after 483 iterations is E=0.012479804981175594\n",
      "Wrote weights to file ./customFunctionLearningAdaptive/baselineMLPFinal/best_weights\n",
      "The loss after 484 iterations is E=0.012472993284726976\n",
      "Wrote weights to file ./customFunctionLearningAdaptive/baselineMLPFinal/best_weights\n",
      "The loss after 485 iterations is E=0.012466186675668891\n",
      "Wrote weights to file ./customFunctionLearningAdaptive/baselineMLPFinal/best_weights\n",
      "The loss after 486 iterations is E=0.01245938193229485\n",
      "Wrote weights to file ./customFunctionLearningAdaptive/baselineMLPFinal/best_weights\n",
      "The loss after 487 iterations is E=0.012452576635246891\n",
      "Wrote weights to file ./customFunctionLearningAdaptive/baselineMLPFinal/best_weights\n",
      "The loss after 488 iterations is E=0.012445769404504755\n",
      "Wrote weights to file ./customFunctionLearningAdaptive/baselineMLPFinal/best_weights\n",
      "The loss after 489 iterations is E=0.012438959341602891\n",
      "Wrote weights to file ./customFunctionLearningAdaptive/baselineMLPFinal/best_weights\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The loss after 490 iterations is E=0.012432147234121347\n",
      "Wrote weights to file ./customFunctionLearningAdaptive/baselineMLPFinal/best_weights\n",
      "The loss after 491 iterations is E=0.01242533387697948\n",
      "Wrote weights to file ./customFunctionLearningAdaptive/baselineMLPFinal/best_weights\n",
      "The loss after 492 iterations is E=0.012418522717648707\n",
      "Wrote weights to file ./customFunctionLearningAdaptive/baselineMLPFinal/best_weights\n",
      "The loss after 493 iterations is E=0.012411717111417328\n",
      "Wrote weights to file ./customFunctionLearningAdaptive/baselineMLPFinal/best_weights\n",
      "The loss after 494 iterations is E=0.012404925371084091\n",
      "Wrote weights to file ./customFunctionLearningAdaptive/baselineMLPFinal/best_weights\n",
      "The loss after 495 iterations is E=0.01239815666655058\n",
      "Wrote weights to file ./customFunctionLearningAdaptive/baselineMLPFinal/best_weights\n",
      "The loss after 496 iterations is E=0.012391431044555652\n",
      "Wrote weights to file ./customFunctionLearningAdaptive/baselineMLPFinal/best_weights\n",
      "The loss after 497 iterations is E=0.012384773160375178\n",
      "Wrote weights to file ./customFunctionLearningAdaptive/baselineMLPFinal/best_weights\n",
      "The loss after 498 iterations is E=0.012378234935709324\n",
      "Wrote weights to file ./customFunctionLearningAdaptive/baselineMLPFinal/best_weights\n",
      "The loss after 499 iterations is E=0.012371884626052168\n",
      "Wrote weights to file ./customFunctionLearningAdaptive/baselineMLPFinal/best_weights\n",
      "The loss after 500 iterations is E=0.012365867126035891\n",
      "Wrote weights to file ./customFunctionLearningAdaptive/baselineMLPFinal/best_weights\n",
      "The loss after 501 iterations is E=0.012360377895227015\n",
      "Wrote weights to file ./customFunctionLearningAdaptive/baselineMLPFinal/best_weights\n",
      "The loss after 502 iterations is E=0.012355850421650506\n",
      "Wrote weights to file ./customFunctionLearningAdaptive/baselineMLPFinal/best_weights\n",
      "The loss after 503 iterations is E=0.012352860632931242\n",
      "Wrote weights to file ./customFunctionLearningAdaptive/baselineMLPFinal/best_weights\n",
      "The loss after 504 iterations is E=0.012352792734891222\n",
      "Wrote weights to file ./customFunctionLearningAdaptive/baselineMLPFinal/best_weights\n",
      "The loss after 518 iterations is E=0.012247529581830894\n",
      "Wrote weights to file ./customFunctionLearningAdaptive/baselineMLPFinal/best_weights\n",
      "The loss after 523 iterations is E=0.012235637507571812\n",
      "Wrote weights to file ./customFunctionLearningAdaptive/baselineMLPFinal/best_weights\n",
      "The loss after 528 iterations is E=0.012196483143452133\n",
      "Wrote weights to file ./customFunctionLearningAdaptive/baselineMLPFinal/best_weights\n",
      "The loss after 533 iterations is E=0.012153433178042595\n",
      "Wrote weights to file ./customFunctionLearningAdaptive/baselineMLPFinal/best_weights\n",
      "The loss after 538 iterations is E=0.012119024571553987\n",
      "Wrote weights to file ./customFunctionLearningAdaptive/baselineMLPFinal/best_weights\n",
      "The loss after 543 iterations is E=0.012087926976752952\n",
      "Wrote weights to file ./customFunctionLearningAdaptive/baselineMLPFinal/best_weights\n",
      "The loss after 548 iterations is E=0.012057389795366363\n",
      "Wrote weights to file ./customFunctionLearningAdaptive/baselineMLPFinal/best_weights\n",
      "The loss after 552 iterations is E=0.012053306623138043\n",
      "Wrote weights to file ./customFunctionLearningAdaptive/baselineMLPFinal/best_weights\n",
      "The loss after 553 iterations is E=0.01202732573789094\n",
      "Wrote weights to file ./customFunctionLearningAdaptive/baselineMLPFinal/best_weights\n",
      "The loss after 557 iterations is E=0.01201879445641268\n",
      "Wrote weights to file ./customFunctionLearningAdaptive/baselineMLPFinal/best_weights\n",
      "The loss after 558 iterations is E=0.011998120750516872\n",
      "Wrote weights to file ./customFunctionLearningAdaptive/baselineMLPFinal/best_weights\n",
      "The loss after 559 iterations is E=0.011991335087633623\n",
      "Wrote weights to file ./customFunctionLearningAdaptive/baselineMLPFinal/best_weights\n",
      "The loss after 562 iterations is E=0.011985101431372406\n",
      "Wrote weights to file ./customFunctionLearningAdaptive/baselineMLPFinal/best_weights\n",
      "The loss after 563 iterations is E=0.011969483846362044\n",
      "Wrote weights to file ./customFunctionLearningAdaptive/baselineMLPFinal/best_weights\n",
      "The loss after 564 iterations is E=0.011958793626053335\n",
      "Wrote weights to file ./customFunctionLearningAdaptive/baselineMLPFinal/best_weights\n",
      "The loss after 565 iterations is E=0.011956091293820272\n",
      "Wrote weights to file ./customFunctionLearningAdaptive/baselineMLPFinal/best_weights\n",
      "The loss after 566 iterations is E=0.011955655797414467\n",
      "Wrote weights to file ./customFunctionLearningAdaptive/baselineMLPFinal/best_weights\n",
      "The loss after 567 iterations is E=0.011950522987183266\n",
      "Wrote weights to file ./customFunctionLearningAdaptive/baselineMLPFinal/best_weights\n",
      "The loss after 568 iterations is E=0.011940152783396201\n",
      "Wrote weights to file ./customFunctionLearningAdaptive/baselineMLPFinal/best_weights\n",
      "The loss after 569 iterations is E=0.011929075256439046\n",
      "Wrote weights to file ./customFunctionLearningAdaptive/baselineMLPFinal/best_weights\n",
      "The loss after 570 iterations is E=0.011921821071587114\n",
      "Wrote weights to file ./customFunctionLearningAdaptive/baselineMLPFinal/best_weights\n",
      "The loss after 571 iterations is E=0.011918284498230835\n",
      "Wrote weights to file ./customFunctionLearningAdaptive/baselineMLPFinal/best_weights\n",
      "The loss after 572 iterations is E=0.011914859527163289\n",
      "Wrote weights to file ./customFunctionLearningAdaptive/baselineMLPFinal/best_weights\n",
      "The loss after 573 iterations is E=0.011908675172521236\n",
      "Wrote weights to file ./customFunctionLearningAdaptive/baselineMLPFinal/best_weights\n",
      "The loss after 574 iterations is E=0.011899897020783512\n",
      "Wrote weights to file ./customFunctionLearningAdaptive/baselineMLPFinal/best_weights\n",
      "The loss after 575 iterations is E=0.011891165698582295\n",
      "Wrote weights to file ./customFunctionLearningAdaptive/baselineMLPFinal/best_weights\n",
      "The loss after 576 iterations is E=0.011884486382847935\n",
      "Wrote weights to file ./customFunctionLearningAdaptive/baselineMLPFinal/best_weights\n",
      "The loss after 577 iterations is E=0.011879680298844718\n",
      "Wrote weights to file ./customFunctionLearningAdaptive/baselineMLPFinal/best_weights\n",
      "The loss after 578 iterations is E=0.011874994868822687\n",
      "Wrote weights to file ./customFunctionLearningAdaptive/baselineMLPFinal/best_weights\n",
      "The loss after 579 iterations is E=0.011868911939501271\n",
      "Wrote weights to file ./customFunctionLearningAdaptive/baselineMLPFinal/best_weights\n",
      "The loss after 580 iterations is E=0.011861471760989502\n",
      "Wrote weights to file ./customFunctionLearningAdaptive/baselineMLPFinal/best_weights\n",
      "The loss after 581 iterations is E=0.011853789128861442\n",
      "Wrote weights to file ./customFunctionLearningAdaptive/baselineMLPFinal/best_weights\n",
      "The loss after 582 iterations is E=0.011846993417566088\n",
      "Wrote weights to file ./customFunctionLearningAdaptive/baselineMLPFinal/best_weights\n",
      "The loss after 583 iterations is E=0.011841259989209114\n",
      "Wrote weights to file ./customFunctionLearningAdaptive/baselineMLPFinal/best_weights\n",
      "The loss after 584 iterations is E=0.011835894723596758\n",
      "Wrote weights to file ./customFunctionLearningAdaptive/baselineMLPFinal/best_weights\n",
      "The loss after 585 iterations is E=0.011830078165951574\n",
      "Wrote weights to file ./customFunctionLearningAdaptive/baselineMLPFinal/best_weights\n",
      "The loss after 586 iterations is E=0.011823488948956155\n",
      "Wrote weights to file ./customFunctionLearningAdaptive/baselineMLPFinal/best_weights\n",
      "The loss after 587 iterations is E=0.01181648160199719\n",
      "Wrote weights to file ./customFunctionLearningAdaptive/baselineMLPFinal/best_weights\n",
      "The loss after 588 iterations is E=0.011809626028109612\n",
      "Wrote weights to file ./customFunctionLearningAdaptive/baselineMLPFinal/best_weights\n",
      "The loss after 589 iterations is E=0.011803278215051312\n",
      "Wrote weights to file ./customFunctionLearningAdaptive/baselineMLPFinal/best_weights\n",
      "The loss after 590 iterations is E=0.011797359195520168\n",
      "Wrote weights to file ./customFunctionLearningAdaptive/baselineMLPFinal/best_weights\n",
      "The loss after 591 iterations is E=0.011791509978770745\n",
      "Wrote weights to file ./customFunctionLearningAdaptive/baselineMLPFinal/best_weights\n",
      "The loss after 592 iterations is E=0.011785407618547801\n",
      "Wrote weights to file ./customFunctionLearningAdaptive/baselineMLPFinal/best_weights\n",
      "The loss after 593 iterations is E=0.011778956988542026\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wrote weights to file ./customFunctionLearningAdaptive/baselineMLPFinal/best_weights\n",
      "The loss after 594 iterations is E=0.01177232275135956\n",
      "Wrote weights to file ./customFunctionLearningAdaptive/baselineMLPFinal/best_weights\n",
      "The loss after 595 iterations is E=0.011765742585157564\n",
      "Wrote weights to file ./customFunctionLearningAdaptive/baselineMLPFinal/best_weights\n",
      "The loss after 596 iterations is E=0.01175937605476384\n",
      "Wrote weights to file ./customFunctionLearningAdaptive/baselineMLPFinal/best_weights\n",
      "The loss after 597 iterations is E=0.011753216943290625\n",
      "Wrote weights to file ./customFunctionLearningAdaptive/baselineMLPFinal/best_weights\n",
      "The loss after 598 iterations is E=0.011747137138652584\n",
      "Wrote weights to file ./customFunctionLearningAdaptive/baselineMLPFinal/best_weights\n",
      "The loss after 599 iterations is E=0.011740992533568095\n",
      "Wrote weights to file ./customFunctionLearningAdaptive/baselineMLPFinal/best_weights\n",
      "The loss after 600 iterations is E=0.011734702775892414\n",
      "Wrote weights to file ./customFunctionLearningAdaptive/baselineMLPFinal/best_weights\n",
      "The loss after 601 iterations is E=0.011728290407565662\n",
      "Wrote weights to file ./customFunctionLearningAdaptive/baselineMLPFinal/best_weights\n",
      "The loss after 602 iterations is E=0.011721833433491874\n",
      "Wrote weights to file ./customFunctionLearningAdaptive/baselineMLPFinal/best_weights\n",
      "The loss after 603 iterations is E=0.011715419962155377\n",
      "Wrote weights to file ./customFunctionLearningAdaptive/baselineMLPFinal/best_weights\n",
      "The loss after 604 iterations is E=0.011709094031791406\n",
      "Wrote weights to file ./customFunctionLearningAdaptive/baselineMLPFinal/best_weights\n",
      "The loss after 605 iterations is E=0.01170284727472312\n",
      "Wrote weights to file ./customFunctionLearningAdaptive/baselineMLPFinal/best_weights\n",
      "The loss after 606 iterations is E=0.011696635061354375\n",
      "Wrote weights to file ./customFunctionLearningAdaptive/baselineMLPFinal/best_weights\n",
      "The loss after 607 iterations is E=0.011690405236097696\n",
      "Wrote weights to file ./customFunctionLearningAdaptive/baselineMLPFinal/best_weights\n",
      "The loss after 608 iterations is E=0.011684126906362122\n",
      "Wrote weights to file ./customFunctionLearningAdaptive/baselineMLPFinal/best_weights\n",
      "The loss after 609 iterations is E=0.011677794372410127\n",
      "Wrote weights to file ./customFunctionLearningAdaptive/baselineMLPFinal/best_weights\n",
      "The loss after 610 iterations is E=0.011671428559711738\n",
      "Wrote weights to file ./customFunctionLearningAdaptive/baselineMLPFinal/best_weights\n",
      "The loss after 611 iterations is E=0.011665056304368005\n",
      "Wrote weights to file ./customFunctionLearningAdaptive/baselineMLPFinal/best_weights\n",
      "The loss after 612 iterations is E=0.011658702285465675\n",
      "Wrote weights to file ./customFunctionLearningAdaptive/baselineMLPFinal/best_weights\n",
      "The loss after 613 iterations is E=0.011652377026571771\n",
      "Wrote weights to file ./customFunctionLearningAdaptive/baselineMLPFinal/best_weights\n",
      "The loss after 614 iterations is E=0.011646078261929241\n",
      "Wrote weights to file ./customFunctionLearningAdaptive/baselineMLPFinal/best_weights\n",
      "The loss after 615 iterations is E=0.011639794654908622\n",
      "Wrote weights to file ./customFunctionLearningAdaptive/baselineMLPFinal/best_weights\n",
      "The loss after 616 iterations is E=0.01163351169741387\n",
      "Wrote weights to file ./customFunctionLearningAdaptive/baselineMLPFinal/best_weights\n",
      "The loss after 617 iterations is E=0.011627218744956144\n",
      "Wrote weights to file ./customFunctionLearningAdaptive/baselineMLPFinal/best_weights\n",
      "The loss after 618 iterations is E=0.01162090930415126\n",
      "Wrote weights to file ./customFunctionLearningAdaptive/baselineMLPFinal/best_weights\n",
      "The loss after 619 iterations is E=0.01161458450098125\n",
      "Wrote weights to file ./customFunctionLearningAdaptive/baselineMLPFinal/best_weights\n",
      "The loss after 620 iterations is E=0.011608247687779648\n",
      "Wrote weights to file ./customFunctionLearningAdaptive/baselineMLPFinal/best_weights\n",
      "The loss after 621 iterations is E=0.011601905752850913\n",
      "Wrote weights to file ./customFunctionLearningAdaptive/baselineMLPFinal/best_weights\n",
      "The loss after 622 iterations is E=0.011595563646889396\n",
      "Wrote weights to file ./customFunctionLearningAdaptive/baselineMLPFinal/best_weights\n",
      "The loss after 623 iterations is E=0.011589225835871885\n",
      "Wrote weights to file ./customFunctionLearningAdaptive/baselineMLPFinal/best_weights\n",
      "The loss after 624 iterations is E=0.011582893768772216\n",
      "Wrote weights to file ./customFunctionLearningAdaptive/baselineMLPFinal/best_weights\n",
      "The loss after 625 iterations is E=0.011576567551115803\n",
      "Wrote weights to file ./customFunctionLearningAdaptive/baselineMLPFinal/best_weights\n",
      "The loss after 626 iterations is E=0.011570245787812697\n",
      "Wrote weights to file ./customFunctionLearningAdaptive/baselineMLPFinal/best_weights\n",
      "The loss after 627 iterations is E=0.01156392649644294\n",
      "Wrote weights to file ./customFunctionLearningAdaptive/baselineMLPFinal/best_weights\n",
      "The loss after 628 iterations is E=0.011557607983363971\n",
      "Wrote weights to file ./customFunctionLearningAdaptive/baselineMLPFinal/best_weights\n",
      "The loss after 629 iterations is E=0.011551288451145498\n",
      "Wrote weights to file ./customFunctionLearningAdaptive/baselineMLPFinal/best_weights\n",
      "The loss after 630 iterations is E=0.011544967310187452\n",
      "Wrote weights to file ./customFunctionLearningAdaptive/baselineMLPFinal/best_weights\n",
      "The loss after 631 iterations is E=0.011538643665669234\n",
      "Wrote weights to file ./customFunctionLearningAdaptive/baselineMLPFinal/best_weights\n",
      "The loss after 632 iterations is E=0.011532318000175406\n",
      "Wrote weights to file ./customFunctionLearningAdaptive/baselineMLPFinal/best_weights\n",
      "The loss after 633 iterations is E=0.011525990034734711\n",
      "Wrote weights to file ./customFunctionLearningAdaptive/baselineMLPFinal/best_weights\n",
      "The loss after 634 iterations is E=0.0115196607575043\n",
      "Wrote weights to file ./customFunctionLearningAdaptive/baselineMLPFinal/best_weights\n",
      "The loss after 635 iterations is E=0.011513330066148188\n",
      "Wrote weights to file ./customFunctionLearningAdaptive/baselineMLPFinal/best_weights\n",
      "The loss after 636 iterations is E=0.011506999026898626\n",
      "Wrote weights to file ./customFunctionLearningAdaptive/baselineMLPFinal/best_weights\n",
      "The loss after 637 iterations is E=0.011500667495953193\n",
      "Wrote weights to file ./customFunctionLearningAdaptive/baselineMLPFinal/best_weights\n",
      "The loss after 638 iterations is E=0.011494336492917036\n",
      "Wrote weights to file ./customFunctionLearningAdaptive/baselineMLPFinal/best_weights\n",
      "The loss after 639 iterations is E=0.011488005834301528\n",
      "Wrote weights to file ./customFunctionLearningAdaptive/baselineMLPFinal/best_weights\n",
      "The loss after 640 iterations is E=0.011481676594324447\n",
      "Wrote weights to file ./customFunctionLearningAdaptive/baselineMLPFinal/best_weights\n",
      "The loss after 641 iterations is E=0.0114753486867463\n",
      "Wrote weights to file ./customFunctionLearningAdaptive/baselineMLPFinal/best_weights\n",
      "The loss after 642 iterations is E=0.011469023510447108\n",
      "Wrote weights to file ./customFunctionLearningAdaptive/baselineMLPFinal/best_weights\n",
      "The loss after 643 iterations is E=0.011462701351973087\n",
      "Wrote weights to file ./customFunctionLearningAdaptive/baselineMLPFinal/best_weights\n",
      "The loss after 644 iterations is E=0.011456384469193188\n",
      "Wrote weights to file ./customFunctionLearningAdaptive/baselineMLPFinal/best_weights\n",
      "The loss after 645 iterations is E=0.011450074120695692\n",
      "Wrote weights to file ./customFunctionLearningAdaptive/baselineMLPFinal/best_weights\n",
      "The loss after 646 iterations is E=0.01144377461061634\n",
      "Wrote weights to file ./customFunctionLearningAdaptive/baselineMLPFinal/best_weights\n",
      "The loss after 647 iterations is E=0.01143748964963309\n",
      "Wrote weights to file ./customFunctionLearningAdaptive/baselineMLPFinal/best_weights\n",
      "The loss after 648 iterations is E=0.011431228522868387\n",
      "Wrote weights to file ./customFunctionLearningAdaptive/baselineMLPFinal/best_weights\n",
      "The loss after 649 iterations is E=0.011425001278626718\n",
      "Wrote weights to file ./customFunctionLearningAdaptive/baselineMLPFinal/best_weights\n",
      "The loss after 650 iterations is E=0.011418829966534594\n",
      "Wrote weights to file ./customFunctionLearningAdaptive/baselineMLPFinal/best_weights\n",
      "The loss after 651 iterations is E=0.011412741547339724\n",
      "Wrote weights to file ./customFunctionLearningAdaptive/baselineMLPFinal/best_weights\n",
      "The loss after 652 iterations is E=0.011406792685952289\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wrote weights to file ./customFunctionLearningAdaptive/baselineMLPFinal/best_weights\n",
      "The loss after 653 iterations is E=0.01140105690867625\n",
      "Wrote weights to file ./customFunctionLearningAdaptive/baselineMLPFinal/best_weights\n",
      "The loss after 654 iterations is E=0.011395690214064341\n",
      "Wrote weights to file ./customFunctionLearningAdaptive/baselineMLPFinal/best_weights\n",
      "The loss after 655 iterations is E=0.01139089785714531\n",
      "Wrote weights to file ./customFunctionLearningAdaptive/baselineMLPFinal/best_weights\n",
      "The loss after 656 iterations is E=0.01138713730243556\n",
      "Wrote weights to file ./customFunctionLearningAdaptive/baselineMLPFinal/best_weights\n",
      "The loss after 657 iterations is E=0.011384992546165337\n",
      "Wrote weights to file ./customFunctionLearningAdaptive/baselineMLPFinal/best_weights\n",
      "The loss after 672 iterations is E=0.011341785137929819\n",
      "Wrote weights to file ./customFunctionLearningAdaptive/baselineMLPFinal/best_weights\n",
      "The loss after 678 iterations is E=0.011249557207702815\n",
      "Wrote weights to file ./customFunctionLearningAdaptive/baselineMLPFinal/best_weights\n",
      "The loss after 683 iterations is E=0.011224008408883924\n",
      "Wrote weights to file ./customFunctionLearningAdaptive/baselineMLPFinal/best_weights\n",
      "The loss after 688 iterations is E=0.011198590545287504\n",
      "Wrote weights to file ./customFunctionLearningAdaptive/baselineMLPFinal/best_weights\n",
      "The loss after 693 iterations is E=0.011171816848094719\n",
      "Wrote weights to file ./customFunctionLearningAdaptive/baselineMLPFinal/best_weights\n",
      "The loss after 698 iterations is E=0.011145672386958207\n",
      "Wrote weights to file ./customFunctionLearningAdaptive/baselineMLPFinal/best_weights\n",
      "The loss after 699 iterations is E=0.011144882237450278\n",
      "Wrote weights to file ./customFunctionLearningAdaptive/baselineMLPFinal/best_weights\n",
      "The loss after 703 iterations is E=0.011120560653748942\n",
      "Wrote weights to file ./customFunctionLearningAdaptive/baselineMLPFinal/best_weights\n",
      "The loss after 704 iterations is E=0.011108575321998996\n",
      "Wrote weights to file ./customFunctionLearningAdaptive/baselineMLPFinal/best_weights\n",
      "The loss after 708 iterations is E=0.011095904207606295\n",
      "Wrote weights to file ./customFunctionLearningAdaptive/baselineMLPFinal/best_weights\n",
      "The loss after 709 iterations is E=0.011078629863686074\n",
      "Wrote weights to file ./customFunctionLearningAdaptive/baselineMLPFinal/best_weights\n",
      "The loss after 713 iterations is E=0.011070500437085686\n",
      "Wrote weights to file ./customFunctionLearningAdaptive/baselineMLPFinal/best_weights\n",
      "The loss after 714 iterations is E=0.011053077421343677\n",
      "Wrote weights to file ./customFunctionLearningAdaptive/baselineMLPFinal/best_weights\n",
      "The loss after 715 iterations is E=0.011047915748063441\n",
      "Wrote weights to file ./customFunctionLearningAdaptive/baselineMLPFinal/best_weights\n",
      "The loss after 718 iterations is E=0.01104282405297741\n",
      "Wrote weights to file ./customFunctionLearningAdaptive/baselineMLPFinal/best_weights\n",
      "The loss after 719 iterations is E=0.011029252036807756\n",
      "Wrote weights to file ./customFunctionLearningAdaptive/baselineMLPFinal/best_weights\n",
      "The loss after 720 iterations is E=0.011019218267849714\n",
      "Wrote weights to file ./customFunctionLearningAdaptive/baselineMLPFinal/best_weights\n",
      "The loss after 721 iterations is E=0.01101602152717948\n",
      "Wrote weights to file ./customFunctionLearningAdaptive/baselineMLPFinal/best_weights\n",
      "The loss after 722 iterations is E=0.011015682966921205\n",
      "Wrote weights to file ./customFunctionLearningAdaptive/baselineMLPFinal/best_weights\n",
      "The loss after 723 iterations is E=0.011012226609578585\n",
      "Wrote weights to file ./customFunctionLearningAdaptive/baselineMLPFinal/best_weights\n",
      "The loss after 724 iterations is E=0.011004091148685783\n",
      "Wrote weights to file ./customFunctionLearningAdaptive/baselineMLPFinal/best_weights\n",
      "The loss after 725 iterations is E=0.010994228467842993\n",
      "Wrote weights to file ./customFunctionLearningAdaptive/baselineMLPFinal/best_weights\n",
      "The loss after 726 iterations is E=0.010986816308815754\n",
      "Wrote weights to file ./customFunctionLearningAdaptive/baselineMLPFinal/best_weights\n",
      "The loss after 727 iterations is E=0.010982902459797867\n",
      "Wrote weights to file ./customFunctionLearningAdaptive/baselineMLPFinal/best_weights\n",
      "The loss after 728 iterations is E=0.010980194994030005\n",
      "Wrote weights to file ./customFunctionLearningAdaptive/baselineMLPFinal/best_weights\n",
      "The loss after 729 iterations is E=0.010975863884875424\n",
      "Wrote weights to file ./customFunctionLearningAdaptive/baselineMLPFinal/best_weights\n",
      "The loss after 730 iterations is E=0.010969018968973626\n",
      "Wrote weights to file ./customFunctionLearningAdaptive/baselineMLPFinal/best_weights\n",
      "The loss after 731 iterations is E=0.010961266780637418\n",
      "Wrote weights to file ./customFunctionLearningAdaptive/baselineMLPFinal/best_weights\n",
      "The loss after 732 iterations is E=0.010954610389400236\n",
      "Wrote weights to file ./customFunctionLearningAdaptive/baselineMLPFinal/best_weights\n",
      "The loss after 733 iterations is E=0.010949772887813104\n",
      "Wrote weights to file ./customFunctionLearningAdaptive/baselineMLPFinal/best_weights\n",
      "The loss after 734 iterations is E=0.010945823244173423\n",
      "Wrote weights to file ./customFunctionLearningAdaptive/baselineMLPFinal/best_weights\n",
      "The loss after 735 iterations is E=0.010941289129086958\n",
      "Wrote weights to file ./customFunctionLearningAdaptive/baselineMLPFinal/best_weights\n",
      "The loss after 736 iterations is E=0.010935522256989348\n",
      "Wrote weights to file ./customFunctionLearningAdaptive/baselineMLPFinal/best_weights\n",
      "The loss after 737 iterations is E=0.010928960262174808\n",
      "Wrote weights to file ./customFunctionLearningAdaptive/baselineMLPFinal/best_weights\n",
      "The loss after 738 iterations is E=0.010922628296334617\n",
      "Wrote weights to file ./customFunctionLearningAdaptive/baselineMLPFinal/best_weights\n",
      "The loss after 739 iterations is E=0.010917152120811463\n",
      "Wrote weights to file ./customFunctionLearningAdaptive/baselineMLPFinal/best_weights\n",
      "The loss after 740 iterations is E=0.010912391591643445\n",
      "Wrote weights to file ./customFunctionLearningAdaptive/baselineMLPFinal/best_weights\n",
      "The loss after 741 iterations is E=0.01090770929675678\n",
      "Wrote weights to file ./customFunctionLearningAdaptive/baselineMLPFinal/best_weights\n",
      "The loss after 742 iterations is E=0.010902539185010142\n",
      "Wrote weights to file ./customFunctionLearningAdaptive/baselineMLPFinal/best_weights\n",
      "The loss after 743 iterations is E=0.010896812179750625\n",
      "Wrote weights to file ./customFunctionLearningAdaptive/baselineMLPFinal/best_weights\n",
      "The loss after 744 iterations is E=0.010890859510362115\n",
      "Wrote weights to file ./customFunctionLearningAdaptive/baselineMLPFinal/best_weights\n",
      "The loss after 745 iterations is E=0.010885123306024229\n",
      "Wrote weights to file ./customFunctionLearningAdaptive/baselineMLPFinal/best_weights\n",
      "The loss after 746 iterations is E=0.010879803060549994\n",
      "Wrote weights to file ./customFunctionLearningAdaptive/baselineMLPFinal/best_weights\n",
      "The loss after 747 iterations is E=0.010874781660893926\n",
      "Wrote weights to file ./customFunctionLearningAdaptive/baselineMLPFinal/best_weights\n",
      "The loss after 748 iterations is E=0.010869773997581954\n",
      "Wrote weights to file ./customFunctionLearningAdaptive/baselineMLPFinal/best_weights\n",
      "The loss after 749 iterations is E=0.010864545611741827\n",
      "Wrote weights to file ./customFunctionLearningAdaptive/baselineMLPFinal/best_weights\n",
      "The loss after 750 iterations is E=0.010859064994925477\n",
      "Wrote weights to file ./customFunctionLearningAdaptive/baselineMLPFinal/best_weights\n",
      "The loss after 751 iterations is E=0.010853461851170922\n",
      "Wrote weights to file ./customFunctionLearningAdaptive/baselineMLPFinal/best_weights\n",
      "The loss after 752 iterations is E=0.010847924847610603\n",
      "Wrote weights to file ./customFunctionLearningAdaptive/baselineMLPFinal/best_weights\n",
      "The loss after 753 iterations is E=0.010842562803713233\n",
      "Wrote weights to file ./customFunctionLearningAdaptive/baselineMLPFinal/best_weights\n",
      "The loss after 754 iterations is E=0.010837362591477556\n",
      "Wrote weights to file ./customFunctionLearningAdaptive/baselineMLPFinal/best_weights\n",
      "The loss after 755 iterations is E=0.010832223533667305\n",
      "Wrote weights to file ./customFunctionLearningAdaptive/baselineMLPFinal/best_weights\n",
      "The loss after 756 iterations is E=0.010827033249028721\n",
      "Wrote weights to file ./customFunctionLearningAdaptive/baselineMLPFinal/best_weights\n",
      "The loss after 757 iterations is E=0.010821737687880991\n",
      "Wrote weights to file ./customFunctionLearningAdaptive/baselineMLPFinal/best_weights\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The loss after 758 iterations is E=0.010816350689641456\n",
      "Wrote weights to file ./customFunctionLearningAdaptive/baselineMLPFinal/best_weights\n",
      "The loss after 759 iterations is E=0.010810938015277972\n",
      "Wrote weights to file ./customFunctionLearningAdaptive/baselineMLPFinal/best_weights\n",
      "The loss after 760 iterations is E=0.01080556444579734\n",
      "Wrote weights to file ./customFunctionLearningAdaptive/baselineMLPFinal/best_weights\n",
      "The loss after 761 iterations is E=0.010800264992261544\n",
      "Wrote weights to file ./customFunctionLearningAdaptive/baselineMLPFinal/best_weights\n",
      "The loss after 762 iterations is E=0.010795032333963438\n",
      "Wrote weights to file ./customFunctionLearningAdaptive/baselineMLPFinal/best_weights\n",
      "The loss after 763 iterations is E=0.01078983082875837\n",
      "Wrote weights to file ./customFunctionLearningAdaptive/baselineMLPFinal/best_weights\n",
      "The loss after 764 iterations is E=0.010784620985209397\n",
      "Wrote weights to file ./customFunctionLearningAdaptive/baselineMLPFinal/best_weights\n",
      "The loss after 765 iterations is E=0.010779376662281988\n",
      "Wrote weights to file ./customFunctionLearningAdaptive/baselineMLPFinal/best_weights\n",
      "The loss after 766 iterations is E=0.010774096520436573\n",
      "Wrote weights to file ./customFunctionLearningAdaptive/baselineMLPFinal/best_weights\n",
      "The loss after 767 iterations is E=0.010768795097624915\n",
      "Wrote weights to file ./customFunctionLearningAdaptive/baselineMLPFinal/best_weights\n",
      "The loss after 768 iterations is E=0.010763496084641274\n",
      "Wrote weights to file ./customFunctionLearningAdaptive/baselineMLPFinal/best_weights\n",
      "The loss after 769 iterations is E=0.010758217564002862\n",
      "Wrote weights to file ./customFunctionLearningAdaptive/baselineMLPFinal/best_weights\n",
      "The loss after 770 iterations is E=0.010752967972863826\n",
      "Wrote weights to file ./customFunctionLearningAdaptive/baselineMLPFinal/best_weights\n",
      "The loss after 771 iterations is E=0.01074774428370777\n",
      "Wrote weights to file ./customFunctionLearningAdaptive/baselineMLPFinal/best_weights\n",
      "The loss after 772 iterations is E=0.010742536273325503\n",
      "Wrote weights to file ./customFunctionLearningAdaptive/baselineMLPFinal/best_weights\n",
      "The loss after 773 iterations is E=0.01073733245023194\n",
      "Wrote weights to file ./customFunctionLearningAdaptive/baselineMLPFinal/best_weights\n",
      "The loss after 774 iterations is E=0.010732123512699504\n",
      "Wrote weights to file ./customFunctionLearningAdaptive/baselineMLPFinal/best_weights\n",
      "The loss after 775 iterations is E=0.010726906312295007\n",
      "Wrote weights to file ./customFunctionLearningAdaptive/baselineMLPFinal/best_weights\n",
      "The loss after 776 iterations is E=0.010721681574419665\n",
      "Wrote weights to file ./customFunctionLearningAdaptive/baselineMLPFinal/best_weights\n",
      "The loss after 777 iterations is E=0.010716454463803085\n",
      "Wrote weights to file ./customFunctionLearningAdaptive/baselineMLPFinal/best_weights\n",
      "The loss after 778 iterations is E=0.01071123009238911\n",
      "Wrote weights to file ./customFunctionLearningAdaptive/baselineMLPFinal/best_weights\n",
      "The loss after 779 iterations is E=0.010706013527215454\n",
      "Wrote weights to file ./customFunctionLearningAdaptive/baselineMLPFinal/best_weights\n",
      "The loss after 780 iterations is E=0.010700807250496476\n",
      "Wrote weights to file ./customFunctionLearningAdaptive/baselineMLPFinal/best_weights\n",
      "The loss after 781 iterations is E=0.010695611991881298\n",
      "Wrote weights to file ./customFunctionLearningAdaptive/baselineMLPFinal/best_weights\n",
      "The loss after 782 iterations is E=0.010690426621093523\n",
      "Wrote weights to file ./customFunctionLearningAdaptive/baselineMLPFinal/best_weights\n",
      "The loss after 783 iterations is E=0.01068524908089987\n",
      "Wrote weights to file ./customFunctionLearningAdaptive/baselineMLPFinal/best_weights\n",
      "The loss after 784 iterations is E=0.010680077315687785\n",
      "Wrote weights to file ./customFunctionLearningAdaptive/baselineMLPFinal/best_weights\n",
      "The loss after 785 iterations is E=0.01067490932952982\n",
      "Wrote weights to file ./customFunctionLearningAdaptive/baselineMLPFinal/best_weights\n",
      "The loss after 786 iterations is E=0.01066974422166113\n",
      "Wrote weights to file ./customFunctionLearningAdaptive/baselineMLPFinal/best_weights\n",
      "The loss after 787 iterations is E=0.010664581249769859\n",
      "Wrote weights to file ./customFunctionLearningAdaptive/baselineMLPFinal/best_weights\n",
      "The loss after 788 iterations is E=0.010659420835541698\n",
      "Wrote weights to file ./customFunctionLearningAdaptive/baselineMLPFinal/best_weights\n",
      "The loss after 789 iterations is E=0.010654263143055355\n",
      "Wrote weights to file ./customFunctionLearningAdaptive/baselineMLPFinal/best_weights\n",
      "The loss after 790 iterations is E=0.0106491091270626\n",
      "Wrote weights to file ./customFunctionLearningAdaptive/baselineMLPFinal/best_weights\n",
      "The loss after 791 iterations is E=0.010643959148671032\n",
      "Wrote weights to file ./customFunctionLearningAdaptive/baselineMLPFinal/best_weights\n",
      "The loss after 792 iterations is E=0.010638814026256454\n",
      "Wrote weights to file ./customFunctionLearningAdaptive/baselineMLPFinal/best_weights\n",
      "The loss after 793 iterations is E=0.01063367393373391\n",
      "Wrote weights to file ./customFunctionLearningAdaptive/baselineMLPFinal/best_weights\n",
      "The loss after 794 iterations is E=0.010628539335928094\n",
      "Wrote weights to file ./customFunctionLearningAdaptive/baselineMLPFinal/best_weights\n",
      "The loss after 795 iterations is E=0.01062341019221992\n",
      "Wrote weights to file ./customFunctionLearningAdaptive/baselineMLPFinal/best_weights\n",
      "The loss after 796 iterations is E=0.010618286688211692\n",
      "Wrote weights to file ./customFunctionLearningAdaptive/baselineMLPFinal/best_weights\n",
      "The loss after 797 iterations is E=0.01061316868617228\n",
      "Wrote weights to file ./customFunctionLearningAdaptive/baselineMLPFinal/best_weights\n",
      "The loss after 798 iterations is E=0.010608056232991747\n",
      "Wrote weights to file ./customFunctionLearningAdaptive/baselineMLPFinal/best_weights\n",
      "The loss after 799 iterations is E=0.010602949198226899\n",
      "Wrote weights to file ./customFunctionLearningAdaptive/baselineMLPFinal/best_weights\n",
      "The loss after 800 iterations is E=0.010597847586464277\n",
      "Wrote weights to file ./customFunctionLearningAdaptive/baselineMLPFinal/best_weights\n",
      "The loss after 801 iterations is E=0.010592751320835696\n",
      "Wrote weights to file ./customFunctionLearningAdaptive/baselineMLPFinal/best_weights\n",
      "The loss after 802 iterations is E=0.010587660404055\n",
      "Wrote weights to file ./customFunctionLearningAdaptive/baselineMLPFinal/best_weights\n",
      "The loss after 803 iterations is E=0.010582574817200504\n",
      "Wrote weights to file ./customFunctionLearningAdaptive/baselineMLPFinal/best_weights\n",
      "The loss after 804 iterations is E=0.010577494569275264\n",
      "Wrote weights to file ./customFunctionLearningAdaptive/baselineMLPFinal/best_weights\n",
      "The loss after 805 iterations is E=0.010572419688540025\n",
      "Wrote weights to file ./customFunctionLearningAdaptive/baselineMLPFinal/best_weights\n",
      "The loss after 806 iterations is E=0.01056735018828128\n",
      "Wrote weights to file ./customFunctionLearningAdaptive/baselineMLPFinal/best_weights\n",
      "The loss after 807 iterations is E=0.010562286135712676\n",
      "Wrote weights to file ./customFunctionLearningAdaptive/baselineMLPFinal/best_weights\n",
      "The loss after 808 iterations is E=0.010557227548611159\n",
      "Wrote weights to file ./customFunctionLearningAdaptive/baselineMLPFinal/best_weights\n",
      "The loss after 809 iterations is E=0.010552174537162288\n",
      "Wrote weights to file ./customFunctionLearningAdaptive/baselineMLPFinal/best_weights\n",
      "The loss after 810 iterations is E=0.010547127134321252\n",
      "Wrote weights to file ./customFunctionLearningAdaptive/baselineMLPFinal/best_weights\n",
      "The loss after 811 iterations is E=0.010542085520456677\n",
      "Wrote weights to file ./customFunctionLearningAdaptive/baselineMLPFinal/best_weights\n",
      "The loss after 812 iterations is E=0.010537049779972421\n",
      "Wrote weights to file ./customFunctionLearningAdaptive/baselineMLPFinal/best_weights\n",
      "The loss after 813 iterations is E=0.010532020241392936\n",
      "Wrote weights to file ./customFunctionLearningAdaptive/baselineMLPFinal/best_weights\n",
      "The loss after 814 iterations is E=0.010526997145411486\n",
      "Wrote weights to file ./customFunctionLearningAdaptive/baselineMLPFinal/best_weights\n",
      "The loss after 815 iterations is E=0.010521981177152367\n",
      "Wrote weights to file ./customFunctionLearningAdaptive/baselineMLPFinal/best_weights\n",
      "The loss after 816 iterations is E=0.010516973034294223\n",
      "Wrote weights to file ./customFunctionLearningAdaptive/baselineMLPFinal/best_weights\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The loss after 817 iterations is E=0.01051197433296578\n",
      "Wrote weights to file ./customFunctionLearningAdaptive/baselineMLPFinal/best_weights\n",
      "The loss after 818 iterations is E=0.010506987108149297\n",
      "Wrote weights to file ./customFunctionLearningAdaptive/baselineMLPFinal/best_weights\n",
      "The loss after 819 iterations is E=0.010502015566120793\n",
      "Wrote weights to file ./customFunctionLearningAdaptive/baselineMLPFinal/best_weights\n",
      "The loss after 820 iterations is E=0.010497065728026313\n",
      "Wrote weights to file ./customFunctionLearningAdaptive/baselineMLPFinal/best_weights\n",
      "The loss after 821 iterations is E=0.010492149416868072\n",
      "Wrote weights to file ./customFunctionLearningAdaptive/baselineMLPFinal/best_weights\n",
      "The loss after 822 iterations is E=0.010487284849907418\n",
      "Wrote weights to file ./customFunctionLearningAdaptive/baselineMLPFinal/best_weights\n",
      "The loss after 823 iterations is E=0.010482507420054925\n",
      "Wrote weights to file ./customFunctionLearningAdaptive/baselineMLPFinal/best_weights\n",
      "The loss after 824 iterations is E=0.010477873741407729\n",
      "Wrote weights to file ./customFunctionLearningAdaptive/baselineMLPFinal/best_weights\n",
      "The loss after 825 iterations is E=0.010473495799801247\n",
      "Wrote weights to file ./customFunctionLearningAdaptive/baselineMLPFinal/best_weights\n",
      "The loss after 826 iterations is E=0.010469554399438164\n",
      "Wrote weights to file ./customFunctionLearningAdaptive/baselineMLPFinal/best_weights\n",
      "The loss after 827 iterations is E=0.010466422849384352\n",
      "Wrote weights to file ./customFunctionLearningAdaptive/baselineMLPFinal/best_weights\n",
      "The loss after 828 iterations is E=0.010464691790265294\n",
      "Wrote weights to file ./customFunctionLearningAdaptive/baselineMLPFinal/best_weights\n",
      "The loss after 843 iterations is E=0.01038779342180561\n",
      "Wrote weights to file ./customFunctionLearningAdaptive/baselineMLPFinal/best_weights\n",
      "The loss after 848 iterations is E=0.010374170902350077\n",
      "Wrote weights to file ./customFunctionLearningAdaptive/baselineMLPFinal/best_weights\n",
      "The loss after 853 iterations is E=0.010356936292924715\n",
      "Wrote weights to file ./customFunctionLearningAdaptive/baselineMLPFinal/best_weights\n",
      "The loss after 862 iterations is E=0.010308840305626024\n",
      "Wrote weights to file ./customFunctionLearningAdaptive/baselineMLPFinal/best_weights\n",
      "The loss after 871 iterations is E=0.010272068700498184\n",
      "Wrote weights to file ./customFunctionLearningAdaptive/baselineMLPFinal/best_weights\n",
      "The loss after 875 iterations is E=0.01026814497338429\n",
      "Wrote weights to file ./customFunctionLearningAdaptive/baselineMLPFinal/best_weights\n",
      "The loss after 876 iterations is E=0.01025885913344416\n",
      "Wrote weights to file ./customFunctionLearningAdaptive/baselineMLPFinal/best_weights\n",
      "The loss after 880 iterations is E=0.010237387486666178\n",
      "Wrote weights to file ./customFunctionLearningAdaptive/baselineMLPFinal/best_weights\n",
      "The loss after 884 iterations is E=0.010231837069713492\n",
      "Wrote weights to file ./customFunctionLearningAdaptive/baselineMLPFinal/best_weights\n",
      "The loss after 885 iterations is E=0.010216885133116499\n",
      "Wrote weights to file ./customFunctionLearningAdaptive/baselineMLPFinal/best_weights\n",
      "The loss after 889 iterations is E=0.010205362549070706\n",
      "Wrote weights to file ./customFunctionLearningAdaptive/baselineMLPFinal/best_weights\n",
      "The loss after 890 iterations is E=0.010197940998831147\n",
      "Wrote weights to file ./customFunctionLearningAdaptive/baselineMLPFinal/best_weights\n",
      "The loss after 893 iterations is E=0.010195663907797695\n",
      "Wrote weights to file ./customFunctionLearningAdaptive/baselineMLPFinal/best_weights\n",
      "The loss after 894 iterations is E=0.010183595436780032\n",
      "Wrote weights to file ./customFunctionLearningAdaptive/baselineMLPFinal/best_weights\n",
      "The loss after 895 iterations is E=0.010178673126834086\n",
      "Wrote weights to file ./customFunctionLearningAdaptive/baselineMLPFinal/best_weights\n",
      "The loss after 898 iterations is E=0.010172241518668504\n",
      "Wrote weights to file ./customFunctionLearningAdaptive/baselineMLPFinal/best_weights\n",
      "The loss after 899 iterations is E=0.01016358120101844\n",
      "Wrote weights to file ./customFunctionLearningAdaptive/baselineMLPFinal/best_weights\n",
      "The loss after 900 iterations is E=0.01015920501300538\n",
      "Wrote weights to file ./customFunctionLearningAdaptive/baselineMLPFinal/best_weights\n",
      "The loss after 901 iterations is E=0.0101585399041906\n",
      "Wrote weights to file ./customFunctionLearningAdaptive/baselineMLPFinal/best_weights\n",
      "The loss after 902 iterations is E=0.010156630834528486\n",
      "Wrote weights to file ./customFunctionLearningAdaptive/baselineMLPFinal/best_weights\n",
      "The loss after 903 iterations is E=0.01015098009132462\n",
      "Wrote weights to file ./customFunctionLearningAdaptive/baselineMLPFinal/best_weights\n",
      "The loss after 904 iterations is E=0.010144248492508652\n",
      "Wrote weights to file ./customFunctionLearningAdaptive/baselineMLPFinal/best_weights\n",
      "The loss after 905 iterations is E=0.010139840947962395\n",
      "Wrote weights to file ./customFunctionLearningAdaptive/baselineMLPFinal/best_weights\n",
      "The loss after 906 iterations is E=0.010137782022338483\n",
      "Wrote weights to file ./customFunctionLearningAdaptive/baselineMLPFinal/best_weights\n",
      "The loss after 907 iterations is E=0.010135349560450776\n",
      "Wrote weights to file ./customFunctionLearningAdaptive/baselineMLPFinal/best_weights\n",
      "The loss after 908 iterations is E=0.01013078915403526\n",
      "Wrote weights to file ./customFunctionLearningAdaptive/baselineMLPFinal/best_weights\n",
      "The loss after 909 iterations is E=0.010125241172901181\n",
      "Wrote weights to file ./customFunctionLearningAdaptive/baselineMLPFinal/best_weights\n",
      "The loss after 910 iterations is E=0.01012075020791906\n",
      "Wrote weights to file ./customFunctionLearningAdaptive/baselineMLPFinal/best_weights\n",
      "The loss after 911 iterations is E=0.010117785968544669\n",
      "Wrote weights to file ./customFunctionLearningAdaptive/baselineMLPFinal/best_weights\n",
      "The loss after 912 iterations is E=0.010115011328597094\n",
      "Wrote weights to file ./customFunctionLearningAdaptive/baselineMLPFinal/best_weights\n",
      "The loss after 913 iterations is E=0.01011114405258249\n",
      "Wrote weights to file ./customFunctionLearningAdaptive/baselineMLPFinal/best_weights\n",
      "The loss after 914 iterations is E=0.010106413508804205\n",
      "Wrote weights to file ./customFunctionLearningAdaptive/baselineMLPFinal/best_weights\n",
      "The loss after 915 iterations is E=0.010101956701176333\n",
      "Wrote weights to file ./customFunctionLearningAdaptive/baselineMLPFinal/best_weights\n",
      "The loss after 916 iterations is E=0.010098411480529236\n",
      "Wrote weights to file ./customFunctionLearningAdaptive/baselineMLPFinal/best_weights\n",
      "The loss after 917 iterations is E=0.010095309169267128\n",
      "Wrote weights to file ./customFunctionLearningAdaptive/baselineMLPFinal/best_weights\n",
      "The loss after 918 iterations is E=0.010091804716148674\n",
      "Wrote weights to file ./customFunctionLearningAdaptive/baselineMLPFinal/best_weights\n",
      "The loss after 919 iterations is E=0.010087671374222796\n",
      "Wrote weights to file ./customFunctionLearningAdaptive/baselineMLPFinal/best_weights\n",
      "The loss after 920 iterations is E=0.010083395044009883\n",
      "Wrote weights to file ./customFunctionLearningAdaptive/baselineMLPFinal/best_weights\n",
      "The loss after 921 iterations is E=0.010079533566883824\n",
      "Wrote weights to file ./customFunctionLearningAdaptive/baselineMLPFinal/best_weights\n",
      "The loss after 922 iterations is E=0.010076110286855932\n",
      "Wrote weights to file ./customFunctionLearningAdaptive/baselineMLPFinal/best_weights\n",
      "The loss after 923 iterations is E=0.01007270182276007\n",
      "Wrote weights to file ./customFunctionLearningAdaptive/baselineMLPFinal/best_weights\n",
      "The loss after 924 iterations is E=0.010068966364811091\n",
      "Wrote weights to file ./customFunctionLearningAdaptive/baselineMLPFinal/best_weights\n",
      "The loss after 925 iterations is E=0.010064964618921087\n",
      "Wrote weights to file ./customFunctionLearningAdaptive/baselineMLPFinal/best_weights\n",
      "The loss after 926 iterations is E=0.010061024615468886\n",
      "Wrote weights to file ./customFunctionLearningAdaptive/baselineMLPFinal/best_weights\n",
      "The loss after 927 iterations is E=0.010057353454519845\n",
      "Wrote weights to file ./customFunctionLearningAdaptive/baselineMLPFinal/best_weights\n",
      "The loss after 928 iterations is E=0.01005386186104185\n",
      "Wrote weights to file ./customFunctionLearningAdaptive/baselineMLPFinal/best_weights\n",
      "The loss after 929 iterations is E=0.010050312313546412\n",
      "Wrote weights to file ./customFunctionLearningAdaptive/baselineMLPFinal/best_weights\n",
      "The loss after 930 iterations is E=0.010046575808396725\n",
      "Wrote weights to file ./customFunctionLearningAdaptive/baselineMLPFinal/best_weights\n",
      "The loss after 931 iterations is E=0.010042736278403505\n",
      "Wrote weights to file ./customFunctionLearningAdaptive/baselineMLPFinal/best_weights\n",
      "The loss after 932 iterations is E=0.010038962618448324\n",
      "Wrote weights to file ./customFunctionLearningAdaptive/baselineMLPFinal/best_weights\n",
      "The loss after 933 iterations is E=0.010035336322977047\n",
      "Wrote weights to file ./customFunctionLearningAdaptive/baselineMLPFinal/best_weights\n",
      "The loss after 934 iterations is E=0.010031793094430249\n",
      "Wrote weights to file ./customFunctionLearningAdaptive/baselineMLPFinal/best_weights\n",
      "The loss after 935 iterations is E=0.010028211674399897\n",
      "Wrote weights to file ./customFunctionLearningAdaptive/baselineMLPFinal/best_weights\n",
      "The loss after 936 iterations is E=0.0100245365706775\n",
      "Wrote weights to file ./customFunctionLearningAdaptive/baselineMLPFinal/best_weights\n",
      "The loss after 937 iterations is E=0.01002081157814285\n",
      "Wrote weights to file ./customFunctionLearningAdaptive/baselineMLPFinal/best_weights\n",
      "The loss after 938 iterations is E=0.010017122415731194\n",
      "Wrote weights to file ./customFunctionLearningAdaptive/baselineMLPFinal/best_weights\n",
      "The loss after 939 iterations is E=0.010013511474358333\n",
      "Wrote weights to file ./customFunctionLearningAdaptive/baselineMLPFinal/best_weights\n",
      "The loss after 940 iterations is E=0.010009951005770451\n",
      "Wrote weights to file ./customFunctionLearningAdaptive/baselineMLPFinal/best_weights\n",
      "The loss after 941 iterations is E=0.010006381594967261\n",
      "Wrote weights to file ./customFunctionLearningAdaptive/baselineMLPFinal/best_weights\n",
      "The loss after 942 iterations is E=0.01000276813343933\n",
      "Wrote weights to file ./customFunctionLearningAdaptive/baselineMLPFinal/best_weights\n",
      "The loss after 943 iterations is E=0.009999124979185306\n",
      "Using every 8th point for                         training.\n",
      "Selecting subset for training: 337 out of 2700 selected.\n",
      "Wrote weights to file ./customFunctionLearningAdaptive/baselineMLPFinal/best_weights\n",
      "The loss after 944 iterations is E=0.007171432948280287\n",
      "Using every 7th point for                         training.\n",
      "Selecting subset for training: 385 out of 2700 selected.\n",
      "Wrote weights to file ./customFunctionLearningAdaptive/baselineMLPFinal/best_weights\n",
      "The loss after 945 iterations is E=0.03759345229406363\n",
      "Wrote weights to file ./customFunctionLearningAdaptive/baselineMLPFinal/best_weights\n",
      "The loss after 953 iterations is E=0.009357507237771684\n",
      "Using every 6th point for                         training.\n",
      "Selecting subset for training: 450 out of 2700 selected.\n",
      "Wrote weights to file ./customFunctionLearningAdaptive/baselineMLPFinal/best_weights\n",
      "The loss after 954 iterations is E=0.11516772770142729\n",
      "Wrote weights to file ./customFunctionLearningAdaptive/baselineMLPFinal/best_weights\n",
      "The loss after 955 iterations is E=0.013967082964396412\n",
      "Wrote weights to file ./customFunctionLearningAdaptive/baselineMLPFinal/best_weights\n",
      "The loss after 979 iterations is E=0.01377062754857972\n",
      "Wrote weights to file ./customFunctionLearningAdaptive/baselineMLPFinal/best_weights\n",
      "The loss after 984 iterations is E=0.013642569196371294\n",
      "Wrote weights to file ./customFunctionLearningAdaptive/baselineMLPFinal/best_weights\n",
      "The loss after 989 iterations is E=0.013595851274888625\n",
      "Wrote weights to file ./customFunctionLearningAdaptive/baselineMLPFinal/best_weights\n",
      "The loss after 994 iterations is E=0.013566207392789178\n",
      "Wrote weights to file ./customFunctionLearningAdaptive/baselineMLPFinal/best_weights\n",
      "The loss after 999 iterations is E=0.013540849265515187\n",
      "Wrote weights to file ./customFunctionLearningAdaptive/baselineMLPFinal/best_weights\n",
      "The loss after 1004 iterations is E=0.013516493095572575\n",
      "Wrote weights to file ./customFunctionLearningAdaptive/baselineMLPFinal/best_weights\n",
      "The loss after 1006 iterations is E=0.01347374339691089\n",
      "Wrote weights to file ./customFunctionLearningAdaptive/baselineMLPFinal/best_weights\n",
      "The loss after 1011 iterations is E=0.01342704626774487\n",
      "Wrote weights to file ./customFunctionLearningAdaptive/baselineMLPFinal/best_weights\n",
      "The loss after 1016 iterations is E=0.013399528449503425\n",
      "Wrote weights to file ./customFunctionLearningAdaptive/baselineMLPFinal/best_weights\n",
      "The loss after 1021 iterations is E=0.013380529457766596\n",
      "Wrote weights to file ./customFunctionLearningAdaptive/baselineMLPFinal/best_weights\n",
      "The loss after 1023 iterations is E=0.013377642552678875\n",
      "Wrote weights to file ./customFunctionLearningAdaptive/baselineMLPFinal/best_weights\n",
      "The loss after 1026 iterations is E=0.01336474185449654\n",
      "Wrote weights to file ./customFunctionLearningAdaptive/baselineMLPFinal/best_weights\n",
      "The loss after 1028 iterations is E=0.013353034881695109\n",
      "Wrote weights to file ./customFunctionLearningAdaptive/baselineMLPFinal/best_weights\n",
      "The loss after 1031 iterations is E=0.013349547731423281\n",
      "Wrote weights to file ./customFunctionLearningAdaptive/baselineMLPFinal/best_weights\n",
      "The loss after 1033 iterations is E=0.013335590776795397\n",
      "Wrote weights to file ./customFunctionLearningAdaptive/baselineMLPFinal/best_weights\n",
      "The loss after 1035 iterations is E=0.01333395690364381\n",
      "Wrote weights to file ./customFunctionLearningAdaptive/baselineMLPFinal/best_weights\n",
      "The loss after 1038 iterations is E=0.013321333406466676\n",
      "Wrote weights to file ./customFunctionLearningAdaptive/baselineMLPFinal/best_weights\n",
      "The loss after 1040 iterations is E=0.013316000137118346\n",
      "Wrote weights to file ./customFunctionLearningAdaptive/baselineMLPFinal/best_weights\n",
      "The loss after 1042 iterations is E=0.013314320116396806\n",
      "Wrote weights to file ./customFunctionLearningAdaptive/baselineMLPFinal/best_weights\n",
      "The loss after 1043 iterations is E=0.013308419340225159\n",
      "Wrote weights to file ./customFunctionLearningAdaptive/baselineMLPFinal/best_weights\n",
      "The loss after 1045 iterations is E=0.013301761657295016\n",
      "Wrote weights to file ./customFunctionLearningAdaptive/baselineMLPFinal/best_weights\n",
      "The loss after 1047 iterations is E=0.013298305432626433\n",
      "Wrote weights to file ./customFunctionLearningAdaptive/baselineMLPFinal/best_weights\n",
      "The loss after 1048 iterations is E=0.01329609338916151\n",
      "Wrote weights to file ./customFunctionLearningAdaptive/baselineMLPFinal/best_weights\n",
      "The loss after 1049 iterations is E=0.013295119033762974\n",
      "Wrote weights to file ./customFunctionLearningAdaptive/baselineMLPFinal/best_weights\n",
      "The loss after 1050 iterations is E=0.013289482780075796\n",
      "Wrote weights to file ./customFunctionLearningAdaptive/baselineMLPFinal/best_weights\n",
      "The loss after 1052 iterations is E=0.013285012741891365\n",
      "Wrote weights to file ./customFunctionLearningAdaptive/baselineMLPFinal/best_weights\n",
      "The loss after 1053 iterations is E=0.013284141984166544\n",
      "Wrote weights to file ./customFunctionLearningAdaptive/baselineMLPFinal/best_weights\n",
      "The loss after 1054 iterations is E=0.013281484888553822\n",
      "Wrote weights to file ./customFunctionLearningAdaptive/baselineMLPFinal/best_weights\n",
      "The loss after 1055 iterations is E=0.013278199708603458\n",
      "Wrote weights to file ./customFunctionLearningAdaptive/baselineMLPFinal/best_weights\n",
      "The loss after 1056 iterations is E=0.01327747904287386\n",
      "Wrote weights to file ./customFunctionLearningAdaptive/baselineMLPFinal/best_weights\n",
      "The loss after 1057 iterations is E=0.013273394968542818\n",
      "Wrote weights to file ./customFunctionLearningAdaptive/baselineMLPFinal/best_weights\n",
      "The loss after 1058 iterations is E=0.013272616783928869\n",
      "Wrote weights to file ./customFunctionLearningAdaptive/baselineMLPFinal/best_weights\n",
      "The loss after 1059 iterations is E=0.013269512832646258\n",
      "Wrote weights to file ./customFunctionLearningAdaptive/baselineMLPFinal/best_weights\n",
      "The loss after 1060 iterations is E=0.013267487164246925\n",
      "Wrote weights to file ./customFunctionLearningAdaptive/baselineMLPFinal/best_weights\n",
      "The loss after 1061 iterations is E=0.013265766845610502\n",
      "Wrote weights to file ./customFunctionLearningAdaptive/baselineMLPFinal/best_weights\n",
      "The loss after 1062 iterations is E=0.01326280219021196\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wrote weights to file ./customFunctionLearningAdaptive/baselineMLPFinal/best_weights\n",
      "The loss after 1063 iterations is E=0.013261644549884275\n",
      "Wrote weights to file ./customFunctionLearningAdaptive/baselineMLPFinal/best_weights\n",
      "The loss after 1064 iterations is E=0.013258757863000788\n",
      "Wrote weights to file ./customFunctionLearningAdaptive/baselineMLPFinal/best_weights\n",
      "The loss after 1065 iterations is E=0.013257218521859754\n",
      "Wrote weights to file ./customFunctionLearningAdaptive/baselineMLPFinal/best_weights\n",
      "The loss after 1066 iterations is E=0.013255054400562091\n",
      "Wrote weights to file ./customFunctionLearningAdaptive/baselineMLPFinal/best_weights\n",
      "The loss after 1067 iterations is E=0.013252870860069663\n",
      "Wrote weights to file ./customFunctionLearningAdaptive/baselineMLPFinal/best_weights\n",
      "The loss after 1068 iterations is E=0.013251310203241348\n",
      "Wrote weights to file ./customFunctionLearningAdaptive/baselineMLPFinal/best_weights\n",
      "The loss after 1069 iterations is E=0.013248867941910233\n",
      "Wrote weights to file ./customFunctionLearningAdaptive/baselineMLPFinal/best_weights\n",
      "The loss after 1070 iterations is E=0.013247391105375317\n",
      "Wrote weights to file ./customFunctionLearningAdaptive/baselineMLPFinal/best_weights\n",
      "The loss after 1071 iterations is E=0.01324518483167471\n",
      "Wrote weights to file ./customFunctionLearningAdaptive/baselineMLPFinal/best_weights\n",
      "The loss after 1072 iterations is E=0.013243423797067427\n",
      "Wrote weights to file ./customFunctionLearningAdaptive/baselineMLPFinal/best_weights\n",
      "The loss after 1073 iterations is E=0.013241625015760207\n",
      "Wrote weights to file ./customFunctionLearningAdaptive/baselineMLPFinal/best_weights\n",
      "The loss after 1074 iterations is E=0.013239597645372124\n",
      "Wrote weights to file ./customFunctionLearningAdaptive/baselineMLPFinal/best_weights\n",
      "The loss after 1075 iterations is E=0.01323803268839244\n",
      "Wrote weights to file ./customFunctionLearningAdaptive/baselineMLPFinal/best_weights\n",
      "The loss after 1076 iterations is E=0.013235990574907559\n",
      "Wrote weights to file ./customFunctionLearningAdaptive/baselineMLPFinal/best_weights\n",
      "The loss after 1077 iterations is E=0.013234396587994158\n",
      "Wrote weights to file ./customFunctionLearningAdaptive/baselineMLPFinal/best_weights\n",
      "The loss after 1078 iterations is E=0.013232545408802409\n",
      "Wrote weights to file ./customFunctionLearningAdaptive/baselineMLPFinal/best_weights\n",
      "The loss after 1079 iterations is E=0.013230803573600836\n",
      "Wrote weights to file ./customFunctionLearningAdaptive/baselineMLPFinal/best_weights\n",
      "The loss after 1080 iterations is E=0.01322916001825494\n",
      "Wrote weights to file ./customFunctionLearningAdaptive/baselineMLPFinal/best_weights\n",
      "The loss after 1081 iterations is E=0.01322733430116098\n",
      "Wrote weights to file ./customFunctionLearningAdaptive/baselineMLPFinal/best_weights\n",
      "The loss after 1082 iterations is E=0.013225779318903274\n",
      "Wrote weights to file ./customFunctionLearningAdaptive/baselineMLPFinal/best_weights\n",
      "The loss after 1083 iterations is E=0.013224001806760808\n",
      "Wrote weights to file ./customFunctionLearningAdaptive/baselineMLPFinal/best_weights\n",
      "The loss after 1084 iterations is E=0.013222417779378574\n",
      "Wrote weights to file ./customFunctionLearningAdaptive/baselineMLPFinal/best_weights\n",
      "The loss after 1085 iterations is E=0.013220764202530369\n",
      "Wrote weights to file ./customFunctionLearningAdaptive/baselineMLPFinal/best_weights\n",
      "The loss after 1086 iterations is E=0.01321912172628363\n",
      "Wrote weights to file ./customFunctionLearningAdaptive/baselineMLPFinal/best_weights\n",
      "The loss after 1087 iterations is E=0.013217574040263375\n",
      "Wrote weights to file ./customFunctionLearningAdaptive/baselineMLPFinal/best_weights\n",
      "The loss after 1088 iterations is E=0.013215921565384897\n",
      "Wrote weights to file ./customFunctionLearningAdaptive/baselineMLPFinal/best_weights\n",
      "The loss after 1089 iterations is E=0.01321441379054198\n",
      "Wrote weights to file ./customFunctionLearningAdaptive/baselineMLPFinal/best_weights\n",
      "The loss after 1090 iterations is E=0.013212813585333605\n",
      "Wrote weights to file ./customFunctionLearningAdaptive/baselineMLPFinal/best_weights\n",
      "The loss after 1091 iterations is E=0.013211296547248858\n",
      "Wrote weights to file ./customFunctionLearningAdaptive/baselineMLPFinal/best_weights\n",
      "The loss after 1092 iterations is E=0.013209773513317478\n",
      "Wrote weights to file ./customFunctionLearningAdaptive/baselineMLPFinal/best_weights\n",
      "The loss after 1093 iterations is E=0.01320824397270119\n",
      "Wrote weights to file ./customFunctionLearningAdaptive/baselineMLPFinal/best_weights\n",
      "The loss after 1094 iterations is E=0.013206780218256721\n",
      "Wrote weights to file ./customFunctionLearningAdaptive/baselineMLPFinal/best_weights\n",
      "The loss after 1095 iterations is E=0.0132052663196949\n",
      "Wrote weights to file ./customFunctionLearningAdaptive/baselineMLPFinal/best_weights\n",
      "The loss after 1096 iterations is E=0.013203828629098423\n",
      "Wrote weights to file ./customFunctionLearningAdaptive/baselineMLPFinal/best_weights\n",
      "The loss after 1097 iterations is E=0.013202357975675351\n",
      "Wrote weights to file ./customFunctionLearningAdaptive/baselineMLPFinal/best_weights\n",
      "The loss after 1098 iterations is E=0.013200926355442517\n",
      "Wrote weights to file ./customFunctionLearningAdaptive/baselineMLPFinal/best_weights\n",
      "The loss after 1099 iterations is E=0.013199506095923528\n",
      "Wrote weights to file ./customFunctionLearningAdaptive/baselineMLPFinal/best_weights\n",
      "The loss after 1100 iterations is E=0.013198082704059672\n",
      "Wrote weights to file ./customFunctionLearningAdaptive/baselineMLPFinal/best_weights\n",
      "The loss after 1101 iterations is E=0.01319670126528532\n",
      "Wrote weights to file ./customFunctionLearningAdaptive/baselineMLPFinal/best_weights\n",
      "The loss after 1102 iterations is E=0.013195300497923093\n",
      "Wrote weights to file ./customFunctionLearningAdaptive/baselineMLPFinal/best_weights\n",
      "The loss after 1103 iterations is E=0.013193941830446917\n",
      "Wrote weights to file ./customFunctionLearningAdaptive/baselineMLPFinal/best_weights\n",
      "The loss after 1104 iterations is E=0.013192575536625784\n",
      "Wrote weights to file ./customFunctionLearningAdaptive/baselineMLPFinal/best_weights\n",
      "The loss after 1105 iterations is E=0.013191231362787381\n",
      "Wrote weights to file ./customFunctionLearningAdaptive/baselineMLPFinal/best_weights\n",
      "The loss after 1106 iterations is E=0.013189901139842945\n",
      "Wrote weights to file ./customFunctionLearningAdaptive/baselineMLPFinal/best_weights\n",
      "The loss after 1107 iterations is E=0.013188573410614772\n",
      "Wrote weights to file ./customFunctionLearningAdaptive/baselineMLPFinal/best_weights\n",
      "The loss after 1108 iterations is E=0.013187272744032493\n",
      "Wrote weights to file ./customFunctionLearningAdaptive/baselineMLPFinal/best_weights\n",
      "The loss after 1109 iterations is E=0.013185968285846412\n",
      "Wrote weights to file ./customFunctionLearningAdaptive/baselineMLPFinal/best_weights\n",
      "The loss after 1110 iterations is E=0.013184689444976241\n",
      "Wrote weights to file ./customFunctionLearningAdaptive/baselineMLPFinal/best_weights\n",
      "The loss after 1111 iterations is E=0.013183413217641434\n",
      "Wrote weights to file ./customFunctionLearningAdaptive/baselineMLPFinal/best_weights\n",
      "The loss after 1112 iterations is E=0.013182152474317978\n",
      "Wrote weights to file ./customFunctionLearningAdaptive/baselineMLPFinal/best_weights\n",
      "The loss after 1113 iterations is E=0.013180904558987861\n",
      "Wrote weights to file ./customFunctionLearningAdaptive/baselineMLPFinal/best_weights\n",
      "The loss after 1114 iterations is E=0.013179662885476444\n",
      "Wrote weights to file ./customFunctionLearningAdaptive/baselineMLPFinal/best_weights\n",
      "The loss after 1115 iterations is E=0.013178439805319828\n",
      "Wrote weights to file ./customFunctionLearningAdaptive/baselineMLPFinal/best_weights\n",
      "The loss after 1116 iterations is E=0.01317722019037498\n",
      "Wrote weights to file ./customFunctionLearningAdaptive/baselineMLPFinal/best_weights\n",
      "The loss after 1117 iterations is E=0.01317601810540444\n",
      "Wrote weights to file ./customFunctionLearningAdaptive/baselineMLPFinal/best_weights\n",
      "The loss after 1118 iterations is E=0.013174822558324605\n",
      "Wrote weights to file ./customFunctionLearningAdaptive/baselineMLPFinal/best_weights\n",
      "The loss after 1119 iterations is E=0.013173639563683538\n",
      "Wrote weights to file ./customFunctionLearningAdaptive/baselineMLPFinal/best_weights\n",
      "The loss after 1120 iterations is E=0.013172467830234109\n",
      "Wrote weights to file ./customFunctionLearningAdaptive/baselineMLPFinal/best_weights\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The loss after 1121 iterations is E=0.013171304160084692\n",
      "Wrote weights to file ./customFunctionLearningAdaptive/baselineMLPFinal/best_weights\n",
      "The loss after 1122 iterations is E=0.01317015437841712\n",
      "Wrote weights to file ./customFunctionLearningAdaptive/baselineMLPFinal/best_weights\n",
      "The loss after 1123 iterations is E=0.013169011195753767\n",
      "Wrote weights to file ./customFunctionLearningAdaptive/baselineMLPFinal/best_weights\n",
      "The loss after 1124 iterations is E=0.013167881350218666\n",
      "Wrote weights to file ./customFunctionLearningAdaptive/baselineMLPFinal/best_weights\n",
      "The loss after 1125 iterations is E=0.013166759373389141\n",
      "Wrote weights to file ./customFunctionLearningAdaptive/baselineMLPFinal/best_weights\n",
      "The loss after 1126 iterations is E=0.013165648317260286\n",
      "Wrote weights to file ./customFunctionLearningAdaptive/baselineMLPFinal/best_weights\n",
      "The loss after 1127 iterations is E=0.013164547245435346\n",
      "Wrote weights to file ./customFunctionLearningAdaptive/baselineMLPFinal/best_weights\n",
      "The loss after 1128 iterations is E=0.013163454829580753\n",
      "Wrote weights to file ./customFunctionLearningAdaptive/baselineMLPFinal/best_weights\n",
      "The loss after 1129 iterations is E=0.013162373612545884\n",
      "Wrote weights to file ./customFunctionLearningAdaptive/baselineMLPFinal/best_weights\n",
      "The loss after 1130 iterations is E=0.013161300139426846\n",
      "Wrote weights to file ./customFunctionLearningAdaptive/baselineMLPFinal/best_weights\n",
      "The loss after 1131 iterations is E=0.01316023761857897\n",
      "Wrote weights to file ./customFunctionLearningAdaptive/baselineMLPFinal/best_weights\n",
      "The loss after 1132 iterations is E=0.013159183236910753\n",
      "Wrote weights to file ./customFunctionLearningAdaptive/baselineMLPFinal/best_weights\n",
      "The loss after 1133 iterations is E=0.01315813863211076\n",
      "Wrote weights to file ./customFunctionLearningAdaptive/baselineMLPFinal/best_weights\n",
      "The loss after 1134 iterations is E=0.013157103040964341\n",
      "Wrote weights to file ./customFunctionLearningAdaptive/baselineMLPFinal/best_weights\n",
      "The loss after 1135 iterations is E=0.013156076027752903\n",
      "Wrote weights to file ./customFunctionLearningAdaptive/baselineMLPFinal/best_weights\n",
      "The loss after 1136 iterations is E=0.0131550585703347\n",
      "Wrote weights to file ./customFunctionLearningAdaptive/baselineMLPFinal/best_weights\n",
      "The loss after 1137 iterations is E=0.01315404907162281\n",
      "Wrote weights to file ./customFunctionLearningAdaptive/baselineMLPFinal/best_weights\n",
      "The loss after 1138 iterations is E=0.013153049012954098\n",
      "Wrote weights to file ./customFunctionLearningAdaptive/baselineMLPFinal/best_weights\n",
      "The loss after 1139 iterations is E=0.013152056914994938\n",
      "Wrote weights to file ./customFunctionLearningAdaptive/baselineMLPFinal/best_weights\n",
      "The loss after 1140 iterations is E=0.013151073666811203\n",
      "Wrote weights to file ./customFunctionLearningAdaptive/baselineMLPFinal/best_weights\n",
      "The loss after 1141 iterations is E=0.01315009867241425\n",
      "Wrote weights to file ./customFunctionLearningAdaptive/baselineMLPFinal/best_weights\n",
      "The loss after 1142 iterations is E=0.013149131861385437\n",
      "Wrote weights to file ./customFunctionLearningAdaptive/baselineMLPFinal/best_weights\n",
      "The loss after 1143 iterations is E=0.013148173504529988\n",
      "Wrote weights to file ./customFunctionLearningAdaptive/baselineMLPFinal/best_weights\n",
      "The loss after 1144 iterations is E=0.013147222888692994\n",
      "Wrote weights to file ./customFunctionLearningAdaptive/baselineMLPFinal/best_weights\n",
      "The loss after 1145 iterations is E=0.01314628064706328\n",
      "Wrote weights to file ./customFunctionLearningAdaptive/baselineMLPFinal/best_weights\n",
      "The loss after 1146 iterations is E=0.013145345998430077\n",
      "Wrote weights to file ./customFunctionLearningAdaptive/baselineMLPFinal/best_weights\n",
      "The loss after 1147 iterations is E=0.013144419403705847\n",
      "Wrote weights to file ./customFunctionLearningAdaptive/baselineMLPFinal/best_weights\n",
      "The loss after 1148 iterations is E=0.013143500424929982\n",
      "Wrote weights to file ./customFunctionLearningAdaptive/baselineMLPFinal/best_weights\n",
      "The loss after 1149 iterations is E=0.013142589103075062\n",
      "Wrote weights to file ./customFunctionLearningAdaptive/baselineMLPFinal/best_weights\n",
      "The loss after 1150 iterations is E=0.013141685421183772\n",
      "Wrote weights to file ./customFunctionLearningAdaptive/baselineMLPFinal/best_weights\n",
      "The loss after 1151 iterations is E=0.01314078907401486\n",
      "Wrote weights to file ./customFunctionLearningAdaptive/baselineMLPFinal/best_weights\n",
      "The loss after 1152 iterations is E=0.013139900281787676\n",
      "Wrote weights to file ./customFunctionLearningAdaptive/baselineMLPFinal/best_weights\n",
      "The loss after 1153 iterations is E=0.013139018632693424\n",
      "Wrote weights to file ./customFunctionLearningAdaptive/baselineMLPFinal/best_weights\n",
      "The loss after 1154 iterations is E=0.013138144337240561\n",
      "Wrote weights to file ./customFunctionLearningAdaptive/baselineMLPFinal/best_weights\n",
      "The loss after 1155 iterations is E=0.013137277092614667\n",
      "Wrote weights to file ./customFunctionLearningAdaptive/baselineMLPFinal/best_weights\n",
      "The loss after 1156 iterations is E=0.01313641694448996\n",
      "Wrote weights to file ./customFunctionLearningAdaptive/baselineMLPFinal/best_weights\n",
      "The loss after 1157 iterations is E=0.013135563779798954\n",
      "Wrote weights to file ./customFunctionLearningAdaptive/baselineMLPFinal/best_weights\n",
      "The loss after 1158 iterations is E=0.013134717468874629\n",
      "Wrote weights to file ./customFunctionLearningAdaptive/baselineMLPFinal/best_weights\n",
      "The loss after 1159 iterations is E=0.013133878041465742\n",
      "Wrote weights to file ./customFunctionLearningAdaptive/baselineMLPFinal/best_weights\n",
      "The loss after 1160 iterations is E=0.013133045279974716\n",
      "Wrote weights to file ./customFunctionLearningAdaptive/baselineMLPFinal/best_weights\n",
      "The loss after 1161 iterations is E=0.013132219250615062\n",
      "Wrote weights to file ./customFunctionLearningAdaptive/baselineMLPFinal/best_weights\n",
      "The loss after 1162 iterations is E=0.013131399751374232\n",
      "Wrote weights to file ./customFunctionLearningAdaptive/baselineMLPFinal/best_weights\n",
      "The loss after 1163 iterations is E=0.013130586798567866\n",
      "Wrote weights to file ./customFunctionLearningAdaptive/baselineMLPFinal/best_weights\n",
      "The loss after 1164 iterations is E=0.013129780266346432\n",
      "Wrote weights to file ./customFunctionLearningAdaptive/baselineMLPFinal/best_weights\n",
      "The loss after 1165 iterations is E=0.013128980091298462\n",
      "Wrote weights to file ./customFunctionLearningAdaptive/baselineMLPFinal/best_weights\n",
      "The loss after 1166 iterations is E=0.013128186223731245\n",
      "Wrote weights to file ./customFunctionLearningAdaptive/baselineMLPFinal/best_weights\n",
      "The loss after 1167 iterations is E=0.013127398543068878\n",
      "Wrote weights to file ./customFunctionLearningAdaptive/baselineMLPFinal/best_weights\n",
      "The loss after 1168 iterations is E=0.013126617038485713\n",
      "Wrote weights to file ./customFunctionLearningAdaptive/baselineMLPFinal/best_weights\n",
      "The loss after 1169 iterations is E=0.013125841576841766\n",
      "Wrote weights to file ./customFunctionLearningAdaptive/baselineMLPFinal/best_weights\n",
      "The loss after 1170 iterations is E=0.013125072142466297\n",
      "Wrote weights to file ./customFunctionLearningAdaptive/baselineMLPFinal/best_weights\n",
      "The loss after 1171 iterations is E=0.013124308625283743\n",
      "Wrote weights to file ./customFunctionLearningAdaptive/baselineMLPFinal/best_weights\n",
      "The loss after 1172 iterations is E=0.013123550980069742\n",
      "Wrote weights to file ./customFunctionLearningAdaptive/baselineMLPFinal/best_weights\n",
      "The loss after 1173 iterations is E=0.01312279913268666\n",
      "Wrote weights to file ./customFunctionLearningAdaptive/baselineMLPFinal/best_weights\n",
      "The loss after 1174 iterations is E=0.013122053007451902\n",
      "Wrote weights to file ./customFunctionLearningAdaptive/baselineMLPFinal/best_weights\n",
      "The loss after 1175 iterations is E=0.013121312557015832\n",
      "Wrote weights to file ./customFunctionLearningAdaptive/baselineMLPFinal/best_weights\n",
      "The loss after 1176 iterations is E=0.01312057769027585\n",
      "Wrote weights to file ./customFunctionLearningAdaptive/baselineMLPFinal/best_weights\n",
      "The loss after 1177 iterations is E=0.013119848368880936\n",
      "Wrote weights to file ./customFunctionLearningAdaptive/baselineMLPFinal/best_weights\n",
      "The loss after 1178 iterations is E=0.013119124504364976\n",
      "Wrote weights to file ./customFunctionLearningAdaptive/baselineMLPFinal/best_weights\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The loss after 1179 iterations is E=0.013118406051811648\n",
      "Wrote weights to file ./customFunctionLearningAdaptive/baselineMLPFinal/best_weights\n",
      "The loss after 1180 iterations is E=0.013117692936217574\n",
      "Wrote weights to file ./customFunctionLearningAdaptive/baselineMLPFinal/best_weights\n",
      "The loss after 1181 iterations is E=0.013116985100032758\n",
      "Wrote weights to file ./customFunctionLearningAdaptive/baselineMLPFinal/best_weights\n",
      "The loss after 1182 iterations is E=0.013116282483332839\n",
      "Wrote weights to file ./customFunctionLearningAdaptive/baselineMLPFinal/best_weights\n",
      "The loss after 1183 iterations is E=0.013115585018489691\n",
      "Wrote weights to file ./customFunctionLearningAdaptive/baselineMLPFinal/best_weights\n",
      "The loss after 1184 iterations is E=0.013114892654934089\n",
      "Wrote weights to file ./customFunctionLearningAdaptive/baselineMLPFinal/best_weights\n",
      "The loss after 1185 iterations is E=0.013114205321892848\n",
      "Wrote weights to file ./customFunctionLearningAdaptive/baselineMLPFinal/best_weights\n",
      "The loss after 1186 iterations is E=0.013113522971001311\n",
      "Wrote weights to file ./customFunctionLearningAdaptive/baselineMLPFinal/best_weights\n",
      "The loss after 1187 iterations is E=0.013112845535003992\n",
      "Wrote weights to file ./customFunctionLearningAdaptive/baselineMLPFinal/best_weights\n",
      "The loss after 1188 iterations is E=0.013112172962472348\n",
      "Wrote weights to file ./customFunctionLearningAdaptive/baselineMLPFinal/best_weights\n",
      "The loss after 1189 iterations is E=0.013111505192770467\n",
      "Wrote weights to file ./customFunctionLearningAdaptive/baselineMLPFinal/best_weights\n",
      "The loss after 1190 iterations is E=0.0131108421700743\n",
      "Wrote weights to file ./customFunctionLearningAdaptive/baselineMLPFinal/best_weights\n",
      "The loss after 1191 iterations is E=0.013110183840165294\n",
      "Wrote weights to file ./customFunctionLearningAdaptive/baselineMLPFinal/best_weights\n",
      "The loss after 1192 iterations is E=0.013109530144441028\n",
      "Wrote weights to file ./customFunctionLearningAdaptive/baselineMLPFinal/best_weights\n",
      "The loss after 1193 iterations is E=0.013108881032449803\n",
      "Wrote weights to file ./customFunctionLearningAdaptive/baselineMLPFinal/best_weights\n",
      "The loss after 1194 iterations is E=0.013108236445586787\n",
      "Wrote weights to file ./customFunctionLearningAdaptive/baselineMLPFinal/best_weights\n",
      "The loss after 1195 iterations is E=0.013107596334488263\n",
      "Wrote weights to file ./customFunctionLearningAdaptive/baselineMLPFinal/best_weights\n",
      "The loss after 1196 iterations is E=0.013106960642949967\n",
      "Wrote weights to file ./customFunctionLearningAdaptive/baselineMLPFinal/best_weights\n",
      "The loss after 1197 iterations is E=0.013106329320873376\n",
      "Wrote weights to file ./customFunctionLearningAdaptive/baselineMLPFinal/best_weights\n",
      "The loss after 1198 iterations is E=0.013105702315351306\n",
      "Wrote weights to file ./customFunctionLearningAdaptive/baselineMLPFinal/best_weights\n",
      "The loss after 1199 iterations is E=0.013105079575248911\n",
      "Wrote weights to file ./customFunctionLearningAdaptive/baselineMLPFinal/best_weights\n",
      "The loss after 1200 iterations is E=0.013104461050742096\n",
      "Wrote weights to file ./customFunctionLearningAdaptive/baselineMLPFinal/best_weights\n",
      "The loss after 1201 iterations is E=0.01310384669036352\n",
      "Wrote weights to file ./customFunctionLearningAdaptive/baselineMLPFinal/best_weights\n",
      "The loss after 1202 iterations is E=0.013103236446275549\n",
      "Wrote weights to file ./customFunctionLearningAdaptive/baselineMLPFinal/best_weights\n",
      "The loss after 1203 iterations is E=0.013102630267710767\n",
      "Wrote weights to file ./customFunctionLearningAdaptive/baselineMLPFinal/best_weights\n",
      "The loss after 1204 iterations is E=0.01310202810783607\n",
      "Wrote weights to file ./customFunctionLearningAdaptive/baselineMLPFinal/best_weights\n",
      "The loss after 1205 iterations is E=0.01310142991746477\n",
      "Wrote weights to file ./customFunctionLearningAdaptive/baselineMLPFinal/best_weights\n",
      "The loss after 1206 iterations is E=0.013100835650083806\n",
      "Wrote weights to file ./customFunctionLearningAdaptive/baselineMLPFinal/best_weights\n",
      "The loss after 1207 iterations is E=0.013100245258366727\n",
      "Wrote weights to file ./customFunctionLearningAdaptive/baselineMLPFinal/best_weights\n",
      "The loss after 1208 iterations is E=0.013099658696015946\n",
      "Wrote weights to file ./customFunctionLearningAdaptive/baselineMLPFinal/best_weights\n",
      "The loss after 1209 iterations is E=0.01309907591749087\n",
      "Wrote weights to file ./customFunctionLearningAdaptive/baselineMLPFinal/best_weights\n",
      "The loss after 1210 iterations is E=0.013098496876955409\n",
      "Wrote weights to file ./customFunctionLearningAdaptive/baselineMLPFinal/best_weights\n",
      "The loss after 1211 iterations is E=0.013097921530220916\n",
      "Wrote weights to file ./customFunctionLearningAdaptive/baselineMLPFinal/best_weights\n",
      "The loss after 1212 iterations is E=0.013097349832273696\n",
      "Wrote weights to file ./customFunctionLearningAdaptive/baselineMLPFinal/best_weights\n",
      "The loss after 1213 iterations is E=0.013096781739917933\n",
      "Wrote weights to file ./customFunctionLearningAdaptive/baselineMLPFinal/best_weights\n",
      "The loss after 1214 iterations is E=0.013096217209290648\n",
      "Wrote weights to file ./customFunctionLearningAdaptive/baselineMLPFinal/best_weights\n",
      "The loss after 1215 iterations is E=0.013095656197901658\n",
      "Wrote weights to file ./customFunctionLearningAdaptive/baselineMLPFinal/best_weights\n",
      "The loss after 1216 iterations is E=0.013095098663133806\n",
      "Wrote weights to file ./customFunctionLearningAdaptive/baselineMLPFinal/best_weights\n",
      "The loss after 1217 iterations is E=0.013094544563140022\n",
      "Wrote weights to file ./customFunctionLearningAdaptive/baselineMLPFinal/best_weights\n",
      "The loss after 1218 iterations is E=0.013093993856542068\n",
      "Wrote weights to file ./customFunctionLearningAdaptive/baselineMLPFinal/best_weights\n",
      "The loss after 1219 iterations is E=0.013093446502196019\n",
      "Wrote weights to file ./customFunctionLearningAdaptive/baselineMLPFinal/best_weights\n",
      "The loss after 1220 iterations is E=0.013092902459791972\n",
      "Wrote weights to file ./customFunctionLearningAdaptive/baselineMLPFinal/best_weights\n",
      "The loss after 1221 iterations is E=0.013092361689001802\n",
      "Wrote weights to file ./customFunctionLearningAdaptive/baselineMLPFinal/best_weights\n",
      "The loss after 1222 iterations is E=0.013091824150451795\n",
      "Wrote weights to file ./customFunctionLearningAdaptive/baselineMLPFinal/best_weights\n",
      "The loss after 1223 iterations is E=0.013091289804749016\n",
      "Wrote weights to file ./customFunctionLearningAdaptive/baselineMLPFinal/best_weights\n",
      "The loss after 1224 iterations is E=0.013090758613321526\n",
      "Wrote weights to file ./customFunctionLearningAdaptive/baselineMLPFinal/best_weights\n",
      "The loss after 1225 iterations is E=0.013090230537742291\n",
      "Wrote weights to file ./customFunctionLearningAdaptive/baselineMLPFinal/best_weights\n",
      "The loss after 1226 iterations is E=0.013089705540197885\n",
      "Wrote weights to file ./customFunctionLearningAdaptive/baselineMLPFinal/best_weights\n",
      "The loss after 1227 iterations is E=0.01308918358323592\n",
      "Wrote weights to file ./customFunctionLearningAdaptive/baselineMLPFinal/best_weights\n",
      "The loss after 1228 iterations is E=0.013088664629797233\n",
      "Wrote weights to file ./customFunctionLearningAdaptive/baselineMLPFinal/best_weights\n",
      "The loss after 1229 iterations is E=0.013088148643333977\n",
      "Wrote weights to file ./customFunctionLearningAdaptive/baselineMLPFinal/best_weights\n",
      "The loss after 1230 iterations is E=0.013087635587565588\n",
      "Wrote weights to file ./customFunctionLearningAdaptive/baselineMLPFinal/best_weights\n",
      "The loss after 1231 iterations is E=0.013087125426799321\n",
      "Wrote weights to file ./customFunctionLearningAdaptive/baselineMLPFinal/best_weights\n",
      "The loss after 1232 iterations is E=0.013086618125569688\n",
      "Wrote weights to file ./customFunctionLearningAdaptive/baselineMLPFinal/best_weights\n",
      "The loss after 1233 iterations is E=0.01308611364897003\n",
      "Wrote weights to file ./customFunctionLearningAdaptive/baselineMLPFinal/best_weights\n",
      "The loss after 1234 iterations is E=0.01308561196235401\n",
      "Wrote weights to file ./customFunctionLearningAdaptive/baselineMLPFinal/best_weights\n",
      "The loss after 1235 iterations is E=0.013085113031571787\n",
      "Wrote weights to file ./customFunctionLearningAdaptive/baselineMLPFinal/best_weights\n",
      "The loss after 1236 iterations is E=0.013084616822801824\n",
      "Wrote weights to file ./customFunctionLearningAdaptive/baselineMLPFinal/best_weights\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The loss after 1237 iterations is E=0.01308412330262947\n",
      "Wrote weights to file ./customFunctionLearningAdaptive/baselineMLPFinal/best_weights\n",
      "The loss after 1238 iterations is E=0.013083632438024533\n",
      "Wrote weights to file ./customFunctionLearningAdaptive/baselineMLPFinal/best_weights\n",
      "The loss after 1239 iterations is E=0.013083144196303215\n",
      "Wrote weights to file ./customFunctionLearningAdaptive/baselineMLPFinal/best_weights\n",
      "The loss after 1240 iterations is E=0.013082658545206573\n",
      "Wrote weights to file ./customFunctionLearningAdaptive/baselineMLPFinal/best_weights\n",
      "The loss after 1241 iterations is E=0.013082175452784285\n",
      "Wrote weights to file ./customFunctionLearningAdaptive/baselineMLPFinal/best_weights\n",
      "The loss after 1242 iterations is E=0.013081694887509633\n",
      "Wrote weights to file ./customFunctionLearningAdaptive/baselineMLPFinal/best_weights\n",
      "The loss after 1243 iterations is E=0.013081216818159111\n",
      "Wrote weights to file ./customFunctionLearningAdaptive/baselineMLPFinal/best_weights\n",
      "The loss after 1244 iterations is E=0.013080741213918407\n",
      "Wrote weights to file ./customFunctionLearningAdaptive/baselineMLPFinal/best_weights\n",
      "The loss after 1245 iterations is E=0.013080268044289104\n",
      "Wrote weights to file ./customFunctionLearningAdaptive/baselineMLPFinal/best_weights\n",
      "The loss after 1246 iterations is E=0.013079797279146474\n",
      "Wrote weights to file ./customFunctionLearningAdaptive/baselineMLPFinal/best_weights\n",
      "The loss after 1247 iterations is E=0.013079328888697193\n",
      "Wrote weights to file ./customFunctionLearningAdaptive/baselineMLPFinal/best_weights\n",
      "The loss after 1248 iterations is E=0.013078862843494217\n",
      "Wrote weights to file ./customFunctionLearningAdaptive/baselineMLPFinal/best_weights\n",
      "The loss after 1249 iterations is E=0.013078399114437889\n",
      "Wrote weights to file ./customFunctionLearningAdaptive/baselineMLPFinal/best_weights\n",
      "The loss after 1250 iterations is E=0.013077937672748965\n",
      "Wrote weights to file ./customFunctionLearningAdaptive/baselineMLPFinal/best_weights\n",
      "The loss after 1251 iterations is E=0.013077478489997029\n",
      "Wrote weights to file ./customFunctionLearningAdaptive/baselineMLPFinal/best_weights\n",
      "The loss after 1252 iterations is E=0.013077021538059813\n",
      "Wrote weights to file ./customFunctionLearningAdaptive/baselineMLPFinal/best_weights\n",
      "The loss after 1253 iterations is E=0.013076566789161645\n",
      "Wrote weights to file ./customFunctionLearningAdaptive/baselineMLPFinal/best_weights\n",
      "The loss after 1254 iterations is E=0.013076114215830057\n",
      "Wrote weights to file ./customFunctionLearningAdaptive/baselineMLPFinal/best_weights\n",
      "The loss after 1255 iterations is E=0.013075663790924575\n",
      "Wrote weights to file ./customFunctionLearningAdaptive/baselineMLPFinal/best_weights\n",
      "The loss after 1256 iterations is E=0.013075215487607358\n",
      "Wrote weights to file ./customFunctionLearningAdaptive/baselineMLPFinal/best_weights\n",
      "The loss after 1257 iterations is E=0.013074769279360455\n",
      "Wrote weights to file ./customFunctionLearningAdaptive/baselineMLPFinal/best_weights\n",
      "The loss after 1258 iterations is E=0.013074325139971085\n",
      "Wrote weights to file ./customFunctionLearningAdaptive/baselineMLPFinal/best_weights\n",
      "The loss after 1259 iterations is E=0.013073883043530597\n",
      "Wrote weights to file ./customFunctionLearningAdaptive/baselineMLPFinal/best_weights\n",
      "The loss after 1260 iterations is E=0.013073442964433555\n",
      "Wrote weights to file ./customFunctionLearningAdaptive/baselineMLPFinal/best_weights\n",
      "The loss after 1261 iterations is E=0.013073004877368764\n",
      "Wrote weights to file ./customFunctionLearningAdaptive/baselineMLPFinal/best_weights\n",
      "The loss after 1262 iterations is E=0.013072568757326928\n",
      "Wrote weights to file ./customFunctionLearningAdaptive/baselineMLPFinal/best_weights\n",
      "The loss after 1263 iterations is E=0.013072134579583922\n",
      "Wrote weights to file ./customFunctionLearningAdaptive/baselineMLPFinal/best_weights\n",
      "The loss after 1264 iterations is E=0.013071702319710148\n",
      "Wrote weights to file ./customFunctionLearningAdaptive/baselineMLPFinal/best_weights\n",
      "The loss after 1265 iterations is E=0.01307127195355598\n",
      "Wrote weights to file ./customFunctionLearningAdaptive/baselineMLPFinal/best_weights\n",
      "The loss after 1266 iterations is E=0.013070843457260032\n",
      "Wrote weights to file ./customFunctionLearningAdaptive/baselineMLPFinal/best_weights\n",
      "The loss after 1267 iterations is E=0.01307041680723702\n",
      "Wrote weights to file ./customFunctionLearningAdaptive/baselineMLPFinal/best_weights\n",
      "The loss after 1268 iterations is E=0.01306999198018056\n",
      "Wrote weights to file ./customFunctionLearningAdaptive/baselineMLPFinal/best_weights\n",
      "The loss after 1269 iterations is E=0.013069568953055739\n",
      "Wrote weights to file ./customFunctionLearningAdaptive/baselineMLPFinal/best_weights\n",
      "The loss after 1270 iterations is E=0.013069147703099766\n",
      "Wrote weights to file ./customFunctionLearningAdaptive/baselineMLPFinal/best_weights\n",
      "The loss after 1271 iterations is E=0.013068728207818242\n",
      "Wrote weights to file ./customFunctionLearningAdaptive/baselineMLPFinal/best_weights\n",
      "The loss after 1272 iterations is E=0.013068310444980549\n",
      "Wrote weights to file ./customFunctionLearningAdaptive/baselineMLPFinal/best_weights\n",
      "The loss after 1273 iterations is E=0.013067894392619047\n",
      "Wrote weights to file ./customFunctionLearningAdaptive/baselineMLPFinal/best_weights\n",
      "The loss after 1274 iterations is E=0.013067480029023954\n",
      "Wrote weights to file ./customFunctionLearningAdaptive/baselineMLPFinal/best_weights\n",
      "The loss after 1275 iterations is E=0.013067067332744126\n",
      "Wrote weights to file ./customFunctionLearningAdaptive/baselineMLPFinal/best_weights\n",
      "The loss after 1276 iterations is E=0.013066656282580445\n",
      "Wrote weights to file ./customFunctionLearningAdaptive/baselineMLPFinal/best_weights\n",
      "The loss after 1277 iterations is E=0.013066246857585713\n",
      "Wrote weights to file ./customFunctionLearningAdaptive/baselineMLPFinal/best_weights\n",
      "The loss after 1278 iterations is E=0.013065839037059794\n",
      "Wrote weights to file ./customFunctionLearningAdaptive/baselineMLPFinal/best_weights\n",
      "The loss after 1279 iterations is E=0.013065432800549264\n",
      "Wrote weights to file ./customFunctionLearningAdaptive/baselineMLPFinal/best_weights\n",
      "The loss after 1280 iterations is E=0.01306502812784308\n",
      "Wrote weights to file ./customFunctionLearningAdaptive/baselineMLPFinal/best_weights\n",
      "The loss after 1281 iterations is E=0.01306462499897067\n",
      "Wrote weights to file ./customFunctionLearningAdaptive/baselineMLPFinal/best_weights\n",
      "The loss after 1282 iterations is E=0.0130642233941985\n",
      "Wrote weights to file ./customFunctionLearningAdaptive/baselineMLPFinal/best_weights\n",
      "The loss after 1283 iterations is E=0.013063823294028247\n",
      "Wrote weights to file ./customFunctionLearningAdaptive/baselineMLPFinal/best_weights\n",
      "The loss after 1284 iterations is E=0.013063424679194574\n",
      "Wrote weights to file ./customFunctionLearningAdaptive/baselineMLPFinal/best_weights\n",
      "The loss after 1285 iterations is E=0.013063027530661415\n",
      "Wrote weights to file ./customFunctionLearningAdaptive/baselineMLPFinal/best_weights\n",
      "The loss after 1286 iterations is E=0.013062631829620424\n",
      "Wrote weights to file ./customFunctionLearningAdaptive/baselineMLPFinal/best_weights\n",
      "The loss after 1287 iterations is E=0.013062237557487774\n",
      "Wrote weights to file ./customFunctionLearningAdaptive/baselineMLPFinal/best_weights\n",
      "The loss after 1288 iterations is E=0.013061844695902507\n",
      "Wrote weights to file ./customFunctionLearningAdaptive/baselineMLPFinal/best_weights\n",
      "The loss after 1289 iterations is E=0.01306145322672334\n",
      "Wrote weights to file ./customFunctionLearningAdaptive/baselineMLPFinal/best_weights\n",
      "The loss after 1290 iterations is E=0.013061063132026896\n",
      "Wrote weights to file ./customFunctionLearningAdaptive/baselineMLPFinal/best_weights\n",
      "The loss after 1291 iterations is E=0.013060674394104311\n",
      "Wrote weights to file ./customFunctionLearningAdaptive/baselineMLPFinal/best_weights\n",
      "The loss after 1292 iterations is E=0.013060286995460057\n",
      "Wrote weights to file ./customFunctionLearningAdaptive/baselineMLPFinal/best_weights\n",
      "The loss after 1293 iterations is E=0.013059900918808962\n",
      "Wrote weights to file ./customFunctionLearningAdaptive/baselineMLPFinal/best_weights\n",
      "The loss after 1294 iterations is E=0.01305951614707402\n",
      "Wrote weights to file ./customFunctionLearningAdaptive/baselineMLPFinal/best_weights\n",
      "The loss after 1295 iterations is E=0.013059132663383693\n",
      "Wrote weights to file ./customFunctionLearningAdaptive/baselineMLPFinal/best_weights\n",
      "The loss after 1296 iterations is E=0.013058750451070203\n",
      "Wrote weights to file ./customFunctionLearningAdaptive/baselineMLPFinal/best_weights\n",
      "The loss after 1297 iterations is E=0.01305836949366723\n",
      "Wrote weights to file ./customFunctionLearningAdaptive/baselineMLPFinal/best_weights\n",
      "The loss after 1298 iterations is E=0.013057989774907195\n",
      "Wrote weights to file ./customFunctionLearningAdaptive/baselineMLPFinal/best_weights\n",
      "The loss after 1299 iterations is E=0.013057611278719634\n",
      "Wrote weights to file ./customFunctionLearningAdaptive/baselineMLPFinal/best_weights\n",
      "The loss after 1300 iterations is E=0.01305723398922833\n",
      "Wrote weights to file ./customFunctionLearningAdaptive/baselineMLPFinal/best_weights\n",
      "The loss after 1301 iterations is E=0.013056857890749955\n",
      "Wrote weights to file ./customFunctionLearningAdaptive/baselineMLPFinal/best_weights\n",
      "The loss after 1302 iterations is E=0.013056482967791355\n",
      "Wrote weights to file ./customFunctionLearningAdaptive/baselineMLPFinal/best_weights\n",
      "The loss after 1303 iterations is E=0.013056109205047527\n",
      "Wrote weights to file ./customFunctionLearningAdaptive/baselineMLPFinal/best_weights\n",
      "The loss after 1304 iterations is E=0.013055736587399492\n",
      "Wrote weights to file ./customFunctionLearningAdaptive/baselineMLPFinal/best_weights\n",
      "The loss after 1305 iterations is E=0.013055365099912536\n",
      "Wrote weights to file ./customFunctionLearningAdaptive/baselineMLPFinal/best_weights\n",
      "The loss after 1306 iterations is E=0.01305499472783375\n",
      "Wrote weights to file ./customFunctionLearningAdaptive/baselineMLPFinal/best_weights\n",
      "The loss after 1307 iterations is E=0.013054625456590193\n",
      "Wrote weights to file ./customFunctionLearningAdaptive/baselineMLPFinal/best_weights\n",
      "The loss after 1308 iterations is E=0.013054257271786638\n",
      "Wrote weights to file ./customFunctionLearningAdaptive/baselineMLPFinal/best_weights\n",
      "The loss after 1309 iterations is E=0.013053890159203968\n",
      "Wrote weights to file ./customFunctionLearningAdaptive/baselineMLPFinal/best_weights\n",
      "The loss after 1310 iterations is E=0.013053524104797053\n",
      "Wrote weights to file ./customFunctionLearningAdaptive/baselineMLPFinal/best_weights\n",
      "The loss after 1311 iterations is E=0.0130531590946924\n",
      "Wrote weights to file ./customFunctionLearningAdaptive/baselineMLPFinal/best_weights\n",
      "The loss after 1312 iterations is E=0.013052795115186887\n",
      "Wrote weights to file ./customFunctionLearningAdaptive/baselineMLPFinal/best_weights\n",
      "The loss after 1313 iterations is E=0.013052432152745127\n",
      "Wrote weights to file ./customFunctionLearningAdaptive/baselineMLPFinal/best_weights\n",
      "The loss after 1314 iterations is E=0.013052070193998282\n",
      "Wrote weights to file ./customFunctionLearningAdaptive/baselineMLPFinal/best_weights\n",
      "The loss after 1315 iterations is E=0.01305170922574166\n",
      "Wrote weights to file ./customFunctionLearningAdaptive/baselineMLPFinal/best_weights\n",
      "The loss after 1316 iterations is E=0.013051349234933007\n",
      "Wrote weights to file ./customFunctionLearningAdaptive/baselineMLPFinal/best_weights\n",
      "The loss after 1317 iterations is E=0.013050990208690692\n",
      "Wrote weights to file ./customFunctionLearningAdaptive/baselineMLPFinal/best_weights\n",
      "The loss after 1318 iterations is E=0.013050632134291902\n",
      "Wrote weights to file ./customFunctionLearningAdaptive/baselineMLPFinal/best_weights\n",
      "The loss after 1319 iterations is E=0.01305027499917091\n",
      "Wrote weights to file ./customFunctionLearningAdaptive/baselineMLPFinal/best_weights\n",
      "The loss after 1320 iterations is E=0.013049918790917006\n",
      "Wrote weights to file ./customFunctionLearningAdaptive/baselineMLPFinal/best_weights\n",
      "The loss after 1321 iterations is E=0.013049563497272865\n",
      "Wrote weights to file ./customFunctionLearningAdaptive/baselineMLPFinal/best_weights\n",
      "The loss after 1322 iterations is E=0.013049209106133065\n",
      "Wrote weights to file ./customFunctionLearningAdaptive/baselineMLPFinal/best_weights\n",
      "The loss after 1323 iterations is E=0.013048855605542038\n",
      "Wrote weights to file ./customFunctionLearningAdaptive/baselineMLPFinal/best_weights\n",
      "The loss after 1324 iterations is E=0.01304850298369238\n",
      "Wrote weights to file ./customFunctionLearningAdaptive/baselineMLPFinal/best_weights\n",
      "The loss after 1325 iterations is E=0.013048151228923204\n",
      "Wrote weights to file ./customFunctionLearningAdaptive/baselineMLPFinal/best_weights\n",
      "The loss after 1326 iterations is E=0.013047800329718534\n",
      "Wrote weights to file ./customFunctionLearningAdaptive/baselineMLPFinal/best_weights\n",
      "The loss after 1327 iterations is E=0.013047450274705753\n",
      "Wrote weights to file ./customFunctionLearningAdaptive/baselineMLPFinal/best_weights\n",
      "The loss after 1328 iterations is E=0.013047101052653345\n",
      "Wrote weights to file ./customFunctionLearningAdaptive/baselineMLPFinal/best_weights\n",
      "The loss after 1329 iterations is E=0.013046752652470192\n",
      "Wrote weights to file ./customFunctionLearningAdaptive/baselineMLPFinal/best_weights\n",
      "The loss after 1330 iterations is E=0.013046405063203108\n",
      "Wrote weights to file ./customFunctionLearningAdaptive/baselineMLPFinal/best_weights\n",
      "The loss after 1331 iterations is E=0.013046058274035901\n",
      "Wrote weights to file ./customFunctionLearningAdaptive/baselineMLPFinal/best_weights\n",
      "The loss after 1332 iterations is E=0.01304571227428734\n",
      "Wrote weights to file ./customFunctionLearningAdaptive/baselineMLPFinal/best_weights\n",
      "The loss after 1333 iterations is E=0.013045367053409869\n",
      "Wrote weights to file ./customFunctionLearningAdaptive/baselineMLPFinal/best_weights\n",
      "The loss after 1334 iterations is E=0.013045022600987752\n",
      "Wrote weights to file ./customFunctionLearningAdaptive/baselineMLPFinal/best_weights\n",
      "The loss after 1335 iterations is E=0.01304467890673603\n",
      "Wrote weights to file ./customFunctionLearningAdaptive/baselineMLPFinal/best_weights\n",
      "The loss after 1336 iterations is E=0.01304433596049855\n",
      "Wrote weights to file ./customFunctionLearningAdaptive/baselineMLPFinal/best_weights\n",
      "The loss after 1337 iterations is E=0.013043993752246763\n",
      "Wrote weights to file ./customFunctionLearningAdaptive/baselineMLPFinal/best_weights\n",
      "The loss after 1338 iterations is E=0.013043652272077981\n",
      "Wrote weights to file ./customFunctionLearningAdaptive/baselineMLPFinal/best_weights\n",
      "The loss after 1339 iterations is E=0.013043311510214118\n",
      "Wrote weights to file ./customFunctionLearningAdaptive/baselineMLPFinal/best_weights\n",
      "The loss after 1340 iterations is E=0.013042971457000199\n",
      "Wrote weights to file ./customFunctionLearningAdaptive/baselineMLPFinal/best_weights\n",
      "The loss after 1341 iterations is E=0.013042632102902955\n",
      "Wrote weights to file ./customFunctionLearningAdaptive/baselineMLPFinal/best_weights\n",
      "The loss after 1342 iterations is E=0.013042293438509213\n",
      "Wrote weights to file ./customFunctionLearningAdaptive/baselineMLPFinal/best_weights\n",
      "The loss after 1343 iterations is E=0.013041955454524684\n",
      "Wrote weights to file ./customFunctionLearningAdaptive/baselineMLPFinal/best_weights\n",
      "The loss after 1344 iterations is E=0.013041618141772527\n",
      "Wrote weights to file ./customFunctionLearningAdaptive/baselineMLPFinal/best_weights\n",
      "The loss after 1345 iterations is E=0.013041281491191947\n",
      "Wrote weights to file ./customFunctionLearningAdaptive/baselineMLPFinal/best_weights\n",
      "The loss after 1346 iterations is E=0.013040945493836971\n",
      "Wrote weights to file ./customFunctionLearningAdaptive/baselineMLPFinal/best_weights\n",
      "The loss after 1347 iterations is E=0.013040610140874747\n",
      "Wrote weights to file ./customFunctionLearningAdaptive/baselineMLPFinal/best_weights\n",
      "The loss after 1348 iterations is E=0.013040275423584524\n",
      "Wrote weights to file ./customFunctionLearningAdaptive/baselineMLPFinal/best_weights\n",
      "The loss after 1349 iterations is E=0.013039941333356423\n",
      "Wrote weights to file ./customFunctionLearningAdaptive/baselineMLPFinal/best_weights\n",
      "The loss after 1350 iterations is E=0.01303960786168966\n",
      "Wrote weights to file ./customFunctionLearningAdaptive/baselineMLPFinal/best_weights\n",
      "The loss after 1351 iterations is E=0.013039275000191858\n",
      "Wrote weights to file ./customFunctionLearningAdaptive/baselineMLPFinal/best_weights\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The loss after 1352 iterations is E=0.013038942740577217\n",
      "Wrote weights to file ./customFunctionLearningAdaptive/baselineMLPFinal/best_weights\n",
      "The loss after 1353 iterations is E=0.013038611074665748\n",
      "Wrote weights to file ./customFunctionLearningAdaptive/baselineMLPFinal/best_weights\n",
      "The loss after 1354 iterations is E=0.013038279994381508\n",
      "Wrote weights to file ./customFunctionLearningAdaptive/baselineMLPFinal/best_weights\n",
      "The loss after 1355 iterations is E=0.013037949491751818\n",
      "Wrote weights to file ./customFunctionLearningAdaptive/baselineMLPFinal/best_weights\n",
      "The loss after 1356 iterations is E=0.013037619558905738\n",
      "Wrote weights to file ./customFunctionLearningAdaptive/baselineMLPFinal/best_weights\n",
      "The loss after 1357 iterations is E=0.01303729018807312\n",
      "Wrote weights to file ./customFunctionLearningAdaptive/baselineMLPFinal/best_weights\n",
      "The loss after 1358 iterations is E=0.0130369613715831\n",
      "Wrote weights to file ./customFunctionLearningAdaptive/baselineMLPFinal/best_weights\n",
      "The loss after 1359 iterations is E=0.013036633101863192\n",
      "Wrote weights to file ./customFunctionLearningAdaptive/baselineMLPFinal/best_weights\n",
      "The loss after 1360 iterations is E=0.01303630537143804\n",
      "Wrote weights to file ./customFunctionLearningAdaptive/baselineMLPFinal/best_weights\n",
      "The loss after 1361 iterations is E=0.013035978172928024\n",
      "Wrote weights to file ./customFunctionLearningAdaptive/baselineMLPFinal/best_weights\n",
      "The loss after 1362 iterations is E=0.013035651499048478\n",
      "Wrote weights to file ./customFunctionLearningAdaptive/baselineMLPFinal/best_weights\n",
      "The loss after 1363 iterations is E=0.013035325342608341\n",
      "Wrote weights to file ./customFunctionLearningAdaptive/baselineMLPFinal/best_weights\n",
      "The loss after 1364 iterations is E=0.013034999696509094\n",
      "Wrote weights to file ./customFunctionLearningAdaptive/baselineMLPFinal/best_weights\n",
      "The loss after 1365 iterations is E=0.013034674553743644\n",
      "Wrote weights to file ./customFunctionLearningAdaptive/baselineMLPFinal/best_weights\n",
      "The loss after 1366 iterations is E=0.013034349907395194\n",
      "Wrote weights to file ./customFunctionLearningAdaptive/baselineMLPFinal/best_weights\n",
      "The loss after 1367 iterations is E=0.013034025750636133\n",
      "Wrote weights to file ./customFunctionLearningAdaptive/baselineMLPFinal/best_weights\n",
      "The loss after 1368 iterations is E=0.013033702076727206\n",
      "Wrote weights to file ./customFunctionLearningAdaptive/baselineMLPFinal/best_weights\n",
      "The loss after 1369 iterations is E=0.013033378879016027\n",
      "Wrote weights to file ./customFunctionLearningAdaptive/baselineMLPFinal/best_weights\n",
      "The loss after 1370 iterations is E=0.013033056150936499\n",
      "Wrote weights to file ./customFunctionLearningAdaptive/baselineMLPFinal/best_weights\n",
      "The loss after 1371 iterations is E=0.013032733886007387\n",
      "Wrote weights to file ./customFunctionLearningAdaptive/baselineMLPFinal/best_weights\n",
      "The loss after 1372 iterations is E=0.013032412077831275\n",
      "Wrote weights to file ./customFunctionLearningAdaptive/baselineMLPFinal/best_weights\n",
      "The loss after 1373 iterations is E=0.013032090720094107\n",
      "Wrote weights to file ./customFunctionLearningAdaptive/baselineMLPFinal/best_weights\n",
      "The loss after 1374 iterations is E=0.013031769806563571\n",
      "Wrote weights to file ./customFunctionLearningAdaptive/baselineMLPFinal/best_weights\n",
      "The loss after 1375 iterations is E=0.013031449331088307\n",
      "Wrote weights to file ./customFunctionLearningAdaptive/baselineMLPFinal/best_weights\n",
      "The loss after 1376 iterations is E=0.01303112928759701\n",
      "Wrote weights to file ./customFunctionLearningAdaptive/baselineMLPFinal/best_weights\n",
      "The loss after 1377 iterations is E=0.01303080967009733\n",
      "Wrote weights to file ./customFunctionLearningAdaptive/baselineMLPFinal/best_weights\n",
      "The loss after 1378 iterations is E=0.013030490472674997\n",
      "Wrote weights to file ./customFunctionLearningAdaptive/baselineMLPFinal/best_weights\n",
      "The loss after 1379 iterations is E=0.013030171689492867\n",
      "Wrote weights to file ./customFunctionLearningAdaptive/baselineMLPFinal/best_weights\n",
      "The loss after 1380 iterations is E=0.01302985331478996\n",
      "Wrote weights to file ./customFunctionLearningAdaptive/baselineMLPFinal/best_weights\n",
      "The loss after 1381 iterations is E=0.013029535342880507\n",
      "Wrote weights to file ./customFunctionLearningAdaptive/baselineMLPFinal/best_weights\n",
      "The loss after 1382 iterations is E=0.013029217768152931\n",
      "Wrote weights to file ./customFunctionLearningAdaptive/baselineMLPFinal/best_weights\n",
      "The loss after 1383 iterations is E=0.013028900585069315\n",
      "Wrote weights to file ./customFunctionLearningAdaptive/baselineMLPFinal/best_weights\n",
      "The loss after 1384 iterations is E=0.01302858378816395\n",
      "Wrote weights to file ./customFunctionLearningAdaptive/baselineMLPFinal/best_weights\n",
      "The loss after 1385 iterations is E=0.01302826737204301\n",
      "Wrote weights to file ./customFunctionLearningAdaptive/baselineMLPFinal/best_weights\n",
      "The loss after 1386 iterations is E=0.0130279513313832\n",
      "Wrote weights to file ./customFunctionLearningAdaptive/baselineMLPFinal/best_weights\n",
      "The loss after 1387 iterations is E=0.013027635660931233\n",
      "Wrote weights to file ./customFunctionLearningAdaptive/baselineMLPFinal/best_weights\n",
      "The loss after 1388 iterations is E=0.013027320355502704\n",
      "Wrote weights to file ./customFunctionLearningAdaptive/baselineMLPFinal/best_weights\n",
      "The loss after 1389 iterations is E=0.013027005409981364\n",
      "Wrote weights to file ./customFunctionLearningAdaptive/baselineMLPFinal/best_weights\n",
      "The loss after 1390 iterations is E=0.013026690819318402\n",
      "Wrote weights to file ./customFunctionLearningAdaptive/baselineMLPFinal/best_weights\n",
      "The loss after 1391 iterations is E=0.013026376578531356\n",
      "Wrote weights to file ./customFunctionLearningAdaptive/baselineMLPFinal/best_weights\n",
      "The loss after 1392 iterations is E=0.013026062682703494\n",
      "Wrote weights to file ./customFunctionLearningAdaptive/baselineMLPFinal/best_weights\n",
      "The loss after 1393 iterations is E=0.01302574912698281\n",
      "Wrote weights to file ./customFunctionLearningAdaptive/baselineMLPFinal/best_weights\n",
      "The loss after 1394 iterations is E=0.013025435906581584\n",
      "Wrote weights to file ./customFunctionLearningAdaptive/baselineMLPFinal/best_weights\n",
      "The loss after 1395 iterations is E=0.013025123016775024\n",
      "Wrote weights to file ./customFunctionLearningAdaptive/baselineMLPFinal/best_weights\n",
      "The loss after 1396 iterations is E=0.013024810452901055\n",
      "Wrote weights to file ./customFunctionLearningAdaptive/baselineMLPFinal/best_weights\n",
      "The loss after 1397 iterations is E=0.013024498210359068\n",
      "Wrote weights to file ./customFunctionLearningAdaptive/baselineMLPFinal/best_weights\n",
      "The loss after 1398 iterations is E=0.01302418628460953\n",
      "Wrote weights to file ./customFunctionLearningAdaptive/baselineMLPFinal/best_weights\n",
      "The loss after 1399 iterations is E=0.013023874671173061\n",
      "Wrote weights to file ./customFunctionLearningAdaptive/baselineMLPFinal/best_weights\n",
      "The loss after 1400 iterations is E=0.013023563365629511\n",
      "Wrote weights to file ./customFunctionLearningAdaptive/baselineMLPFinal/best_weights\n",
      "The loss after 1401 iterations is E=0.013023252363617684\n",
      "Wrote weights to file ./customFunctionLearningAdaptive/baselineMLPFinal/best_weights\n",
      "The loss after 1402 iterations is E=0.013022941660833929\n",
      "Wrote weights to file ./customFunctionLearningAdaptive/baselineMLPFinal/best_weights\n",
      "The loss after 1403 iterations is E=0.01302263125303221\n",
      "Wrote weights to file ./customFunctionLearningAdaptive/baselineMLPFinal/best_weights\n",
      "The loss after 1404 iterations is E=0.013022321136022552\n",
      "Wrote weights to file ./customFunctionLearningAdaptive/baselineMLPFinal/best_weights\n",
      "The loss after 1405 iterations is E=0.013022011305671101\n",
      "Wrote weights to file ./customFunctionLearningAdaptive/baselineMLPFinal/best_weights\n",
      "The loss after 1406 iterations is E=0.013021701757898789\n",
      "Wrote weights to file ./customFunctionLearningAdaptive/baselineMLPFinal/best_weights\n",
      "The loss after 1407 iterations is E=0.013021392488681276\n",
      "Wrote weights to file ./customFunctionLearningAdaptive/baselineMLPFinal/best_weights\n",
      "The loss after 1408 iterations is E=0.013021083494047542\n",
      "Wrote weights to file ./customFunctionLearningAdaptive/baselineMLPFinal/best_weights\n",
      "The loss after 1409 iterations is E=0.013020774770079853\n",
      "Wrote weights to file ./customFunctionLearningAdaptive/baselineMLPFinal/best_weights\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The loss after 1410 iterations is E=0.013020466312912616\n",
      "Wrote weights to file ./customFunctionLearningAdaptive/baselineMLPFinal/best_weights\n",
      "The loss after 1411 iterations is E=0.013020158118732177\n",
      "Wrote weights to file ./customFunctionLearningAdaptive/baselineMLPFinal/best_weights\n",
      "The loss after 1412 iterations is E=0.013019850183775626\n",
      "Wrote weights to file ./customFunctionLearningAdaptive/baselineMLPFinal/best_weights\n",
      "The loss after 1413 iterations is E=0.01301954250433061\n",
      "Wrote weights to file ./customFunctionLearningAdaptive/baselineMLPFinal/best_weights\n",
      "The loss after 1414 iterations is E=0.01301923507673453\n",
      "Wrote weights to file ./customFunctionLearningAdaptive/baselineMLPFinal/best_weights\n",
      "The loss after 1415 iterations is E=0.01301892789737376\n",
      "Wrote weights to file ./customFunctionLearningAdaptive/baselineMLPFinal/best_weights\n",
      "The loss after 1416 iterations is E=0.013018620962683369\n",
      "Wrote weights to file ./customFunctionLearningAdaptive/baselineMLPFinal/best_weights\n",
      "The loss after 1417 iterations is E=0.013018314269146071\n",
      "Wrote weights to file ./customFunctionLearningAdaptive/baselineMLPFinal/best_weights\n",
      "The loss after 1418 iterations is E=0.013018007813291996\n",
      "Wrote weights to file ./customFunctionLearningAdaptive/baselineMLPFinal/best_weights\n",
      "The loss after 1419 iterations is E=0.013017701591697916\n",
      "Wrote weights to file ./customFunctionLearningAdaptive/baselineMLPFinal/best_weights\n",
      "The loss after 1420 iterations is E=0.013017395600986593\n",
      "Wrote weights to file ./customFunctionLearningAdaptive/baselineMLPFinal/best_weights\n",
      "The loss after 1421 iterations is E=0.013017089837826235\n",
      "Wrote weights to file ./customFunctionLearningAdaptive/baselineMLPFinal/best_weights\n",
      "The loss after 1422 iterations is E=0.013016784298930188\n",
      "Wrote weights to file ./customFunctionLearningAdaptive/baselineMLPFinal/best_weights\n",
      "The loss after 1423 iterations is E=0.013016478981055719\n",
      "Wrote weights to file ./customFunctionLearningAdaptive/baselineMLPFinal/best_weights\n",
      "The loss after 1424 iterations is E=0.013016173881004215\n",
      "Wrote weights to file ./customFunctionLearningAdaptive/baselineMLPFinal/best_weights\n",
      "The loss after 1425 iterations is E=0.01301586899562\n",
      "Wrote weights to file ./customFunctionLearningAdaptive/baselineMLPFinal/best_weights\n",
      "The loss after 1426 iterations is E=0.013015564321790128\n",
      "Wrote weights to file ./customFunctionLearningAdaptive/baselineMLPFinal/best_weights\n",
      "The loss after 1427 iterations is E=0.013015259856443648\n",
      "Wrote weights to file ./customFunctionLearningAdaptive/baselineMLPFinal/best_weights\n",
      "The loss after 1428 iterations is E=0.013014955596551319\n",
      "Wrote weights to file ./customFunctionLearningAdaptive/baselineMLPFinal/best_weights\n",
      "The loss after 1429 iterations is E=0.01301465153912472\n",
      "Wrote weights to file ./customFunctionLearningAdaptive/baselineMLPFinal/best_weights\n",
      "The loss after 1430 iterations is E=0.013014347681215913\n",
      "Wrote weights to file ./customFunctionLearningAdaptive/baselineMLPFinal/best_weights\n",
      "The loss after 1431 iterations is E=0.013014044019917103\n",
      "Wrote weights to file ./customFunctionLearningAdaptive/baselineMLPFinal/best_weights\n",
      "The loss after 1432 iterations is E=0.01301374055235962\n",
      "Wrote weights to file ./customFunctionLearningAdaptive/baselineMLPFinal/best_weights\n",
      "The loss after 1433 iterations is E=0.013013437275713961\n",
      "Wrote weights to file ./customFunctionLearningAdaptive/baselineMLPFinal/best_weights\n",
      "The loss after 1434 iterations is E=0.013013134187188972\n",
      "Wrote weights to file ./customFunctionLearningAdaptive/baselineMLPFinal/best_weights\n",
      "The loss after 1435 iterations is E=0.01301283128403134\n",
      "Wrote weights to file ./customFunctionLearningAdaptive/baselineMLPFinal/best_weights\n",
      "The loss after 1436 iterations is E=0.013012528563525168\n",
      "Wrote weights to file ./customFunctionLearningAdaptive/baselineMLPFinal/best_weights\n",
      "The loss after 1437 iterations is E=0.013012226022991645\n",
      "Wrote weights to file ./customFunctionLearningAdaptive/baselineMLPFinal/best_weights\n",
      "The loss after 1438 iterations is E=0.013011923659788293\n",
      "Wrote weights to file ./customFunctionLearningAdaptive/baselineMLPFinal/best_weights\n",
      "The loss after 1439 iterations is E=0.013011621471308533\n",
      "Wrote weights to file ./customFunctionLearningAdaptive/baselineMLPFinal/best_weights\n",
      "The loss after 1440 iterations is E=0.013011319454981435\n",
      "Wrote weights to file ./customFunctionLearningAdaptive/baselineMLPFinal/best_weights\n",
      "The loss after 1441 iterations is E=0.01301101760827104\n",
      "Wrote weights to file ./customFunctionLearningAdaptive/baselineMLPFinal/best_weights\n",
      "The loss after 1442 iterations is E=0.013010715928676038\n",
      "Wrote weights to file ./customFunctionLearningAdaptive/baselineMLPFinal/best_weights\n",
      "The loss after 1443 iterations is E=0.01301041441372902\n",
      "Wrote weights to file ./customFunctionLearningAdaptive/baselineMLPFinal/best_weights\n",
      "The loss after 1444 iterations is E=0.01301011306099659\n",
      "Wrote weights to file ./customFunctionLearningAdaptive/baselineMLPFinal/best_weights\n",
      "The loss after 1445 iterations is E=0.013009811868078328\n",
      "Wrote weights to file ./customFunctionLearningAdaptive/baselineMLPFinal/best_weights\n",
      "The loss after 1446 iterations is E=0.01300951083260675\n",
      "Wrote weights to file ./customFunctionLearningAdaptive/baselineMLPFinal/best_weights\n",
      "The loss after 1447 iterations is E=0.013009209952246694\n",
      "Wrote weights to file ./customFunctionLearningAdaptive/baselineMLPFinal/best_weights\n",
      "The loss after 1448 iterations is E=0.013008909224694882\n",
      "Wrote weights to file ./customFunctionLearningAdaptive/baselineMLPFinal/best_weights\n",
      "The loss after 1449 iterations is E=0.013008608647679614\n",
      "Wrote weights to file ./customFunctionLearningAdaptive/baselineMLPFinal/best_weights\n",
      "The loss after 1450 iterations is E=0.013008308218960301\n",
      "Wrote weights to file ./customFunctionLearningAdaptive/baselineMLPFinal/best_weights\n",
      "The loss after 1451 iterations is E=0.013008007936326968\n",
      "Wrote weights to file ./customFunctionLearningAdaptive/baselineMLPFinal/best_weights\n",
      "The loss after 1452 iterations is E=0.0130077077976001\n",
      "Wrote weights to file ./customFunctionLearningAdaptive/baselineMLPFinal/best_weights\n",
      "The loss after 1453 iterations is E=0.01300740780062981\n",
      "Wrote weights to file ./customFunctionLearningAdaptive/baselineMLPFinal/best_weights\n",
      "The loss after 1454 iterations is E=0.01300710794329584\n",
      "Wrote weights to file ./customFunctionLearningAdaptive/baselineMLPFinal/best_weights\n",
      "The loss after 1455 iterations is E=0.01300680822350698\n",
      "Wrote weights to file ./customFunctionLearningAdaptive/baselineMLPFinal/best_weights\n",
      "The loss after 1456 iterations is E=0.01300650863920077\n",
      "Wrote weights to file ./customFunctionLearningAdaptive/baselineMLPFinal/best_weights\n",
      "The loss after 1457 iterations is E=0.013006209188343061\n",
      "Wrote weights to file ./customFunctionLearningAdaptive/baselineMLPFinal/best_weights\n",
      "The loss after 1458 iterations is E=0.013005909868927589\n",
      "Wrote weights to file ./customFunctionLearningAdaptive/baselineMLPFinal/best_weights\n",
      "The loss after 1459 iterations is E=0.013005610678975685\n",
      "Wrote weights to file ./customFunctionLearningAdaptive/baselineMLPFinal/best_weights\n",
      "The loss after 1460 iterations is E=0.013005311616535915\n",
      "Wrote weights to file ./customFunctionLearningAdaptive/baselineMLPFinal/best_weights\n",
      "The loss after 1461 iterations is E=0.013005012679683665\n",
      "Wrote weights to file ./customFunctionLearningAdaptive/baselineMLPFinal/best_weights\n",
      "The loss after 1462 iterations is E=0.01300471386652072\n",
      "Wrote weights to file ./customFunctionLearningAdaptive/baselineMLPFinal/best_weights\n",
      "The loss after 1463 iterations is E=0.013004415175175203\n",
      "Wrote weights to file ./customFunctionLearningAdaptive/baselineMLPFinal/best_weights\n",
      "The loss after 1464 iterations is E=0.013004116603800695\n",
      "Wrote weights to file ./customFunctionLearningAdaptive/baselineMLPFinal/best_weights\n",
      "The loss after 1465 iterations is E=0.013003818150576376\n",
      "Wrote weights to file ./customFunctionLearningAdaptive/baselineMLPFinal/best_weights\n",
      "The loss after 1466 iterations is E=0.013003519813706616\n",
      "Wrote weights to file ./customFunctionLearningAdaptive/baselineMLPFinal/best_weights\n",
      "The loss after 1467 iterations is E=0.013003221591420254\n",
      "Wrote weights to file ./customFunctionLearningAdaptive/baselineMLPFinal/best_weights\n",
      "The loss after 1468 iterations is E=0.013002923481970779\n",
      "Wrote weights to file ./customFunctionLearningAdaptive/baselineMLPFinal/best_weights\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The loss after 1469 iterations is E=0.013002625483635575\n",
      "Wrote weights to file ./customFunctionLearningAdaptive/baselineMLPFinal/best_weights\n",
      "The loss after 1470 iterations is E=0.01300232759471596\n",
      "Wrote weights to file ./customFunctionLearningAdaptive/baselineMLPFinal/best_weights\n",
      "The loss after 1471 iterations is E=0.013002029813536473\n",
      "Wrote weights to file ./customFunctionLearningAdaptive/baselineMLPFinal/best_weights\n",
      "The loss after 1472 iterations is E=0.013001732138444976\n",
      "Wrote weights to file ./customFunctionLearningAdaptive/baselineMLPFinal/best_weights\n",
      "The loss after 1473 iterations is E=0.013001434567812048\n",
      "Wrote weights to file ./customFunctionLearningAdaptive/baselineMLPFinal/best_weights\n",
      "The loss after 1474 iterations is E=0.013001137100030603\n",
      "Wrote weights to file ./customFunctionLearningAdaptive/baselineMLPFinal/best_weights\n",
      "The loss after 1475 iterations is E=0.013000839733515984\n",
      "Wrote weights to file ./customFunctionLearningAdaptive/baselineMLPFinal/best_weights\n",
      "The loss after 1476 iterations is E=0.013000542466705239\n",
      "Wrote weights to file ./customFunctionLearningAdaptive/baselineMLPFinal/best_weights\n",
      "The loss after 1477 iterations is E=0.013000245298057149\n",
      "Wrote weights to file ./customFunctionLearningAdaptive/baselineMLPFinal/best_weights\n",
      "The loss after 1478 iterations is E=0.012999948226051572\n",
      "Wrote weights to file ./customFunctionLearningAdaptive/baselineMLPFinal/best_weights\n",
      "The loss after 1479 iterations is E=0.012999651249189554\n",
      "Wrote weights to file ./customFunctionLearningAdaptive/baselineMLPFinal/best_weights\n",
      "The loss after 1480 iterations is E=0.012999354365992803\n",
      "Wrote weights to file ./customFunctionLearningAdaptive/baselineMLPFinal/best_weights\n",
      "The loss after 1481 iterations is E=0.012999057575003393\n",
      "Wrote weights to file ./customFunctionLearningAdaptive/baselineMLPFinal/best_weights\n",
      "The loss after 1482 iterations is E=0.012998760874783598\n",
      "Wrote weights to file ./customFunctionLearningAdaptive/baselineMLPFinal/best_weights\n",
      "The loss after 1483 iterations is E=0.01299846426391548\n",
      "Wrote weights to file ./customFunctionLearningAdaptive/baselineMLPFinal/best_weights\n",
      "The loss after 1484 iterations is E=0.012998167741000823\n",
      "Wrote weights to file ./customFunctionLearningAdaptive/baselineMLPFinal/best_weights\n",
      "The loss after 1485 iterations is E=0.01299787130466066\n",
      "Wrote weights to file ./customFunctionLearningAdaptive/baselineMLPFinal/best_weights\n",
      "The loss after 1486 iterations is E=0.012997574953534999\n",
      "Wrote weights to file ./customFunctionLearningAdaptive/baselineMLPFinal/best_weights\n",
      "The loss after 1487 iterations is E=0.012997278686282821\n",
      "Wrote weights to file ./customFunctionLearningAdaptive/baselineMLPFinal/best_weights\n",
      "The loss after 1488 iterations is E=0.012996982501581503\n",
      "Wrote weights to file ./customFunctionLearningAdaptive/baselineMLPFinal/best_weights\n",
      "The loss after 1489 iterations is E=0.012996686398126713\n",
      "Wrote weights to file ./customFunctionLearningAdaptive/baselineMLPFinal/best_weights\n",
      "The loss after 1490 iterations is E=0.012996390374632228\n",
      "Wrote weights to file ./customFunctionLearningAdaptive/baselineMLPFinal/best_weights\n",
      "The loss after 1491 iterations is E=0.0129960944298296\n",
      "Wrote weights to file ./customFunctionLearningAdaptive/baselineMLPFinal/best_weights\n",
      "The loss after 1492 iterations is E=0.012995798562467796\n",
      "Wrote weights to file ./customFunctionLearningAdaptive/baselineMLPFinal/best_weights\n",
      "The loss after 1493 iterations is E=0.01299550277131309\n",
      "Wrote weights to file ./customFunctionLearningAdaptive/baselineMLPFinal/best_weights\n",
      "The loss after 1494 iterations is E=0.012995207055149003\n",
      "Wrote weights to file ./customFunctionLearningAdaptive/baselineMLPFinal/best_weights\n",
      "The loss after 1495 iterations is E=0.012994911412775551\n",
      "Wrote weights to file ./customFunctionLearningAdaptive/baselineMLPFinal/best_weights\n",
      "The loss after 1496 iterations is E=0.01299461584300955\n",
      "Wrote weights to file ./customFunctionLearningAdaptive/baselineMLPFinal/best_weights\n",
      "The loss after 1497 iterations is E=0.01299432034468406\n",
      "Wrote weights to file ./customFunctionLearningAdaptive/baselineMLPFinal/best_weights\n",
      "The loss after 1498 iterations is E=0.012994024916648294\n",
      "Wrote weights to file ./customFunctionLearningAdaptive/baselineMLPFinal/best_weights\n",
      "The loss after 1499 iterations is E=0.012993729557767291\n",
      "Wrote weights to file ./customFunctionLearningAdaptive/baselineMLPFinal/best_weights\n",
      "The loss after 1500 iterations is E=0.01299343426692188\n",
      "Wrote weights to file ./customFunctionLearningAdaptive/baselineMLPFinal/best_weights\n",
      "The loss after 1501 iterations is E=0.012993139043008216\n",
      "Wrote weights to file ./customFunctionLearningAdaptive/baselineMLPFinal/best_weights\n",
      "The loss after 1502 iterations is E=0.012992843884937717\n",
      "Wrote weights to file ./customFunctionLearningAdaptive/baselineMLPFinal/best_weights\n",
      "The loss after 1503 iterations is E=0.01299254879163679\n",
      "Wrote weights to file ./customFunctionLearningAdaptive/baselineMLPFinal/best_weights\n",
      "The loss after 1504 iterations is E=0.012992253762046786\n",
      "Wrote weights to file ./customFunctionLearningAdaptive/baselineMLPFinal/best_weights\n",
      "The loss after 1505 iterations is E=0.01299195879512338\n",
      "Wrote weights to file ./customFunctionLearningAdaptive/baselineMLPFinal/best_weights\n",
      "The loss after 1506 iterations is E=0.012991663889836982\n",
      "Wrote weights to file ./customFunctionLearningAdaptive/baselineMLPFinal/best_weights\n",
      "The loss after 1507 iterations is E=0.012991369045171845\n",
      "Wrote weights to file ./customFunctionLearningAdaptive/baselineMLPFinal/best_weights\n",
      "The loss after 1508 iterations is E=0.012991074260126441\n",
      "Wrote weights to file ./customFunctionLearningAdaptive/baselineMLPFinal/best_weights\n",
      "The loss after 1509 iterations is E=0.012990779533712925\n",
      "Wrote weights to file ./customFunctionLearningAdaptive/baselineMLPFinal/best_weights\n",
      "The loss after 1510 iterations is E=0.012990484864957158\n",
      "Wrote weights to file ./customFunctionLearningAdaptive/baselineMLPFinal/best_weights\n",
      "The loss after 1511 iterations is E=0.012990190252898113\n",
      "Wrote weights to file ./customFunctionLearningAdaptive/baselineMLPFinal/best_weights\n",
      "The loss after 1512 iterations is E=0.012989895696588332\n",
      "Wrote weights to file ./customFunctionLearningAdaptive/baselineMLPFinal/best_weights\n",
      "The loss after 1513 iterations is E=0.012989601195093205\n",
      "Wrote weights to file ./customFunctionLearningAdaptive/baselineMLPFinal/best_weights\n",
      "The loss after 1514 iterations is E=0.012989306747490924\n",
      "Wrote weights to file ./customFunctionLearningAdaptive/baselineMLPFinal/best_weights\n",
      "The loss after 1515 iterations is E=0.012989012352872345\n",
      "Wrote weights to file ./customFunctionLearningAdaptive/baselineMLPFinal/best_weights\n",
      "The loss after 1516 iterations is E=0.012988718010340917\n",
      "Wrote weights to file ./customFunctionLearningAdaptive/baselineMLPFinal/best_weights\n",
      "The loss after 1517 iterations is E=0.012988423719012225\n",
      "Wrote weights to file ./customFunctionLearningAdaptive/baselineMLPFinal/best_weights\n",
      "The loss after 1518 iterations is E=0.012988129478014018\n",
      "Wrote weights to file ./customFunctionLearningAdaptive/baselineMLPFinal/best_weights\n",
      "The loss after 1519 iterations is E=0.012987835286486093\n",
      "Wrote weights to file ./customFunctionLearningAdaptive/baselineMLPFinal/best_weights\n",
      "The loss after 1520 iterations is E=0.012987541143579863\n",
      "Wrote weights to file ./customFunctionLearningAdaptive/baselineMLPFinal/best_weights\n",
      "The loss after 1521 iterations is E=0.012987247048458451\n",
      "Wrote weights to file ./customFunctionLearningAdaptive/baselineMLPFinal/best_weights\n",
      "The loss after 1522 iterations is E=0.012986953000296356\n",
      "Wrote weights to file ./customFunctionLearningAdaptive/baselineMLPFinal/best_weights\n",
      "The loss after 1523 iterations is E=0.012986658998279403\n",
      "Wrote weights to file ./customFunctionLearningAdaptive/baselineMLPFinal/best_weights\n",
      "The loss after 1524 iterations is E=0.01298636504160453\n",
      "Wrote weights to file ./customFunctionLearningAdaptive/baselineMLPFinal/best_weights\n",
      "The loss after 1525 iterations is E=0.012986071129479457\n",
      "Wrote weights to file ./customFunctionLearningAdaptive/baselineMLPFinal/best_weights\n",
      "The loss after 1526 iterations is E=0.012985777261122974\n",
      "Wrote weights to file ./customFunctionLearningAdaptive/baselineMLPFinal/best_weights\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The loss after 1527 iterations is E=0.012985483435764321\n",
      "Wrote weights to file ./customFunctionLearningAdaptive/baselineMLPFinal/best_weights\n",
      "The loss after 1528 iterations is E=0.012985189652643248\n",
      "Wrote weights to file ./customFunctionLearningAdaptive/baselineMLPFinal/best_weights\n",
      "The loss after 1529 iterations is E=0.01298489591100991\n",
      "Wrote weights to file ./customFunctionLearningAdaptive/baselineMLPFinal/best_weights\n",
      "The loss after 1530 iterations is E=0.012984602210124456\n",
      "Wrote weights to file ./customFunctionLearningAdaptive/baselineMLPFinal/best_weights\n",
      "The loss after 1531 iterations is E=0.012984308549257327\n",
      "Wrote weights to file ./customFunctionLearningAdaptive/baselineMLPFinal/best_weights\n",
      "The loss after 1532 iterations is E=0.012984014927688699\n",
      "Wrote weights to file ./customFunctionLearningAdaptive/baselineMLPFinal/best_weights\n",
      "The loss after 1533 iterations is E=0.012983721344708418\n",
      "Wrote weights to file ./customFunctionLearningAdaptive/baselineMLPFinal/best_weights\n",
      "The loss after 1534 iterations is E=0.012983427799616183\n",
      "Wrote weights to file ./customFunctionLearningAdaptive/baselineMLPFinal/best_weights\n",
      "The loss after 1535 iterations is E=0.012983134291720815\n",
      "Wrote weights to file ./customFunctionLearningAdaptive/baselineMLPFinal/best_weights\n",
      "The loss after 1536 iterations is E=0.012982840820340704\n",
      "Wrote weights to file ./customFunctionLearningAdaptive/baselineMLPFinal/best_weights\n",
      "The loss after 1537 iterations is E=0.012982547384803303\n",
      "Wrote weights to file ./customFunctionLearningAdaptive/baselineMLPFinal/best_weights\n",
      "The loss after 1538 iterations is E=0.012982253984445354\n",
      "Wrote weights to file ./customFunctionLearningAdaptive/baselineMLPFinal/best_weights\n",
      "The loss after 1539 iterations is E=0.012981960618612162\n",
      "Wrote weights to file ./customFunctionLearningAdaptive/baselineMLPFinal/best_weights\n",
      "The loss after 1540 iterations is E=0.012981667286658057\n",
      "Wrote weights to file ./customFunctionLearningAdaptive/baselineMLPFinal/best_weights\n",
      "The loss after 1541 iterations is E=0.012981373987945915\n",
      "Wrote weights to file ./customFunctionLearningAdaptive/baselineMLPFinal/best_weights\n",
      "The loss after 1542 iterations is E=0.012981080721847213\n",
      "Wrote weights to file ./customFunctionLearningAdaptive/baselineMLPFinal/best_weights\n",
      "The loss after 1543 iterations is E=0.012980787487741839\n",
      "Wrote weights to file ./customFunctionLearningAdaptive/baselineMLPFinal/best_weights\n",
      "The loss after 1544 iterations is E=0.012980494285017912\n",
      "Wrote weights to file ./customFunctionLearningAdaptive/baselineMLPFinal/best_weights\n",
      "The loss after 1545 iterations is E=0.012980201113071707\n",
      "Wrote weights to file ./customFunctionLearningAdaptive/baselineMLPFinal/best_weights\n",
      "The loss after 1546 iterations is E=0.012979907971307617\n",
      "Wrote weights to file ./customFunctionLearningAdaptive/baselineMLPFinal/best_weights\n",
      "The loss after 1547 iterations is E=0.012979614859137907\n",
      "Wrote weights to file ./customFunctionLearningAdaptive/baselineMLPFinal/best_weights\n",
      "The loss after 1548 iterations is E=0.01297932177598262\n",
      "Wrote weights to file ./customFunctionLearningAdaptive/baselineMLPFinal/best_weights\n",
      "The loss after 1549 iterations is E=0.012979028721269478\n",
      "Wrote weights to file ./customFunctionLearningAdaptive/baselineMLPFinal/best_weights\n",
      "The loss after 1550 iterations is E=0.012978735694433799\n",
      "Wrote weights to file ./customFunctionLearningAdaptive/baselineMLPFinal/best_weights\n",
      "The loss after 1551 iterations is E=0.012978442694918489\n",
      "Wrote weights to file ./customFunctionLearningAdaptive/baselineMLPFinal/best_weights\n",
      "The loss after 1552 iterations is E=0.012978149722173503\n",
      "Wrote weights to file ./customFunctionLearningAdaptive/baselineMLPFinal/best_weights\n",
      "The loss after 1553 iterations is E=0.01297785677565638\n",
      "Wrote weights to file ./customFunctionLearningAdaptive/baselineMLPFinal/best_weights\n",
      "The loss after 1554 iterations is E=0.012977563854831507\n",
      "Wrote weights to file ./customFunctionLearningAdaptive/baselineMLPFinal/best_weights\n",
      "The loss after 1555 iterations is E=0.012977270959170522\n",
      "Wrote weights to file ./customFunctionLearningAdaptive/baselineMLPFinal/best_weights\n",
      "The loss after 1556 iterations is E=0.012976978088151759\n",
      "Wrote weights to file ./customFunctionLearningAdaptive/baselineMLPFinal/best_weights\n",
      "The loss after 1557 iterations is E=0.012976685241260593\n",
      "Wrote weights to file ./customFunctionLearningAdaptive/baselineMLPFinal/best_weights\n",
      "The loss after 1558 iterations is E=0.012976392417988949\n",
      "Wrote weights to file ./customFunctionLearningAdaptive/baselineMLPFinal/best_weights\n",
      "The loss after 1559 iterations is E=0.01297609961783544\n",
      "Wrote weights to file ./customFunctionLearningAdaptive/baselineMLPFinal/best_weights\n",
      "The loss after 1560 iterations is E=0.012975806840305285\n",
      "Wrote weights to file ./customFunctionLearningAdaptive/baselineMLPFinal/best_weights\n",
      "The loss after 1561 iterations is E=0.01297551408490982\n",
      "Wrote weights to file ./customFunctionLearningAdaptive/baselineMLPFinal/best_weights\n",
      "The loss after 1562 iterations is E=0.01297522135116703\n",
      "Wrote weights to file ./customFunctionLearningAdaptive/baselineMLPFinal/best_weights\n",
      "The loss after 1563 iterations is E=0.01297492863860095\n",
      "Wrote weights to file ./customFunctionLearningAdaptive/baselineMLPFinal/best_weights\n",
      "The loss after 1564 iterations is E=0.012974635946741865\n",
      "Wrote weights to file ./customFunctionLearningAdaptive/baselineMLPFinal/best_weights\n",
      "The loss after 1565 iterations is E=0.012974343275125838\n",
      "Wrote weights to file ./customFunctionLearningAdaptive/baselineMLPFinal/best_weights\n",
      "The loss after 1566 iterations is E=0.012974050623295257\n",
      "Wrote weights to file ./customFunctionLearningAdaptive/baselineMLPFinal/best_weights\n",
      "The loss after 1567 iterations is E=0.012973757990798066\n",
      "Wrote weights to file ./customFunctionLearningAdaptive/baselineMLPFinal/best_weights\n",
      "The loss after 1568 iterations is E=0.012973465377188126\n",
      "Wrote weights to file ./customFunctionLearningAdaptive/baselineMLPFinal/best_weights\n",
      "The loss after 1569 iterations is E=0.012973172782024938\n",
      "Wrote weights to file ./customFunctionLearningAdaptive/baselineMLPFinal/best_weights\n",
      "The loss after 1570 iterations is E=0.012972880204873586\n",
      "Wrote weights to file ./customFunctionLearningAdaptive/baselineMLPFinal/best_weights\n",
      "The loss after 1571 iterations is E=0.012972587645304663\n",
      "Wrote weights to file ./customFunctionLearningAdaptive/baselineMLPFinal/best_weights\n",
      "The loss after 1572 iterations is E=0.012972295102894226\n",
      "Wrote weights to file ./customFunctionLearningAdaptive/baselineMLPFinal/best_weights\n",
      "The loss after 1573 iterations is E=0.01297200257722365\n",
      "Wrote weights to file ./customFunctionLearningAdaptive/baselineMLPFinal/best_weights\n",
      "The loss after 1574 iterations is E=0.01297171006787955\n",
      "Wrote weights to file ./customFunctionLearningAdaptive/baselineMLPFinal/best_weights\n",
      "The loss after 1575 iterations is E=0.012971417574453711\n",
      "Wrote weights to file ./customFunctionLearningAdaptive/baselineMLPFinal/best_weights\n",
      "The loss after 1576 iterations is E=0.01297112509654315\n",
      "Wrote weights to file ./customFunctionLearningAdaptive/baselineMLPFinal/best_weights\n",
      "The loss after 1577 iterations is E=0.012970832633749731\n",
      "Wrote weights to file ./customFunctionLearningAdaptive/baselineMLPFinal/best_weights\n",
      "The loss after 1578 iterations is E=0.012970540185680344\n",
      "Wrote weights to file ./customFunctionLearningAdaptive/baselineMLPFinal/best_weights\n",
      "The loss after 1579 iterations is E=0.0129702477519469\n",
      "Wrote weights to file ./customFunctionLearningAdaptive/baselineMLPFinal/best_weights\n",
      "The loss after 1580 iterations is E=0.012969955332165679\n",
      "Wrote weights to file ./customFunctionLearningAdaptive/baselineMLPFinal/best_weights\n",
      "The loss after 1581 iterations is E=0.012969662925958206\n",
      "Wrote weights to file ./customFunctionLearningAdaptive/baselineMLPFinal/best_weights\n",
      "The loss after 1582 iterations is E=0.012969370532950225\n",
      "Wrote weights to file ./customFunctionLearningAdaptive/baselineMLPFinal/best_weights\n",
      "The loss after 1583 iterations is E=0.012969078152772314\n",
      "Wrote weights to file ./customFunctionLearningAdaptive/baselineMLPFinal/best_weights\n",
      "The loss after 1584 iterations is E=0.012968785785059648\n",
      "Wrote weights to file ./customFunctionLearningAdaptive/baselineMLPFinal/best_weights\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The loss after 1585 iterations is E=0.012968493429451396\n",
      "Wrote weights to file ./customFunctionLearningAdaptive/baselineMLPFinal/best_weights\n",
      "The loss after 1586 iterations is E=0.01296820108559154\n",
      "Wrote weights to file ./customFunctionLearningAdaptive/baselineMLPFinal/best_weights\n",
      "The loss after 1587 iterations is E=0.012967908753128239\n",
      "Wrote weights to file ./customFunctionLearningAdaptive/baselineMLPFinal/best_weights\n",
      "The loss after 1588 iterations is E=0.012967616431713683\n",
      "Wrote weights to file ./customFunctionLearningAdaptive/baselineMLPFinal/best_weights\n",
      "The loss after 1589 iterations is E=0.012967324121004535\n",
      "Wrote weights to file ./customFunctionLearningAdaptive/baselineMLPFinal/best_weights\n",
      "The loss after 1590 iterations is E=0.012967031820661348\n",
      "Wrote weights to file ./customFunctionLearningAdaptive/baselineMLPFinal/best_weights\n",
      "The loss after 1591 iterations is E=0.012966739530348862\n",
      "Wrote weights to file ./customFunctionLearningAdaptive/baselineMLPFinal/best_weights\n",
      "The loss after 1592 iterations is E=0.01296644724973566\n",
      "Wrote weights to file ./customFunctionLearningAdaptive/baselineMLPFinal/best_weights\n",
      "The loss after 1593 iterations is E=0.012966154978494402\n",
      "Wrote weights to file ./customFunctionLearningAdaptive/baselineMLPFinal/best_weights\n",
      "The loss after 1594 iterations is E=0.012965862716301377\n",
      "Wrote weights to file ./customFunctionLearningAdaptive/baselineMLPFinal/best_weights\n",
      "The loss after 1595 iterations is E=0.01296557046283686\n",
      "Wrote weights to file ./customFunctionLearningAdaptive/baselineMLPFinal/best_weights\n",
      "The loss after 1596 iterations is E=0.012965278217784725\n",
      "Wrote weights to file ./customFunctionLearningAdaptive/baselineMLPFinal/best_weights\n",
      "The loss after 1597 iterations is E=0.012964985980832776\n",
      "Wrote weights to file ./customFunctionLearningAdaptive/baselineMLPFinal/best_weights\n",
      "The loss after 1598 iterations is E=0.012964693751672036\n",
      "Wrote weights to file ./customFunctionLearningAdaptive/baselineMLPFinal/best_weights\n",
      "The loss after 1599 iterations is E=0.012964401529997362\n",
      "Wrote weights to file ./customFunctionLearningAdaptive/baselineMLPFinal/best_weights\n",
      "The loss after 1600 iterations is E=0.012964109315507169\n",
      "Wrote weights to file ./customFunctionLearningAdaptive/baselineMLPFinal/best_weights\n",
      "The loss after 1601 iterations is E=0.012963817107903099\n",
      "Wrote weights to file ./customFunctionLearningAdaptive/baselineMLPFinal/best_weights\n",
      "The loss after 1602 iterations is E=0.012963524906890343\n",
      "Wrote weights to file ./customFunctionLearningAdaptive/baselineMLPFinal/best_weights\n",
      "The loss after 1603 iterations is E=0.012963232712177416\n",
      "Wrote weights to file ./customFunctionLearningAdaptive/baselineMLPFinal/best_weights\n",
      "The loss after 1604 iterations is E=0.012962940523476033\n",
      "Wrote weights to file ./customFunctionLearningAdaptive/baselineMLPFinal/best_weights\n",
      "The loss after 1605 iterations is E=0.012962648340501296\n",
      "Wrote weights to file ./customFunctionLearningAdaptive/baselineMLPFinal/best_weights\n",
      "The loss after 1606 iterations is E=0.012962356162971388\n",
      "Wrote weights to file ./customFunctionLearningAdaptive/baselineMLPFinal/best_weights\n",
      "The loss after 1607 iterations is E=0.01296206399060763\n",
      "Wrote weights to file ./customFunctionLearningAdaptive/baselineMLPFinal/best_weights\n",
      "The loss after 1608 iterations is E=0.012961771823134545\n",
      "Wrote weights to file ./customFunctionLearningAdaptive/baselineMLPFinal/best_weights\n",
      "The loss after 1609 iterations is E=0.012961479660279386\n",
      "Wrote weights to file ./customFunctionLearningAdaptive/baselineMLPFinal/best_weights\n",
      "The loss after 1610 iterations is E=0.01296118750177292\n",
      "Wrote weights to file ./customFunctionLearningAdaptive/baselineMLPFinal/best_weights\n",
      "The loss after 1611 iterations is E=0.012960895347348357\n",
      "Wrote weights to file ./customFunctionLearningAdaptive/baselineMLPFinal/best_weights\n",
      "The loss after 1612 iterations is E=0.012960603196741955\n",
      "Wrote weights to file ./customFunctionLearningAdaptive/baselineMLPFinal/best_weights\n",
      "The loss after 1613 iterations is E=0.012960311049693028\n",
      "Wrote weights to file ./customFunctionLearningAdaptive/baselineMLPFinal/best_weights\n",
      "The loss after 1614 iterations is E=0.012960018905943362\n",
      "Wrote weights to file ./customFunctionLearningAdaptive/baselineMLPFinal/best_weights\n",
      "The loss after 1615 iterations is E=0.012959726765237753\n",
      "Wrote weights to file ./customFunctionLearningAdaptive/baselineMLPFinal/best_weights\n",
      "The loss after 1616 iterations is E=0.012959434627323633\n",
      "Wrote weights to file ./customFunctionLearningAdaptive/baselineMLPFinal/best_weights\n",
      "The loss after 1617 iterations is E=0.012959142491950982\n",
      "Wrote weights to file ./customFunctionLearningAdaptive/baselineMLPFinal/best_weights\n",
      "The loss after 1618 iterations is E=0.012958850358872702\n",
      "Wrote weights to file ./customFunctionLearningAdaptive/baselineMLPFinal/best_weights\n",
      "The loss after 1619 iterations is E=0.012958558227843944\n",
      "Wrote weights to file ./customFunctionLearningAdaptive/baselineMLPFinal/best_weights\n",
      "The loss after 1620 iterations is E=0.012958266098622658\n",
      "Wrote weights to file ./customFunctionLearningAdaptive/baselineMLPFinal/best_weights\n",
      "The loss after 1621 iterations is E=0.012957973970969183\n",
      "Wrote weights to file ./customFunctionLearningAdaptive/baselineMLPFinal/best_weights\n",
      "The loss after 1622 iterations is E=0.012957681844646306\n",
      "Wrote weights to file ./customFunctionLearningAdaptive/baselineMLPFinal/best_weights\n",
      "The loss after 1623 iterations is E=0.0129573897194192\n",
      "Wrote weights to file ./customFunctionLearningAdaptive/baselineMLPFinal/best_weights\n",
      "The loss after 1624 iterations is E=0.012957097595055672\n",
      "Wrote weights to file ./customFunctionLearningAdaptive/baselineMLPFinal/best_weights\n",
      "The loss after 1625 iterations is E=0.012956805471325607\n",
      "Wrote weights to file ./customFunctionLearningAdaptive/baselineMLPFinal/best_weights\n",
      "The loss after 1626 iterations is E=0.012956513348001238\n",
      "Wrote weights to file ./customFunctionLearningAdaptive/baselineMLPFinal/best_weights\n",
      "The loss after 1627 iterations is E=0.012956221224857178\n",
      "Wrote weights to file ./customFunctionLearningAdaptive/baselineMLPFinal/best_weights\n",
      "The loss after 1628 iterations is E=0.012955929101670267\n",
      "Wrote weights to file ./customFunctionLearningAdaptive/baselineMLPFinal/best_weights\n",
      "The loss after 1629 iterations is E=0.012955636978219357\n",
      "Wrote weights to file ./customFunctionLearningAdaptive/baselineMLPFinal/best_weights\n",
      "The loss after 1630 iterations is E=0.012955344854285717\n",
      "Wrote weights to file ./customFunctionLearningAdaptive/baselineMLPFinal/best_weights\n",
      "The loss after 1631 iterations is E=0.012955052729652566\n",
      "Wrote weights to file ./customFunctionLearningAdaptive/baselineMLPFinal/best_weights\n",
      "The loss after 1632 iterations is E=0.012954760604105282\n",
      "Wrote weights to file ./customFunctionLearningAdaptive/baselineMLPFinal/best_weights\n",
      "The loss after 1633 iterations is E=0.012954468477431274\n",
      "Wrote weights to file ./customFunctionLearningAdaptive/baselineMLPFinal/best_weights\n",
      "The loss after 1634 iterations is E=0.012954176349420114\n",
      "Wrote weights to file ./customFunctionLearningAdaptive/baselineMLPFinal/best_weights\n",
      "The loss after 1635 iterations is E=0.012953884219863092\n",
      "Wrote weights to file ./customFunctionLearningAdaptive/baselineMLPFinal/best_weights\n",
      "The loss after 1636 iterations is E=0.012953592088553696\n",
      "Wrote weights to file ./customFunctionLearningAdaptive/baselineMLPFinal/best_weights\n",
      "The loss after 1637 iterations is E=0.012953299955287299\n",
      "Wrote weights to file ./customFunctionLearningAdaptive/baselineMLPFinal/best_weights\n",
      "The loss after 1638 iterations is E=0.012953007819861004\n",
      "Wrote weights to file ./customFunctionLearningAdaptive/baselineMLPFinal/best_weights\n",
      "The loss after 1639 iterations is E=0.012952715682073977\n",
      "Wrote weights to file ./customFunctionLearningAdaptive/baselineMLPFinal/best_weights\n",
      "The loss after 1640 iterations is E=0.012952423541727258\n",
      "Wrote weights to file ./customFunctionLearningAdaptive/baselineMLPFinal/best_weights\n",
      "The loss after 1641 iterations is E=0.012952131398623397\n",
      "Wrote weights to file ./customFunctionLearningAdaptive/baselineMLPFinal/best_weights\n",
      "The loss after 1642 iterations is E=0.012951839252566946\n",
      "Wrote weights to file ./customFunctionLearningAdaptive/baselineMLPFinal/best_weights\n",
      "The loss after 1643 iterations is E=0.012951547103364365\n",
      "Wrote weights to file ./customFunctionLearningAdaptive/baselineMLPFinal/best_weights\n",
      "The loss after 1644 iterations is E=0.012951254950823339\n",
      "Wrote weights to file ./customFunctionLearningAdaptive/baselineMLPFinal/best_weights\n",
      "The loss after 1645 iterations is E=0.012950962794753697\n",
      "Wrote weights to file ./customFunctionLearningAdaptive/baselineMLPFinal/best_weights\n",
      "The loss after 1646 iterations is E=0.012950670634966799\n",
      "Wrote weights to file ./customFunctionLearningAdaptive/baselineMLPFinal/best_weights\n",
      "The loss after 1647 iterations is E=0.012950378471275526\n",
      "Wrote weights to file ./customFunctionLearningAdaptive/baselineMLPFinal/best_weights\n",
      "The loss after 1648 iterations is E=0.012950086303494415\n",
      "Wrote weights to file ./customFunctionLearningAdaptive/baselineMLPFinal/best_weights\n",
      "The loss after 1649 iterations is E=0.012949794131439681\n",
      "Wrote weights to file ./customFunctionLearningAdaptive/baselineMLPFinal/best_weights\n",
      "The loss after 1650 iterations is E=0.012949501954929018\n",
      "Wrote weights to file ./customFunctionLearningAdaptive/baselineMLPFinal/best_weights\n",
      "The loss after 1651 iterations is E=0.012949209773781608\n",
      "Wrote weights to file ./customFunctionLearningAdaptive/baselineMLPFinal/best_weights\n",
      "The loss after 1652 iterations is E=0.01294891758781827\n",
      "Wrote weights to file ./customFunctionLearningAdaptive/baselineMLPFinal/best_weights\n",
      "The loss after 1653 iterations is E=0.01294862539686118\n",
      "Wrote weights to file ./customFunctionLearningAdaptive/baselineMLPFinal/best_weights\n",
      "The loss after 1654 iterations is E=0.012948333200733946\n",
      "Wrote weights to file ./customFunctionLearningAdaptive/baselineMLPFinal/best_weights\n",
      "The loss after 1655 iterations is E=0.012948040999261702\n",
      "Wrote weights to file ./customFunctionLearningAdaptive/baselineMLPFinal/best_weights\n",
      "The loss after 1656 iterations is E=0.012947748792271022\n",
      "Wrote weights to file ./customFunctionLearningAdaptive/baselineMLPFinal/best_weights\n",
      "The loss after 1657 iterations is E=0.012947456579589754\n",
      "Wrote weights to file ./customFunctionLearningAdaptive/baselineMLPFinal/best_weights\n",
      "The loss after 1658 iterations is E=0.012947164361047122\n",
      "Wrote weights to file ./customFunctionLearningAdaptive/baselineMLPFinal/best_weights\n",
      "The loss after 1659 iterations is E=0.01294687213647368\n",
      "Wrote weights to file ./customFunctionLearningAdaptive/baselineMLPFinal/best_weights\n",
      "The loss after 1660 iterations is E=0.01294657990570144\n",
      "Wrote weights to file ./customFunctionLearningAdaptive/baselineMLPFinal/best_weights\n",
      "The loss after 1661 iterations is E=0.012946287668563485\n",
      "Wrote weights to file ./customFunctionLearningAdaptive/baselineMLPFinal/best_weights\n",
      "The loss after 1662 iterations is E=0.012945995424894337\n",
      "Wrote weights to file ./customFunctionLearningAdaptive/baselineMLPFinal/best_weights\n",
      "The loss after 1663 iterations is E=0.012945703174529765\n",
      "Wrote weights to file ./customFunctionLearningAdaptive/baselineMLPFinal/best_weights\n",
      "The loss after 1664 iterations is E=0.012945410917306586\n",
      "Wrote weights to file ./customFunctionLearningAdaptive/baselineMLPFinal/best_weights\n",
      "The loss after 1665 iterations is E=0.012945118653063041\n",
      "Wrote weights to file ./customFunctionLearningAdaptive/baselineMLPFinal/best_weights\n",
      "The loss after 1666 iterations is E=0.0129448263816385\n",
      "Wrote weights to file ./customFunctionLearningAdaptive/baselineMLPFinal/best_weights\n",
      "The loss after 1667 iterations is E=0.012944534102873441\n",
      "Wrote weights to file ./customFunctionLearningAdaptive/baselineMLPFinal/best_weights\n",
      "The loss after 1668 iterations is E=0.012944241816609536\n",
      "Wrote weights to file ./customFunctionLearningAdaptive/baselineMLPFinal/best_weights\n",
      "The loss after 1669 iterations is E=0.012943949522689508\n",
      "Wrote weights to file ./customFunctionLearningAdaptive/baselineMLPFinal/best_weights\n",
      "The loss after 1670 iterations is E=0.012943657220957448\n",
      "Wrote weights to file ./customFunctionLearningAdaptive/baselineMLPFinal/best_weights\n",
      "The loss after 1671 iterations is E=0.012943364911258196\n",
      "Wrote weights to file ./customFunctionLearningAdaptive/baselineMLPFinal/best_weights\n",
      "The loss after 1672 iterations is E=0.012943072593437896\n",
      "Wrote weights to file ./customFunctionLearningAdaptive/baselineMLPFinal/best_weights\n",
      "The loss after 1673 iterations is E=0.012942780267343641\n",
      "Wrote weights to file ./customFunctionLearningAdaptive/baselineMLPFinal/best_weights\n",
      "The loss after 1674 iterations is E=0.012942487932823599\n",
      "Wrote weights to file ./customFunctionLearningAdaptive/baselineMLPFinal/best_weights\n",
      "The loss after 1675 iterations is E=0.012942195589727102\n",
      "Wrote weights to file ./customFunctionLearningAdaptive/baselineMLPFinal/best_weights\n",
      "The loss after 1676 iterations is E=0.012941903237904165\n",
      "Wrote weights to file ./customFunctionLearningAdaptive/baselineMLPFinal/best_weights\n",
      "The loss after 1677 iterations is E=0.012941610877206076\n",
      "Wrote weights to file ./customFunctionLearningAdaptive/baselineMLPFinal/best_weights\n",
      "The loss after 1678 iterations is E=0.012941318507485\n",
      "Wrote weights to file ./customFunctionLearningAdaptive/baselineMLPFinal/best_weights\n",
      "The loss after 1679 iterations is E=0.012941026128593987\n",
      "Wrote weights to file ./customFunctionLearningAdaptive/baselineMLPFinal/best_weights\n",
      "The loss after 1680 iterations is E=0.012940733740387152\n",
      "Wrote weights to file ./customFunctionLearningAdaptive/baselineMLPFinal/best_weights\n",
      "The loss after 1681 iterations is E=0.012940441342719421\n",
      "Wrote weights to file ./customFunctionLearningAdaptive/baselineMLPFinal/best_weights\n",
      "The loss after 1682 iterations is E=0.012940148935446727\n",
      "Wrote weights to file ./customFunctionLearningAdaptive/baselineMLPFinal/best_weights\n",
      "The loss after 1683 iterations is E=0.012939856518425866\n",
      "Wrote weights to file ./customFunctionLearningAdaptive/baselineMLPFinal/best_weights\n",
      "The loss after 1684 iterations is E=0.012939564091514444\n",
      "Wrote weights to file ./customFunctionLearningAdaptive/baselineMLPFinal/best_weights\n",
      "The loss after 1685 iterations is E=0.012939271654570912\n",
      "Wrote weights to file ./customFunctionLearningAdaptive/baselineMLPFinal/best_weights\n",
      "The loss after 1686 iterations is E=0.012938979207454798\n",
      "Wrote weights to file ./customFunctionLearningAdaptive/baselineMLPFinal/best_weights\n",
      "The loss after 1687 iterations is E=0.012938686750026221\n",
      "Wrote weights to file ./customFunctionLearningAdaptive/baselineMLPFinal/best_weights\n",
      "The loss after 1688 iterations is E=0.012938394282146087\n",
      "Wrote weights to file ./customFunctionLearningAdaptive/baselineMLPFinal/best_weights\n",
      "The loss after 1689 iterations is E=0.012938101803676383\n",
      "Wrote weights to file ./customFunctionLearningAdaptive/baselineMLPFinal/best_weights\n",
      "The loss after 1690 iterations is E=0.012937809314479751\n",
      "Wrote weights to file ./customFunctionLearningAdaptive/baselineMLPFinal/best_weights\n",
      "The loss after 1691 iterations is E=0.012937516814419485\n",
      "Wrote weights to file ./customFunctionLearningAdaptive/baselineMLPFinal/best_weights\n",
      "The loss after 1692 iterations is E=0.012937224303359795\n",
      "Wrote weights to file ./customFunctionLearningAdaptive/baselineMLPFinal/best_weights\n",
      "The loss after 1693 iterations is E=0.012936931781165575\n",
      "Wrote weights to file ./customFunctionLearningAdaptive/baselineMLPFinal/best_weights\n",
      "The loss after 1694 iterations is E=0.012936639247702494\n",
      "Wrote weights to file ./customFunctionLearningAdaptive/baselineMLPFinal/best_weights\n",
      "The loss after 1695 iterations is E=0.012936346702836922\n",
      "Wrote weights to file ./customFunctionLearningAdaptive/baselineMLPFinal/best_weights\n",
      "The loss after 1696 iterations is E=0.012936054146436032\n",
      "Wrote weights to file ./customFunctionLearningAdaptive/baselineMLPFinal/best_weights\n",
      "The loss after 1697 iterations is E=0.012935761578367636\n",
      "Wrote weights to file ./customFunctionLearningAdaptive/baselineMLPFinal/best_weights\n",
      "The loss after 1698 iterations is E=0.012935468998500102\n",
      "Wrote weights to file ./customFunctionLearningAdaptive/baselineMLPFinal/best_weights\n",
      "The loss after 1699 iterations is E=0.01293517640670275\n",
      "Wrote weights to file ./customFunctionLearningAdaptive/baselineMLPFinal/best_weights\n",
      "The loss after 1700 iterations is E=0.012934883802845364\n",
      "Wrote weights to file ./customFunctionLearningAdaptive/baselineMLPFinal/best_weights\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The loss after 1701 iterations is E=0.012934591186798594\n",
      "Wrote weights to file ./customFunctionLearningAdaptive/baselineMLPFinal/best_weights\n",
      "The loss after 1702 iterations is E=0.012934298558433355\n",
      "Wrote weights to file ./customFunctionLearningAdaptive/baselineMLPFinal/best_weights\n",
      "The loss after 1703 iterations is E=0.012934005917621637\n",
      "Wrote weights to file ./customFunctionLearningAdaptive/baselineMLPFinal/best_weights\n",
      "The loss after 1704 iterations is E=0.012933713264235725\n",
      "Wrote weights to file ./customFunctionLearningAdaptive/baselineMLPFinal/best_weights\n",
      "The loss after 1705 iterations is E=0.012933420598148835\n",
      "Wrote weights to file ./customFunctionLearningAdaptive/baselineMLPFinal/best_weights\n",
      "The loss after 1706 iterations is E=0.012933127919234438\n",
      "Wrote weights to file ./customFunctionLearningAdaptive/baselineMLPFinal/best_weights\n",
      "The loss after 1707 iterations is E=0.012932835227366874\n",
      "Wrote weights to file ./customFunctionLearningAdaptive/baselineMLPFinal/best_weights\n",
      "The loss after 1708 iterations is E=0.012932542522420893\n",
      "Wrote weights to file ./customFunctionLearningAdaptive/baselineMLPFinal/best_weights\n",
      "The loss after 1709 iterations is E=0.012932249804271961\n",
      "Wrote weights to file ./customFunctionLearningAdaptive/baselineMLPFinal/best_weights\n",
      "The loss after 1710 iterations is E=0.012931957072795987\n",
      "Wrote weights to file ./customFunctionLearningAdaptive/baselineMLPFinal/best_weights\n",
      "The loss after 1711 iterations is E=0.012931664327869576\n",
      "Wrote weights to file ./customFunctionLearningAdaptive/baselineMLPFinal/best_weights\n",
      "The loss after 1712 iterations is E=0.012931371569369682\n",
      "Wrote weights to file ./customFunctionLearningAdaptive/baselineMLPFinal/best_weights\n",
      "The loss after 1713 iterations is E=0.012931078797173956\n",
      "Wrote weights to file ./customFunctionLearningAdaptive/baselineMLPFinal/best_weights\n",
      "The loss after 1714 iterations is E=0.012930786011160579\n",
      "Wrote weights to file ./customFunctionLearningAdaptive/baselineMLPFinal/best_weights\n",
      "The loss after 1715 iterations is E=0.012930493211208231\n",
      "Wrote weights to file ./customFunctionLearningAdaptive/baselineMLPFinal/best_weights\n",
      "The loss after 1716 iterations is E=0.012930200397195916\n",
      "Wrote weights to file ./customFunctionLearningAdaptive/baselineMLPFinal/best_weights\n",
      "The loss after 1717 iterations is E=0.012929907569003475\n",
      "Wrote weights to file ./customFunctionLearningAdaptive/baselineMLPFinal/best_weights\n",
      "The loss after 1718 iterations is E=0.012929614726511031\n",
      "Wrote weights to file ./customFunctionLearningAdaptive/baselineMLPFinal/best_weights\n",
      "The loss after 1719 iterations is E=0.012929321869599211\n",
      "Wrote weights to file ./customFunctionLearningAdaptive/baselineMLPFinal/best_weights\n",
      "The loss after 1720 iterations is E=0.012929028998149208\n",
      "Wrote weights to file ./customFunctionLearningAdaptive/baselineMLPFinal/best_weights\n",
      "The loss after 1721 iterations is E=0.012928736112042566\n",
      "Wrote weights to file ./customFunctionLearningAdaptive/baselineMLPFinal/best_weights\n",
      "The loss after 1722 iterations is E=0.012928443211161347\n",
      "Wrote weights to file ./customFunctionLearningAdaptive/baselineMLPFinal/best_weights\n",
      "The loss after 1723 iterations is E=0.012928150295388137\n",
      "Wrote weights to file ./customFunctionLearningAdaptive/baselineMLPFinal/best_weights\n",
      "The loss after 1724 iterations is E=0.012927857364605872\n",
      "Wrote weights to file ./customFunctionLearningAdaptive/baselineMLPFinal/best_weights\n",
      "The loss after 1725 iterations is E=0.012927564418697968\n",
      "Wrote weights to file ./customFunctionLearningAdaptive/baselineMLPFinal/best_weights\n",
      "The loss after 1726 iterations is E=0.012927271457548335\n",
      "Wrote weights to file ./customFunctionLearningAdaptive/baselineMLPFinal/best_weights\n",
      "The loss after 1727 iterations is E=0.012926978481041235\n",
      "Wrote weights to file ./customFunctionLearningAdaptive/baselineMLPFinal/best_weights\n",
      "The loss after 1728 iterations is E=0.01292668548906124\n",
      "Wrote weights to file ./customFunctionLearningAdaptive/baselineMLPFinal/best_weights\n",
      "The loss after 1729 iterations is E=0.012926392481493607\n",
      "Wrote weights to file ./customFunctionLearningAdaptive/baselineMLPFinal/best_weights\n",
      "The loss after 1730 iterations is E=0.012926099458223813\n",
      "Wrote weights to file ./customFunctionLearningAdaptive/baselineMLPFinal/best_weights\n",
      "The loss after 1731 iterations is E=0.012925806419137756\n",
      "Wrote weights to file ./customFunctionLearningAdaptive/baselineMLPFinal/best_weights\n",
      "The loss after 1732 iterations is E=0.012925513364121857\n",
      "Wrote weights to file ./customFunctionLearningAdaptive/baselineMLPFinal/best_weights\n",
      "The loss after 1733 iterations is E=0.012925220293062657\n",
      "Wrote weights to file ./customFunctionLearningAdaptive/baselineMLPFinal/best_weights\n",
      "The loss after 1734 iterations is E=0.012924927205847356\n",
      "Wrote weights to file ./customFunctionLearningAdaptive/baselineMLPFinal/best_weights\n",
      "The loss after 1735 iterations is E=0.012924634102363362\n",
      "Wrote weights to file ./customFunctionLearningAdaptive/baselineMLPFinal/best_weights\n",
      "The loss after 1736 iterations is E=0.012924340982498509\n",
      "Wrote weights to file ./customFunctionLearningAdaptive/baselineMLPFinal/best_weights\n",
      "The loss after 1737 iterations is E=0.01292404784614098\n",
      "Wrote weights to file ./customFunctionLearningAdaptive/baselineMLPFinal/best_weights\n",
      "The loss after 1738 iterations is E=0.012923754693179437\n",
      "Wrote weights to file ./customFunctionLearningAdaptive/baselineMLPFinal/best_weights\n",
      "The loss after 1739 iterations is E=0.012923461523502652\n",
      "Wrote weights to file ./customFunctionLearningAdaptive/baselineMLPFinal/best_weights\n",
      "The loss after 1740 iterations is E=0.01292316833699993\n",
      "Wrote weights to file ./customFunctionLearningAdaptive/baselineMLPFinal/best_weights\n",
      "The loss after 1741 iterations is E=0.012922875133560862\n",
      "Wrote weights to file ./customFunctionLearningAdaptive/baselineMLPFinal/best_weights\n",
      "The loss after 1742 iterations is E=0.012922581913075371\n",
      "Wrote weights to file ./customFunctionLearningAdaptive/baselineMLPFinal/best_weights\n",
      "The loss after 1743 iterations is E=0.012922288675433756\n",
      "Wrote weights to file ./customFunctionLearningAdaptive/baselineMLPFinal/best_weights\n",
      "The loss after 1744 iterations is E=0.012921995420526522\n",
      "Wrote weights to file ./customFunctionLearningAdaptive/baselineMLPFinal/best_weights\n",
      "The loss after 1745 iterations is E=0.012921702148244759\n",
      "Wrote weights to file ./customFunctionLearningAdaptive/baselineMLPFinal/best_weights\n",
      "The loss after 1746 iterations is E=0.012921408858479434\n",
      "Wrote weights to file ./customFunctionLearningAdaptive/baselineMLPFinal/best_weights\n",
      "The loss after 1747 iterations is E=0.012921115551122296\n",
      "Wrote weights to file ./customFunctionLearningAdaptive/baselineMLPFinal/best_weights\n",
      "The loss after 1748 iterations is E=0.012920822226065064\n",
      "Wrote weights to file ./customFunctionLearningAdaptive/baselineMLPFinal/best_weights\n",
      "The loss after 1749 iterations is E=0.012920528883199972\n",
      "Wrote weights to file ./customFunctionLearningAdaptive/baselineMLPFinal/best_weights\n",
      "The loss after 1750 iterations is E=0.012920235522419445\n",
      "Wrote weights to file ./customFunctionLearningAdaptive/baselineMLPFinal/best_weights\n",
      "The loss after 1751 iterations is E=0.012919942143616208\n",
      "Wrote weights to file ./customFunctionLearningAdaptive/baselineMLPFinal/best_weights\n",
      "The loss after 1752 iterations is E=0.012919648746683277\n",
      "Wrote weights to file ./customFunctionLearningAdaptive/baselineMLPFinal/best_weights\n",
      "The loss after 1753 iterations is E=0.01291935533151399\n",
      "Wrote weights to file ./customFunctionLearningAdaptive/baselineMLPFinal/best_weights\n",
      "The loss after 1754 iterations is E=0.012919061898001948\n",
      "Wrote weights to file ./customFunctionLearningAdaptive/baselineMLPFinal/best_weights\n",
      "The loss after 1755 iterations is E=0.012918768446040979\n",
      "Wrote weights to file ./customFunctionLearningAdaptive/baselineMLPFinal/best_weights\n",
      "The loss after 1756 iterations is E=0.012918474975525207\n",
      "Wrote weights to file ./customFunctionLearningAdaptive/baselineMLPFinal/best_weights\n",
      "The loss after 1757 iterations is E=0.0129181814863492\n",
      "Wrote weights to file ./customFunctionLearningAdaptive/baselineMLPFinal/best_weights\n",
      "The loss after 1758 iterations is E=0.012917887978407492\n",
      "Wrote weights to file ./customFunctionLearningAdaptive/baselineMLPFinal/best_weights\n",
      "The loss after 1759 iterations is E=0.012917594451595012\n",
      "Wrote weights to file ./customFunctionLearningAdaptive/baselineMLPFinal/best_weights\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The loss after 1760 iterations is E=0.012917300905807024\n",
      "Wrote weights to file ./customFunctionLearningAdaptive/baselineMLPFinal/best_weights\n",
      "The loss after 1761 iterations is E=0.012917007340938962\n",
      "Wrote weights to file ./customFunctionLearningAdaptive/baselineMLPFinal/best_weights\n",
      "The loss after 1762 iterations is E=0.012916713756886529\n",
      "Wrote weights to file ./customFunctionLearningAdaptive/baselineMLPFinal/best_weights\n",
      "The loss after 1763 iterations is E=0.012916420153545665\n",
      "Wrote weights to file ./customFunctionLearningAdaptive/baselineMLPFinal/best_weights\n",
      "The loss after 1764 iterations is E=0.012916126530812533\n",
      "Wrote weights to file ./customFunctionLearningAdaptive/baselineMLPFinal/best_weights\n",
      "The loss after 1765 iterations is E=0.01291583288858355\n",
      "Wrote weights to file ./customFunctionLearningAdaptive/baselineMLPFinal/best_weights\n",
      "The loss after 1766 iterations is E=0.01291553922675554\n",
      "Wrote weights to file ./customFunctionLearningAdaptive/baselineMLPFinal/best_weights\n",
      "The loss after 1767 iterations is E=0.012915245545225218\n",
      "Wrote weights to file ./customFunctionLearningAdaptive/baselineMLPFinal/best_weights\n",
      "The loss after 1768 iterations is E=0.01291495184388982\n",
      "Wrote weights to file ./customFunctionLearningAdaptive/baselineMLPFinal/best_weights\n",
      "The loss after 1769 iterations is E=0.012914658122646713\n",
      "Wrote weights to file ./customFunctionLearningAdaptive/baselineMLPFinal/best_weights\n",
      "The loss after 1770 iterations is E=0.012914364381393382\n",
      "Wrote weights to file ./customFunctionLearningAdaptive/baselineMLPFinal/best_weights\n",
      "The loss after 1771 iterations is E=0.012914070620027734\n",
      "Wrote weights to file ./customFunctionLearningAdaptive/baselineMLPFinal/best_weights\n",
      "The loss after 1772 iterations is E=0.01291377683844774\n",
      "Wrote weights to file ./customFunctionLearningAdaptive/baselineMLPFinal/best_weights\n",
      "The loss after 1773 iterations is E=0.012913483036551668\n",
      "Wrote weights to file ./customFunctionLearningAdaptive/baselineMLPFinal/best_weights\n",
      "The loss after 1774 iterations is E=0.012913189214238006\n",
      "Wrote weights to file ./customFunctionLearningAdaptive/baselineMLPFinal/best_weights\n",
      "The loss after 1775 iterations is E=0.012912895371405364\n",
      "Wrote weights to file ./customFunctionLearningAdaptive/baselineMLPFinal/best_weights\n",
      "The loss after 1776 iterations is E=0.012912601507952612\n",
      "Wrote weights to file ./customFunctionLearningAdaptive/baselineMLPFinal/best_weights\n",
      "The loss after 1777 iterations is E=0.012912307623778877\n",
      "Wrote weights to file ./customFunctionLearningAdaptive/baselineMLPFinal/best_weights\n",
      "The loss after 1778 iterations is E=0.012912013718783436\n",
      "Wrote weights to file ./customFunctionLearningAdaptive/baselineMLPFinal/best_weights\n",
      "The loss after 1779 iterations is E=0.012911719792865744\n",
      "Wrote weights to file ./customFunctionLearningAdaptive/baselineMLPFinal/best_weights\n",
      "The loss after 1780 iterations is E=0.012911425845925521\n",
      "Wrote weights to file ./customFunctionLearningAdaptive/baselineMLPFinal/best_weights\n",
      "The loss after 1781 iterations is E=0.012911131877862629\n",
      "Wrote weights to file ./customFunctionLearningAdaptive/baselineMLPFinal/best_weights\n",
      "The loss after 1782 iterations is E=0.012910837888577189\n",
      "Wrote weights to file ./customFunctionLearningAdaptive/baselineMLPFinal/best_weights\n",
      "The loss after 1783 iterations is E=0.01291054387796933\n",
      "Wrote weights to file ./customFunctionLearningAdaptive/baselineMLPFinal/best_weights\n",
      "The loss after 1784 iterations is E=0.012910249845939673\n",
      "Wrote weights to file ./customFunctionLearningAdaptive/baselineMLPFinal/best_weights\n",
      "The loss after 1785 iterations is E=0.01290995579238869\n",
      "Wrote weights to file ./customFunctionLearningAdaptive/baselineMLPFinal/best_weights\n",
      "The loss after 1786 iterations is E=0.012909661717217379\n",
      "Wrote weights to file ./customFunctionLearningAdaptive/baselineMLPFinal/best_weights\n",
      "The loss after 1787 iterations is E=0.012909367620326637\n",
      "Wrote weights to file ./customFunctionLearningAdaptive/baselineMLPFinal/best_weights\n",
      "The loss after 1788 iterations is E=0.012909073501617737\n",
      "Wrote weights to file ./customFunctionLearningAdaptive/baselineMLPFinal/best_weights\n",
      "The loss after 1789 iterations is E=0.012908779360991935\n",
      "Wrote weights to file ./customFunctionLearningAdaptive/baselineMLPFinal/best_weights\n",
      "The loss after 1790 iterations is E=0.012908485198350925\n",
      "Wrote weights to file ./customFunctionLearningAdaptive/baselineMLPFinal/best_weights\n",
      "The loss after 1791 iterations is E=0.012908191013596295\n",
      "Wrote weights to file ./customFunctionLearningAdaptive/baselineMLPFinal/best_weights\n",
      "The loss after 1792 iterations is E=0.012907896806630072\n",
      "Wrote weights to file ./customFunctionLearningAdaptive/baselineMLPFinal/best_weights\n",
      "The loss after 1793 iterations is E=0.012907602577354155\n",
      "Wrote weights to file ./customFunctionLearningAdaptive/baselineMLPFinal/best_weights\n",
      "The loss after 1794 iterations is E=0.012907308325670993\n",
      "Wrote weights to file ./customFunctionLearningAdaptive/baselineMLPFinal/best_weights\n",
      "The loss after 1795 iterations is E=0.012907014051482844\n",
      "Wrote weights to file ./customFunctionLearningAdaptive/baselineMLPFinal/best_weights\n",
      "The loss after 1796 iterations is E=0.012906719754692294\n",
      "Wrote weights to file ./customFunctionLearningAdaptive/baselineMLPFinal/best_weights\n",
      "The loss after 1797 iterations is E=0.01290642543520223\n",
      "Wrote weights to file ./customFunctionLearningAdaptive/baselineMLPFinal/best_weights\n",
      "The loss after 1798 iterations is E=0.012906131092915355\n",
      "Wrote weights to file ./customFunctionLearningAdaptive/baselineMLPFinal/best_weights\n",
      "The loss after 1799 iterations is E=0.012905836727734807\n",
      "Wrote weights to file ./customFunctionLearningAdaptive/baselineMLPFinal/best_weights\n",
      "The loss after 1800 iterations is E=0.012905542339563954\n",
      "Wrote weights to file ./customFunctionLearningAdaptive/baselineMLPFinal/best_weights\n",
      "The loss after 1801 iterations is E=0.012905247928306038\n",
      "Wrote weights to file ./customFunctionLearningAdaptive/baselineMLPFinal/best_weights\n",
      "The loss after 1802 iterations is E=0.01290495349386457\n",
      "Wrote weights to file ./customFunctionLearningAdaptive/baselineMLPFinal/best_weights\n",
      "The loss after 1803 iterations is E=0.012904659036143427\n",
      "Wrote weights to file ./customFunctionLearningAdaptive/baselineMLPFinal/best_weights\n",
      "The loss after 1804 iterations is E=0.012904364555046245\n",
      "Wrote weights to file ./customFunctionLearningAdaptive/baselineMLPFinal/best_weights\n",
      "The loss after 1805 iterations is E=0.012904070050477338\n",
      "Wrote weights to file ./customFunctionLearningAdaptive/baselineMLPFinal/best_weights\n",
      "The loss after 1806 iterations is E=0.012903775522340602\n",
      "Wrote weights to file ./customFunctionLearningAdaptive/baselineMLPFinal/best_weights\n",
      "The loss after 1807 iterations is E=0.012903480970540433\n",
      "Wrote weights to file ./customFunctionLearningAdaptive/baselineMLPFinal/best_weights\n",
      "The loss after 1808 iterations is E=0.012903186394981316\n",
      "Wrote weights to file ./customFunctionLearningAdaptive/baselineMLPFinal/best_weights\n",
      "The loss after 1809 iterations is E=0.012902891795567932\n",
      "Wrote weights to file ./customFunctionLearningAdaptive/baselineMLPFinal/best_weights\n",
      "The loss after 1810 iterations is E=0.012902597172204981\n",
      "Wrote weights to file ./customFunctionLearningAdaptive/baselineMLPFinal/best_weights\n",
      "The loss after 1811 iterations is E=0.01290230252479741\n",
      "Wrote weights to file ./customFunctionLearningAdaptive/baselineMLPFinal/best_weights\n",
      "The loss after 1812 iterations is E=0.012902007853250278\n",
      "Wrote weights to file ./customFunctionLearningAdaptive/baselineMLPFinal/best_weights\n",
      "The loss after 1813 iterations is E=0.012901713157468752\n",
      "Wrote weights to file ./customFunctionLearningAdaptive/baselineMLPFinal/best_weights\n",
      "The loss after 1814 iterations is E=0.012901418437358207\n",
      "Wrote weights to file ./customFunctionLearningAdaptive/baselineMLPFinal/best_weights\n",
      "The loss after 1815 iterations is E=0.012901123692824206\n",
      "Wrote weights to file ./customFunctionLearningAdaptive/baselineMLPFinal/best_weights\n",
      "The loss after 1816 iterations is E=0.01290082892377229\n",
      "Wrote weights to file ./customFunctionLearningAdaptive/baselineMLPFinal/best_weights\n",
      "The loss after 1817 iterations is E=0.012900534130108334\n",
      "Wrote weights to file ./customFunctionLearningAdaptive/baselineMLPFinal/best_weights\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The loss after 1818 iterations is E=0.01290023931173818\n",
      "Wrote weights to file ./customFunctionLearningAdaptive/baselineMLPFinal/best_weights\n",
      "The loss after 1819 iterations is E=0.012899944468567856\n",
      "Wrote weights to file ./customFunctionLearningAdaptive/baselineMLPFinal/best_weights\n",
      "The loss after 1820 iterations is E=0.012899649600503712\n",
      "Wrote weights to file ./customFunctionLearningAdaptive/baselineMLPFinal/best_weights\n",
      "The loss after 1821 iterations is E=0.012899354707451956\n",
      "Wrote weights to file ./customFunctionLearningAdaptive/baselineMLPFinal/best_weights\n",
      "The loss after 1822 iterations is E=0.012899059789319123\n",
      "Wrote weights to file ./customFunctionLearningAdaptive/baselineMLPFinal/best_weights\n",
      "The loss after 1823 iterations is E=0.012898764846011827\n",
      "Wrote weights to file ./customFunctionLearningAdaptive/baselineMLPFinal/best_weights\n",
      "The loss after 1824 iterations is E=0.01289846987743676\n",
      "Wrote weights to file ./customFunctionLearningAdaptive/baselineMLPFinal/best_weights\n",
      "The loss after 1825 iterations is E=0.012898174883500844\n",
      "Wrote weights to file ./customFunctionLearningAdaptive/baselineMLPFinal/best_weights\n",
      "The loss after 1826 iterations is E=0.012897879864111197\n",
      "Wrote weights to file ./customFunctionLearningAdaptive/baselineMLPFinal/best_weights\n",
      "The loss after 1827 iterations is E=0.012897584819174805\n",
      "Wrote weights to file ./customFunctionLearningAdaptive/baselineMLPFinal/best_weights\n",
      "The loss after 1828 iterations is E=0.012897289748599071\n",
      "Wrote weights to file ./customFunctionLearningAdaptive/baselineMLPFinal/best_weights\n",
      "The loss after 1829 iterations is E=0.012896994652291313\n",
      "Wrote weights to file ./customFunctionLearningAdaptive/baselineMLPFinal/best_weights\n",
      "The loss after 1830 iterations is E=0.01289669953015921\n",
      "Wrote weights to file ./customFunctionLearningAdaptive/baselineMLPFinal/best_weights\n",
      "The loss after 1831 iterations is E=0.01289640438211036\n",
      "Wrote weights to file ./customFunctionLearningAdaptive/baselineMLPFinal/best_weights\n",
      "The loss after 1832 iterations is E=0.01289610920805258\n",
      "Wrote weights to file ./customFunctionLearningAdaptive/baselineMLPFinal/best_weights\n",
      "The loss after 1833 iterations is E=0.012895814007893898\n",
      "Wrote weights to file ./customFunctionLearningAdaptive/baselineMLPFinal/best_weights\n",
      "The loss after 1834 iterations is E=0.012895518781542352\n",
      "Wrote weights to file ./customFunctionLearningAdaptive/baselineMLPFinal/best_weights\n",
      "The loss after 1835 iterations is E=0.01289522352890609\n",
      "Wrote weights to file ./customFunctionLearningAdaptive/baselineMLPFinal/best_weights\n",
      "The loss after 1836 iterations is E=0.01289492824989351\n",
      "Wrote weights to file ./customFunctionLearningAdaptive/baselineMLPFinal/best_weights\n",
      "The loss after 1837 iterations is E=0.012894632944413016\n",
      "Wrote weights to file ./customFunctionLearningAdaptive/baselineMLPFinal/best_weights\n",
      "The loss after 1838 iterations is E=0.012894337612373285\n",
      "Wrote weights to file ./customFunctionLearningAdaptive/baselineMLPFinal/best_weights\n",
      "The loss after 1839 iterations is E=0.012894042253683062\n",
      "Wrote weights to file ./customFunctionLearningAdaptive/baselineMLPFinal/best_weights\n",
      "The loss after 1840 iterations is E=0.012893746868251107\n",
      "Wrote weights to file ./customFunctionLearningAdaptive/baselineMLPFinal/best_weights\n",
      "The loss after 1841 iterations is E=0.01289345145598638\n",
      "Wrote weights to file ./customFunctionLearningAdaptive/baselineMLPFinal/best_weights\n",
      "The loss after 1842 iterations is E=0.012893156016798076\n",
      "Wrote weights to file ./customFunctionLearningAdaptive/baselineMLPFinal/best_weights\n",
      "The loss after 1843 iterations is E=0.0128928605505954\n",
      "Wrote weights to file ./customFunctionLearningAdaptive/baselineMLPFinal/best_weights\n",
      "The loss after 1844 iterations is E=0.012892565057287754\n",
      "Wrote weights to file ./customFunctionLearningAdaptive/baselineMLPFinal/best_weights\n",
      "The loss after 1845 iterations is E=0.012892269536784509\n",
      "Wrote weights to file ./customFunctionLearningAdaptive/baselineMLPFinal/best_weights\n",
      "The loss after 1846 iterations is E=0.012891973988995324\n",
      "Wrote weights to file ./customFunctionLearningAdaptive/baselineMLPFinal/best_weights\n",
      "The loss after 1847 iterations is E=0.012891678413829998\n",
      "Wrote weights to file ./customFunctionLearningAdaptive/baselineMLPFinal/best_weights\n",
      "The loss after 1848 iterations is E=0.012891382811198327\n",
      "Wrote weights to file ./customFunctionLearningAdaptive/baselineMLPFinal/best_weights\n",
      "The loss after 1849 iterations is E=0.012891087181010315\n",
      "Wrote weights to file ./customFunctionLearningAdaptive/baselineMLPFinal/best_weights\n",
      "The loss after 1850 iterations is E=0.012890791523176123\n",
      "Wrote weights to file ./customFunctionLearningAdaptive/baselineMLPFinal/best_weights\n",
      "The loss after 1851 iterations is E=0.012890495837605872\n",
      "Wrote weights to file ./customFunctionLearningAdaptive/baselineMLPFinal/best_weights\n",
      "The loss after 1852 iterations is E=0.012890200124210072\n",
      "Wrote weights to file ./customFunctionLearningAdaptive/baselineMLPFinal/best_weights\n",
      "The loss after 1853 iterations is E=0.012889904382899109\n",
      "Wrote weights to file ./customFunctionLearningAdaptive/baselineMLPFinal/best_weights\n",
      "The loss after 1854 iterations is E=0.012889608613583584\n",
      "Wrote weights to file ./customFunctionLearningAdaptive/baselineMLPFinal/best_weights\n",
      "The loss after 1855 iterations is E=0.01288931281617429\n",
      "Wrote weights to file ./customFunctionLearningAdaptive/baselineMLPFinal/best_weights\n",
      "The loss after 1856 iterations is E=0.012889016990582147\n",
      "Wrote weights to file ./customFunctionLearningAdaptive/baselineMLPFinal/best_weights\n",
      "The loss after 1857 iterations is E=0.012888721136718049\n",
      "Wrote weights to file ./customFunctionLearningAdaptive/baselineMLPFinal/best_weights\n",
      "The loss after 1858 iterations is E=0.012888425254493139\n",
      "Wrote weights to file ./customFunctionLearningAdaptive/baselineMLPFinal/best_weights\n",
      "The loss after 1859 iterations is E=0.012888129343818512\n",
      "Wrote weights to file ./customFunctionLearningAdaptive/baselineMLPFinal/best_weights\n",
      "The loss after 1860 iterations is E=0.012887833404605755\n",
      "Wrote weights to file ./customFunctionLearningAdaptive/baselineMLPFinal/best_weights\n",
      "The loss after 1861 iterations is E=0.012887537436766199\n",
      "Wrote weights to file ./customFunctionLearningAdaptive/baselineMLPFinal/best_weights\n",
      "The loss after 1862 iterations is E=0.012887241440211512\n",
      "Wrote weights to file ./customFunctionLearningAdaptive/baselineMLPFinal/best_weights\n",
      "The loss after 1863 iterations is E=0.012886945414853374\n",
      "Wrote weights to file ./customFunctionLearningAdaptive/baselineMLPFinal/best_weights\n",
      "The loss after 1864 iterations is E=0.012886649360603656\n",
      "Wrote weights to file ./customFunctionLearningAdaptive/baselineMLPFinal/best_weights\n",
      "The loss after 1865 iterations is E=0.012886353277374366\n",
      "Wrote weights to file ./customFunctionLearningAdaptive/baselineMLPFinal/best_weights\n",
      "The loss after 1866 iterations is E=0.012886057165077536\n",
      "Wrote weights to file ./customFunctionLearningAdaptive/baselineMLPFinal/best_weights\n",
      "The loss after 1867 iterations is E=0.012885761023625382\n",
      "Wrote weights to file ./customFunctionLearningAdaptive/baselineMLPFinal/best_weights\n",
      "The loss after 1868 iterations is E=0.012885464852930349\n",
      "Wrote weights to file ./customFunctionLearningAdaptive/baselineMLPFinal/best_weights\n",
      "The loss after 1869 iterations is E=0.012885168652904901\n",
      "Wrote weights to file ./customFunctionLearningAdaptive/baselineMLPFinal/best_weights\n",
      "The loss after 1870 iterations is E=0.01288487242346146\n",
      "Wrote weights to file ./customFunctionLearningAdaptive/baselineMLPFinal/best_weights\n",
      "The loss after 1871 iterations is E=0.012884576164512915\n",
      "Wrote weights to file ./customFunctionLearningAdaptive/baselineMLPFinal/best_weights\n",
      "The loss after 1872 iterations is E=0.012884279875972023\n",
      "Wrote weights to file ./customFunctionLearningAdaptive/baselineMLPFinal/best_weights\n",
      "The loss after 1873 iterations is E=0.012883983557751822\n",
      "Wrote weights to file ./customFunctionLearningAdaptive/baselineMLPFinal/best_weights\n",
      "The loss after 1874 iterations is E=0.012883687209765352\n",
      "Wrote weights to file ./customFunctionLearningAdaptive/baselineMLPFinal/best_weights\n",
      "The loss after 1875 iterations is E=0.012883390831925773\n",
      "Wrote weights to file ./customFunctionLearningAdaptive/baselineMLPFinal/best_weights\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The loss after 1876 iterations is E=0.012883094424146584\n",
      "Wrote weights to file ./customFunctionLearningAdaptive/baselineMLPFinal/best_weights\n",
      "The loss after 1877 iterations is E=0.012882797986341066\n",
      "Wrote weights to file ./customFunctionLearningAdaptive/baselineMLPFinal/best_weights\n",
      "The loss after 1878 iterations is E=0.012882501518422906\n",
      "Wrote weights to file ./customFunctionLearningAdaptive/baselineMLPFinal/best_weights\n",
      "The loss after 1879 iterations is E=0.01288220502030574\n",
      "Wrote weights to file ./customFunctionLearningAdaptive/baselineMLPFinal/best_weights\n",
      "The loss after 1880 iterations is E=0.012881908491903493\n",
      "Wrote weights to file ./customFunctionLearningAdaptive/baselineMLPFinal/best_weights\n",
      "The loss after 1881 iterations is E=0.01288161193313007\n",
      "Wrote weights to file ./customFunctionLearningAdaptive/baselineMLPFinal/best_weights\n",
      "The loss after 1882 iterations is E=0.012881315343899497\n",
      "Wrote weights to file ./customFunctionLearningAdaptive/baselineMLPFinal/best_weights\n",
      "The loss after 1883 iterations is E=0.012881018724126092\n",
      "Wrote weights to file ./customFunctionLearningAdaptive/baselineMLPFinal/best_weights\n",
      "The loss after 1884 iterations is E=0.012880722073724182\n",
      "Wrote weights to file ./customFunctionLearningAdaptive/baselineMLPFinal/best_weights\n",
      "The loss after 1885 iterations is E=0.012880425392608148\n",
      "Wrote weights to file ./customFunctionLearningAdaptive/baselineMLPFinal/best_weights\n",
      "The loss after 1886 iterations is E=0.012880128680692587\n",
      "Wrote weights to file ./customFunctionLearningAdaptive/baselineMLPFinal/best_weights\n",
      "The loss after 1887 iterations is E=0.012879831937892284\n",
      "Wrote weights to file ./customFunctionLearningAdaptive/baselineMLPFinal/best_weights\n",
      "The loss after 1888 iterations is E=0.012879535164121922\n",
      "Wrote weights to file ./customFunctionLearningAdaptive/baselineMLPFinal/best_weights\n",
      "The loss after 1889 iterations is E=0.012879238359296703\n",
      "Wrote weights to file ./customFunctionLearningAdaptive/baselineMLPFinal/best_weights\n",
      "The loss after 1890 iterations is E=0.012878941523331414\n",
      "Wrote weights to file ./customFunctionLearningAdaptive/baselineMLPFinal/best_weights\n",
      "The loss after 1891 iterations is E=0.01287864465614154\n",
      "Wrote weights to file ./customFunctionLearningAdaptive/baselineMLPFinal/best_weights\n",
      "The loss after 1892 iterations is E=0.012878347757642336\n",
      "Wrote weights to file ./customFunctionLearningAdaptive/baselineMLPFinal/best_weights\n",
      "The loss after 1893 iterations is E=0.012878050827749133\n",
      "Wrote weights to file ./customFunctionLearningAdaptive/baselineMLPFinal/best_weights\n",
      "The loss after 1894 iterations is E=0.012877753866377734\n",
      "Wrote weights to file ./customFunctionLearningAdaptive/baselineMLPFinal/best_weights\n",
      "The loss after 1895 iterations is E=0.012877456873443624\n",
      "Wrote weights to file ./customFunctionLearningAdaptive/baselineMLPFinal/best_weights\n",
      "The loss after 1896 iterations is E=0.012877159848862878\n",
      "Wrote weights to file ./customFunctionLearningAdaptive/baselineMLPFinal/best_weights\n",
      "The loss after 1897 iterations is E=0.012876862792551305\n",
      "Wrote weights to file ./customFunctionLearningAdaptive/baselineMLPFinal/best_weights\n",
      "The loss after 1898 iterations is E=0.01287656570442518\n",
      "Wrote weights to file ./customFunctionLearningAdaptive/baselineMLPFinal/best_weights\n",
      "The loss after 1899 iterations is E=0.01287626858440053\n",
      "Wrote weights to file ./customFunctionLearningAdaptive/baselineMLPFinal/best_weights\n",
      "The loss after 1900 iterations is E=0.012875971432393857\n",
      "Wrote weights to file ./customFunctionLearningAdaptive/baselineMLPFinal/best_weights\n",
      "The loss after 1901 iterations is E=0.012875674248321593\n",
      "Wrote weights to file ./customFunctionLearningAdaptive/baselineMLPFinal/best_weights\n",
      "The loss after 1902 iterations is E=0.012875377032100371\n",
      "Wrote weights to file ./customFunctionLearningAdaptive/baselineMLPFinal/best_weights\n",
      "The loss after 1903 iterations is E=0.012875079783646954\n",
      "Wrote weights to file ./customFunctionLearningAdaptive/baselineMLPFinal/best_weights\n",
      "The loss after 1904 iterations is E=0.012874782502878119\n",
      "Wrote weights to file ./customFunctionLearningAdaptive/baselineMLPFinal/best_weights\n",
      "The loss after 1905 iterations is E=0.012874485189711031\n",
      "Wrote weights to file ./customFunctionLearningAdaptive/baselineMLPFinal/best_weights\n",
      "The loss after 1906 iterations is E=0.012874187844062671\n",
      "Wrote weights to file ./customFunctionLearningAdaptive/baselineMLPFinal/best_weights\n",
      "The loss after 1907 iterations is E=0.012873890465850463\n",
      "Wrote weights to file ./customFunctionLearningAdaptive/baselineMLPFinal/best_weights\n",
      "The loss after 1908 iterations is E=0.012873593054991595\n",
      "Wrote weights to file ./customFunctionLearningAdaptive/baselineMLPFinal/best_weights\n",
      "The loss after 1909 iterations is E=0.012873295611403738\n",
      "Wrote weights to file ./customFunctionLearningAdaptive/baselineMLPFinal/best_weights\n",
      "The loss after 1910 iterations is E=0.01287299813500451\n",
      "Wrote weights to file ./customFunctionLearningAdaptive/baselineMLPFinal/best_weights\n",
      "The loss after 1911 iterations is E=0.012872700625711704\n",
      "Wrote weights to file ./customFunctionLearningAdaptive/baselineMLPFinal/best_weights\n",
      "The loss after 1912 iterations is E=0.012872403083443208\n",
      "Wrote weights to file ./customFunctionLearningAdaptive/baselineMLPFinal/best_weights\n",
      "The loss after 1913 iterations is E=0.012872105508117114\n",
      "Wrote weights to file ./customFunctionLearningAdaptive/baselineMLPFinal/best_weights\n",
      "The loss after 1914 iterations is E=0.012871807899651559\n",
      "Wrote weights to file ./customFunctionLearningAdaptive/baselineMLPFinal/best_weights\n",
      "The loss after 1915 iterations is E=0.012871510257964878\n",
      "Wrote weights to file ./customFunctionLearningAdaptive/baselineMLPFinal/best_weights\n",
      "The loss after 1916 iterations is E=0.01287121258297549\n",
      "Wrote weights to file ./customFunctionLearningAdaptive/baselineMLPFinal/best_weights\n",
      "The loss after 1917 iterations is E=0.01287091487460205\n",
      "Wrote weights to file ./customFunctionLearningAdaptive/baselineMLPFinal/best_weights\n",
      "The loss after 1918 iterations is E=0.012870617132763117\n",
      "Wrote weights to file ./customFunctionLearningAdaptive/baselineMLPFinal/best_weights\n",
      "The loss after 1919 iterations is E=0.012870319357377723\n",
      "Wrote weights to file ./customFunctionLearningAdaptive/baselineMLPFinal/best_weights\n",
      "The loss after 1920 iterations is E=0.012870021548364726\n",
      "Wrote weights to file ./customFunctionLearningAdaptive/baselineMLPFinal/best_weights\n",
      "The loss after 1921 iterations is E=0.012869723705643248\n",
      "Wrote weights to file ./customFunctionLearningAdaptive/baselineMLPFinal/best_weights\n",
      "The loss after 1922 iterations is E=0.01286942582913255\n",
      "Wrote weights to file ./customFunctionLearningAdaptive/baselineMLPFinal/best_weights\n",
      "The loss after 1923 iterations is E=0.012869127918752037\n",
      "Wrote weights to file ./customFunctionLearningAdaptive/baselineMLPFinal/best_weights\n",
      "The loss after 1924 iterations is E=0.012868829974421203\n",
      "Wrote weights to file ./customFunctionLearningAdaptive/baselineMLPFinal/best_weights\n",
      "The loss after 1925 iterations is E=0.012868531996059671\n",
      "Wrote weights to file ./customFunctionLearningAdaptive/baselineMLPFinal/best_weights\n",
      "The loss after 1926 iterations is E=0.0128682339835873\n",
      "Wrote weights to file ./customFunctionLearningAdaptive/baselineMLPFinal/best_weights\n",
      "The loss after 1927 iterations is E=0.01286793593692394\n",
      "Wrote weights to file ./customFunctionLearningAdaptive/baselineMLPFinal/best_weights\n",
      "The loss after 1928 iterations is E=0.012867637855989666\n",
      "Wrote weights to file ./customFunctionLearningAdaptive/baselineMLPFinal/best_weights\n",
      "The loss after 1929 iterations is E=0.012867339740704666\n",
      "Wrote weights to file ./customFunctionLearningAdaptive/baselineMLPFinal/best_weights\n",
      "The loss after 1930 iterations is E=0.012867041590989309\n",
      "Wrote weights to file ./customFunctionLearningAdaptive/baselineMLPFinal/best_weights\n",
      "The loss after 1931 iterations is E=0.012866743406764079\n",
      "Wrote weights to file ./customFunctionLearningAdaptive/baselineMLPFinal/best_weights\n",
      "The loss after 1932 iterations is E=0.012866445187949448\n",
      "Wrote weights to file ./customFunctionLearningAdaptive/baselineMLPFinal/best_weights\n",
      "The loss after 1933 iterations is E=0.012866146934466315\n",
      "Wrote weights to file ./customFunctionLearningAdaptive/baselineMLPFinal/best_weights\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The loss after 1934 iterations is E=0.012865848646235481\n",
      "Wrote weights to file ./customFunctionLearningAdaptive/baselineMLPFinal/best_weights\n",
      "The loss after 1935 iterations is E=0.012865550323177963\n",
      "Wrote weights to file ./customFunctionLearningAdaptive/baselineMLPFinal/best_weights\n",
      "The loss after 1936 iterations is E=0.01286525196521494\n",
      "Wrote weights to file ./customFunctionLearningAdaptive/baselineMLPFinal/best_weights\n",
      "The loss after 1937 iterations is E=0.012864953572267702\n",
      "Wrote weights to file ./customFunctionLearningAdaptive/baselineMLPFinal/best_weights\n",
      "The loss after 1938 iterations is E=0.012864655144257695\n",
      "Wrote weights to file ./customFunctionLearningAdaptive/baselineMLPFinal/best_weights\n",
      "The loss after 1939 iterations is E=0.012864356681106493\n",
      "Wrote weights to file ./customFunctionLearningAdaptive/baselineMLPFinal/best_weights\n",
      "The loss after 1940 iterations is E=0.012864058182735831\n",
      "Wrote weights to file ./customFunctionLearningAdaptive/baselineMLPFinal/best_weights\n",
      "The loss after 1941 iterations is E=0.012863759649067542\n",
      "Wrote weights to file ./customFunctionLearningAdaptive/baselineMLPFinal/best_weights\n",
      "The loss after 1942 iterations is E=0.012863461080023553\n",
      "Wrote weights to file ./customFunctionLearningAdaptive/baselineMLPFinal/best_weights\n",
      "The loss after 1943 iterations is E=0.01286316247552614\n",
      "Wrote weights to file ./customFunctionLearningAdaptive/baselineMLPFinal/best_weights\n",
      "The loss after 1944 iterations is E=0.012862863835497498\n",
      "Wrote weights to file ./customFunctionLearningAdaptive/baselineMLPFinal/best_weights\n",
      "The loss after 1945 iterations is E=0.01286256515986009\n",
      "Wrote weights to file ./customFunctionLearningAdaptive/baselineMLPFinal/best_weights\n",
      "The loss after 1946 iterations is E=0.012862266448536337\n",
      "Wrote weights to file ./customFunctionLearningAdaptive/baselineMLPFinal/best_weights\n",
      "The loss after 1947 iterations is E=0.01286196770144913\n",
      "Wrote weights to file ./customFunctionLearningAdaptive/baselineMLPFinal/best_weights\n",
      "The loss after 1948 iterations is E=0.012861668918521234\n",
      "Wrote weights to file ./customFunctionLearningAdaptive/baselineMLPFinal/best_weights\n",
      "The loss after 1949 iterations is E=0.012861370099675666\n",
      "Wrote weights to file ./customFunctionLearningAdaptive/baselineMLPFinal/best_weights\n",
      "The loss after 1950 iterations is E=0.012861071244835494\n",
      "Wrote weights to file ./customFunctionLearningAdaptive/baselineMLPFinal/best_weights\n",
      "The loss after 1951 iterations is E=0.012860772353924084\n",
      "Wrote weights to file ./customFunctionLearningAdaptive/baselineMLPFinal/best_weights\n",
      "The loss after 1952 iterations is E=0.012860473426864789\n",
      "Wrote weights to file ./customFunctionLearningAdaptive/baselineMLPFinal/best_weights\n",
      "The loss after 1953 iterations is E=0.01286017446358117\n",
      "Wrote weights to file ./customFunctionLearningAdaptive/baselineMLPFinal/best_weights\n",
      "The loss after 1954 iterations is E=0.012859875463996983\n",
      "Re-setting points: using every 6th point for training.\n",
      "Selecting subset for training: 450 out of 2700 selected.\n",
      "Wrote weights to file ./customFunctionLearningAdaptive/baselineMLPFinal/best_weights\n",
      "The loss after 1955 iterations is E=0.012545242569204576\n",
      "Wrote weights to file ./customFunctionLearningAdaptive/baselineMLPFinal/best_weights\n",
      "The loss after 1956 iterations is E=0.012389232538504932\n",
      "Wrote weights to file ./customFunctionLearningAdaptive/baselineMLPFinal/best_weights\n",
      "The loss after 1958 iterations is E=0.012330779489806622\n",
      "Wrote weights to file ./customFunctionLearningAdaptive/baselineMLPFinal/best_weights\n",
      "The loss after 1960 iterations is E=0.012314595844215246\n",
      "Wrote weights to file ./customFunctionLearningAdaptive/baselineMLPFinal/best_weights\n",
      "The loss after 1967 iterations is E=0.012308290946673704\n",
      "Wrote weights to file ./customFunctionLearningAdaptive/baselineMLPFinal/best_weights\n",
      "The loss after 1969 iterations is E=0.012304587212758832\n",
      "Wrote weights to file ./customFunctionLearningAdaptive/baselineMLPFinal/best_weights\n",
      "The loss after 1976 iterations is E=0.012295187340782813\n",
      "Wrote weights to file ./customFunctionLearningAdaptive/baselineMLPFinal/best_weights\n",
      "The loss after 1978 iterations is E=0.012287322751213667\n",
      "Wrote weights to file ./customFunctionLearningAdaptive/baselineMLPFinal/best_weights\n",
      "The loss after 1980 iterations is E=0.012286878284221283\n",
      "Wrote weights to file ./customFunctionLearningAdaptive/baselineMLPFinal/best_weights\n",
      "The loss after 1987 iterations is E=0.012284427596453052\n",
      "Wrote weights to file ./customFunctionLearningAdaptive/baselineMLPFinal/best_weights\n",
      "The loss after 1989 iterations is E=0.012282761619059156\n",
      "Wrote weights to file ./customFunctionLearningAdaptive/baselineMLPFinal/best_weights\n",
      "The loss after 1991 iterations is E=0.012282272603551523\n",
      "Wrote weights to file ./customFunctionLearningAdaptive/baselineMLPFinal/best_weights\n",
      "The loss after 1993 iterations is E=0.012282121339701333\n",
      "Wrote weights to file ./customFunctionLearningAdaptive/baselineMLPFinal/best_weights\n",
      "The loss after 1994 iterations is E=0.012279653291537854\n",
      "Wrote weights to file ./customFunctionLearningAdaptive/baselineMLPFinal/best_weights\n",
      "The loss after 1996 iterations is E=0.012277714872642708\n",
      "Wrote weights to file ./customFunctionLearningAdaptive/baselineMLPFinal/best_weights\n",
      "The loss after 1998 iterations is E=0.012276949988648533\n",
      "Wrote weights to file ./customFunctionLearningAdaptive/baselineMLPFinal/best_weights\n",
      "The loss after 2000 iterations is E=0.012276695024281897\n",
      "Elapsed time: 14.42 min\n"
     ]
    }
   ],
   "source": [
    "start = time.time()\n",
    "customModel.solve(max_iter=2000, learning_rate=0.01, verbose=1, adaptive=True, tolerance=0.01, every=1000)\n",
    "print(\"Elapsed time: {:5.2f} min\".format((time.time() - start) / 60.0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded weights from file ./customFunctionLearningAdaptive/baselineMLPFinal/best_weights.npy\n"
     ]
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAuoAAAD3CAYAAABLsHHKAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz\nAAALEgAACxIB0t1+/AAAADl0RVh0U29mdHdhcmUAbWF0cGxvdGxpYiB2ZXJzaW9uIDIuMS4wLCBo\ndHRwOi8vbWF0cGxvdGxpYi5vcmcvpW3flQAAIABJREFUeJzsvW1sHHd+5/mth+6ufmSTFElRpPgk\nyZYsyx7L8sN4spPgRt7bZHGXF2fPYHHAAUEuHgSHBEleaHby4oJbILsrvzgE2AsW1uSyM5g9zDnW\nYS67udtJrFwwefD4QaLjGY9nbFndzWdSJJvdbPZzddW9qP5X/+u5urtapKj/ByBIdnVXV1V3/etb\n3//vgVNVFQwGg8FgMBgMBuNowR/2BjAYDAaDwWAwGAwrTKgzGAwGg8FgMBhHECbUGQwGg8FgMBiM\nIwgT6gwGg8FgMBgMxhGECXUG4yGF47hXOI67zHHcdY7j0oe9PQwGg8FgMIKFCXUG4yGE47jLABZU\nVV0EcBXAyCFvEoPBYHRN22y41h7TGCYGcXzYMX+4YELdJ+0v9TWO416jnMxr/TqZHMctBLWNPt/v\nensfrg34fRZM/9/hOO7qIN/TYTsM+zuo493+Tug/Pa7jbZvHFtrfs6v0960t0G9wHPcagDdUVc24\nrNdy7Af5vTsqnz2DYUf7nLrOcZxKjenX2o/1dO6a1t/V9/1Bny8P6hrgsQ36PrfHsjMArhzW9gyS\nfj/PQRyfXtfJxvbDQTzsDTjqtIXRHQCvtr/c5PEFANcB3Ohj3ZcBLABwFFlB0hZ1uwD+DAMcFB32\n61U3MTmg7TDs76COd/uCl1FV9Wb7+/LXAG528fqr7e2yG/DeUlX12fbzbgP4FoBXAUBV1QI0sX6d\n47gFl+NrOPaD/N4dlc+ewXBCVdUMx3H/BsA1VVVfp5dR59LrDi/3g+/v+4M+Xx7UNcBjG+z2uXAY\n2/KACOLzvBfIlvSxTja2Hx7MUffmW9Acy0X6wfaXsx+RngbwzT63rVvSAAqqqhZUVb01iDdw2q9D\nOpn1/QVwGwM43mR/VVW9CWjimQhrv6iqektVVct3qT0w5qnnFdAW820HkLh/uwAcnUCTSB/Y9+6I\nffYMRteoqvoNAN/sZ8apC5F+GOfLwK8BbhzSde9QOQ7jHxvbDxfmqHvzCoBvOCy7rqpqoe2IvtH+\n/0ZbQH0LwFdUVV1sf8m/Cu1ONA3trnSx/ffLHMeNALjVdnouQ3M6Mu3nkcf190DnjvbV9v8L7Z+0\nkxPUXu/LAAocx6G9DrdtNr9fGsDLqqp+3bTOq9S+3Gpvu2G/2v+TG54b1Gv97KflfU37RVxow/47\n7K/d8SZudh7Ac6qqfsO0DXkAX1NV9VW79yf70D5+BQCXAdwMaABbgNVpyrf37SaAhfb7noHDd7T9\nXPrYWz6fbo+D0zG3WzcC/OztzqM+nU8Gw45b0Mb91x3OCzJW3mqfD2TW9SaAN2H9vh/K+WLG7RoA\n4/k9kGuQ0z5TY2W6/R521xrL52Czf47Hw8c1uqt98zteAvg3sP88DddNSkfA53E077vt2Oj0Wbod\nO7vjgwf8XWWYUFWV/Tj8QBNdKrSTxuu51wG8Rv3/NoDL1LLL1LJX2r/fIH+3/18A8LZpvXdM7/EG\nvcz0+j2Pbbxm2kbHbXZ5v4X232nTtl2DNpVs2S/ze/W4nwsO+3SPfD7t973qsr+uxxvAa6Z9eIN8\nD1yO6Wv0d6R9XO71+H1Tbdb9ls3+Om6Pz+9mX8fB45gP7LOHw3nEfthPtz/t81R1WPYGgLc8zovX\noIkasow+n8zn26GcLw77Zjcm6ud3j+/n+xrksc9O573j52Czbq/xo9vrnWXfPL4XduMl/Xm6XTd9\nX8sc9tswNvr4LLvVAw/0u8p+Oj/MUXdB1e60Aa2ihiWGjuO4tKqFI3jxBoC3OY7LQPsiO90pvwLt\nLpsmw3HcVbUzTXmHWpbH4OPbze9Hkme/Ci2cBADgsk+EXervXvbTKWn3WQBXqc+pmynrV6C5S8TJ\nyFPLCmjH8KmmsCfzdkOLTy+0n1toJ6tZYsY5LVl0UbVxgxyw2+9eqrvseizv9jh0e8yD+uz9nkcM\nRj+MAPgALueFqrmO9wB8ox0mQ3+nzefbYZ0vfjCc3+18m8O6Bjnth9v41M16etkGu33rdrykP0+3\n62Y/1zLL2Ojzs+yXB/ldfWRhQt0bfRrUZtlV+EsazKuqeqY9LfR1juPeUk2hFJxzmaQ0HuIvM8dx\nlz2ELqHr/aQSN19Vtem157rZrvafeZdBy+uCANgP5E43b1/3uU563RZh7vN4+qLb49DNMR/AZ+95\nHjEYAXAVWsjCVbifF7fayZl5tZ2jYuaQzxe/eI1JA7sGdbHPbp/DYdDvdcNAP9cyansMYyO0m00z\ngXyWh/hdfSRhyaTevArti28Q0py1LOMujF9IOqP+m22HdVE1xmQV0BFiI9BEv1mwk3iwQeC2zV78\nGUxVSiiHwbxfZoLaT3Ih1WMcTdthxu54G/bZ5bW2tN+7QL4P7d8Zs5tOnutzBoY83zAQtp27IL4L\n/RwHr2M+yM/e6TxiMAKh7ULeaJ97XufFdWi5IW7n9GGeL73wIN7Pa5/ttqmvcbpNP9e7oLbH6brZ\n7bXMjN3Y2O1n6XV8jtp39dHhsGNvHpYfaIPyNWjxaK+Aih9rL0+jHVfWXv5W+ydtep3+WmhTW2+0\nHyNxdFfbz3+l/Zs8fhnatNHb6JTy22u/Po1OrLRT7B79+qs+ttn1/ah1Xqden7bbL/O6+tlPm2P+\nFjmm7R+yP3b763a8r6KTO0Be+xZ8xIO313u9/RlcR5dxd+33u9b+/K7DGJt4mdq+a+Zj4HPd5mPf\n83FwO+aD/uzhcB6xH/bTzQ91vqrUd+oaTDG67edazgvT8jdM/5u/w4d2vthsq2FMtDu/+3k/eFyD\nTMffcZ/t9sPrc/C5fT1f78z7Zrc9dsfT4fO0XDfdvid267DZdyeN4euzbD/meHwe9HeV/Rh/uPZB\nZDAYDAaDwWAwGEcIFvrCYDAYDAaDwWAcQZhQZzAYDAaDwWAwjiDdVn1hcTIMBoNxuHABrION5QwG\ng3G4+BrLmaPOYDAYDAaDwWAcQZhQZzAYDAaDwWAwjiBMqDMYDAaDwWAwGEcQJtQZDAaDwWAwGIwj\nCBPqDAaDwWAwGAzGEYQJdQaDwWAwGAwG4wjChDqDwWAwGAwGg3EEYUKdwWAwGAwGg8E4gjChzmAw\nGAwGg8FgHEGYUGcwGAwGg8FgMI4gTKgzGAwGg8FgMBhHECbUGQwGg8FgMBiMI4h42BvAeLRotVpo\ntVoQBAE8z4PjuMPeJAaDwWB0iaqqkGUZAMDzPBvPGYwBwYQ6Y+CoqgpFUdBoNNBqtdBoNMDz2mTO\n9vY2Tp48CUEQ9B8y4LNBn8FgMI4Wqqqi0WhAURTU63WoqgqO41CpVKCqKtLptGEsZwKewegPJtQZ\nA0NVVd1BLxaLWFpawhNPPAGO4yAIAlRVxfLyMsbHx9FqtSyv53neIuDZoM9gMBgPHuKgy7KMn/zk\nJ5ifn0c4HAbHceB5HuVyGbVaDYlEAs1mUxfwAPTnMEOGwegeJtQZgUMEuizL+mDN87xh4AagD9LE\nXTevg1wYms2m4XF6wCd/s0GfwWAwgoUeh4mZQo/n+/v7UFUViURCH8fN47mqqgAARVGYIcNg9AAT\n6ozAoB0XAAYRTgZ2M9X/+jeB/G3L406im6yD3AiYYYM+g8Fg9Ac9G6ooCoDOmKwoCmq1Gj7++GNE\no1GIoohcLodqtQpVVVGr1RCLxfSfaDTqOp4zQ4bBcIcJdUZfODku5oGU4ziLUP+HqecBAH87csVx\n/V82iXh6KtVpW5rNJhqNhsW9Z4M+g8FgOGM3G0p+ZFnG6uoq1tbWAABnzpzB8PCwLqp3dnZQKBQw\nPj6OcrmMYrGIjY0N1Go1AIAkSYjH4wYRL4oiM2QYDA+YUGf0hJvjYgfP81AURV9ORLoZIao58K2q\ntk4nEW8W8G7v7zbob25uYmpqyiLg2aDPYDAeFWizhQ5XBIBarYalpSXs7OxgamoKL7zwAu7evYtw\nOKy/FuiMv6lUCqlUyrB+4sJXKhVUKhXs7e2hUqmg1WohFArpwp0I+Ugk4unCmw0Zsr7h4WFmyDCO\nFUyoM7rCzXFxg3bU35t9SX9ciPK6KCci3fw4/Rigifi/Hbli+xygOxd+bW0Nk5OThgsUgQh2URT1\nv9mgz2AwjgskbtwuXHF/fx+5XA6VSgWzs7M4d+6cvsxuhtTuMQLP87oYN9NoNHQBn8/nsbKygnq9\nDo7jDO57PB5HNBqFIAi242+lUkG1WkUqlbJ14e1mVJkhw3gYYEKd4Qs3x8UP9CAem5FQWa7py2iB\nbofXcjP/MPW8LwFPtsspmRVwnnq1m3Zlgz6DwTjq0I60eTZUVVVsb28jl8tBEATMzc1heHjYMq6R\nGVIaN6HuRjgcRjgcRjqdNjyuKAqq1SrK5TIqlQp2d3dRqVSgKAoikYhFxLtdl0iJYGbIMB5GmFBn\nuOLmuHSDeWCPzUi2z/Mj4M0uPC3K3US9Yyy8g4Cnf9N4DfpOAp4N+gwG47Awz4YCHYHearWwsbGB\n5eVlDA0N4cKFC0gkEo7r6tZR7wWe5xGPxxGPxy37QVz4crmM7e1tLC0t6XXcy+WyIRY+Go06mijM\nkGE8DDChzrBAhGg+n4cgCK5Z+34hg/hHv/SLlmWRZBj1UgOAPwFvh1mgu4l5ABBT2ldf3pe7SmYF\nvGPhzRUMyGvcRDyDwWAMAjIburGxgbGxMUO4YqPRwMrKCjY2NnDy5ElcuXJFjz1340EIdbf3jkQi\niEQiGB4e1h/f2tpCpVLBiRMnUKlUUCqVsLW1pVejkSTJEAcfi8UQCoX0dZphhgzjqMCEOkPH7Ljc\nv3/f1tHohV4HcSLizQKeF7TB8CBbdXXe7SAi3Q/dJrPSv2mcBv1qtQpFUQzd/IioZ4M+g8HoFfNs\n6L179zAxMQEAKJfLyOVyKBaLOH36NL74xS9CEATf6ybjubmy1oMQ6m7wPI9kMolkMml4nJSNJLHw\nm5ubKJfLkGUZoihawmgkSerJkNnZ2cHY2BhCoRAzZBiBwYQ6wzb+nO4eGgROg3gkGdZ/E1fdvMyN\nxHzU9rUH2arhf+Kqm0W6mBIh71unPL0eD8qFL5fLqFarrJsfg8HoG6/48729PWSzWbRaLczOzuqd\norslyBj1BwHHcYhGo4hGoxgdHTUsk2VZD6MhJSWr1ar+GlrEk5KSZJ1mVlZWMDo6ikaj4erCM0OG\n0Q1MqD/CeMWf2w3GvcJxHKL/8huGx9yEuB+R7kZiPmp5TEpHsPvxvuvrunHbnejGhQfs4/5ZNz8G\ng+EXrwZFJCxkeXkZZ86cwdDQUF/vd5ihL0EjiqJnSclyuWwpKWmuCR+JRKCqqm1VGrMLzwwZRjcw\nof6I4ea4mAlSqJvpR4hHkmE0K03HZQAsDjsh4RADX/j4wNNtJ8ud3HYv7AQ8F9KO++zWB8bHPcJo\nvLr5mWMo2aDPYBw//DYoGh0dRTQaxdNPPx3I+x4noe4EXVLyxIkT+uPk+knCaHZ3d/WSkpVKBZ98\n8okhFp6UlASs4zkzZBh+YEL9EaHbBkVkud3g0SvxMS3WPXmy854H98uW53k57bRIdwud6Yb0k9YK\nBwdUAms3TrtZyIdHQmjk7W8sCH838Zzt4//EJOCB3ho7AWzQZzCOC+RG3W42lG5QdOrUKbzwwgsQ\nRRHvvPNOYO9PnPr79+/roXuDNHaOEhzHOZaUfP/99zE7O6uL+O3tbT0PiS4pSUR8KBRyHc+ZIcMA\nmFA/9vTaoAjQhF3QDglJAiUkxjuJqsmTmlgubR4YnkNEuFnAd+PKS+mI47JIKoz6vlXkE/edE4xh\nKaVsRRfjdPUYwCroyf9OYj2UFNEsObvz3Qp4+jcNPZNiToBqtVpoNpsYGhpi3fwYjCMKLdyIgUKf\no3SDopmZGUODInod/Z7TrVYLhUIBu7u7GB8fRzQaxd7eHkqlEkqlEm7fvo1oNKqLUdKoqJeyvg8b\nHMchkUhYSluSkpKkJvz9+/dRLpfRbDYhCIIlDr6fkpLlchnJZBLhcJgZMscEJtSPKUHUPw/SIfns\nv/tnlsdCMU1oNyuaSBalEORaUxfsnedFkM/sGh4LSqTr63MQ63Yk563d9UrZiuUxLxc+PBJyXOYm\n4MMjIUcBD3TvwpdKJWxubkKSrGFBrJsfg3G4uM2G0g2KeJ7H/Py8bYMioGO89HruNhoNLC8vY3Nz\nE/F4HDMzM5iZmUGz2YQoiqjVavjss89w8eJFVKtVQ51zUuNckiSDgKeTM48zdEnJkZERwzJZlvXG\nTnYlJc2x8F4lJZeWljA/P2+b7Gs3njND5uhz/M+QRwgvx6VbBjmVKUpGkUpEuxO0eOfaNxwHWyXD\nc3qJe4+krK+R0hJqBfe67dJQBLVivbN9lHinZw2Kn3dCe2hXnRbpTqLcy213olsXniRAmcuzsW5+\nDMbh4TYbqigK1tbWsLKygmQy6dmgCOiM590aNpVKBblcDoVCQS/luLW1hXq9bngePZY4ucp0cub6\n+jrK5bJtcmY8Hkc4HH4kxhJRFH2VlNzY2EClUjGUlKSPGSkpqSiKXh6SXhfg3diJGTJHEybUjwFk\nQN/b24OiKEilUoEIpqCFujnspRc46iKTmEhalpvFu9lNp51zO5FuRkpLfcW+D5011aCf036VclYH\nnhBKOp+W5mXmkBq3eHg3F/7UO/+n5bFeY+FZNz8Go3eI2bK8vIxTp04ZZkPpBkUTExO4fPkyIhHv\nGUOg+/G8WCwim82iXq9jbm4OFy5c0M9hu7BIr2RStxKJdKfR3d1dLC8vo9Fo6AmdtAsvSdIjE0bj\ndLzoZNZCoYC1tTXUajVwHIdarYaVlRUkEgn9uDklswLMkHkYYEL9IcbsuJRKJTQaDUuCS68EJdTz\nv/9rhv9FKQSO93eSh2LOF6FwPIxG2SiiiXgfPRvRlx3cL1le60ekOyENRfTfxFU3O+xeJOdilmOw\nn7Em1tKuOi3Sze68V7KqG7zIYfPL/wKbNsu+tPa+5bFeGjsBrJsfg+GGOVxxeXkZ09PTAOxd7W4a\nFAGdBFA3VFXF7u4ustksBEHQQ2mc1hVUwyOn5MxWq6ULeBKiV6tps53mMJpHIZGVEAqFMDQ0ZCmz\nqSgKbt++jWQyiVqt5lpSkp61YIbM0YYJ9YcQp4x/QRCCdcADdNTTM524vMrugcszO7iJdIKdWDcz\nMj9qeE7yJIfSpns99V7pZ9YgtdBx38l6Cne1Y+XmshPcYt7NSONh1O5rx0SRVfCi/Xb/w9Tzto/b\nCXjA24W36+ZHO4aiKBrcGzboM44z5MZWlmVL/DnHccjn81haWkKz2bS42t3iNp4rioLNzU0sLS35\nCqV5UOUZBUGwDQsx1zjP5/MoFov6zLI5Dj4c7q8vx8MCGTPHx8cN3xNSSIAks9IlJXmeNyT/kh8y\nJvdqyKiqinA4zAyZAGBC/SHCLkGU/uIfVaEuV41Oc2w0YQhhqaDjeJOEUrNID8UikGtUiEfcrYSj\ns8AnLnbyZMqyrL6/AyltTaikSz8SN90Pbi57dFgyLIsOR1Hdq1qelz5nf7Es5soGYW7+2y0cRhrv\n/6L1D1PPgwtZp59fyr1r+3w/Lnwmk0EqlTJM87JufozjiHk2FDAmiJIW90tLS1hYWOi7QRFgP57T\ntdZPnDiBZ555xjap3Awtyulz+0HVUbercU6SMCcnJ3UBb1ddJR6P66KUxHUfN8z7RJeUNM+QtFot\nS/KvuaQkLeL9uPCyLOP27dt49tlnDdvgNqvKcIYJ9SOOm+Nihuf5QOueDzKZVIiE0Kpr4jE22nFL\nQnEJzUod1byz6+4m0vth7PyY/nfyZBKlTVO8ex8i3fx/N2EydgzNae47Rzn4hXvWY2Z22Z1cd6fw\nmchEGPUt/zH678y9CAAGh/7Fz3/k+ho6QY6UFANYNz/G8YNO9rdrULS2tobV1VWMjo4ilUrhiSee\n8B2D7gU9ntfrdSwtLWF7e9tQa72bdZHzkxbsh93wiK6uYhaksizrYtQc121XTrLb0KKHFbfkX9LE\nqVKp6N1tzSUl6Zse4sLTtd7p9fkJi2SGjBUm1I8ovdQ/P4qOujk+HQDEqPeFJzqSaLvomkis7Gii\n2Uukm910d+ddc47qJfsKL8mTnRuI1CltAOolZMYs8M1uejc3ADScKcwmfYaqjHNO2969u53tNYt0\nOvzFjsiE87GLTkVQu+8vLv7ds180/O8k3FutlmFgd3Lhzd38vAZ95towDhu3crnmBkXPP/88QqEQ\nPvzww8CNl3K5jOXlZezv72N2dhZnz57tKTHTLt79KAh1N0RRRCqVQiplnE1VFEUvj1ipVLCzs2Nw\nlM1hNKQ84nGH4zhIkgRJkmxLShIBv7+/j42NDdRqNaiqimg0CkmSIMsy9vf39RKcfsMimSFjhQn1\nI4ad4+J3ID1qjnqlYlNb3IdItyN2whijyPE8qrtG0ewW8mKOZQ/Hjc91S26NJCX9teaQmaEpDsW1\nouExO+FNhHl0WLI8l7P5fCMpCfV94w2ElI6iVrCGx9ihKgo4nsfwOW176f0r3DMeN7tQGD/x7tJ4\nyFasRyfDqG/bi3ghyuODS1/S/+dCHK4s/j0Aq1B3wiuMhu7mR84h4vCwQZ/xoKCbjNnNhpZKJWSz\nWZTLZczOzloaFAVpvJAGRcViEefOncPFixf7+s47xag/jPA8r4fD0NCOcrlc1sORSHlEs4APaubj\nYcDppoeUlCwUCsjn81hfXzeUlDTHwdOhR8yQcYYJ9SNCEA2KBEE4EkK9VCohk8mgVqvhHPW4l0gP\nxZ1jI0OxCJoVY7hIdFQbJJKnI2jVGhbhbnbTiVg1i3Qz4XgEjbL/0JShKS1+dPxCDI2DKvbXO8K9\nV7fcDr8iHQDiJxKo5O1LQKbPdAZX/jHtmOQ/K+rhL2aRbg5/cXPavWhVFQhR4/f69uVf0P8WfvTX\nPa8b8E5mJecZ/biTa3NcB33GYPFqULSzs4NsNgue5zE3N4eRkZGBhDLSzZBIRZVTp07pMd39cNTd\n8yBwc5RJeUSSyEoSMyuVCn76058a4uAfla6sQKekpKIoSCQSOH/+vL6MPmZ7e3uW0CNzLLxXSclH\nyZBhQv0Q8XJcuuUwQ19UVcXe3h4ymQw4jsP8/DzC3/oDYCiByHAS9T1riUQaJxEvSmE9/IWIdTsH\nGtCEuxiN6MmrZuFO4+Sgm0U6CY/phtSpofZv7X8i3ImbTpd0NGx/O6E0Ohy1rNPspsdGY6g6CPfY\niLVzqhcjj2nbTI7L7s8Lts/zEunSuCbyI2MhW1c9diqC+p6D2x7h8ckvvQwAeOZHf+tvw33ix4Vv\nNptoNps4ODhAoVDA9PQ0OI7DzZs3cfXqVczOzga6TYzjRdANino1XhRFwfr6OpaXl5FOp/Hkk08i\nFovh7t27gV0feJ6HLMu4e/euXmUlHo/rXTaPa5Imwa48IimNODs7i3K5jIODA8cuo8e9K6vd7Khb\nSUk6mXV3dxeVSkXPWTLHwvtJZqUNmWw2i1OnTiESiaDRaODb3/42rl27NqA9HwzH81tyxCED+s7O\njv6lDeJu7zBCX1RVxf3795HL5SBJEh5//HG9lBZJbRQiYUSGtccESYJSb6CWL+oJpWaR7qcsI0F0\neW5qZsJSccZNvJuhRbpdGUi/pRhTp4YwNN25uRia5lBc3fO9HWbMIj2ajhr+d7qR6YbR82njes4B\nu3et20yHvxCR7kT0pH8n/ue/rAn28//lbd+v6RXzuUdKnxIX7C//8i/xpS99yenljEccRVFQLpfR\naDQQi8UMs6EPskFRs9nEysoK1tfXMTExgStXrhjKEgZVHKBcLuPevXvI5/N4/PHHMTU1hUajoXcZ\nvXv3Lmq1mqHs36PiLvM879qVlcTBr62t2dY3N4vRbjkqteRbrZbvmxA69GhsrFPQwVxScmdnRz/P\n6Ko/dAKwXUnJUqmEUCgEnueRz+fxwx/+kAl1hjPm+POf/exn+OIXvxiY8xB06Ivb9KbZtbl06RJi\nMauT6+SUSyPGu+p6wdlxdxLubiLdCRIuQ5xjWrh7hcTQRJKSIRQnnHB23sPJGJrlTry5qqgYmh42\nbEdx1d7BBjQ33S+9uOn6a0fjqO45d0wdPdeposDxHPL3OsI9SJFO8/NffvmBiHWaZrOJUCikn5fF\nYtG26Qvj0YWeem+1Wsjn8zg4OMDZs2cBGBsUTU9P99SgyO94XqvVkMvlsLu7i+npabz44ou2Iqlf\noV4sFpHJZNBsNjE5OQmO4zA5OYl6vQ5JkpBKpbC6uoqnnnoKgHaNIC4p7S4DnWZF5Oc4VFkxx1DT\nkPCOaNQ6lpObHLMYtausEo1GXfUCCf04bPzmG9nxD1PP40tr7/sqKVkul/WSkpVKBaqqGhKAY7EY\nms2mnsxaKpV6Gstv3ryJdDqNxcVFW5Fvt/wb3/gGrl+/jhs3buC1117r6VgQmFB/AAQRf+6HoMsp\n2g0IsixjZWUFa2trtq4N4eD6b3X1XrHJE2jVO441x/Oo54sur7DSTaIqCZGJjqYQHU3pgrmyrb1n\nNyEv4YTk6WKH4pJBrJsZmk4bfhPhbhbpsVGjEI92IeLN9NqciRyrkTPagCec1/Z9++e7lko0dq+L\nDIf08JfoSATVvDUfoFqoI5rWPs8HLdZJ4hOhUCgE1u2X8XDjFH8uiqLebCeXy6HZbGJ2dravBkVe\nQp0ko1YqFczOzuKxxx5zva70cn1QVRX5fB6ZTMbQqZSEKLjh5C6TZkVEZNGhDpIkWWqdPyzhIW5C\n3Q03MUpudOjKKgD0mG5axJPQ16Mg1GVZ7lqo/+3IFQBawQEi1p3wU1KyXC7rN4cffPABvvOd7+id\nbf/iL/4C58+fx9zcnOf3a3FxEQBw9epVZDIZLC4u4vLly57Lb9y4gZs3b+KNN97o6jjY8XCcAQ8h\nZscFsE6xk4EzKCdhkDGB9XoduVwOOzs7mJqacnRt3BB8NNIAOqEbEZPr3ijs68v8uul0zDrBLWk1\nNjEMWgpXt61Od1ciPtmbwz3nB4f4AAAgAElEQVQ0nQYvdr4XheW85TlEpJOwFyc3PTZijWePjcRQ\nK1LhM8O9O/GEsfOj+uez/emOHqfux00XIsaLS3wsBqXZESkPUqzLsmyYHWo2m49URQeGFbf4c1VV\nUSwWsb6+jmq1ivn5+UBu7OyENckFymazUFUV8/PzjsmoftbnhKqq2NraQi6XQywWs8TU03XUu4UO\nWzCHOtTrdV3Ar6+v6+E0JFaZFvBHrdtor0LdCa+urMSFz+fz+o2OKIqo1WpYW1s71OPUraNORHq/\n2CUA7+/v47nnnsPZs2fxve99D++++y7effddfOc738Grr76Kr371q67rfPPNN/Hyy1oo5sLCAm7d\numUQ6k7Lv/Wtb+GVV14JZL+YUA8Yt4x/M8QxOcpTfoqi4Kc//SmKxaJtCTE3/Drcgs/nhdMpRGNR\nKG1XgQ6XEaTOYCRG7UseAu4i3fLcWBTceOdziwGo7miOOwl7MYe8hBNRNA7cq7OEk1FXd92O9MwI\nuDl6n9zdLDPmOHZfrxmOuYbCuDH2uFZZgr9I3PZt36+Nj9nfNOT+xX+Lue/9p562pxvMjjrj0YWY\nLXazoXSDokQigaGhIXzhC18I7L1pR90snM+dO2cpjecFx3GeoTR0SOPw8DCefvpp23ANuzrq/UKL\nLLo7MR2rbO42Spf8azQa+vMPI5H1QYWd0Dc65vcn4Ukkd4zuymqOgx9kwi+5ufLDexdesn3cy1X3\nA30zOTIygpGREXz5y1/G7/3e7/leR6FQMFT9Mc8kOS3PZDK4deuWY7hMN7CrUUD00qBIFEXIsnzk\nnAFAi0fMZrOoVqt47LHH8MQTT3R1UksTmlMi72sx4EKESmqKhKG0w1xokS5EwobwFy8iw1bHfdBE\nT7QruiTjkMtVVHecY8u93HS68kwoHkWz3BHSkSHNvaIfoxma6ZRY4/hd6m/vC4XZeY8Ox1zryLuR\nPJlCZde5iyxh7PwYOJ7zFOxOIp2EAD0IsU4EAICeXUPGww3pBm03G1qr1bC8vIzt7W1MTk7i+eef\nR7PZxKeffhroNgiCgHq9juXlZaysrGBkZMRROPvBzQWnQxpPnjzpGNJIeJDlGd1ilemSfwcHB6hW\nq9jd3dXFLB0HP+hKNId1g0AgIViSJGF6etqwzK0rqzmEJhaL9X3D0asBaS7d2y/m7SgWi4GUJ/UD\nEedvv/02bt26hatXr/a8LibU+8TNcfEi6OTPflFVFbu7u8hms3o8YqVSwfj4eFfrkb/9r/S/ReL6\ntIWgXKRccA8nnRb0ACDEnC9QHM8ZQmUaNsmpTm66GI+iVe3O4SZET6Q7iak7nZj6XkNeeiE13XGf\neFFAcXkHgCbInWqp90piPIHyTrnn14+d70x15zNaOE90JIJG2V+XU2DwYp121KvVas/CiPFwQVqc\ny7Ls2KAol8vh4ODA0tWTGDVB0Wg0sLW1hXw+j9nZWb1baT/Yhb7U63UsLS1he3u7q5BGItQPuwQj\nXfJPEATUajXMzs4a4ruLxaIhvpsWpuZqIf2gKMqhHw+nGHW/XVm3t7f1rqwkX6CXrqx+hbqTm07o\n11UnhQEIpOFXN6TTaeTz2rWqUCgYZnuclt+4cQMjIyN45ZVXMDo6ikwm0/M+AEyo94yb4+KXoyLU\nVVXF5uYmcrkcEomErxq/vSIOJcFLEtT2NKVcNLrgxBHmI0Y3R5C6ixEOpztxfZHRNBp7PhJTqQGO\nhO2IUQmyTxFP3PbouOb62MW322F20/sR+YrcMrjt6TkOhVzHxSZuOolPN8eme4W70CI9Ma4d49ho\nwtZVT06mUd52nuU4eWkSALD5kw0ARjddjAhAREDjwH6GZeO1VzF54y3HdfeDLMv64F4oFFjFl2OO\nV/z5zs4OcrkcOI5zbFAkCIJu1vQDXS1meHgYk5OTOHPmTN/rBYxCnX4f801Ht+s6irjFdztVC6Gb\n7tAJmn45Cjcu3SaTenVlJQLeT1dWet+PSkivXWGAbsfzr33ta7h9+zYALZyFOOOkyIDT8oWFBQDA\nvXv38PWvf72v/WBCvQu8HJduIaEvQUJiB/2crK1WS2/CMTo6imeeeQaSTcJnvwMQJ0V0YW5GHDLe\n4Td28rpI18Nj7EQ6R7XajkYMzrsdYTpMhufR2N0zJJoKUUkPuxGjEXCc8/ELJeOOy8REHHKlLYLH\n0uCpwapwbw3hpObOhuIS5GodobjRrXUS6eYSj05Iw0k0SkahnZ7TXOwRUcBeZstzHYAmvs14uelu\nYTduoTUnL02CFwVsf+pv2wYNXbGACfXji7lcLj0bSsdqJxIJQ38IO4IofZjNZlGv1zE3N4cLFy5g\nb28PW1vBnRM8z6NWq+Gjjz5CrVbT36eXsf1h7UzqJkzpSjR7e3sol8tQFAWRSMQi4O2c5YdRqDvh\nlC8AOHdlpWPnSYlJt+2hO1IPiiCE+uXLl3H79m3cunUL6XRaTyT9yle+gjt37jguJ676mTNnDMmn\nvcCEug96iT/3wyAcdT8lmprNJpaXl7GxsaHHWDpNaXU7xUmHvQBWkc55xOOHJzqhEa2id8y5WaTz\nkqQnmwIA3w6XUU1CPjyqnaxka4jj7pUAK8Z7D4NIn5nSQ4Aqm90lg/ohMuR8A0EYXpjQ/y7k7vf8\nXsRNd1w+4Z7oNjQzptewJ9Vtxh6fcBTroWgIzaoxPGZQrjqdFMZqqB8/3MrlkqZBGxsbGB8f992g\nqJdrgV2oIf1dC/L6sLe3h7t376LRaODSpUsYHh7uz3x5SIW6E3SdczqGWVVVvc55uVw2OMvmRkVH\ngQdRntGpwyhd27zRaGB9fR25XM4wW0GO1db/8LWBbiPBLNR7Hc/t6qDfuXPHdXm/tdNpmFB3gY4/\n//GPf4ynn3460JNgEEKddCe1izOkG2OcPn3aVxMO4hR1s99cJGwRxoBRpHPhsMVl59qxwOTx0Pg4\nlHpbdHMcWvta3Dnf7m5qjnHnTbMBvEtMu5no1Eld8Dfy9l1D7UQ67RTbLReiEbSq1jrhsZMmlyJb\n1d10r4RQ4so3DowOe7cdSdNzndyD4rJ7kmdi3Oqw95qA6sbY49qNxF5uJ/B19wKroX48IOVyiRAP\nh8MYHx/XxWqlUsHS0hLy+TxOnz6NF154YWCVfxRFwebmJpaWlpBMJh1DDYnp0iuqqmJ7exvZbBaS\nJGFmZsZSoaJXiFBXFAX7+/tIJpO62XMUnOWg4DgOkUgEkUjEctwajYbuLG9vb2N/fx+VSgV37twx\nCPh4PG4JDRkUh1lHna5tvr6+jieeeAKhUMgwW7Hx6ldxMNS5bo8/NYr7P9ZMK7tE0shEGLcv/wKu\nLP59T9t0XHpiMKFug53jUqlUAj/RBhH6Yif+Dw4OkM1mcXBwgLm5Oc/GGDTdTukKbTdC4ew7jTqJ\nSS7iXTZRSGkuLh+NQa3X0NrvOO5EpBM33Umk85IExSEMhxAeMd5xy9Wap5NOLxdjUT38xS/pc6f1\nv6v37W8U3Jom+XHT3Rg+M6n/PcLz2Lu30fO6EhMp15uG5ClvoTB2YQrbP1uzXRZJdgb6oF11s8jY\n29tjjvpDjF25XOKSchyHQqGAbDaLRqOBubk5nD9/fmCCSpZlrK6uYm1tDSdOnHAMNSQQ06VbFEXB\nxsYGlpaWMDQ0pHeN3t/f15Pe+kVRFDQaDbz33nuIRqNoNptoNpuoVqv49NNPkUgkdJF6FKuaBQGp\nREOE3/7+PtbX13H27FldwO/t7ekVVujQELoSTZDC+qg0PKJj1OnZir22SC8uO3ciJ8z/8gzWFzcB\naDfSXl1Z7Wg2m4bvX7lcHlj+3SBhQr0N7bgEEX/uB1J+K0hoYV0oFJDJZNBqtTA/P4/R0dGu96er\nJhl/+Sed1yWo0AiOh1Lqrsso70u4a+EVAscbRHvQxE6f0v+m3XZLtRiXuHYAEByqy9CNjQAgOkG5\n7Vu7ns51vyI9MjKEZskYe06Eu7BsDUXxCnvxwry/Zkj40diFKax9kEVysrt60f1gToIqFAqYmJhw\neQXjKGIXf05+BEHA7u4u1tfXEYlEAmtQRN7XPMbSlVVOnTrl263vdsa11WphdXUVq6urGBsbw7PP\nPmsI2wkiAZSUcVxfX4eqqnjuuecMruUHH3yA8fFxVKtVbG9v611azQmIRMAfF+cd6Hz2ThVW6NCQ\nUqmkd80EYJvI2ovgPipC3a6m/Pr/aGz+M3pOM0CIqy7vd0zL+V+eMTz33r17lq6sdDlJp8gAc/M6\noPswtZs3byKdTjvWQ3db/vrrr/ddQx1gQr2rBkVBM6jQl52dHXzyyScIhUI4c+aMJZas2/X1NbhH\nJKDRAJ9sb0NbdKqljrD2dNNdPgtOikKt1zuiPa29TzfCnZR9NJeD7LxHBGqt3eDoRMcNbua1qi79\nxK07bU+LJKROjIJrC9vqhv+mQYAm4OnE0lAyhmapu3KNQwudm5RiZt3z+V6x6X7cdJqp5+axv+oc\nz584OYzyv/pNxP/nf9/Vep0wT5Xu7+/jwoULgaybMXjc4s9J8nw2m0U4HMbTTz9tuYj3AxnPyffn\n4OAAuVwOpVIJMzMzXVdW8Rv60mg0sLy8jM3NTZw6dcox56ifsZy8x9bWFqampvDCCy/g/fffRygU\nMswKC4KAVCplCRORZVmP897d3cXy8jIajYbeiIf+eVgFvFfIj1Pbe1KJhrjwOzs7hhKJ5gorbjd5\nR0Wo2+GnrK9ZoBMuXboEwNqVdXd3V+/KSpJ+6RuefpvXLS4uAgCuXr2KTCaDxcVFQ2Ko2/Jbt27h\n7bffZkK9H9wcFzu6qabilyBDX0jc4/b2NprNJi5evBhIgkugHegoZ5hLpvTKKmpp37DMdRUuU8U0\nQqqzfrlYMMTN8+GwHv7iVpsdAPi484U8MnUKar3jkDf9lIA0ISbihuRXweUzi53qxJRXNzXRTtx0\np1CTcDJmqQJjWJ72744PLZzC8Ll2NZR7xrCUXiu+mLFL5j1x4TR2frZieTxxMviQlOMS0/goQWZD\nncrlEkf7/v37OHXqFC5cuIDd3d1ARTrQEeqk3rosy5ibm8PFixd7Ep5eoS90ztHMzIxnzlEvQr1e\nr1vymtyugU5JpqIo2iYg0o146AoigiBYGhY9qDjvXuk1Np+uRDM21immQGK7yfFZW1tDpVLRu36a\nZyhCoRAURTmSXZXNbvrUlUnUip3r3vhTo+aX2OLWlZUk/VYqFWxtbaFSqaBUKqFYLOLnP/+5nti6\ntLSE06dP+9Jyb775Jl5++WUAWrnFW7duGYS61/KgOHqf6IBxc1zcGEQX0SAcdXPc48TEBCYmJgLL\nQnfrZkdDh73oEKc8HAbMceHh9rJmA1wyZXTND6jY84gEpaG52bwk6eElXETSRTIXiUCt1zvJqHVj\neIk4lDbcCLQKmqD2FOldJKMCQGikI+jkShWhhLsQEBPGz8hre2iiJ8cQ68JpjwxbBXk4nfRMQFXl\nlu7o06TPTIETBRTudgS0nZseH0uhsqvFI3brpgOdcpUnLpzGxmKnaYRZpAflqtNdSQFWnvEo4zUb\nSjcompmZwUsvvQSe51EqlQLPDSI3CouLi4hGo1hYWOhrJhNwHnvpnKP5+Xk8/vjjvgRiN0Kd1Fkv\nFouYnZ3FuXPnHK+T9DZ2K1TdwkToUonmOO8H2XHUL0En0dKx3XSJRCJKiYAnorTZbOqhHs1m80jP\nUNAinUDnHhGG51PYy3rPjjsl/f74xz/GmTNnIEmS3szpt3/7t7GysoJf/dVfxR/8wR+4rtecfL27\nu+tr+eLiIq5evYrr1697brsfHgmh7uW4+OGoCfVGo4GlpSXDVKQoivj8888DDafpebrUR4y5E1yK\ncjDbITJ2TjonuXQqdYkXF05OQmiLf2Xf3gW3FemU2Ldsj+m7FJ2Z6vyT39MrvwhRyRCnTrvpvRKd\nHNNvYGqb254uejeER4bQ3Lc2MyLinSTBjoREFD+3ut5e0PHq8VMnUN9zTjIiJRoH4aQTgirnxRgc\nXg2KSMlDAJibm7Pk5gTVnAgw1ltvtVo4f/68wRUNEpL4KstyTzlHfsbyg4MDZDIZVKtVzM/Pd11n\nPaiyjSSExk7A0x1H19fXdQFP8g+I09xL8mE/PKhqN7QoNY9Nd+/eRTgc1vMw7GYoyO9B3eCYv2Nm\nNz0xnrAV6oOg2WxCkiRcunQJkiThk08+wZtvvgkAgXxPnQgqaZtwrIV6kPHng6jQIopi16K6Wq0i\nl8thb28Pp0+f1l0iQr8lvcz4FupxzbFVOQ5cxSrsDKEtIdPNjtvnMToOvkkl3JZt1u1FRAKa9km7\nfKrjehHR3q2T7kVo7ARIxGhzVzuB/brpYjKBVtVaQUZIJqDYPC6d1ERCdEpAbcO5WYpbyEsolbAV\n514MndVEey+C3Y741LhhO05cOG25AfFKTO0WuispwBz1owRdLhcwzoZ206DIHFfdC6TM4/r6Ok6e\nPIkrV67g888/DzzsgHRGzWazEEWxr8RXt7G8WCwik8lAlmUsLCzYdl/1w6Drqzt1HCWJtKVSCfv7\n+9jY2NCTD+0c+EHEcR+FspQcxyGZTFpyBJxucDiOswj4aDTa1/FRFKXrrqTVvaqto94vdLiy2XTx\n81ml02lddBcKBUvzJ7vlxE0PkmMp1AfRoGhQpRT9rrNUKiGbzaJSqbiWEeu1pJcT3TjqSkgCJ9eh\nxtrJMhwPrmyatjKLdOsbOi/jeCBBOSzljvtKwl4smEW6i9PPp4bAC9opoewXHG8gnEJw/OxDaHQE\nofYgJrdFuy7SParGEIS2yOejUVuxTpAmtWol0dPuop0mnO6/wkr68TkAQPHusv4YCXvpRljb3Syk\nHp/D/qc52+cHEf5idtQrlUrgscyM7iDdoO1mQ+kGRWNjY54lD4H+HHU6Lnx6etoQFx5kcQBVVbG5\nuam3b3eqtd4NZhGtqir29vaQyWTA8zwWFhb6zscINKepCwRB0D/32dlZ/XFFUVCpVFCpVHBwcGCp\ntEIL+H4F6lEQ6k4i2ekGhz4+5XIZ9+/f14+PXSKrHwFOJ1QTN50kktr14/BieD6F6r/+nxD9/T/u\n+rVAR5D3km/0ta99Dbdv3wYAZDIZXYCTddktz2QyyGQyyOfzyOfzlgTUXjhWQp0kXxD8xp/74UHV\nPKchA2k2m4Wqqpifn/d0OoKuJONHqKt//6b1sZAETm5AjbeFHxG25aKzIA1HAJnqPukl6pNUDGjZ\nJmTCLMq7CMfhU8a4dgUFqI2GZzIrH411mjRBE9OGzqyxuC7sxdGRTsJr3r1TqesFwHQ8hVQKSlkb\nGEmIChHtnGDfSCiUjFtKNJqJjKbRLFiPc/jECGSbLrJD57QMfiHiXZM9fuqE53Oipyeheny3P/nk\nE4s71M3FU5Zli9A77IvvowgplSvLsn78aYFONyianp7uqkGR37wbGtoomZ2dte1FEcQ1otVq6TMD\no6OjiEajePLJJ/taJ4EcO+LSZzIZSJLkOvvQzTrp8KOjAs/zrpVWSBw8LVBJ+b9EIqEnLvrREUdF\nqHejeZyOj6qqhko0+XzeUF3FnMhKn3uyLEMQBFT/7W8hPpZCeds9xnzy8jwyf/0JCssFpGesQjp1\naggHG3mgD7EO9DY7evnyZdy+fRu3bt1COp3WBfdXvvIV3Llzx3Y5ec6NGzdQKBR63l6aYyHUyaDe\narXw3nvv4fnnn+966sWLQQh1JxGsqiru37+PXC4HSZJw7tw5S6ye2zqbzab3E/vcRidUMQJOrrf/\nDoOTjUmkanwIansw48v7mhiXm5pIpwmFOwI0FLGGrkQ6yagAjE776Ekgf99+A8k6wxGgYVqn4Hw6\n8CengLbAVoumhkRkf6JG59Us0t0QR0b1GwPitOubFY3qsexCwj1J2K1KDQBEJjs1wes+XXY/iEMp\nW7EOAKnH5gEA+59lXddBx6ebw14M63Nx1U+fPq3XKd7c3LStvetWp7jfcl6M/mm1Wmg2m8jn89ja\n2jLESRcKBeRyOdTrdczOzg60QZGqqsjn83q1CC+jpB+ThA6lmZycxHPPPYdwOIx33nknMAFIrpPv\nvvsuksmk3ggpSI6aUHeCrrRCQ5f/Ix1HK5UKVFW1deBpnfEwCnUnSEhMLBbDiRMdE0VVVdTrdb26\nysbGBiqVih4yGI/HwfM8ZFlGo2g0fnpx0ycunUZ1t7ceKYqiGD6PXsMYX3vtNctjd+7ccV1OHnda\n1i3H4opEQl04jtPjD4MW6kHENZoxn9R0nGU6ne5pIH2Qjnq1WkU2m8UFaGEvjjg46Eo8BTUsgWuL\nbb7SPiG9nHSzM243OLbddiUsgd9zEO000ZixMo1LSUFuaFgPb1ELmmg3i3Tb17kkv9KIoyMQRwGI\nIbR2Og64LtLd6sq73GyYiUxOgBNDqK/Z10cPj3hXrQil/Ve2SD02j4PMsuVxP266F7FJbR3hv/uP\nSP7KbxqWmesU07V3JUkyCPh6va7HqDebTdt61N3gt1lGJpMJbFB/2FEUBaqqIhwO62Pu1tYWcrkc\nwuEw5ubmBpo3oKqq/n6xWAyPPfaYL8e5l7GXlD/c2dmxhNIAnfG3n+uZOeHVT3iQX8zi9LCFar/Q\n5f/MpRLdxhBSs5uEngatP/wy6DrqHMdBkiRIkmSJ1yblEXd2dvDYf/kTkKtpfCzlWEN98vK853tG\nR1O6WO8mBMauMMDp06ddXnF0ORZCnZ4aDYVCaDabho5sQSCKIiqVYCppmKG7vY2Pj+PKlSs9V5cJ\novuc1/rK5bJeHuyZZAOKaNxW1UW0qy4DuZzuJI7qop1AxL5H+IoatlmepKbTDmzuzs0iOxLpOPUA\nIJmWUwMhlx4Gx7cH5eKeHvbCm2LmOSmqv84Q1x6NAtWq9cYgngTqNQhtN4MrmFx8G7zcdD6pzToo\nFeOgGZnSmhpFeA6N9U3P97Er1+hEaHzc0Hwq+fgCAKD0acYxXj0+NW55LHp60vB/6vE5KBXn+Hwa\ntzrFtDu0vr6OfD6Pg4MD/M3f/A0+++wzyLKMH/7wh7hw4QLGxsa6EiJ+mmUsLCzg8uXLuHXrViCx\njMcJnuexv7+PH/3oRz0bF07Y9cUgDZFWVlYwOjqKp59+GlGn3BcbRFE0hF66UalUkM1mUSwWMTc3\n51j+kIj/XoQf3amUXFdu374dmEgHgI2NDSwtLel1vUnd737jvY8abg4zceA3NzdRrVbx4Ycf2poA\n8Xh84AL+MBsehcNhhMNhtFotg5senzyB+ckT2P5Ym1H1W/ElNW1fW92vWLcT6g9rT4xjIdRpiFAP\nGlEUA19vvV5HrVbDe++9Zyix2A+DdNRLpRIymQxqtRoWFhZw8eJFNH/812hFk7rLK1Sdq4WoJMyF\n/G8nqtsoic4JxZOEVD8x5i4JmSrHGeLaDdVpiJtuvsGTYu5VaWiGhsELAlAwxpv7ddId4Xjww51B\nSynkDcsAb5HuSftGIXzqpLZa0bs2u52bzvm8ECUfX0D9/Z8E4qb3ip07VKlU8NRTT2F+fh5/9Vd/\nhe9+97v4/ve/jz/8wz/Er/zKr+B3fud3fK/fTzOMb3zjG3j77bcNiUqPOoqi4PPPP8f9+/chyzJe\neumlQMviAsZyu347e3rhJ0l1f38fmUwG9Xod8/PzeOKJJ1xv/noxXugE28nJyUCuKzTEoScVRM6f\nPw9AO3d2dnawtraGXC4HIPiEzaMGXeu80WhAURRMT08bTABzsyIS402H4QX1+bRarUM/vvE/+V86\nbvqk8/jux023IzY7BT/BVXbN6x7WCl7HSqirqjpQoR6UAKbdFJ7n8eKLLwZ2pz0IoX5wcIDFxUUo\niqKX7jJAXWha0YRBLIsHmhOshsyuu/8LrxJPGZx4p5sBN+Fv+3zqZoArd99VFAAQbYejEIe81QLS\nbVHN8eCKPdRTjXrEoZ/QHGdlxzmkh4/HDdVo+Fh3DbDCU1od+Maa1oE0Mtq9EyGOGAdFLmQcbk48\nfwnVVWOiqR83Xd/GM2fRuPd519vlBnEvJyYmcP78eXzhC1/AH/3RH/W0Lq9mGZcvX8bCwgKGh4fx\nrW99q6/tPk6QChVnz57Fu+++G7hIB7TxvFQqYWtrC4VCwdAQqVecxl66ugrHcfpn3s867Wg0Gsjl\nctje3sbp06cDva4AmkBfXV3FysoKxsbGkEgkcO7cOTQaDb1+eT6fx+TkJFKplGfCZjweRyKR6Cnp\n+yhChwE5hYjQHTTL5bJ+w0N3G6V/uhXwh+mo05gFuphKYPKlS9h45ycA7EX68NwwVMVffsPGxobn\nLMXDLtQ5jhMB8AC4YyHU6WoAg0j6JOvt9wbAzk15//33A02+CSr0hVxciDNy6dIlS6e9+k//Fuah\ntSVqYlloaffU9fQk+JYWziLWNIFtJ9rVtrhXQhFj3XRYw2XkeEc0igVNrNqJdMVFuKsRzYEm8fEG\nB/+gnaltDnmJSMaQmIgPp3yIuqkpGTPAOTvxHI27l6ik4E+Maw4+PES7g0jnkymoLuUdAU2wh2dE\nyBuaYA+lk5APKrqb3k0YjOX9E0nEzydR/vlnlmV2iaRCIgm56JxFH56cBBQVof/336NpilP3C32x\n7aWcVzeQ9X/zm9/Eb/zGb+jC/VGH4zhMTEwMTHAUi0Xs7+/j008/xZkzZ7pu6uOE+dpDFwWIRqM9\nVVfx0xejVqshm81ib28Ps7OzOHv2bKDHrtVqYWVlBWtra5iYmNBnHOyuXXQyqVvCJhHw5pKJ5lCR\nh0nAq6rqedydOmjSAp6U5CyXy4YkTfrHacbnsIV69d/+luvyyZcuobZlX3msG5rNpmGWwnyTE4vF\nAgl98ZtjRC+/desWAODtt9/uqTMpx3Gcqp1EVwC8CuCdYyHUaQbpqPdyA0AqB2QyGQiCgPn5ecNd\nHXHqg5r66tdRN5fumpqagqIovtphK4K78yVHO5VZhJpzB0qaVkgCL3eEuxJuC+x2NRl5uFPFRKDi\n2tVQJ4RFEcPgqeozbv6Njt4AACAASURBVDH0AKAkOwOoJVae4CDSVSmqi38LaSrmbmvNutzDSQcA\nxBMd9x7QHHxB0Fx2joea738Q5IaGAVMcuzipOexEsHshjgyDEz3CByISUK8hfv4x7X8bwe7kpj8I\n+nVgvJpl3LhxA9/85jeRTqexsLCAmzdv2l4MHjXMwiyIahpkXMvlcnrXy9nZWevsYB+QsTeIogAE\nt74YJFeoVCphfn7eVwUcu9h8J0ju1NraGk6dOmUJobErc+mnjrqbgCfJmuaqTUTAEwd+UF01+6Gf\n76mTgAdgcOC3trYsAp6+uSHrOgyISI9NaeGTzb3uZ6ln/ptfxPJ//qH382Zm9L+dbnKq1SoEQUA2\nm8UHH3yAg4MDVKtV35+Tnxwj8/J8Po+33noLb7zxBq5fv95T3pHaOakqAP4YwJljI9TJnXwoFPKd\n0NMN3Qp1c+WA8+fP27opJK4xqOTXXh11sr3ZbBbJZBJPPvkk4vE4tra2UCq5iOr2F95OpBN33Y56\nYgx8S7uhCtVMYrh9EWmZBLVrZRmYnHaTwCauvJ1Id01wHZ7QRb7utBOR3oVzoUhx8E3qezk5AxUA\nV9x1rTBjiLknznhb4No+fYRKdNpY7bjpAU2Bi6dnIQJobdhXiumH+OVnUP/5z7t6TZDhL+YBvF+h\n7tUsg+aVV17BjRs3en6v40q/VU8URcHm5iaWlpaQTCb1xkF3794dyMxrqVTCj370I4yPj+PZZ5/t\ne1y3M15KpRLu3buHRqOB+fl5XLx40bc4I8fTTag3m009Zn9qaspSiYZARHlQVV/cap4TAU+6jlar\nVb1CCxGpjUbjUEtDDqo8I0nSNI9FzWbTUEYyl8uhXC5jcXHR1oF/EAKeiHQ3pIkTtq766X/+Ze/1\nz05ZHnO6yVlaWtKLgPzjP/4jCoUCfv3Xfx2bm5u4dOkSvvvd77q+l1eOkd3ya9eu6eN8JpPptzhA\nGkBRVdW3j6VQH4Sj7teppisHjIyMeFYOCDqmvNv1KYqiZ+4PDw/jC1/4gmF7nYR/6e4iEE4g1HBv\nkqPjkuRZS45DoBzvcHlXF+nETbcV6W4CO96ZASBOuy7SXS5SdLlIM0oirYt6wdxx1bJtnfdQXEJk\n1KFRqO2qMXzR1PQoInWSXJ1izONJoGZfjYg/3Y4D3PUoT0ndKHCkHn0sbnHVaYRJrVKMst2pxU4S\nSc2x6bZvOTrm+ZwHjd1UKd3lsFu8mmVcu3YNr7/+OhYWFpDP51l5RhvIeN6tUJdlGaurq1hbW7Pt\nWBpkiGSj0cDS0hK2tragKApeeumlQGdIyfhbKBRw7949AOgqzp3Gzcgh+3H//n2cPn0aX/ziF10F\nvV3N9EHUUXcS8K1WSxfwxWIR+Xwe9XodOzs7esMiIlQjkcjAheqDrqMeCoWQTqcNN/0ffPABLl26\npB+XnZ0dLC0tGfII6J9wOBxMjf7//Q98iXQn5v+rC109X1j8f9C6/M9dnyPLMhKJBKampvC7v/u7\n+P73v48f/OAHALTvuhdeOUZuy19//XW88cYbvvbFDBX68hiAf8lxXIEJ9S7W7wZxIUimPWlY4cVh\nCXU6MejEiROO7o+XQ1+Njerx6LIQQbTqnTwpuzjtAFBPdFzhsFz3dNIBLbyFa9lfeFuxlC6GzU47\njVmkm2Pcaee9FU9B5UWIZffOY7RIV0KS7qqrYQlcQ/ubU1pQeQHK0KhW4WXfoUtpNwMqHXYyOg5V\nFMHtuDc44hI2TbU86rLz05qQVVaX/G8bIRS2zAxEzp/XXfV+wl56iVMfRDkvr2YZLNTFCj3WkvHc\nb0nBer2OpaUlbG9vu1bSCkKoV6tV5HI57O3tYWZmBi+++CI++OCDQCuscBynN3kKhUJdNb+zw248\nr9fryGazyOfzmJmZ8RTo9LYRcUqL1AflapOkYzJTnUgkUKvVMD09rQvVQqGAtbU11Go1Q8jNIAT8\nUWh4BGjnzNDQkCVcVZZl3YHf3d3FysoK6vU6BEGw5AYM8samYRMSk/rCJaj1us2z7VF+9hHgQ6iT\nc9E8izSIBHWaa9eu4dVXX8WVK1e6voZQoS/vqKr6JxzXTXeUh4RBCXUnarUalpaWHBtWeBF08quX\no0E7TXRikBNuQt1OcDcjmuvRAI9wQ0sIVIQIeKVpeY0ihPTwl5YYhiA3LGEo9UTHeQ3V7UNwzCJd\nESPgW1RMOt/5PORYSne7xSoV024S5W6JqPpzQhE93EZNChAPtJsUNRQGJ8sdke4ym2C73lQnltkx\nRt6Mj/dQT7Tj+Zczek11Lho1VIdxJZFydu+nZ8Hd37BdJqRSUKqd13m56ZHz5yH6jIUHtPAXN/ff\nLw97lYDjRjfGy8HBAXK5HEqlkq+EylAo1HNfjIODA2QyGVQqFczNzRliw4MSqSQRdW1tDZIk4eLF\nixZHuRfouPJarYZMJoNCoYC5uTk89thjXbefN4clHYXOpGYBT2i1WrpQ3dvbw+rqqkGo0g58L07z\nURHqToii6CjgyY0NfVzMNzaxWMw2N0D883+HlovwjUxPorXvLyeNYK6hHh3t/uaUHs/39/e7vsH1\nyjGyW07i1klhgBs3bvRjxjzFcVxYVdV/PHZCfVBVXwjkZDRfGJwaVngRtKPuNFDQcYd2iUFO2An1\n0t1FX9vSCGsXFpXjEGraXxTNceg0sqmCTFPqnGghHIBrNS3NlhTROCvQEiX9JsGy/mgKcioKXm4Y\n4uR1kU7i703JqADQilhDUeSENg3WCscRLno3DtLfT4rr7rqZVkpbp7Dvv8yjGksa1qfEU+DqnePP\nzT2mxcfvdrZRd9O7uMlUhkbB1zoCmTs5rb3/5qrvdTghTE6hRYl1IWG86IrnLtg3r+oDcydSJtQP\nBztH3Ym9vT1ks1m0Wi3Mzc35jtf2U/Pc7b1ImdqghRmJqc/lckin05ienkY4HA5EpAPasa1UKrh3\n756ehNpr1Ruzk04/dhQhScRmwUYL1Xw+b3CauwkVOepC3QlRFG2PCx1aZJ6ZIA78mX/8T2ju7IAP\n0KEmIn34n/1Tw+PyvU+7Wg89nvcylnvlGNktp+PYC4UCnnvuua7e08QFAF/mOO74JJMSkRy08DW/\nx97eHpaWliDLclcXBicGub1A/7V1/SanyoJ70lQz1Kl6oHACIg33O+yWRwWZSnxMF/DhuibYiEgn\nbrptMquD89yU2k57FBDNya0whr2QyjOu29922gWP0BjXdUQTeqhMKzWihdoU3RsRqTH78m9qJGYQ\n6wCgjrabG/m5CXBx081wJ6cdxbrv2PRIFMLcWbRywdZJd8Mu9IUJ9cOBLrdrFuqqqmJ7exvZbBaR\nSARnzpzxVZWKJhQK+RLqpFpMNptFOBzu6b38QOc20aGIa2trgc0QHxwcoFgsolKp4OzZs67XrvXV\nFajt4rtT09O2z3lQMeqDxkmomkNFlpeXbWO9E4kEQqHQoQv1oI+728wEEfAAehLpB+u7SJzSBHny\n6ScNy9LPPAlu4lSPW92BHs97EepeOUZ2yxcWFvBnf/ZnemGAV155pZ9d+DaAVVVVm8dGqBMGcaKQ\nwZqUwzpz5kxg9ZUHNQMQVG1dO6HuJaJVdN5HFsIQKEe7KUQgKDLqYe3kVzkekWa7vnr7s7Ndv8vn\nWpc6n0W4HR5DRDrtprdESRf3gHOsvNx27kWbUBs7kU6H1phpxTsJqGK7qZIiOSSGUjcRLYfnyENj\n+nuGSvax7AY33em9KJS2YOd3/c8CqHbx7PQ2nJzWKt2sZvytb2wSnFeCrgvKzFnwy/2JerNQ72W6\nlBEsoVBIT/wKsuyh17hrrhZz8eJFSznBICAlENfX13Hy5ElLKKIgCKh3EbtrB10lJh6P4+zZs5br\n18ZKJ8dE4QQAHDioUMFheXUTCnhwUCGrIs6c1gQWx3GQZRk7OzvgOE4XdA+bUHfCLVSEFvBLS0to\nNptoNpt6CUDagX9QPKga6kTAD/9/3+7p9fEXXsB+7s8Nj/mJTxdHRiHnHfK3bKDr2veab+SVY2Re\nnk6ngywKsAeA5zhu+NgI9UEIdPNgPTQ0hPPnzwc6YAuC4CsD2S+VSgXVahUffvih79q6bpiF+trq\nKoRQAjIfgqC0L3RiHBFZc1tlPqw/bhbpskmAkyZH9VA7RCbMIdKw7zpKIwvO4TKNSBIqpwln4rQD\nVnddFiVX8a/wAhrRzokdOdj25aSbQ29oJ16OD0HlRhCq9tgF1UQzqV0wQwBQr1ncdFuRzhtPeTk+\nBKGhNRtRRk9C5QUI+bZgJyksJlFuK9JNNytycgR8sw5lekEX671UenmQrjppKU/w08CEMRjo4gCl\nUgmZTAYbGxuYmJjAlStX+hZATg3szM62uVqMH/wIpkajgeXlZWxtbbkmvbrVUfeiWCwik8kYQnU+\n+eQTi5CmRTqNSrWz46FAaRsw91Z20YIANbGAjz++jYmJCQiCgPX1dT1md3Nz0xDzHYvFAu2Sepg4\nCfhPP/0UsVgMHMfp5RKbzSZEUbR14IPmQTY7Ev/z/2b8f6oz6yKv9R/6GDQPWxgjx3HDAP5XADsA\nfnRshDoNGdx6HRharRZWV1exurpqGKw//vjjwN3voEJfSIJTtVqFKIp44YUXAjlp/Ya+1EVNxOqC\n3STK6f9bvNgR+fRz+DBA4trBIdK0JgjKguRa97wpRiG2XfNGJKU/N9w46FSn6SIkhlBLdUpPOSe1\nRqDy3qdUM6oN8EriBMKVPcvylhTvxMdTlWKUcBR8w9pJtJkchTocRmi/u2ZHragx7pXMDLRGtH0V\nNpYsIl1/rqBdaBQpbohTt0OZXgCfN1WcCfkXWub4dD8o02cg/Piv0Hrqn3o/uU2z2ezZoWUEC8dx\nqFarWF9fx+7uLs6dO9d12J4bZkfdXLXLK8nebb2tVstx7K3X68jlctjd3fVVArGX68Pe3h4yGe3m\n2Dz7S4/n66sr2oMcr4tyXrW+V1MNgeM0cS9wLciqNsZxnIqR2WfRBFBTBChJIJno7Es6uqvHfVcq\nFSiKgmg0amhcFI1Gj83NMJlVMDu35nrn2Wy2646jfuin30D3b9a52aNFOvnfS6xPfu1XUfq7v+/6\nbWlXnfs//jXU//73fb3uYRPqqqruAfg1juMmAfzasRTqvdbeJS4HSbg0D9Z+4xq7od/QF+KayLKs\nuybvvfdeYNOPdkJd5p0Hk4PQMARo+yPJmoAjIp12aMw0bWLc6yHNEVbCKUg2or0lhMFTgp/EyctC\nWBfrhEY4gYYYRbxqnTpTxDB4qmqMLEaM6xW16i2Coq2ThNpEagUooQi4lmybxCrIxgRR4vQbtium\nDR5h7IFrNhxDXgzrMbnXTSkFUa6hmdLKWooHlPjvY+BuzjwOAAjlO9VcvEJeAM1NN6OMTFjFuk8a\nj19G+FN/Ccz9QC6ewOG34n7Uqdfr+Oijj3DypHbTSHciDAIigOmqXUQ49yN2yHrNgqtSqSCXy6FY\nLGJubs538QG/RgndAVsURccyjmR9a6urIOEtStuk4FS1HfYCyNo8nT5mq2rbOGiLeh4KeE7bLkXV\nXs8DUKHqr1urnADHjwIpIEJtSjKyjXK5jPv376Na1YwHujxgIpE4kp1HvXCKUberdw54dxylb2j8\nFH54UGOW+Of/DgDApYe15n8V6yw4Ee983Nn4SP6TX+hJrOOZl4AP3wEAR0PWfLNcKBRw5syZ7t/r\nkOA4bgzALwK4DeCPj41Q76f2LqmHS9eRtfvwBxFP3quj7uaakHUGcXdNXyi0wd0/B2IaAmcU7QSV\ncrDNwt9O0NdCHQFLXHsau2RWO+edjo2XGtaYaLNIb4nOSbJ1Ka0732GPBFQ5FIMgd+Lw5HBML00J\naIJdEUK2DrsbrbBV2DeSmmAPl5wd9lY04RpbT9McmTSI9V5QxRBa49MQ7h+9aVECHaNeLBZZfPoh\nIkkSnn/+eTSbTezsdDdT5Ac6RLCfql1mzOM5PdPZS4UVr+sDyZ/KZDKQJEnvvOoEz/NoUPHA5rG2\nibAe4gIAAloQuBZaqtB+P04PgwlxMuqK88wYx6m6wKcf26yPAeIY1CEOkSHtMQVANKR1wd7c3DR0\nHn3QjYt6pdtkUqeOo7SA39zc1AV8OBy2OPC0gDd3iR0Eob/7nlYxLO3tTotT01AKxkIFwlPPAh4z\nsN3w4YcfQlEUSJJkOC6CIPTdE+PmzZtIp9NYXFy0LbFot5wkkd67dw/Xr1/vY88QBzAJ4DUA54+N\nUCeoqupbUJdKJWSzWdt6uHYIghB4jfZuhLqqqtjd3UUmk0E4HPZ0TYLAbV3mEBaZc3baa2JcH7Sl\nVudEdXPnAaApSJZkVFrkx+oFXaSrLuErDdHaHbQW7hy7sJ1jbyPSFRtXHAAa7QTUUMPmJiJkdRVo\nkQ4AzVAcgtLoOOxVY7WYltR9+EcjeQIqLyByYBQ65pAXO5qS1iRKbGqOV3NkEi0hjMiee31z4qar\nYhhoWhOEWuOa0yLseXRLJfswojU9ol118Zx9F7t+E0r7rRLACA4yDgfdF6NYLCKbzaLRaCAUCuHF\nF18MVNyQa4/dTGcv7+MUo04q32QyGcTjcTz55JO+cqfCUgLmtfHtR2pqFALXMsSjtyAYf6s8eKho\ntZdzbQedhMaQPVRVTh/v6ZsBi3BHZ+Z3q3ESXFgFwkC0feqdHirZ1j03O84PMmnTiaCqvtgJeFVV\n9WTVcrmMjY0NlMtltFotXcCTkqPmpPigCP3d9wD4E+mANouKgntFMXPFFwDgQj62ve2qX7lyBaqq\nolarGWrBl0ol1Go13LlzB9/5znewu7uLCxcu4ODgwFepU1IP/erVq8hkMlhcXNSrvjgtz+fzuHr1\nKhYWFvDqq6/i1q1beknHHtgD8H+rqroCAMdGqHdbezeTyUBVVczPz/seRAfRTMnPTQVpfpHNZhGP\nx/HEE0+4ftmCLPlIH5c6JG2gVYEI3wnrIAJd1Qd3UQ9/aakiBE5GU9UGUhFN1IS4/tyI4lzyzyzS\nzTQ4CYrUaUJg57Rr63EvHalwPGrhjhCO1fZ0kU5XsDEjCxGISifEpsWH0JI6CUax8n1dpLu56XLI\neBOhgkM92hkMo017N7sVjkPx4YqTTq9Sc91hPVE9oVTfBof11oenrGLdpzNPaEbT7kI9Yr2pckM5\n0XsHUxqzo86E+uFBxh26QU+v0GEhgiBgfn4ew8PDeOedd4LYVMP7NJtNfPLJJ4hEIlhYWOi7Opgg\nCAajRFVVbG1tIZvNIpVK4emnn0Y06u98ya3e10czBTx4aOttqBFwUMFzClqqAIHTrh1NJYQQT1XN\nUnkInIKWyusVYRSVt4THEIh4V1WODmkGTz1NBYeWQj/AQeQV/XXZQgpa7AzARQAyRz6ZKqBcLmNn\nZ0dP2iQmHZ28OYikTScGWZ6R4ziEw2GMjIwY2tarqqo78Ds7O6jVavjoo4/QarUQiUQsDnyvs+yh\nd/8v7Y9I+xOolrsep/tiz2ZW7ZmXAGjHJhqNIhqN6k2JisUiNjY2MD09jV/6pV/Cf/gP/wE/+MEP\n8Kd/+qeoVCp47733XL8bb775Jl5++WUAwMLCgqE+utPydDqNTCaD1157DQsLC3rEQ4/UAKgcx0kA\npo6NUAfcXRhz7d1eWjGLoqjH1AWFm6hWVRUbGxtYWlrC0NCQ70E5SEedkFk1nijlVkIf0COwNush\nMY520OK3zsd090ZSK+2qMU00TZVdZN7bMamJmqMkyWU97MROpLs57wBwENWEbbTpXOvdLNJt15PQ\nBKRU71R5kU2VY+RQ1PVmAADKaS0+V6rtdRI+bUJezJjFdm34FGRRQvTAKpLp59KNpeyoD09BEcOQ\n8sYwFrvYdCdqpx6DtP6Z9t5j/z977xpjx3Veia5dVed9+sUmm6QokWJTEiVRsmXasmQkRmKYngBB\nkB+xDP2Y/AgGGBuYzAUmQGAjQC6QIAgCG/DNxSABLgQkEyDxD4/oP/ciwWRCDJKMHT9E0XZsSbHM\nfpFsPrrZ3afPq9573x+7dtXeu3adVx9KFMUFNMiux646p8/ZtWrV+tZnJtpCTU9/P3se1g/+18jH\nAIC7d++iXq+jVqsNvYjKxVgPFfUPPgSpXV9fR71ez9lCTJ01Jz2OsJ+EYYiTJ09OzVMvrg9yI6SF\nhYWx02jWb5hvjKNEq2OJXx0AYmanBaMhLYGCwE786DGzUhuMQKqKE8ASXvZknfCvy4glQi8r6oLw\nB3FWrCpgE5beHBAwbLTnAcwDVcCuAuKMyr130r+7bhmRLTT3oujy/chRJ4SgUqmgUkl6iFgWnnji\niZTACwV+c3MT/X5/cgIfBmDVGsiIN830UNIFe/kZYPWdg7zEiSC8/s1mE7/xG7+Bv/mbv8Gf/dmf\nYWlpaaS/U6vVUm6IdnZ2hq6X7TFXrlzBq6++OvZ5E0II48rEZwD8BwDXANx4oIi6gEzU38vs3Ulg\nIuryOS8uLuL8+fPpF3HSMacJMYkLdCm/+InJvEpGbEkPfqEQyo5H6ghZCY4dKeRfkHRBviOrBCdR\n2iOU4CBUHq96TgMBKnAQKjYbE3iuu+QVt2spAXdLM6nVpS752U1++MCpKTagwK6m5+hV5hDXHNR9\n1c6iK+kmUMka5FUXENll1N3848TIqcJJilejUi21rMjwKnNwYh9ucwkAUO3nC2uHkXRlvEOPpmR9\nVJLuNxZh0eRR+yNPFVppdJIuUGR7USDJd0dvXsGPyHG4rgtCiFK0JlInTO3fRfe5STHM33jlypVU\ncTlgU4wHEvqFdBwSJM+fCwsLhQKHmM8nJW26uv3888/j9u3bU1dxXdfFd7/7XaUR0jhYub4DgCvl\nspLus0o6bzokUuZQCgJLItEmgi4r6DIhj1lG+i1CwRiBTVSCbtpPhzw+nzHE7+KcqXLOBAxB4xns\nMgB1wKlnloHFxk6OsMq+5maziXq9fqA6hfe74ZFcTCoTeKEyA/wcfd9PPfCbm5vo9XpGn7eI1LT/\n9X9ORtLfR+hdpmWP+r3+GwmbjKzAjwqWXYTeBfCb4F71Tz1QRF3O3u12u1hbW8Pm5iaWlpYmmuB0\n3AuiLj/alWMhl5aWJs4LPkj2rgnHTj4FgE+cOkkPk98FSY+YjS5rwE4uBlXLS20vOmSSrsNHlU/C\nVs2o2AdkNDVJttlUaS/Jdud/Q52kCxU/ssopWbdYDEps9MuzvAA1zFe46yRdqPjyDYVNI/QrfKJg\nIEa1PrbKabJMESKrhH6Nk2ITYQ/KZkuUvpwRG25jCbXeaF5xvzpnJv+HuOe83Cv2IjKHT5h+YzG3\nzmilGYBg4TjKe8WFrfTkE7llzz//PF9HaepjlIvWBIEPggDb29tot9vY3d2dWFEf5m8EgD/5kz/B\n66+/jq997WvG9Q+RQQgPw3y3onnQ5ubmSHnrYj4f97og3wgcOnRIUbenJZKIa8H169cRRRFefvnl\nia4FnKRzq4vg3S6rwiGC+nKVOlPQ+b8WoaAJCdYJuoBueZF96jLJFkWlmeWFIaJ5QmzigYJTMUbS\n8WlilQkN9UIlK3vv5RuDG71FAItAEyg1kT7vnattKxGSjDFUq1VFfR81QvJ+IupFIISgWq2iWq3m\nCLzneSmBF+/HS2U+r0fNQ4UN9lBvGpNf3gtsb23hyNJSbrnu09d7ZAzD/Px82hOg1Wop79Ww9Zcu\nXZqokJQkH56ErD8F4AkA32aM/d0DR9Q9z8Pt27exvb2N5eVlvPzyy1MrrLhXXUQZY2lDj+PHjxc2\nvxgVuq/xoBCTuDz52iROSXq2XZYMEMGGQ2J4tJpO+FVL8mkbyiNCZlai/MSZSKmFqjXYehSiXBgD\n6VkNBKyMpp1PaNGtNgKBVYXDMjLvJc2ZfFQxE+cTWob54QXcEvfE64RdPveg1FBiJgMt/71fOwRK\nLNS9wUkxQblZWATrNvgkJzeHGjUNRhln9hhqbd4oiTnjEYrekWU0tjM/X5GaPgr2F8/AphGae/km\nLpZlodls5uo7KKXodrtot9vodDr4gz/4A/z4xz+GZVn4zne+g2effRZf+tKXcPjw4ZHOYZi/8eLF\ni3jxxRcBwKi2P4S55qhoTgyCABsbG9ja2sKJEydGnvPHnc9HEVIOStT1TqUvvfQS3njjjYlI+rvX\nWyj6Jge0lMYrOiTOzZlC6RYkXbaoyOtlmBRz2RYj/stAFFsLpQalXZyP2Idl9hsBxvI960JqI6JW\nuq1tFQQhJOd60z0CWEeAGUCUKTEAzeoWut2uMUJSkHg9QvL9bpB2kHhG2ect5jny9j8ilgSxqJE9\nYWSzh1Fq5UUeo5q+/Azwox9MdF7DUNRTJYqiA/XEePXVV3H58mUAwOrqaloUKp60Fq1/7bXX0jl9\n3GJSxpRb1RMAXgDwfxBCdh4oou77Pq5cuYKlpSUcPnwYp0+fnur40ybq4gLT6/Vg2/bUGnpM2/ri\nxZUsscXmZFsm6TahKUkfhG7UAAOBTWLUbMnawoZ/DMWFwaP8ETYDQYX46f8BTtJHgUukqEfpPAbl\nvBvHsTnhq8VcTRAk3TSO79SNnna3NIPYKqGh2WKCkupBD5xqIdnuV7nyWw0y0j8u2e41eNfQRm97\nrP1kyGRdh0lNV45/ZBnN3o9yy4PKLMojdKsFOEmfBJZloVqtolarYXl5GX/913+NP/zDP8Qv//Iv\n4/Tp03j77bfHsjMM8ze+8cYbALjyfunSpYdkfQhEF1HdviKyyVutFk6dOjW0eZBp3FHmc7kZ0iOP\nPDJQSHEcB/4I7dB1RFGEjY2NtIfHQcWad6/z+SRgJZRJYgOFZZynxbKIqsezJGIs5jSdsAsMkoXS\nfRNlnWgzpGVlIpCJ7GfrrOR3vtyRSLhpP4D73QlhkCvWdNXdJupripiFm95RwDkKzAHlJB/g1EIb\nvV4P7XYbN2/ehOd5sCwrVd5930cQBCiXy++Lsj7NHHXy9j8iLtVgRcWf5XB+yUjWi2B/5OO5Ze1z\nv4TZt/5ponO81aiX0gAAIABJREFUc+Rc4TpZUWeMjV2Qfv78eVy+fDktEhVCy2c/+1m8+eabxvWX\nLl3CV77yFXz1q1/F7u4uXn/99ZGPRwipAHgZwPcZYx6A7wD474yxDiFk4YEi6tVqFS+99BLCMMRb\nb7019fGnRdR938fa2lqa295oNHDq1KkpnCHHtIpJ9/f3sbJThSN997sRv1iKSbxqBwNJulBG9IuA\nG1cRUQe2FStKO8CJuyhUBXhBk/y7mOZ9xokxZTVULPOEIhdq6oo9A4GH7OJvstik2w4oQHXtJqht\no0aHE0qdxHtWAyUE6CW2GDm5Rm/aNAy9KieHDS+zoRRZYWT0y7MoUf7+9RpHjJaaQYilrrPu7DFU\n+6PvH0kRmN3HX1CUfRPazeNA8zgOXx/cAKm7cMqoqhdBV2xbrRaOHDmCc+fO4dy54gvCpBC1J5cu\nXcLFixcf+tQLIKyMcjiAiNV1XRePP/742NnkAsMa2AVBgPX1dWxvb4/cDGlckUR+GvDoo49ORax5\n+1oHjuHtCCif/+RGReL/+vws1suWE06GB7/PevILkJF7WUl3CJNUeZZGRjqEpQo7BcmNJ/Mt3T5j\nWfncduX1UALLYgip+v7aNv8MRNpTAkH+BZF/d3cOwBzsEgMWADFzPTq/j263iyAIcPXq1bTZomyf\neS8iJKdC1P/tf4PQGFG5AcIyDkGo+TMdzvMnsqV+d6A33UTSDwrCWKGiLnvU+/3+SNGlOr74xS/m\nlr355puF6y9cuIC9vfH6oEg4A+DfA/gheOKLBfDoPMbY3gNF1AkhIITckxhF4OAE2HVdrK2tYX9/\nH6dOncJTTz0Fy7Jw/fr1qd4NH1RRb7VaWFlZ4Re/+YykiIlMnty7NCPVVSfITZTyBaDQkkLzir1A\nSFVyHTIn9VfKy4SkE9ASmk7SERUl2MklIGSloYq5y/ijshopjowEgACqvYUmD5hdi5PiGDYadDDh\nBLitRkff4c9f61GmjuuWl2HoVQ8ZvfQCcodUr5SfwNoN3g1ytmdWx73agjF3HuBPDvxZ/j4ecnm1\nv6ymx04FMcx585NARDOa1PRxyLrclRQ4WDzjMH/j4uIilpeX023feOONiYg6IWQOwAyAbcbY+DLu\nfQwxlwMZoZZjdZeXl7GwsHAg5VLkTuvwPA9ra2vY29vDqVOn8MQTT4w8N4869/q+j/X1ddy9ezdt\nsjeN+f/ta3zeCKmDksVfW8BKCOIynOR3QdC9OCnSZ5oVJYlfFGASgRVz/yBCXARdjbcly42ssxMr\nSXdhUDLfdYtMrllTLDWYgw3HGk1F7YVlVBz1c6Ar9EWKPQCstuYQ0QU0T54AkBH4Y7N76PV62N7e\nTiMkJ+06OgoopQcaK1z/McoJSR+EuFyHpYlIwwpIO0tPYmbr5xOfW//pl1D/t++PvP0HsCfGEwB+\nB0AfABhjbxNC/h0h5BJjzHAb/QGGmLSn7dHWxx8XorC11+sZu9OJyX2aRH2SG5W9vT2srKzAsqw0\nvvLKqp8UkarnJhNwkb3rRZk/vGoPKYos+Oh5cQUhtWETlpL2NNfXYJExdcZzKSe2tSTrXVfS5axg\nAdlz6bI6KElsLegjIpzwC4I+ikWmZ/H0lDm6A9/Je+VMJF1G35kBdWzU42LC7zt1lGLzU4BemSv0\njaBVaJkZhnbjGA611uBL2fACQamRI+vCd28x/t7uHn1mqH9eRre6CFY7gsXWytBt7z52Hoff+UcA\neZK+XT6BI8HoBapAvvjoIJP7MH/jK6+8gosXL6bLhF99ApwA8BIAixByDcA/A3gOwA3G2J1JB72f\nIIrcbt68ibm5uYlidYugCzq9Xg9ra2vodDo4ffr00AZ4JjiOM5CoyzcBjz/++NQ6ogLAWxtdxbMd\nUgcWoQhpCYQwZc4VpF1AkHUxDxbNcfI8qRPvYfYYE/SGSXyZBYApcY8xtWBZLGd/GYQoJfaZvUWo\n6mJM/pqAMM7mSMaAssP/hkFkp/8XkO0ypqJYgfX2AoAF0CqAalbAemRmJ+062u12p5Z5fhCxL1z/\nMezQS0m6UNOpU1HsLyaS7tUXEc4+ivmtnxnH7hxeHvk8yMIhoDtc5BqGDyBRfwRAkzEmX1QDxvgf\n4oEi6vcb2u02VldXEQQBlpeXsbi4aJz4xeQ+rVivcZX/vb09XL16FY7j4KmnnkovhP/yLkXVUR8J\nWoQWkmx5cu+ENbCkecUw0i4gHkmypPLIizkxLkv7y2p6QAe/X/04s7Xo9hr5XCNWnD7jsjr8uIwZ\nJ69Qi2hIGUGScCNuBFr2YYBxlV5kxAuSLpT4EGWUYH6P+jb/Wwwi7Dpikv19euV5xHDQDPOEWVbT\nQ6uS2l9CUkaJ8fPZXngSADDrbhXGPsqIrDLKcbZNu34Us/3hnLFbHexhnxTdhVP4t2sWPnZy8PfB\nRNQnjWcc5m8UjXAuXryYy98dB4nqEgP4jwBeBc/e7QP4q4kGvM/Q7Xbxk5/8BKVSCUePHsXZs2en\nOr7jOPA8D51OBysrKwiCAKdPn8a5c+cmFmWKVHrXdbG6uop2uz32TQAhZCgJe2uDz0+6Ou5GfA51\nLJp6xAEulAhlXSyPpMQXMZZ6HkxZLnvPR3sdxdvJr0wWUQRVZZZorkQRMwJKCWJmgTEkjZdgPGeB\nSI56jAnKtjofRJTAsVhK7oMoI8mCrPuRoyjvOkmPqKX45uX1st9/vZMk0NQAq8ZfOwVwqHkX3W7X\nGJko1PdBEZKTEPVw/cfp/4vsncL2EpcnL840of3YR0farn90cO2RKW1HJ+rjzuXD4nWL1h8gweuH\nAP4LIeQfwKMZY2T9vR4son6Q7N1xMGxcYR0BkD6iHYSiyX1SjPr4dXd3FysrK3AcB08//TRmZvJt\n6r2olPrMa05YSNIFdP8fAHhxGWFswbYoak6oxH3F1E5UH7N6QJP9BZqOW0jQBxF3QdrlIlbAXMhK\nDQ2IdJU+21ZqFFSQWgNwwu/RCmYNiTPK+STkn2p5DR2bf4YatK3EPoZ2NVXVZYKto1vi+zexB5tG\nKUkfVW1v15Yw6xYXDgk13bjviGRdYGf+zEiq+t7ZX8wtu+scz910NUo+MKABF5An6ge9cR7V3zih\n5cUCJ+U/Z4z9jBDyh0nR0QyAZwEY2vh98FCtVnH+/Hl0Op2DeD8L4Xkebty4gVarNdI8PQr0ubfX\n62F1dRW9Xg/Ly8t49tlnx74mCeHFRML6/T5WtgAg6xAq7H4BdSQSnu1rE5aKL4Ksy3OyKdGFMgBS\nxCJlvMMoAcvdHIwDW1HS1TEIYRlRZwSEROn/AbVBkhc7CGNLuRkxqe403ccCp/1JJntBOgzACXcU\nWLAtBj9yEMUEjs1QtouvsYOUdoGYWkoqzUb3MIDDQAOwG/wmJQYwV+cJNHfv3kW/zy2DtVpNsdDU\narWxibp7/We8WxQAwmLE5RKo5cBKoobLfnHDPwGvngksraWzOVV9HDV9UrzxxhuwLAv1eh31eh3N\nZlMRKse1MQ6L1y1af+nSJXzpS19Kud84YIx9nxDyCwD+K/iT0RKAdwD8D+ABI+oyptV1TkdRpq/c\nptpxHDzxxBOYm8tbBgaNOc1zHKSo7+zsYGVlBeVyuZCg/8u7fH9BoG2Lwo1KmZfcyd9YyGS7SNlw\noxKC2IFjUdSdILefQMQsOCT/GtphpgKPqtQrx48TNZtZqDmc4Apip2fC83PTimBpNb2Q1e1MOTaR\ndNN7MKoPvgjCUqPHVHq22VfoowoH2d+qW1pAhU527HZtCXVHvdGI7fJAkp7uO4Csm9T0nfkzmHEn\nT6AZF2EYHijO6z3GAoALADrgneuOEEJmGWObAEY3ct7nKJfL6Q3TtGqOGGPY2dnB2toaAGBmZmaq\nGfYicKDb7WJlZQWe52F5eRmHDx8+kEqvX3P6/T5+fscCRX6ejJkNNyrBJgy2FSvkFQDcuKQQZMos\npUmRbnsRRJyAyb3EErKeZanr+1uG+VsgLSNlJLcd0cg7IdwGo0Q9AmlDpooVYKaUV+zVhkkWNjvz\nSqQjlV5nGFsIpUuwndhdBOG2E7tMJPngg9hGEKsqerWAUcmFu+Pien8JsJaAGSApYcLpxU6aeS4i\nJD3PQ7fbRafTSUm8HiHZ3eQkkhj6YVNLPfmgkhysMmMk7TJJN+G9IOkA8MlPfjLtkdHtdrG/vw/f\n9/HGG2/g61//Onq9Hubm5vC3f/u3OHfuHE6ePDnwhmZYvG7R+gsXLqS1R5OAMfZ/EUL+HwAvAthk\njF0V6x4oom7K3p02URcTsRz9s729jbW1NdRqtVyb6lEwbUXd1PBI3EisrKygUqng2WefHfs8w5h/\nuB2LwYscRbFolotvNMR+OvqRKGYCqg6/EA/yf+tKj3isy08q+69Q6QUC6igXMz8uoWTFcCNO2mNG\nUHPytXhh8ni4CP24xgl/otIPOndfa/rksnqaltOwOHH2Yfatx4Y05C7jk2iTFKseOkkHgB6tIyT8\npqLJ9gv3BVQLDcAtOvslnrE7F44v2m41TmPRv5n+Lie+ALwz7aDkne35M0oqTmw5SqOpg0D/Tt/n\nWAbwZ4yxTUKIzRhbJYT8IiEkZIyNnpd2n0PM5yKe8SBgjGFrawtra2toNBppndDPfz55gZsJopnW\nO++8g+XlZRw6dOjAT3Vl4aXf72NlZQW0yS1pFlRrok0Ygph/jmNGAEkEEU8uCZiSaCKsJHKii/Bx\nZ+o0yQW+6MTdltbL869MoC3kibhxO3l/sJSU62Rc2R9MVeVJtn0ZDE/M3S7cDxg8f6/sH8stC+K8\n9aUbWMpNUK2Un59iSW0Xqnqc3hBQZXkR3t2ZATDDZfdZwJkFmgCOlfn8ur+/n0ZInn3sqHGepJYN\ni8YgLM6RdIHQqYERgqBUR7ObCS1FJF2o6jpJ90pNDJdzJsPW9jaWjhxJe2QIrvPiiy/iL/7iL/D1\nr38d3W4X3/nOd/Daa6/hj/7oj/CRj3ykcLxh8brD1k8KQojFGOsDyOVVPlBEXYYg6qJj3DTHjaII\njDHcvn0b6+vrafvoSRW5YQVI40JW6IWCtLKygmq1OhZBL7Kj8HH5v0Jx2HNr6eQuT07DHgGKcbyo\nhCgpGKo5Ivc3Q5QURsnnJpPvtl+DLan0AjpJF68ppHaapxtSG0hIvyDsupJugiDxQqXXE2sEfFYe\nmJLQo3VQ1kTD5iRUtr0ErJwrfHXjapro0GUziMkcmuAeVUGuTaRfxFkKdMkc6piso9x+6TBqBU2i\nBCIruznxiPm7McibvlU9hSVvA0G5acxSb1mHsUjNyTTjQibqvV5vojiv9xCHwDnPJmNMfDhCYMBd\nzgcYw2IUB4FSms7T8/Pz+OhHP5rmsfu+PzWBRLY7lsvlgxQI52BZFrrdLn7+85/DdV2Ulz6qJLPY\nhKVzZT8qwyE0JeBy4aMfZ/GMYl4Wr96SiCsFSRNWLIVIZ+ckE11hh2Eiu4UwpbBU7lxKAUA6JwZV\nUdcJunxOtrYuLUQ1WGZMY+i/6+t0yxyRfn9q7kbhMYogz/kUNq7u84QqE3mXEY9gmylCWcQmN8pY\nbPBwBwqA2VIRsBS7CKuEUpxdtyy5w7aj9i3oNo+i2b0zsZK+feJjOLL5w5FeR//oGdTv5C0k0Sle\np7J1+NnCfeM4TgXaZrOJUqmEX/3VX8Wv//qvj3Ts9wuicNSEB5Kom7J3pwXbtnHz5k3cvXsXCwsL\nSvvog4w5TaIuFPXt7W2srq6iVqvhueeeG5l8/N2PHTg2n2CrpYTMGlTxIhLe9sppNJY8KcXUSqv2\n5cIbMY4g+m5UQhhbaJbNfz+dpMvoR+WUQFc10m72z6tfgV6YeNk1hT2iTpqUIP9fPXY2sc2UOLHU\nlXQTxPn2Yk5mZ2y+b2DYV0+5Ee9nlzbRtMYn3W3Gi2xmCW+QIpo46fBYLXfD0MIi5pGpCTIxL8JO\n5ZFUVb9XBaTA+MkvH7CUgB8B+M+EkI8B+DaAXQDHAPzkfT2rKUOOZxx3Lo/jGJubm7h+/ToOHz6M\nj3/846hU1BvVafTFEHZH27ZTu+O//Mu/HGhMGeJRfq/XQ/XYx8EbI0u55Ik67sclOBZN/edifgyp\njUDzkMeGvhexnOZCVIIsyLog74SohaQxS1RvsZxl44hzBDILTZEvXSf1meeepuekxEYmY+tpMyY7\nzSAUpdQwWApZHxfpOTMGCxRPz10zH4cQkBGf4pnOVSbeDAQWy3eaNW0bWyXExFGIuoBM0iOrnDbr\n6zaPohyq1km3PJvWTE2C9rlfwsyV/znx/jr0eqNxPerD4nWHrb8XeKCIup69O02iLib+O3fu4NCh\nQ8b20ZNimoo6YwytVgs7OzuwbXssgq6Ow99HL7QRUQKLsJS0AypJHzTH9IPsI6YrCYyZyb64Kej4\n5ZTMMxA0y36ObIfULjy+l1hrYkZSpX2QlUWGG1XgxQ5KVqyo9EFs/pvr4255CyhZcY7wy/nGRYR/\nN+S1DU1H9aH7tDzw/Lu0iVDKkVf2ZZWBjanabB5125yNbiLpLq3CAsuRdRN6mEkL3ABO1qvl0Xzy\nQlWfFNed0TyDcsOj+52oM8buEEK+DeD3weMZfQDbSAqPHhSIudyyrJHtSFEU4fr167h58yaOHTuG\nT37yk4VFwZP2xRBPKVdXV1Eul3H27Fljnc9BIDzuvu+j+ehLA7cNE6ufIn4wK51b046gUjMhGYTo\nc7iWIS41KrLA0m1le4xtyCzXCbOcGlOkSJusLRQEFkNOqde3U46tvQYTwdWJvnkcM1kvekJqel16\nUx6ZlIt18jam9WKZ/voF8RaJLQxESW+hxEIp9pUbpthSvw+ixqgWJhn8mpKuo1tdRNMrnvP3ykex\nELx/6bAHjdodFq9btP6gIIRUAVDG8okQDxRRlzENXyOQn/gfe+wxNJvNqXYZmzT3XIbwygsFvdFo\n4Pnnnx97nL/7cfaRiLRJXZB2mzBUS+YLXJEfPaIWemE2QVSdOEfS5X1NE2E34IpYTAlmK3nSbvL1\nBbEN26KpH75qFytoRSQ429cybqvba2S4UQWUWag7qitBRE+aEFMbthWjG9VSsq4r6TLh19GN+I3Z\nvMM96LrlpQjtmBeqztrt1ELjsfykLRJwBFrgikITeb98r8CZuMcOYYGM1wHVhB3nGBYjbn+56xyf\neBx5ct/f3584mvG9AmPs7wH8PSHkDP+Vrb7f5/R+IggCXLt2DXfu3MGJEyfw0ksvDW3+Mq53XK9H\nGmQjnDRxTERFhmGIM2fOYL2ViSzpnMMyQhhQRyGXEbVyIoqeG66r0rl7ICb84Opi3QYj5miL0NSr\nLqvlRT71wqJPnspr3EbebhBhl7c1jqGR8qLfZd86k6yImd1Gne9FaSZjfB2FnTxlyAtwIlWMwQIY\n0m2U9wEAZXa6XlfdY9hJkW3yNJrR3M1JnJxDaCcRnTQEpJQvvQZJEPZyQW+ODwrkrqTA+ER9WLxu\n0fqLFy/i8uXLY3eaJoQQxpWITwJogz8xVfDAEXVCSGp9CYLxU0EEwjDExsZGbuK/du3aVAs/AU7U\nPW+yL4dM0JvNJj7ykY+gXC4rUXCTQCbpVtrcIVvmhRbCmMCxOekG1M5xgtDz/2uEPCIIIwe2lans\ngzq/yRNQEPNinbafeMo1lV4m0IL4ywS+7VdS1WmuMqAAVrsJoMxKCbuisGte9oA6xtfSTwpXdcKu\n7Bvni1e7UQ0xtdEoDc4vl48hvPetaC4XRwnwToVlot4Yymp7O57FvN0ykvRB2IsXsGDnY/RMajyg\nknXdwy7Scd4rMMbSJID7XVGXwRgbPwvsA4JR4nY9z8PGxgZ2dnbw2GOPTa3Dp37cO3fuYG1tDTMz\nM0PrkcQ1aByi3ul0cPXqVURRlBL09Va2Xp8XQmpnPnJLI99MJdmUEiVrHCApKTQq4UJpTkkzJ5GK\nss5Ylp7CrNTSInvOddsLwIm8TPDTMyryrA/xnw8qAJWTaOTNLNCEgGufrwIrjdluk8RbStkpwuMu\naoz4e24rx7GSJ4ty5oq8jYUYjJGB600IUQYFr1soITDewAyK7822qSB0KmhEXOQRJF8gsPk1YZiq\nPgymDPW1l38Lyz/7fyceU2AaPTFGjdeV8corr0wUtyvhGQD/BijkHcADSNQFSqVSmjk6DuTWzqaJ\n33Ec+P50u3VPkvoiUgxWV1cxMzODj3zkI+nFgzE2kZXmv3+/knjTgWpZVmmyiUaQb7ldsxfZCCKu\nzDh2tl/FMUwWUZaBG1HADflHkDKk1ho3sHOKvV5hz8/Bgh+V4dgMjVKYW6dDbmIBAC2fTzpVJ0y7\noQqCPujGoR+V05uPeilI1XSdtJv3rSb+e/4ZEhfgIB68by+sIWYEzVJGvGVfaZElpxMmvveS+bsQ\n0BLKVv5pTiueR0AdzGo2Gl1N17EXL+AoXERWuVBNHxdb1VOo1Ee7kZULwrbLJ9KK5B8OaXokk6oP\nElH/sECPKOz3+1hbW0O73T5wh88iUs0Yw61bt7C+vj5WPZLwvo/y1LXdbmNlZYUXwC0+BwfAeksq\nQhQ1PQMsbyKxxY9tKducrzPNY0qRI1OVagERu6gTdgH1/ywdyyJIC11lj7pJUddJeXa87IYghqWS\nTsY987KqLsYqItYWoVpBq1C/JTU9iZ80EX+xTJD7XLpMAr3/hj4eA0EMR/Lex+nybAw7Hcc2kHKl\nvwaTl0sCFcqwkWXOy/GcIeGfyWGEvefMocwGz7nd6iLsMb3p2yc+hqMr3x55+2HNjkzQibrruh+U\n6N0SuKIOpnn9HjiiLrq4jetRl1s7nzp1qnDidxwHvZ7ZyzspxvGoy+rO7OwsXnjhhTTFQGAaTZ68\ngI8RxQT1qpSaIhF0J5kzgmj48WLKLSsmCKXHC22u0msqj0zSsxsF9W8jbDWUklRlH6S2yPCiErzQ\n5tnuZfWGSU6HybbnL9yxGPphGYwVV/LnIiVDniEvbDyzFXcoSZfRDTlRkAl7EUmX0QqSOEeJsMud\nW3WIm4521EjJuk7SZZIfSo2j7uARVGG+me1GDdjS+zmKBWYvmMWxavFFQ7a/TALGmOKBbrVaOHHi\nxMTjPcR0oMftBkEAz/OwuroK13Vx+vTpiRoIyTBllFNKcfPmTVy7dg2HDh0yFqKOMuYg7O/vY2Vl\nhT/JOXROCWBVuyYbmg9RktpZQmqDMqBixwlJzki2ab7VSXeR15oyrRmRtL2cGz4o/UQm3Pr/xb4y\nqVdeO0N+WXrMwXUF+vZiDmbg9pHi4tHBnyOZsMvj6k8MAeRuBsQ28jFiyEp5/jWJWF4L1NiEr+gc\nCRhCZNeEsmEuDkk5ewJAbFgs/xoCUh1I1rtsBnMwz92yT90rjRcDfVDoRF2uXbzPUQfMMWwPJFEH\nRi8m7fV6WFtbQ6fTGam18zSSAnSMMrHLBH1ubs5I0KeBNHZRVswT0h5GBA2JtA8i6IwBXpiR/VpZ\nrUzXId8AAJm1hjIHzWosbVc8YYURgW1lKr1c/Jo7v4JJWRS/VqR9TXYaHeKYMmEXKr2AH+eLOVs+\nv9OvO4HRI+/HJaP/vRtWEVIbMyU+CZsu6G5Uyt1kdMO6QtZN0J8MtKNGrrnUoA61vbCCHipYrKjN\nkYR3XseG/yiOVgeT9VveURyvZgVKLevwwO3Hgd4YbX9/H88999zUxn+Ig4Mxhp/+9KcghEwtnxxQ\n+2JQSnHjxg1cv34dS0tLEwcGDBJe9vf3cfXqVTiHzsFe4BFzsUZcgXw7ekC1FspPBy2CVE0HBhP0\nIqhqerIPSGp7lLeJdbIOed/E9iITc0k5F3vJs1VRnKLuM0+LIZP3SI+UtAznIY6v/1+f/+3EcqJv\nV9TESZ6rRYqO/oRAabiU2FdkUq8q6Vn9gDy2vFy/IZDTexgInKRzq07q+7SRNtiTCbw4jljnE84n\nsgaAJYQooWGoPRJ9PKaFzvl/lww8uHO3DsIYGCHY3trCkaUlAJyoy0++PgB9MQQqAIwq8ANH1AWG\nEfVOp4PV1dW0c9y5c+dGmvjvBVEfNLGLvPa1tTXMz89PJQ7ShL/+dgWVkkrQTfBD/uWuaNYUmeAL\n+4sYy7IAP+L7cTKdfXGaVaaq9EIpkm8UwmziKUvWGmo4ZkwBUfPZD+x0rEON4kd9uq3Gj6z04ikr\n7CaSrs8Bu/0qHIuiUVE/ezJJFxdhuTNePyojphYapdHqKvphCSWbohNWUrI+KrphHWFsYabM1RLZ\nn+5GFUXxBnh6jheVMV8ZHv/YCzPlccefTcl6EUkfhr1gdqTtTIWkq/tH8MjM8Ik/iqJc8dGkxaQX\nL17E/Pw8rly5gi9/+cuF233ta18buP4hONrtNq5evYp+v49Tp07h1KlTUx2/VCrB933cvn0bm5ub\nQ5NiRoHJythqtXCtVQFQRmnxWQBaFCHjvnG5S2hErZwtRVju8sWURCHow7iJyc4iICwswvZCSHZ8\nbjtRC0kFhO0FUFVmveFR/nhWOpYpalGMkVPK0zEZdC97UTCAfAwBBoKIOQopF2TVpM6brCmyym5S\nycUymVzzItw8KRdj64Q7ZraxvkggYk5K1iOmUjuZoOtwWd1IxgV6mMGclOwlk/QdHMEisg7SbecQ\nhuHOmV/EmndSWbZUzWqbVj/+mzjWLy69MWWoy6k5sqLu+/5EN9vD5nHT+lHn/ty5Z3cSJXxYiPow\nRV08cqSUYnl5GQsLC2MpM/dKUdfH1P2R58+fvycEXYYXAJURr09eIJHnkl7Fbyb8wp8uE+vdrq2o\n9LqyLiMICUSaWlU7ZhjpE6o63r6bfVlrJV5EJQi6KbJMoB846cWiVooGetdlMt7zS/AjC7O17DM4\n6DjpfiE/z0YpSAlzxFSfph+pX9tOWEEYW5itmAm7npucxl8G1ZSsA1qn1wQi4hIAWj5/hDkKYRfY\n8WeVSVi/CWgH/InCZv8wTtTVbqcmku47daU7KQDc7i/gaL2V21aHKLhuNBqo1WqptU2OZgQm96hf\nuXIFAHAL5sIiAAAgAElEQVThwgWsrq7iypUrxvb0ly5dwj/8wz88JOpDIB5ZP/XUU9je3h7LfjIK\noihCv9/Hj3/8Yzz22GMjJcWMAvkJ6d7eHm7sc4JuPIektbwgbLLlJE4U5VB7Eqc0E0qVXiBmwwm6\nQNElTxB0sV4l2WafuU2k5kPScj0/vUhFT1/XgFQX3U8ujgHw+EgddkH+uRAlTJ1OTQp3zCx+I2B4\nqimsKUW57TLR5jGTsgqfJL4kSnmOlMNGzCw40g1PwPjFWdgMSySzeIrXFTEbMbNzdUduXFWCBSLm\nKDcSPczkuljL2LcW+esdQ5w2xTRu+I8O3Gf1478JALhZfxI4/SQe6Y/eOfgHP/gBKpUKPM9DqVRC\nu90GY2zsuXzYPG5aLzBs7h+CEgqa1j2wRF3P3hWNKSzLwpkzZzA3NzfR+Pfa+iIT9EOHDr0nBP2v\n/qkC8eTfT77fMmGnlCBOvtNRnHnTo2QOodIkGsVAXbueyiTadCERpD+MgGYtmzzoAFLcdq1UOZdt\nNcpxE5IupSoiiAj8kDd00vdTcoiTmwn5AuGGDmIK1Mr5px9+bBuJuLDS6N53HXo3un2Pv4k5ZV4i\n6WFsoWTz8y3ZNE3CWawVW1uER16gE/DPVtnOvyaZpMto+U3FIy8uHEJND2IHZSkGs0hNFyRdTwVy\nWd1I0nX7C8BJ+qiYmZlBr9fD1tYWXJen6NTr9TQetdfroVqtjt0gQ+Cb3/wmPve5zwEAlpeXcenS\npUkm64eQMDvLPwetVmtqfTHkRK9SqYSzZ8/i6NGjUxkb4PP5rW4Vt7o9yASdMTXUL04K1ykshXAL\nkYFSAgq1GBDICHrMiLI9IHvJzedmKgIVVpeMbJsJulgutrMG6A7EQOpl20sR8ZYLP5OBcmOa9tUR\nG7R7vdMpH0y1u8gEX2xDQQBxIwW1Q6o4l0H9LeRzcUishAAAXEiJoZLytDFgQsAdzTJDwNCPaygZ\nggAYI2r9UGJT9GgFVcss5oju2jNOXoQRtUwV2/y0dwdHsGhtFzY9GkbOB2G1wi2IIbWB5z+K59v/\nhOvecTxWvZXb9sUXX4Tv+/jpT3+KOI7xrW99C6+//jp2dnbw+c9/HufOncPnP/95fPSj+cQZGcPm\ncdP6nZ2dacz9MWPM+Ad64Ii6DDm6sFqtTqUxxb1U1Dc3N7GxsTFRAZMJ40SEpdaV5KXJfUCiWCW7\ndjUj6TLEMuFpr5ZZYQEpYPa4C9JeLVO4fqJkV7LzkdV4ATew0vOvlXmmr67Mm47lBhZiSlCXCLuw\n6AyCm1hqauUYNmFG77mOfuAgpgSNsvAR8vPxw2Qits0XnJ7P75ialSAl6SZVXybg270GmhU+qfJm\nKNk2JniRAy9yMFvxU8VbJunyDQHAiXXLr2O+kt0QyJYXHXfdBg7X1Cd6gqTL2OwfxnL9Ro6kF9UT\nyCR9rXUYp+fvGrcTOHLkCI4cOZL+TimF67q4desWer0evv3tb+P3fu/30Ov18JWvfAUvvPACzp8/\nj8985jMDxxVotVo4dCh79Luzk48vu3LlCi5cuICvfvWrI435YYeIOnQcZ+IIW4EgCLC+vo7t7e00\n0Wt9fX2qHta3NrpA+XhKCvVaDguZUq4UWoJ3GNUJX6piE5a7oR1ElC3JXqh7vS0D+ZUVdEtbJpYX\nHbeomZBpnWl2lc+Pak8Pi6wvYhv9/dKVfdM5DYJMqnWlXMzZMtHO2Wikudkhce6GwU8K/+VGd2Ib\nQcr19yxmNoK4jJKdJ8Ih5dcHE2EPknXyeB6tpKSfwsoVsXaiJuacNkJWGhg2IJ9vEW67i7iNxbG6\nxI6K6172HTtR4+LNTzcJnn+0CsuycOrUKXz5y1/GL/3SL+Fb3/oWfvd3fxdvvfXWSHxo2DxuWj/K\n3D8IhBALwP8qWv/AEXUxsd++fRv9fh+3b9+euDunCeN0yRsFlNKUKHS73akQdMCcZmDCa5fKABgc\nO/8BNpFxfbllqb8zxh/B2hYn7ELtqZTGs9a0+5li7voElVL+PRd+dPnPsd/nnUplZT4Youj3U0Wf\nKIWr8qTrR/k0mr1eCbbFUK/klflYu7EQNyy9RGEv2+bJS9wo2NqFZrdfTUn+KOj6/IIgCLsg6XqB\nmhs66QW57VewUOsXKuk6Wn5dUc4HQSbrJpIusNp/dGBTKoFxlPQiWJaFRqOBRqOBcrmMj3/84/iV\nX/kVfPrTn8bv/M7v4K233sLbb789MlEfBaL19EOMBrkvRqdT7KMdBN/3sba2ht3dXZw8eVKJ3J1G\nY7y3r3Vy6SnCbkYIU26sBW2L9WZtBs93lhySFEtakrIudapMiXNRAgthSlGonrNu6xGJBQRdn5NM\nqrlpnULEUUzWi8bS1wPSjUABIde31yMb5eUiB95UCJpakgzHkJF7EgDepI6A5ex+QHYDJ68Tn4mI\nWUoIgFgexgkpt8OUoAuEtISSFRqL/APqoCzdGHTDGpoD+nLsR+a6oB1vFovV4TU/t93F9P9GS9CQ\n7uCjJtwI3PG5+PJ8ItzHcZyGAwgb45kzZ3DmzPhRj+8VGGMUHyaiDgDvvPMOLMtCs9nEM888MxXf\n4bQhIsA2NjZw+PBh1Ot1nD17dmrj27Y9cnvsMALiOJvISiUCWkDSGQP85AlYpVxM5gGp2DPiP7bN\nrTWVErDfJekFoVHj5zAIXkDSbHeTOq4vk+009ergGythzbEtBjfZr6ao7Pnjycp737dAmarMD8O+\ny+03DclGU0TSe4GNks1Skj8ozUa333T9MrzIxkwlT0YEeZcV8z23Dj+yMF8brai17VcKvfE67roN\nY4ylrtgPwi3vKLxout9nPc7LsiycP39+7EeX8/PzKRFvtVpYXFxU1gs1/SFGh0zUx32S6bou1tbW\n0Gq18Pjjj+Ps2bM5Re0gfTHevqbeOPAGNxZsEmuFiZzo8aZB4nWpdhZbI/RMsrwQwhsMAYD4eouC\nzqKoQXEM8/KsKLRoWzXJBYXrTMcwKuwaWU/HNpyfIM7Gc9fG1pNa9Mx2eZ3pZiCNbmREiVRUvOs6\n6TeML8i0IKZyEox+UyZAmQUaW8p+Ir1L7GMi+WFcMp6DG1VyKV+iL4gg62I/maz3orpyDiaIWOAi\n7OAIdvwZOOHweXx1q4blJfVG4Qc3HsEnH705dF8TIprvkSG+55PYGIfN40XrB+1zUNx/DPaAIITg\nmWeegW3buHLlSq5Q7P0GpRSbm5u4du0ajhw5ghdffBHlcnnsRyXDYFnW0MjH1y6VEUaJOCPNRX6Q\nTQKVcvK4T0pTiRJST30ibQe4XrZfo0YgX1eFB16Qdl40xZf1XKBcEmNnBF9XzDt9ko4lSDtjwxVz\n2YojEMX8AqgXoQJ8vCCy09db1Qi4H1kDlXkAqDjmCSuI1HSGXmBzS4yhS6oXmSf4jsc/zzPV4cSl\n69twbKDjlxSyXmSDEWi55aFkXZyfTNb1TPh9r6KQ8O1uFUeagy0Mtzp1HJ8x++y3u9WRXrcJRU2P\n9DivSfHqq6/i8uXLAHjhqiDlIkVmdXUVq6ur2N3dxe7u7qQFRx9KjNMXo9/vY3V1FZ1OB8vLy3jm\nmWcKH3mP2xfjnWtcUdSztAXkwlAlFlAjiVSzscjKuOo5z2wojBHYMlElLDcPDXuyXxSFyM99yL4j\nKujDjgsgvXkxxTUCquVFWW5Sy4loRZQn37KnXgZlWaKOSUkvypfXj68r6ZRZSTKMKrjIn5MilVle\nKp48+ImKrkf0CgJeRMxLVpx77wLqKOfaDbm1RReG9oMaLDAlaEBgz29iQQsT2HG5nVh/WntQeHHe\nWfCT2V9Cz83soMMwSTDAsHm8aL1p2bQwWSu3+xzjZqlPMv6oarUApRTXr1/Hd7/7XXiehxdffBFP\nPfWUEh00TUvNKNnsJhU7itRz8AOW/vA0l/w5RhFDr88gH67TM21nPg9ZpR92roLwewFBp0cGZrnr\nr8/1CbyAKLnwOsR4/LWKY1noeRb8yBrJwx7FQM+30POFOkKUsU3o+TZ6fkbMBQkWFhpTGk7Hc7Dd\nqQzwnqtEv+OXcLdXvL2OlltOE2LEv8InK4pkBUQh67jQz2WvX2y72e6OTqZX948M3yiBHuc1qfVM\nkO5Lly5hfn4+/f2zn/0sALXFdKs1PKXmITKMMpd3u13867/+K37yk5/g6NGjePnll3H06NGp9MV4\n51ob71xrZzncSZxikT9b3o4CqSddqOQWocoymWhahJMn+YckySPyDyEMtkVhWxSWxWBZLLVwFP2I\n49iEwSJQfgQIWO5HP18Zw0i6eB9k6FdPalgm9ouTH/Gem7ZJCbJE0sW/4kmH+NHJtljOm0dl20Xa\njwxxTkHs5PbTt9Fv5iizEFAnXS9+Iuoo1hX5mEWdaQUxl+sXAuqgF1XSdelyQ2O9otoiOTpYx57f\nRMRs7LgzKUkvwrXd6XUE3XNr2HP5zYUesiCgc6j9/X3FOz4Khs3jpvVF+0wL94/UPGWIAqR7QdTH\naRGtN9Eoyui1LCvXeOUgEOMV4f/+/0oAGChlKJctRTGnMYOledajiKGnkXT5ZZjuMfyAoVIm6LnZ\nY9ZahSiKkYyem6lH1UpCbg1/PvlP2nOzFJowYrCTK05zwPwQhIAfkESZ58sGdU4VMCnzMSWKihWE\nRCnk6vkW7+5aMf8t9GPudBzYFpTISh16QW3XsxVvPcBJum5LEqS/F9ip5UYvUOv63GYjsO+VMFdV\n/wg6SRfYbNVxZGa4yjGJqi6T9Bu7FTx6KLMriPNZax3GmfmtwjF/uFrHx07mUw1kon6QDHUA+OIX\nv5hb9uabb+a2MW33EHmM4iXvdDpYWVlBEAQ4c+bMWM2QhhF1oaALCOKX/i4Rcvl35TUk/4pvdGpp\nUdRx8Y/ZciFPx9kxmLQP09ap0Bv36ORat3oM6jiqjzkJio4xTP7KecVN42hk3tLSduT1+t8m/TeJ\nzZQhE1/duiRsSzZhCqk2Kek6cbdJrEToCrKuHz9KbiZ0mJYBQDeoKjVE/Lh58a4dVDFrUM9FfK/J\n9jKMoE8TIpmsCP/0rxV87CS30ug2xknn82HzuGn9QeZ0QsgcgBkA26bklweOqMvtYifxNY6CUYh6\nHMfY3NzE9evXcfTo0aFNNIQCPi2iPoqiHkUMlgUEgegmB5RLeYIu1okiUYA/npUtMvJ+WUILELkM\njDKQ5Erj+tnj2mqFP+6VVXpxfe328qTdtlWSXkT4AaDb53YccT6EmEm/l/DKOAaqBUJqqmqniTj8\nfGoV7dFimL9IegGBY3MfOwBjUayArPD3vCz9pggyye96NgAbZWe0i2cvsFEZcdt9j39uDzf8QpLO\njw9sd8oHIuthRFCSzouBjKWk67jZmR3a9GiaRP0h7g0EYZeh98QYVzkDiol6jqAPIMB6caJO3tPX\nAJWsp2MXec2L7jVMm4rxDLYZ/XyHYRBJPyg5H/S7fg65At2C9YKs63YZ8XvRfgAQGZJkTLYm/e8s\n1H2l+JdZ6TXJeBNSoIoH1MnZT7iAkvfqC2FFtpmIxlglKWJXZO/rUbl+XELFkB7TDqqYr/SxH6hJ\nL52gOtROJXC7XcWx2YMlM+noGXp76Pgvv5b53XWiPmnU7vuAEwBeAmARQq4B+GcAzwG4wRi788AR\ndRn3yvoySIWJ4zhV0I8fPz5yl7txVPpRUETUKaX4r39byVlcBIKQLx/G4eKYwU7IdxSpXknHKf5m\nR1G2n+ez1JuubpOcayIUeT4fXL+JkBFqr4cywPV5tCNgJulxzMm/WOcl97EVw5/AZBPa7/J8+RlJ\nvZfV9KzBU6b6Z5GT6vmabDgA0HX5xCxSbNzAPNkDQN8jCGyiJN7ox1fOv88XztUH39AJXNupYb7B\nt5VfpyDp3EfLyfqJ+eE3yMMsOFvdfDzYXteGgbMdCGEYpt9RPWbrId5fmJTxvb09rK6ughCC5eXl\nA91Y6XN5EUGXSaBOuIoIp6kgUSbr8namMQpJMRmwjeFUihJPTPsXKe7vNUxFokXrc9trTz1kAj7o\n5kBeL//tBvnLdatLNlj2hEUm4TrRFvsoirz0lLPIqy/3/BAIY1sh6wJBrPbOEGRdt720/PpIT1IA\nwA1t1AaEGgDAjb28wHL1zmjpezu+mjzTDUq5mxkTTIr6B4GoM8beJoTEAP4jgFcBfAZAH8BfAQ+g\nog6oSQGTVvQPgomox3GM69ev48aNGzh+/DhefvnlsYpYR1HAx4FufZGLWPv9T6NcFl05RcQi/xII\n60gYMpQSYux5fJxKRSSpFH9hKFUrU9kA2ZtRBt8HKoliPuzl9z2WKvpBmBH+Zr148m13GS9ctTJl\nflgpQLfPYFkE9SQvXraQZMfPlrnSR4wn2JjPR3YiuT6B6xPMSvNWPOC5b9e1FMuNDmHLEdsCqhqv\njy22cWxO2A/PUHR9lc1HkmLvJk8EWj07JetARtIB/rqFJWezVc/ZcfZ6JUUt326XcWS2WH2PGRlp\ncj4o9Divh4r6/QfGGKIowhtvvIFSqYQnn3wybYZ0EIi5fJiCLgj6OKpwEWSab1LYh2LApkOLOg+4\n/r2EyWIkoKvfyn4FUY6DoN8syZ1iZf++TKLFcr2vRT4HneQIuPg/0Uh8BN78zsoRc5JLxuoHJZQd\nrZg0to0pWv3AUcQVL7JzccNFaHtlzFazeVo045NxtzsdgXE3UBtRuqGDinbzUeRPB1TRBbj/5/Mk\nP/0zAH7OGPsZIeQPGWMdQsgMgGcB3AU+BES92x293fmokIm6TNAfeeSRsQm6wLSJuhhPjoFcWlrC\nP934NGLK4PvZsWo1rUV18v11Xb6NULV8X1hkWEr0Tcq8sNJEMYOVKmIZ8ZfVeADo9mhyziRVzV2P\npjcGfKzi19rts1QFjylv8hSG+fMSyrxIsjFBvgnpe1lxitjHpMwLUAp0eomPvTKYeAv0kqd2DUk8\nlusF5P933eTGpJblsguCrpcjRDEn47q67hUo8nc7pUKbjSDpAq2ejUMzkULSTdjpOFicGaysy2R9\nqz16EafuUz8I5MZgHxQF5sOEnZ0drK6uIooiPPHEE1P9+/zsRhfzJ86lvw8idSZl2qSG6zYJsV26\nr8EjLWPYtDEWqcdkNxP3EqMQ51HG0JN0Rn2d+vH5k1vpb6INo2tNYv+YcRukpRHemFpJB9NsuZ/Y\nU3TLCsCvt2IMSkmS7U5Ssi4EE5mAB0lQgB/aqJTi9PXLxf/ZtlY6tjiOF9poVtS52U2a78ldtDv+\neAT81n4Vx+dU+8v63QYeP1ycrLR+t4YnlnopSY+oavkZpqb/7Gc/Q6PRQLPZhO/7Cv/qdDoHvqG/\nePEi5ufnceXKFXz5y18eef2IqV4LAC4A6AC4BuAIIWSWMbYJ4Ptiowc29UUQ9XtlfQmCAGtra/ju\nd78LxhhefvllLC8vTxwFOe2Op4QQ7Ozs4Hvf+x76/T5efPFFvH7lLGKDwu26McKAIhQEO6KIIjmy\nSrrrT5YHAYXrxil5B4Sazgl6FDMw7YrjBzT9YZQhjFhKjAVx77sUfZfv6PsUvp/9LhBoJJxRBi+J\nhux0qZGkC9LMEiuN5zN0exT7bQrX5T+D0OnxZBvTmKabiHaXpTcGJuiEf3efodsffAEWH4+uS9Dq\nEEVFL0LXtVL1vIikC3T6Fjp9K02YcX0rR9LT8+2M9jnf6TjpxWMS3N4dr2ZjpbU0VuILoNorHhL1\n+wsiYeu5557DwsLCVJrBAcDPru3jZ9f2R9q2KG1kEA6iTFuGn4NASZfRft5LyGk4Y+0nJb7IP/o2\nRceTf3gKC9KfmOXPSU5iEfspqTGiGRLN/pX/D/CcenkcgP8dImqlPzLkMdJljIBqryuMrdx86od2\n2uFa31aHfIyufzCd1jUccxI8udTNeq5IJN2kpptw7NgxEEKwtbWFjY0NbG5u4vXXX8dv//Zvg1KK\nH/7wh3Dd4gZPg3DlyhUAwIULF1IyPsr6S5cu4Qtf+MIoh1gG8GeMsR8QQmzG2CqA04SQJXmjB1JR\nF7gXqS9RFKHVaqVNNCZV0HVMS1EXXVk3NjbQaDTwiU98IvW9yyRdf6QHJMkuPfVmQSjnfL3mz0t+\nlZX2Utl8WaES2acxU8i3UNrlwlWuMvD1YcgUG0+pwKve6VJYFknTZgBO/KvV/DmJY8mKdbtLUavK\nXke+jUyqBfkWNhod+k2EvL0lFbQWqe3dPkutPP6Qj67w1BcVwcq427LSJJxBTycATthL2kc6jFSf\nexDxuMlm7eAX++12GY8s5IuQtveGU5R+4OD6tnrBEDdqZ05kL/SHq+YYID3Oq91u39cd7D6MOHLk\nCAghUxFeBDnX4+eKMs9N65XlgwoiDco6MFrnTR1F34SDJldPci7jYhRyPukxTWPrWpRFzMEDg9J7\n8ueXBR3w65NGqqESbUtkusvLhGquqPfJdUhrfkXAFKIt9g2kyN2STdO6IIATb/2a7oY2qpo9xpMI\ndtd3csp6P3AUVR3I218Oin/brOH5xzK3g0XyCWQAV9NlmGwvc3NzmJvjavy1a9dQKpVw9uxZlMtl\n/OAHP8Cf//mf4+2338Yv/MIv4Otf//pY5/nNb34Tn/vc5wAAy8vLuHTpkqKSF62/cOEClpeXRznE\nIfCv9yZjTPyhQgDKBfGBJurTVNSjKMK1a9dw8+ZNzM7O4rHHHsPp06enMjZwcKLOGMPW1hZWV1cx\nPz+P06dPg1KakvT/87/xKZ1I34VKJclglewe8oQUx0xRzIuKRGPKk11sm6SqPGVAuWQpNhcqGiXR\nLA8dyHvgB4EypqTN1CqW0TPf6yevl2Tj12uW8loHWVMoZYXdUqOIIXHrpIRdV8/F60sLVT0Kyyao\nV4dfkLpJJn2jwHuvf0w6PYZGLbnZ0db5AX/C4dgE3b445/yY8mt1ff4zq9X9mIpSuy4n67ov35cS\ncHY6Tpp2o6e6CJhsL+K8i3Bjt5Jah0xY2UxO9kTxo88oipSkpfvd0/hhg57iNcl8rivngzKiC89j\nAOkeRtaB4WR1HOuGDH3GnG7LmYNhmgR9lLGKyqEGpYMNOg+9lomx4s8OV/mz32PDrVVMSRLnKwtm\nJEfmdc87YLbYKPaW2OypB7gfXX7CE8ZESQfr+g7afRtlKZGs59sIQj0GuIqZWp6jjOtPP3ssI+ji\nqcVuj8//s7UQDmGFQQM/v13Hk8fMzfAA7lGv1+s4fPgwfu3Xfg1/+qd/ir/8y78c6/xk6OECemPK\nYetHwI8A/GdCyMcAfBvALoBjAH4ib/RAEnUR5TWNeMYoirCxsYFbt27h0Ucfxac+9Sns7u5ib29v\nGqeaYlLrC2MM29vbWFlZwdzcHD72sY+hWq3i9u3buY57MkmPY4Z+PzteuTz4MRalDIFEkJ1SMakW\nE2OQtBOmHsuRcJnAx5Tnn/s+t9zYNklvIkw2FoEoZOglrLVctmBZRCHilDHYkq2hKOlGhusxRBFD\nTBka9ew9sYh5f89nCAKKek19fUVe9r7Hb35mZ7KGRqm6n6TQCCIurDazTZKq50UQGfSCsANqEymZ\n9MqqvQ65MLbdy5N1E7b3gPkBsbo9F+i5BIdms/dPttTsd/nyxbncrgC4/eXYoRh73cketa5s2oUR\nY1EU5YqPHlpf7k+MO0cOI+jT8EoDw0n2qMcZ1LZ+VJhSZcbFpOcx7vs5ytijjjlMMR/32KbAAWp4\n6qJ0eiXiXOSi1PzxlAQhYZkRY9NMXdcz4uOY5Ih4GFsIpGZ4jsXSbSJDXxBhawwilazz+GKikHUT\n9rpagzo4uX1u7VeNIphNGJ4+3gFjBF7Mu6RGlMANsnl9r1/G04tbuBEeSp8ouIGFpqFzN6BmqAPq\nfP5BEF0YY3cIId8G8Pvg8Yw+gG0A/0Pe7oEk6gKWZeUebY+KMAxx7do13L59OyXoQnmbtp8c4Ir6\nOAk1jDHs7OxgZWUFjUYDL7zwAmq1rCJRVuiFmg5wgiyPIdSqIMi+CJWKrWwng8YMlDHlfXUcKx1b\nLhKl0hhCmS+VdEJrvqyIYlehzPP/Jx74UFhkGETCTBDQ9JwFyQeym4AwpAhDpI2cGFUVfMryRFwo\n8dWqNbBoljGu4IvXHoRUIfnU8F4Kxb9RH/4UYXef76+r8eLJghxX2HNVFVrvJCt+F374+RnhRzcf\nu53c69UK7DVuUhvQ6rB0LAFxDPEZ222rZB3ISPqwr+mkJH0YPqhxXh9GjKqov3tddH1VP48W2ERq\nOjCZ9UWgSI0vPNYBrSd6E59JcC8LUA9C0EdSx0dIgpFJsjL+CPvyZcXr5L+3RcyN9GJGgESFtwlL\nP5uyR1vPhhfjmFRzAUHOo1wna4KKo9V6JWS94+bnVtETRG+uNy5OHurnYiS92IEf2rAsppB0+UnC\nxlYZjywO51hyhjqgzuejZqi/9tpruWXLy8up73x3dxcAvzYsLi4q2w1bPwoYY38P4O8JIWf4r2xV\n3+aBJOqjdqUzIQxDbGxs4M6dO3jsscfwqU99Ktds414R9VGtLzs7O7h69SpqtRqef/551Ot5/61l\nWYjjGF95jV/Y5JjEUtkuvIGJY4pej2adRGslUMqMZFNA2F0AgFRtZVtRYGqlsY/ZtjJxBgweeFmZ\nDznh1Yl+OlbE0snL9+N030F2Gt/nyTLtTgRCSLqtXnDb6USwbIKaEk85ePLyPLM3Xke7w//mQpGX\nPwJhyECkGbnvMXgeRbMhexfV8foug508Ih3lJqDVYenfRiCKmGJzCkOGMOTKPsD96UBG0uWxDicC\nhk7SdQiCLuNuC+n+t+7qNyXmfHsdN2/5OHK4eMO//Mcm/sMvZ49eP8ANMj40kMMBiorCMnJeDEHC\nDkK6D7L9qNAb9oyL95qgj3oTMg0FXZ6qitJYhp/HiOcygh1GwGRXyRoyFR+DEFVBl//lxadiXHX8\nIIKSDDNKb4muZ6NSGvzpGKSqt5NEs/oI/edMBN2NuNItCl9lki7w7OJt0ORWc+OOjVNH41RNv7k3\nvImmvmEAACAASURBVB+N3rxulLl8UEfRV199FZcvXwYArK6u4sKFC+nY8/PzhesnAWNspWjdA0nU\ndcjKcRHCMMT6+jq2trYKCbrAvSDqjuMMJep7e3u4evUqyuUyzp07h2azWbitbdtpAWacPIcSUYme\nG6YEsFJxlG10uG6oNTNS35MoorCl98lzh9tpImGJYZkCHxrGlrdnjMG2rZToCyKpeOANSoPvU9T1\n+EmR1pKk21DGW3QL1d8RMZKJGp++F8l6GjOlyFaG/ITA8yhiylKCT2NWmEHfd3lazdzscMtP381b\nbXxDwkyvT9OMehOEzcdxyEikvt1lODRHcgRdxt0WCm0mAFfVB1247raA44eHngoAYHWDe3uOLk2e\n4asT9W63O/B79RDvPWSi3m6reec/v84tiATm7HM1CpGrluMq3Hz8wZGLwGSFpSYc5AZgUpJ+Lwn6\nNMcaRVEfBnl+kuciXU03Rm8SSU0f2dKU+dvl46W1YAbFHQAozXznAryTt/h/FuNIaaaiy9uLmiJR\nQ9T3LdV37uaDA3QMUtU7fQAgePpRDyUr5oWw2veiF5RTf70g6X6kWWEZwbOLt3PjD7K9GM91yl2m\nz58/j8uXL+PSpUuYn59PC0k/+9nP4s033yxcf/HiRVy+fBkXL17EK6+8cqBzAD4ERN227ZwPVUYQ\nBNjY2MDW1hZOnjw5kKALTMP7XnSeJrRaLVy9ehW2bePpp5/GzMwAQ7A03n/75ycwbOr2/eyYjmPl\nSBSNVVVXqN5RBMljzsl6GMWw7ey9c93sMXWtVkoJOqCmwACcPAtlPoSqtJsQSCq+7peXJyoaq0k2\n1ZqdU+51yAW0peSGgMYMllQsK45fpPDLEAS/Jqn78g2GTMrb3Ti3rTienJXedyk8j2KmKdt8sn9t\nC0r8ZbNhKTYYQdIFhBVHpOXoqrrA9VsxZmfMr7nvCq89K9ym26MIQoqFuXtjZQGA7buBoqqvrHbw\nxBnzd0ZvkMEYG/r9f4j3FqPG7cpk2EQ8ZSJmKvKcVMmeptd6GraXccj6QY43yQ3PvRyraIxxlPHc\nmBJZ1/fRLS7ydae4W202ThrlqO2rRzLKMN2oUEYQxZlIJXvbecM+AumynBJvYW8JIyhkfa9NQEix\nct7pA08/6sGxKBypU6tO0Le69eR8+O9uYGG+HuVIugniurV8aB9bvdGFE0qp0rxuGk9HTYr7m2++\nOXD9K6+8MhWCLvBAEnVZPReTu07UgyDA+vo6tre3cerUqZEIusAgUj0pTNaX/f19XL16FYQQPPXU\nU2MF9//+X9gAYuVpQqXqFCrnNKIIIqqQcpl064hjqtg0alUrXW7bVkqGxZ9CJu2DxhZkn0okMr0h\nSMam0mwVRVSx2lRrdqFNJ4ooOvs0LaqtVOzcxBeGVLlJEKS9yEIzaL2w0KT59Akhr6bvFVNIOmMM\nLOY3Ka5PUatYyg2JCSLmUlfYdXR7VKkfKEK7G2NWIv/i/FwvO492h+aIuCDpRduIplYm6BdA3fYC\nALst/mE7vjQZwV9b7+H047wydn9/H41GI30yJpKRDophjTGEF3JlZQVf/epXp3LMDxN0oh7HMUrx\nPjY3N7F08lllW1NBpCXZ1UypHPK+egv7QRglseVep78IjKuoH8Rm81767gVGiVocdMwigj6KN10m\n2emyAWRdPqZyk0iQ88dzL3t+TH0dY/l4xyASghlJbY8sIe8iYUsn41FM0sCDcikfv8sYb/gncPYR\nDycPSaRc+i6ZIj5NJB0AWn0HtaSxnhdaqCZWnJ19C/97/xF8evlmbiwAWKi5uLlXwm6L8kyUEfAg\n2RgfSKIuIFQYmVSLRkV3794dm6ALHMQDXwSZqHc6HVy9ehWUUjzxxBNpRuhB4XvqzUWpYoNK6rIg\n6aIok9FY2lYl+YyqSrvrZRfQQQQ/iigsQkCT1xqC22/CgnDvKNJuCGoW9GZMMWOp/abXy86jUnFA\nSN77LuD7cWqDKVdtxeYCAGGQPSHwfZq+flGsKhRuXqjKff3VmkhzYYp3X4bnccuNXPRqQq+fJdoM\nQ9+lA/34QUDTz+0w77xO1mWSnm6TEHGdoPs+TZX4dodibtY2kvS9/Tinqrfa2R96fvbeKe63b99G\nt9tFHMeI4xjNZhObm5u5qMZxIDe+WF1dzXWlu3TpUpqt+4UvfCH9/SGGQ3xuxY1VFEW4fv06bt68\niUceeQQvvfRSrpfFu9dbxqzrdExDl1H1mFIh3xjt6otwrxX199LyArx3TwhkDLK9FHWN1c9Fv3QL\nIpnmng95wiJnqZsgyHouDUb/A1nqGDy2MX9egBgPaXQyk0m7dr5xWkzKlwUhyfqRJJd/y+I1P2JM\nQdiDEHjyES/1lhNkdV+mBl4mgr6x20yJuE7SAV7AGkR2GtcL5P3931k9BoBhZzcCzgBLjS5COv68\n3Gq1cPbs2bH3ux/xQBJ1U/au7/tYX1/Hzs4OTp06hSeffPK+esTtOA48z8OPfvQjhGF4oFbZ/+lr\nbRCLFBLmOOaNgQKJuJfKatKL+IKKZXE/I8ClhGDqZB3gyrxHpW3LIqs9fymJY96hVLbBVCqOSsQp\nA5GU4F43yxwsGTzwlGbFkb1ukKrj5bKd65Qq/x54MWJKUak4qc1FnKP+Pvp+jHrdUQpjBTw3Tm4u\nGOr17OslPo/itfGM+jh5zXZ6gyH2kZ8KdLsR4phhZqaUvEbklHbfj9OxGw1H8cLLJB0YrdC13Y2N\nGeayZafdoYXZ+um5D1DSBVmXCTqQPXkYFXe2grF86mLyZozhnXfeQaPRwOXLl/GNb3wDa2tr+MQn\nPoGzZ8/it37rt9JmFsMwrDHG6uoqVldX8cUvfhHLy8tYXc0V9j9EAcRnlxCCbreL733vezhx4oSR\noAs89ZjqTR21CykwHrkcpkiP7YUfU1G/Xwn6JBhmfxlUSCowaP9hnnNAfeqSHss4ZGaH0UmsTQzn\noWWgE8bjCATRdmymnIeNjHRz8k/Sv7VQzcV6m0DpYyGIffq79iF5bNGDBaZsQwiDLd9caK9pWBfb\n251G2jvDDbgP3tFes1D+Oz2GyjwUNX1UPLKkvq+7u7toNpsol8ugVL3O7e/v3/fxjKPigSTqMggh\nWF9fh+/7ePzxx+87gg4AvV4PV69eRa/Xw9mzZ5UA/XHxn77Gi61oREElK4s1gLQTQhS1vayRdr0Y\nN/RVYiVbKhQ/e0gRhaqdho9dfHnxEouMU1LjDS2bpGq8gPDXi6LVIlUe4PGTjAHlUp7chxFfZ9sk\nHVMvbJXPOYooWi0/vSEoUu5lIm6+UWHJMbPz7vcjNBNCrpNx4bVvNJzcGPp2w2wuu7sBLIug2XRS\nP7rrxortx3UTv3ytuMC1040w0yyeRnq9SDnffj9Oi3UBTtZNuLsT4fBiftxbWzFOHLPTQlIZN2+N\nHm8KZN7nxcVFfP7zn8cLL7yAP/7jP8Y3vvENvPvuu6hWR4g3SDCs8YXsY7xy5QpeffXVsc71w471\n9XXcunULAJSo3FFx9mT2VPKda+0BW45PlqfR8GjYODrej+jFSUj6Qa08RSiymYzypCRdn6arqNvo\navkgtV1PY9EHEWOLrVhSzAwibkpYLgRBvK6SYkVJSLklvOcZwSca6bYJzZFwfg55IU7/fI5L0AFg\n9W5T8cD7IUlJu0WgFKJ2evz/m3cYzjwKbLVM1+PRhJpjpU3s7HSxsbGBMAzTiOvNzU2sra1hd3f3\nwNaXYXZG0/p7YXF8YIm67/tYW1vDnTt3sLi4iPPnz0/VsmJZFiilByL9/X4fKysr6Pf7OHPmDLrd\n7lRIOjPMYFSzrTBDworYRk4QEWqyTtZl4ipbUwbZXmhM4Xvy2OrfQz5v34vSyUQh7YwpZB3gJDwK\nQ1gEKFf5RzrS1G4x+QYhP9lhHwWZPNeTRJRiC01kzHAXEI2lxDo9o14cS7wf+61AzYLXtm+3OUmt\nJq9V7C9IexRlhaK6HcbXbrK63QhNiWjLTxMEXDdOybqMXo/HVo5L1pV13QiNAfsCmT99Eqysdgau\nlwvNhaexXC7jueeem/iYgyAsMbLa/hCDQQjBzMwMTp06he9///sT25MEnjnJa30opbh58ybaVLUW\nysV4iu2hgAQOaxA0bRV6Gg2N7mUjo3S/CQj6OEWl4vrw/7P35eGRVXXa7721pJKqyp5O0+l09oXe\ndxqQZ54HWkWHz+2BD5eZb2YcBQd1RhpB3EBBENlFBGlBRUUE0VGG73MhOKNIo3Y60NgNWSpV2Tr7\nWnvd9fujcm6du9SaqqQ6Xe/z5OnOPbfOPfem6tR7fuf9vb940fVkfRmRUCN5TPS4cbQddJVRolWH\nDDB6XfpyT3SvAADtuzneu5tcn2EA7WYyu5zUGY9wa93QdIQ8A4LumnbqqqVGeAahZW27vThG0jmB\nUUg64RdGJJ1gbj55DuB5552n+n1xcRFutxuhUAiPPvooTp48ib/85S84//zzsW/fPnzhC19I2ieN\nZHJGo/b5+fmcSBzXLVEXBAFlZWUoLS1FJBLJuq7cbDaD53kUFcWpBJMAoVAIg4OD8Pv9aGlpQXV1\nNRiGQX9//4rGJPDq5FGaNGstGiVFgiEllMnwyxFmo8i8rEQNjAl8ItIOUMRXkOJKb4BlO0kiZSL6\ncFlW6esJiJxHko3tIQVeT/rIQoAQXXocoijB54uo+pI03wz0rkEkIsYtAEXLUwjsdmM3okhETKph\nD4cFhazHQyQigePEhMTf7xcS6tsBYG4ugtLS+PISn19QClPFxhd71jRZF3hZFVWPR9Zn5wRs3KB+\nPrOzIUQi+nFMTasj7BOTYZy3UR0RpxNKCTLx3TVCqoUvurq6ComkGaCqqkoJiqRit5sIkiRhYmIC\nw8PDqKqqwsaaEoyNjWHnzp0AgNPD/rSlLJlKXzIhs6vp7AKktyugel2GiarJouKZ9qXtM5Hzj1FP\n2nO092XSJFiySH83IRWCbBQZjyExAc/kelq4pqMOWoFQ9LvfURLtY9FH8QBJhjcAVC77XxCSTsPn\nl+B0RD/TSz4JZXGcwgDg9HhyIw2GYWC329Ha2oqnnnoK73//+/HUU0/B5/PB4/GkfH8EyeSMRu3l\n5eU5kTiuW6LudDpht9sxOzsLv9+f/AVpgiQ2pUPUw+Ew3G43lpaW0NLSgm3btmVtAfHxr0VJAukv\n6sqyTM6XP81mc3zZiShKkJZlMGaLCQzL6Igw6VMh+7KsinYTSER7Tl1Lex5dcEkUJUhUpJ2cq00a\nZRgGfESEsOwMQwixNqKiFEri1JFqpS+Ntp6nLCpNJRZlTDQ4ToTVakJ4WatvTUCQearKK6wmneYe\niC18vN6oXCPmZx+7mUCAVz3v4hKL8rck98ZxIliWiUvYyXnJiH/AvyyrWSbMJBGWjsB7vVxCsu73\n8YpsR4tgUEjpva7Vp/cN+HURLp+Pg9OZnlOL3xuGo1QvZaF3xVbiu5usMAYQ3RIl26OFZNLUYeTi\nlYlTjyzLmJiYwNDQEKqqqrB//35YrVaEQiGV4cC2BrUd3Onh6PdHJsmk6UR1UyV1q+nsAuRWl54M\nqUTY41XppEF/RySLJmshg9ER2qjcJPkCxogIJ7ueFkY7B8ki5KmMIxPQJJ3AH1T7w9MFA0cnZZQU\n08GqWNv8Ag+nQ82fXnZvgnaxAQDd3QvYvz9xEMWoynRlZSVqa2vR2tqa8LVGSCZnNGqn5THZlDjm\nl1g7i6C9d7NtpQikV/QoEongrbfeQk9PDyorK3Ho0CFs2LAhayT9X26dgShIkCRZRdB14wjzhu00\nqZZlGQIvIhzkIFCab+1rSAKowIvgOQFcmIe0TPaNIPCicq4sx8ZJEkppcGEe4RBvGP1W9blcsCje\neZIoLV8z9kOupf03eo8yAj5OIeOqvgQJ4SAfWwSEBYSDvF5iQ1tH8iJCAU61EIgH2s+e3JcWoSCP\nUIhXyDcQW4SFwwK83ogieeE4UXVe9Boi/L5Y5Fm7MwBECbvfxyMSEVUknfz9vV4OgYCg8qYHosm4\nQJSs0/7swaCgSH/IosQIZKEAAEtLEeXHCHRRLYIh15zBmclBR2dXElEnkRajwhjk+Oc+9zm0tLSs\nG8uwtQDZyUwHsixjfHwcr776Knw+n5IsTMh+srl8W4MD2xoc2N5gx/YG9Y4MY1DgJV3IMpMSiZaQ\nmewl1f5zhZU+Hway8pMpWEb9Qx9L5/pG4zBqNy0nZxq1GY3FaIx0H0Dsd6N+4449C+9PAHhtyInX\nhpyYX5JVJD0ckRGOyIhw6msEgjICQRnBYOLv8OFR9RzPCzI8Q0HDc+cXo+/+0fliw3YtURdFMW79\nnFwj2xLHdRtRJ0hWJCNTpELUiRXk3Nwcmpqa0NnZmZScp7ut+y+3ziivk2nJhiDCUkRcQmQVMWVY\nRkduTWa9TEaSZAiCqCJ0ZosJkiwb6uBJn7Isw2zWR29JvwIvKhFlIo+RRAmsiYVINOTL7TTJtdos\nqsUDGSN9bUmUDKP8BDR5LbLpo9h05J6MOV6FVUlW90fOI2NRFTWKCLpxqRJUeQlLC+FYxdgkkpZ4\n97S0EEKxPXG0MRTkUVwSm8DoxQbHixBFCSUl8Sc4srsQD34frytoReD1RlBaGo2i0OTc74vA74vo\nnpHfG4azLPWkThoTk+HkJ1FYWlpCZ2dnRtcCEhfGOHz4MBYWFjLuuwBju91k55MIemVlJfbt22e4\nA5pupWlC1kmdi6LqxO+ZVKLqqRDp1Za8ZBMrjeor/cQhpelG/bUEOVdIpW+WSSEiHqc9W5HyePjT\nm2RhGn3nWS0MIpyMIiujkHNi3xsKxwJHRvD5JViXJY/zCzFOtuRTv6sT7domgpaop8KjSOInjebm\nZhw+fDipnDFRe7YljuuWqGvtGbONRJM7z/PweDyYmZlJy2mGeKnHsxzT4v98MVpy12Q2QaY8mAh5\nFXn1+FhTzFaRlp4wDAPRwE+d7ou8jlv2SycSGUAtYyEghFoUJFiKzKo+yL+6BQMvJtS1h0OxaDBd\n+EgbxSd9Ej09sWyk70USJYQCtNWjWXcfgiCCYRiFCCdaAJAdA0CtjafJelhT9MlqM+si8gSRsKBU\nXCUJtHSRKYvVhOCyVSWR4RCpErlOJCygrKJYLcVZRmh518DusCoJtjSCQWIXadFF5gHAuxQlweWV\n6ugG8dMXeAl2R2zCpRclXm8ElVVR8u33xSIqoQAHZ7lxtESLmekgajaUJDwnaLAzQkObDL6eCmSs\nJxjZ7SaCLMuYnJyEx+NBRUVFXIJO958OfD4fBgYGIMvycp0LdZT9b1REcC1lI9lAtsefLcKu6zeB\nFl17Ti4qoK4k2p/WtXNMzGns2Ay8eEKE1coq32EcLyPCSfAFoLPv5XkZRUWxYySaPjuXPAeK6NM9\nQ8GkRN0zJqLewHOD53mUlJBCS6k9J6PgCkEyOWO89lxIHNe19AXIbKs0FRgRdZ7n4XK58Ne//hUl\nJSW48MILUVdXl7IzjNls1lUnjYf/88XJqBZcliHwgiIjUZFRTdSb53jwHB89XyDuJ9HiQ+RHlmVF\nwiKJEkReiBJ7gwg6zwngOUFFUiVJVn4I+SftXFgtZ1FIuywrP4IQ1aDznKB4rJMfGqFABJEQpywc\nCGhJjciLyg8fEVT3pb2nUCCCcDBG3LWRe1mWlfuNRARIcuy+SBIvAceJOlLOcaLubxOVzoiqZ6F6\nTVhQfpRxLVtehqjCTlw4fjTQn0BuEgxwmJ/VbzPSYzEiu/QCZ3E+hKXFMEJhXlX0ClB73qtfz+PM\niE9F0hPBt5ReZDwRPEMB/OPnojZ/2gjMwsLCuvHdXa9IRNQJQX/11VexuLiIffv24fzzz88o4d8I\nfr8fr7/+Onp7e9HU1IR9+/YZFqPb0ViCpmoJ0tIAdjYmX3jmSvaSDVK3UslJ3H5XgXDGk4cYyVHS\n+UnUd0bjXJanpPKzGtixOfoDAEfeG8an3hVEKCQiwkmILOcu0TVIeF7WWfdqpZGZoP/0dNy2vVv0\nPImez30+HxwOh+6cdJBMzmjUniuJ47qNqBPkyjOdJuqCIGB4eBiTk5Oor6/PqNopEI2op5qgKvIi\nWDOrI3gy1BFwiWV0RYmIvEOgIu4mEwuGZRUCr7kagCi5NJkTRNEpEk5HqegIPtHAk9+1UWqJiriL\nlA+82WJKqH8n/ZEoOksl1dLjpXcCjECkNqKo3gnQ3iMfEcCwDCxWardA6zfPRfviqXHFG792J0F7\nr77FqDDQalMnu5KtRkKeSVIq7YsfCQsqKY02Qh4O8rAtS12M9P5BP4cSR/woR9AXQYkz9p7lwoJy\nvwE/B7vDqlpYANHdEYdFL2nxLYYSRtWN9OkEQwOzaGyrVn6fHJ7DxgZj9xVAT9QLEfX8BZ1zpCXq\nsixjamoKHo9H+cJMxwM/GQKBAAYHBxEOh9Ha2pqShS5daVpL1t8YCi2PO/XILpkdUiXr2YxY52pX\nIB7xzCfJTjaxmtHwTEDIuRGOvDcWLPn6s9HvgrBB/CQcFnVF8Obno+/3hi1R4jwxGVK+p5YWQlha\nCGHnzvKE/unVtXZ4fQI2bYgvyaTn82zN5YnkjEbtuZI4rluinm07Ri3MZjNCoRA8Hg/Gx8exefNm\nHDp0aEUev/TknggfunEUACBpZBMsoyeD2qRJ1hyzOKOPi5AAihxqn58ipxGipJhIV8wW9VuIJvqE\nRLIJnonAi4psx7Tcl27xIclqZxZKT69dMJAE0kTSHBI5Z1gGRTaLjpwSEkyuaSkyG+8ESLIS+Sfn\nkTHQfcmSDCzr8otsFkqaRCVrUv8n/RiRZrKDYIqz2IhEBNiK9ZOZbzGkWrARIk0qr5IEWrPWYjHI\ng2UZBP2crggUjVCAi6uNn5/2KwsMGlqCT4/VWV4Mvzf2TeBbiq9VzzSRtEDUzz5YLBYEAgEAaoJe\nVlaGPXv2ZEzQjepiaOtcVFVVpfy9kmgu376lCGfOnMHIyAjOO+88bNmyBadHE0cgM4mmZ4vwZtMu\nMaXrxfE3X22kItMxrnaa34Rci0QE3Qif/9/R78+vPR2dO+kgUyjEw+m0ZsS/jJJIH/8l8LH3qY99\n5oqQ7jwge1a7+Yh1S9S1WKn3Lg1RFDE3N4epqSk0NTWtmKATpELUr75+WBdJJwRMksm2lKwaj9qJ\nRP2FYDKZYq/XOI2Q49qJSNufsgjQRoWXteHaa8ZLOBF5IZr4KsuwWC26a0miqPoXgJIwS2Q22sg9\noCb2WvJLy13Mlph9pRKJl2RV8msq0XjtDgaNyLI7jlGyLbku5wuDpaL1Wg98Iu3RRtfJ/XmpPIJi\nu/HuDCH8NIEmsiMjoh9tlxAOciqbQ1oGEwpwSb3z44HuB9AvLIAoWad3MFLRqScCz/M6Oy8jKUMB\naw+GYSBJEiwWCziOw9TUFNxu94oJOgFdFyMcDmNwcBA+n09V5yLd/rTSSDq5taamBgcPHlRcKXY1\nqb+KT3pii9SVFjjKBtZaa5+M/NJFquhjifpJ1p7qtc82Yk6js5aHIDBKHkg6SoAvfSj6/r7lSfVz\nXFqMoLwi+nkk0XQCOppO4403FmErsSj6dDq6/vgvgYWZqFxx4BSwtyoAh8MBh8OhcnWh5/OVWO3m\nI9YtUacnVjJprtSqR5IkjI2NYXR0FBUVFaipqUFTU9NKh6ogmfvA1dcPR8cRp0ImDUL4tZF0w/Mo\n7qqNymu13JIsKYsAbeRb1JBgk8UUk7JQEXxJjJFZ7RcgOY+QeylJQaZwMFbMShvdJ/3Fk+RowYWp\nZFVNX2QHgY7smy2m2I4ERewB6FxtCEi0nWjgtd729P+JdCaebEZJ7E2wMAgFIrox0M+DC/MKmSf3\nRv41ioIDQMAfgd1RZHjdcJCDrcSquPjQ1yH9kaRgSZYRXNapJ7oHGktzft3vZPFEL8jcb00m7Usb\nUZckKeVE7gJWF0T64vV6MTExAUmSsHv3bhQXp5Z8nAxmsxnBYBButxuLi4tobm7G1q1bMw7usCyr\nChpMT0/D7XajoqJC8W9PhF1NsYWHJEk4OWyc75FrrDVBTxXnGsFeKcqkEdjtdvB8MaxWKySJBKjo\nHXFWef8nIvC3/ZOskHWJclDTkvThEb/KaGFpIdZuM3AZc5brgzD3X2+G32/CzMwMPB6PIhV2OBwI\nhUKIRCIwmUxZiag/99xzKC8vR09Pj8ofPVF7V1cXAODFF18suL6kC6JrzJSoS5KkbFXW1tbiggsu\nQDgchsvlyuo440XUr/z3WHUrow8MsWUkW7dGXy5ack8Td5WXOMXaTTApUXoa2kVAvN0EnotpSc0W\ns47YkzEppJ0ia0ReE7tmzG89Vkk0FkVnGEZ/PVnWSXHoEbAwqaLzqntcju7T5F8b3Q/zRBqjj/5H\nz4n5ylusZrUkRiDyGh7M8t+UjhbHdiPEqGyGicpmtBF/0ifDMLAUmQ11/JFlQl9ks+gIscCLWJrz\nAQCsNjV5iIQ4FBXHjtG7DwG/egFAF5EiZB0AItRrtIm/ieCd96Os2qk6Nj+1CFuJOnKq3eHQ6tRp\n+L1hzJ6ZV37PxuK9gNWBKIo4fvw4iouLUVpaim3btmWtb47j4Pf7cerUKbS2tqZko5sKZFnG7Ows\nXC4XnE5n2pF/SZIgCAJEUcTWTdF5fXJyEkNDQ2BZFva6/Qlfnw35S7acUgrID+ysZxAMBuHz2bC0\ntISxsTFEIhFYrVY4HA7Y7XaUlJSgpKQkaiyhIfAk8q6Nvv+vrX/FT17dqfwe8Ed0UkjfYghVG/QJ\nnrOTS9jcbDxna1FaWorS0liVUlmWlc/vxMQERkZG8NWvfhUDAwOorKyE3W7Hzp078ba3vU1xhEkF\nPT09AKKac7fbrfiiJ2qfn5/Hz372Mzz22GP4xje+oXvNSrDuiXq8BKRUQEpNDw0NYcOGDaqtynQc\nWlKFEVG/8t/dasIqRWUY2sgtPWait6T7JSBRHm30mzXpE1MFKRY9ThTxpMdMrqUl+OrEVZNBWasX\n0wAAIABJREFU1dNoZJ+O6CvSG+24eFG3U0Dr0KOa9tjfmzWxitxGFfk28HMniPmzx8adSLJDXmMU\n1QeikX9a0gPEJDyyJIFhWYQC4WXCbZzYGQpEt8O10X6CqLNNzD8/ulCJPedIEqLMhTmFrJOdCIEP\nKc+g2K4mGTQhJ9en20qc0fNpW01REFHsUPcTCoRR4tRHRhenvSjfkLh0tHfOh9IqZ9z2kb4JbOk4\nz7BNEASFOKVq51XA2sBisWD37t0wmUx47bXXstInsdGdnZ2FzWZDY2MjqqtTIwzJsLCwgGAwiPHx\ncezcuTMtkkATdIZhYDab4ff74XK5YDabsXv3btjtUUtIWZYRiUTg8/kwEYpt9efS/rBA2s9O7G6I\nfjcT2QgNjuPg8/ng9/sxPj6OQCAAWZZht9tVBN5isRhG32VZxn2fin7v3PCw/nuGmCEQkGj67OQS\nALXhgTapdGHGCwB4/FZ9EjfDMCgqKlJ+tm3bhmeffRYPPvggWJZFbW0tXnzxxbQ/g8888wze/va3\nA4j6qnd1dalIt1H7TTfdpFgxut3urJF0YB0T9XS9d2loS00fOHBAt1WZi4qnWunLBz45EB3PcrSS\nWSaxNHHVkliGZZQPEmkjhJuQUqOovJQgkVR7HVEUwbKsoSRD4AWVw4pR5J4X1JINhvpSkeRY5Jw+\nbpQoC0Sj8mTCMJS+SDJESQQvS2AZVicBoiUtWh26lvxLtEQoCbE3mU2qaL5W0gOoiT85LsuxRYYp\nno6dJ39PrayGONLwy2MkBZgE6pyYk47FatHtKJDFgNGzJG3Fdpth9BxQy4cCS7HkICsVmQ/5wzqy\nHvSFdGTdWFoT1kXVtej/23jCdgKtnZfTGZ/wF7D2sFqjSWornXcFQcDQ0BCmpqbQ0NCAQ4cOwe12\nZyXwQoogsSwLm82GnTt3Jn/RMowIeigUwuDgIARBQGtrqyqaCETnWJvNBpvNhhrqOM/z8Pv9GPFm\n/z2drIBTAfkFQtATwWq1oqqqSlW0R5IkBAIB+P1+LC4uGkbfLRYLJiYmUFxcrHx+7v636PfSTY9G\nDQa0JJ2AkPSaTSvXkmsDLX6/H5dccgmuuOKKjPpbXFxUuTvNzc2l3H733Xfjsccey+i68bBuiTqN\nVL3UiYuA2+1OWMkOiJLdXETUOS5KdGiSrvzLSirCzKoi5cvkXDMkxkBzLlKM02Q2qaLv2ki9TFky\nAlDO1f6r6NY1HxiFNC4TeyN5CMTY4kAlf6H08RIkQ4kMDfL3kARJRTTp6L4kSMrvSvRfiC1stDp0\nIwtMURSjixWGVenwJSpaT+9Y0A4tRKYTlcVEjxklqKpkM0UxQq31XWfYaAReV6yJFyGGOWWM2vEx\nbFQqFE/7L/ACzBazLhEYALzzPpQ4o9EJ4s1PPyOj++FCnCFZJ+QfiJF173xMh06i6vNTi7o+g97o\nQiBZVJ1g+M1h2Mti561nl4D1ipVIUgRBwMjICCYmJnQ2uulWJ9XC7/djYGAAoiiira0NZWVlOHbs\nmM5JxgiSJClzChkLx3Ho7+9HMBhEa2tr2u9Ni8WCiooK0C8jxMsdpwR7AesLqRD0RGBZFk6nE06n\nE+edF9uV5DgOXq8XY2NjWFhYgMVigcVigcvlgt1uV6Lv937Simtuj0kNG9tqMTQwFfcznKjQkVE0\nnYb2c7aW8/lNN92Eq666Cvv3789aQuu6JuqJvHdpyLKMmZkZDA4OoqysLCUf3lzYPxLpy/uu7dNf\nj5K7EMQiq/EJrExLUCRW/TvU5eMBGEbttd7qiksMRfB1bjJmvYxH5/4iq20WCYFVj0fvOGPkeENA\nSDetoydRdK0cJ6oPJ9IXfYIs/eUJRKPM2nsSeVFH/LUg2nmyeFDbN4q6HQFdezAq9Yknq+EjPERR\nVF1fWbTIUtTIHcbPijwvS5FZlcgKRKPX9LjItQAg6AsqEhstBF40JOv0+4gLc+DCHPgIr+onHIzo\n7nNx2qv6PeQPodgRn2x455dQWpncvYXOWykQ9fzGSuZbURQxMjKC8fFx1NXVGbp0ZUrUg8EgXC4X\nwuEw2traVO8hIo+MR9QlSVKi6ECsjsbAwICS0JqJ40w8EOK1i1rTyrKMcDiM/un0cjW0RX4KEfb8\nwkpJeiIQ29Kqqirs2LEjKmWNE33/9Pus+NYv6wEAfSdHAAA2SkJp1+yikiJHTR01SAfafKNUrHaP\nHj2qO9bc3IzDhw+jvLwc8/PRRcbi4qJqpwGAYTvRre/duxfNzc2qCqUrxbom6gQWiwWhkH77hST7\nDA4Owul0ZtVFIBOYzWbccI9gKPMgxCueBAQwrm6paocx4aZfK0OtgTfyQNfKJbRRe0BP7iWo5Tj0\nMSDmkkCTfy1oYg8sLxRESspDReZV16HIO/0FTd+/KIoQeCGupAeIklRZlmMRagPiTxYcCummzuFC\nnNJu5MIj8AIkQVJdg74XVUY+taAi1yDtRgsGepFkKbLocgTILoJR8rCWTNPH6fHRCPlDOolKYCmg\nXJ9+Pd0/H+F1RD3oC8CmIeYhf5zt1PEZw+NaeDweBINBCEJUqrVSD/VMHAIKyAzEqjGVSPXo6CjG\nxsawadOmhDa6ZrMZkUhqlXIBqCwcW1tbDT3WSeBFm7AsSdHPOAkekbnP4/FgZmYGDQ0NaG9vz3kt\nEGDZwrW4GLsa1Mc5jsNbE6lb9RWSTvMDuSToHMdhYGAAkUgE27dvV/IkgMTR9/safYr+/Y7v6wMo\nY64pAMDCjF3XtjDjxdEvlyf9vBtZ7Sabz42KGRFcffXV6O7uBhDVmxPtObF9NGqndeyLi4s4cOBA\nwuung3VN1GnvXa83FpWTZRlzc3MYHBxESUlJ2okGNLLlz/6ea96K9ifJivMKLT+hSaWOTImJCXy8\niG+86LTW3YSWWSSN2oOKylPHaUkOw7C6HQJB5FWvNQIt2TFKoqWh/VAb2k9RxF8r5SH/T5SIG689\nmogp6N4bdHs8i01yjtYRB9BLgQB1tFuinGRYNiZ50e5k0Im2Rtc3WiQkeo12AURA68lJdJ70pSX+\n9DGjiHnYH1KRdf9C9PPMsCxKnHad/IVE1a02K0IBDsNvDqv6s9vtOHPmDDweD+6//3709vaitLQU\nR48exc6dO7Fnz56US89n4hCQzUSjcwFGdrvxLA5pl66NGzfiggsuSGq7mWpEneM4uN1uzM/Po6Wl\nJaGFo5E5gCiKCkEnO75jY2NK0byDBw/mrJp2OrBarSryTiwmJ8PxK/0CBf36WiGXBJ28R8fGxtDc\n3IwNGzakzHm02ven7wM+dMOoKqpOcGZgBHVtW3THyWcokfOM1mp3pT7qe/fuRXd3N7q6upRqxwBw\n2WWX4cSJE4btzc3NePbZZ5VI/ZVXXpnx9bVY90QdUCeTzs/Pw+VywWaz6VaF6YJMxCv1Xv5fH3sT\nAJQKnSRJ0ChpVEviCehiR2Rs9HEAui8NmtxLsqTzUzeS2xjZPOrHmVhPLmm823X3Iqrv0Yi8a51w\nVG2yBEkUVa8zHAch58vnqhZDGmKfEvFn1VIe+l8jJHMaETTSJpqUk+uSc0ibInmRKMmLZlI1cumh\nx8OFOEiiaOguI0uyimST69PRfHphEC/5M+gLwKIhWr65Jdgc0QVzMnlLbDwSgr6A0mfYH0BxqdrR\nYGYslujjm1+Es7Ic5eXlsFgs2L59O77//e/jiSeewMjICARBwA9+8ANUV1ejtbU16fWBzBwCCkQ9\nc5D5XEvUE7l0JUMyos7zPIaGhjAzM4PGxkZ0dHQkJSt0n6IoQhAEVWRwampKWUwcPHgwK0XzcgGS\nIGuz2dDRUqaShcqyjDdGNDusBbK+qsglSV9aWkJfXx8qKiqy9h59+r56/MstMyrZy+J0/OrSRPvu\ncDhQXFxs6DwTDocVCQ4ARCKRFasjjCLuJ06ciNteXl6eMEq/Eqxrok5Ayk4fP34cFosFW7du1dkT\nZQIyEa+EqP/9P/9NRyJlA/mHzuFDpxtXt2tJuaHGPYEUxqgPVtKMQZYg8ZJhNF+VBJvgGiRKrETz\nNWOSJVl1zOj1WsQqquoj+trdCUCvy9cS/3jHlL5lNeFPBSRBVysRIm1axxmGZRAJhsGwjCJHMqo4\nq/dJj0XCLVar7m/KLycv0+MmCyWSA0EIuza6rpVAkTFYbepItG/Rq1xfe236GMMyCPuDMC37yfPz\nHIQIh6KS6IRLourz4zOwFseuQewtuVA0ah/y+lVkfW4sWvyIHLvneganTp1CeXm5ohH+3e9+h/37\n9+O6665DuliJQ0AB6cEo5ygVl65kiEfU6QTULVu24NChQylHvE0mE3ieB8dxilbdZDJhdnYWHo8H\nVVVV2L9/f956+RP9vSiKaG9vN3RFYhgGuxo0xeGWyXuBrOcWuZa5uFwuhEIhbNu2bUUBTSN8/7Ya\nfOqeqGlAIpL+xFeqEAjY4PP5sLCwgJGREXAch6KiIiVxNRQKYXp6Gu3t7RBFES6XC2NjY1mtRr/W\nWNdEnWVZJRrg9/tx8ODBrFqwrcQp4O//+W/K/xORTdIuSzHPbyOyqSX3NHGXJQmEszIsq4vcG41D\nhBg3aq8dg5GuGTCW42j11qqIrqYCqpGkJxnx116D1UTKdQsDSQbDJqlgZ6DlTzQGLYFlTSb1okGS\nlXO0OQCSKCpyI5qUE9IsSzIksquifTaypCxIWJNJRdKBKDGWBMlQHx+v8BMAcOGI7u+gTbKlo+hc\nOKKQdbIQIP+3WK2GxyLBmOZc5ASYrGYIkeh5IX9QeZ9woYjyL03WCUlnTdEiVmFfEGFfEDZnTNL2\nw3s3YXh4GC6XC0VFRXjiiSdw/PhxzM3NYdu2bbj88stXvPAuIDcwstslLl0ejwcVFRUJXbqSQTuX\n0/r2eAmoiUAi5yMjI6iurobT6VRkM6WlpWlJq1YbZJxerxetra2qRWYqMCLvAPD6cHZd0s5VtFaF\nMpbqJoMsyzhz5gxGR0fR1NSE2tranJHdh2904MOfHYvbvjDjBcvWKNp3GpFIRFnwkvG9613vQmlp\nKc6cOYN/+7d/w8zMDDZs2JCTsa821vU3EnnTtbe34/Tp01n3Sc6UqBOSTpM3IxtF0p6qfzp9XI5T\nGIkm9FECT+vP04vaR0mvpGunCTx9DmvWR4IVyQ4fp4BTHEkPPQZ6oUAIJTkmyZKyEyDJ6ui/Nuov\nyZLhwiGRy4xe5y/r2sUEhaMkyupR/zeMknIjDXi8nQYiTaKJs5LYutxG/iWEnT43Vv1VI6VZfp4s\no7clNbJwDPuDuvsVeAFcKKK7Fy6kd3oJ+4IwW9VSH5PJBD7CqV5H+tdF/2UJkiAiMB/16n32sTb0\n9vaipqYG27dvh98fjeZUVlbimmuuwcLCAh599FEEAgG8853v1N1PImTiEFBA5jCbzZibm4PL5UJZ\nWVnaFT+NQOpiSJKE8fFxDA8Pp6xvp0F7odfX18Pr9SrVSSVJUrbjZ2dn4XQ64XA48kKTDkQ/Y8PD\nw5ienk5Z3pMOjCLABfKeOuodi/D5fPB4fAgEAtHqtHa7QmQdDseKdmeWlpbQ39+PsrIyHDhwYFUC\nFj+5dzOAzQYtAvC/jZ1fJEnC2NgY5ubmsGPHDsUKtaSkBBdffDG2b9+O06dP4z/+4z/w9NNP53T8\nq4V1TdQZhsHWrVtzNhGmS9SDwSA+cG2/YYVLbUJmsnYjYq97TQJibwQ9MU8uyTFqN4rekzFoib12\nXMmIvSiKitTBKJpOk0ijBFx6YUATRq1zitGuRfQEw1s3bNcuGmRJjjrTsIxybZoIsxKrIsR0u0pX\nDnWSGv184jm+SIKoPE9g+W9NBdxZsymulIZ+XwhilCibqC8Eup01m1S/k+dIk3mj3RSlUBR9jBNU\nZJ1E3WO7DPoEV4Ei8gDwf3+8Gy6XC4ODg0pOyn/913/hzjvvxGc+8xk88sgjKyYjmTgEFJAZZmdn\nMTY2BqvVmlWXLoZhEA6H8ec//xnV1dVp6dsB42JFwWAQZ86cARBNTnM6nRAEAT5f1AVjZGQEgUA0\nv8LhcMDpdKK0tBQOh2NVd3XI4mR0dBR1dXWrmtBqRN5lWcbJkcTfNecaos9JXZBIFEX4/X74/X5M\nTU3B5XJBEAQUFxcr5N3pdMJmsyWc43ieh8vlQjAYxPnnn58VWXCuMD8/j/7+fmzcuBH79+/H0tIS\nPv3pT2N8fBzPPvssmpub13qIOcG6J+q5RKpEPRKJ4L0fi7q60CRd0pBUIwIfPUfUtScj9kZIpJ9O\nRvQzg3qMWqJPCF28AkZGEf3YYkACwOqIv/p6UdJPpCJa4k9If6LX0+PWtqvHxmgWRtHrGr2GOPto\ndztUhZmWI8La68qSpHouRrkJ+qRkEklXPwcahODGI+AMy6jkMSJv7NLDa4gyAEgR/XVFnofI8zpt\nvPbTxIVE3ftGTLJgJPjhN+vR3d2NxsZGdHZ24syZM/jXf/1XlJWVoaurCzU16Xn1xkMmDgEFpI9A\nIICJiQk0NDQohGSloGtoiKKIQ4cOpSVJMSLokUgEfX19iEQiaGlpUblPmM3m5UJEMes4Qrh8Ph8m\nJibg8/kgSRJKSkpQWlqqEK50dfep3Pvs7KxS4C9f9PIMw+gIvCAIOHVmfeiN00EiHbrJZEJZWRnK\nymKWh8QXnywGx8fHEQqFYDabVZF3spMzPj6OkZERZY7MV003sYbkOA67du2CzWbDc889h/vvvx83\n33wzPvjBD+bt2LMBJpnzhAYrZW6rCkmSEIlEwLIsjh07hgsvvDCrf8zh4WGYTCZs3my0dRNdqbrd\nbvzH14w9n1eKRKRf2649z6gtHmgyZSTXiXcs1cTKTKElx/EIfzpQ/OMVUq9dbKjbjc5JBi3xpvtN\n5x5UVWoTjDeRhp5ounVjZPTFsSRJTvqe046BYVndgstokWm8UMwsqvarH2xHb28vrFYr2traYDKZ\ncPToUfz4xz/GPffcsx4i2tmYxM6quRyIzqeCIGBhYQEzMzPo7OxcUX9EPmO329HS0oLXXnsNF110\nUUqv1VYTJYmjHo8HXq8Xzc3Nht7qqUKSJASDQfh8Pni9Xvh8PvA8r0RLCYEvKirK6BpLS0sYGBhA\ncXExWlpaViwdyhVkWVbI5ObNm1FXV6eK9q9X6Uy2E0V5nlcWg+Q9FQqFUFRUhI0bN6K8vBwOhyPv\n8iZIovjw8LBiDTk8PIwbbrgBmzdvxt133322F6pL6cN7zkTU41l6rQRms9mw4qkgCBgeHsZ1tywZ\nj8uAqBm1ac/RaciTRLzTbU8nog/EIVx00mQSuY72nHjEX6vVJ+fIouZYgvs1cr1J1K4ll1qQ9kwW\nJLLGD95IoiTLkirRNF6RKfJsRF5S/p5a+UuiRFEiOdETcOO/ufY9o5K5GDwLSeQhS7JyL5KBNCnR\nzkk6+M3T+zAyMoKTJ0+io6MDFRUVeP3113HDDTfg0ksvxbFjx9a0oFkB2UGyStPJsLCwAJfLBavV\nqrPoTeYUYVRNVBRFuN1uzM7OoqmpKSvabpZllcgnKSBDoqVerxdLS0sYGxtDOBxGUVGRSupQUlIS\n9/q0k0tnZ2deyxzIQipRtH896t5z4eZisVhQUVEBh8OBYDAIi8WCbdu2gWVZ+Hw+zM3NYWhoSHFU\nIZF38n5aizyKQCCA3t5e2O127N+/HwDw4IMP4j//8z/xwAMP4JJLLln1Ma0V1jVRp0FIdTaJurbi\nKe0UcNsj8QlBIhKSbpuW1Bj9btQHqRZPt0sa6Ua6CwMtWJbRvEbUtavGlgLxp/tjWcbwNdrILwBV\nwSUjaNuNFg3x2tKRIWkj1dpCUHSbJIqGOyA0kSZt9HMxkr8A6vePyWxSvUZalglpI/CCEL8gFr2A\nSvQsiGxGH+3XnaobZ6p49rE2HD9+HDU1NTh48CBCoRC+8IUv4LXXXsPjjz+Obdu2pd1nAfkFo7oY\n6cDr9WJgYAAsy6Kzs1NnLpCoLoZRNVEguqs6OTmJ+vr6nGu7SRXR4uJi1NbWKscjkYgSJZ2amkIw\nGFSkDiTybrFYMDQ0lLGTy2oiEAigv78fJpMJO3fuTHtxHY/o5juBz3XRIhKZbmhoUC0mtYtBjuOU\nyPvMzAyCwaCycKQJfK7yKCRJgsfjwezsLDo7O1FWVobjx4/jpptuwhVXXIFjx45lXQaW7zgniDrx\n3s3USjEeyHYn7RTw1YdtAGxItLOcyFIwXjs5J1mCZ7Lf9X3qI/q0DjwZ8df2Y0T8jUg/3Z4pjHYE\nyLF4uwXxrpkOEU624CCkl2HZtPqVpRj5MFokxdtZETWada2zT/RfDQHn9E408bzvY32pnYgIQddG\nyY0ciTKVsSSDUbLob37zG3z1q1/Fddddh/vvvz9vXDUKWBkyJep+v19JtGtra1NpemnEq4tBihXR\n0fbx8XGMjY1h06ZNa16sqKioCEVFRaiurlaO8TwPn8+HpaUlDA8PIxgMoqioCJWVlYpjiNPpzKsi\nS8QW0ufzoa2tbUWVJY2Qr9H3XBJ0APD5fOjr64PD4Uiah8AwjOH7ic6jmJycVHZliouLFeKeSuJq\nMiwsLKCvrw8bN27EgQMH4Pf7ceTIEbhcLvzoRz9Ce3t7xn2fzVjXRN3IezebMJvN8Pl8+POf/4yv\nfKsIUYKeHMmSNI2JToxEparNTqZ7NmrPNvGP9s8mbTN6faLkTSMYkXDtDkK8vrWFh+gxi1LiscQj\nytGEUuOFDuk3Hoyi29p7Mvr7aRNAU3H+URHtBDsJRq/RLsK0zj7RYwm7yhg/emgLuru70dDQgM7O\nTkxOTuITn/gELBYLfvvb32Ljxo25uXABawoS+U6GYDCIwcFBBINBtLW1JY0ia80BtNVEWZbF9PQ0\nhoeHUVNTs2oWdpnAZDIhGAxicnISdXV12Lx5M2RZht/vh9frxfj4OPx+PyRJgt1uVyWtrnZCKdmJ\nHh8fz4ktZCKsNXnPJUkXBAGDg4Pwer3o6OhAaWlpxn3FS1wNhULKbs6ZM2cQDodhsVhU5D0VC1Kj\nZNHnn38ed911F44cOYJHH310XSeLJkN+zjI5QDaJOsmWHxgYAM/zuOM7udX5GVkYJvo92euTETij\nhUCyhUGiRcFKpD6pJFnGtVJMgFQWHOrCSPETIFNZ6ND+9gTJFlDk/8n+HrQUxajCqPI6gwRR3bVl\nKeG1tMhUT74SkGTR2dlZ7Nu3D2azGU888QS+973v4c4778S73/3uVR1PAasD8kWd7As7HA4rBXta\nWlpQXV2d0pc8Ieq0kwupJjo/Pw+326249uTr1jtxsXG73aiurtZFULVkS5IkBAIBReYwODgIQRBQ\nUlKiEK3S0tKcJBnKsozp6Wl4PB7U1tau+c4EgRF5liQJb4xmL/861zKXyclJDA0NYcuWLWhvb88J\nyWUYBiUlJSgpKVFJschujt/vx+joKPx+P2RZVnIuaBcjo2TRM2fO4IYbbkBlZSVeeuklVWT/XMW6\nJ+oMwxiWnc4UCwsLGBgYgM1mw60PWQBYoLUhzBRpJyUaJFmm9Lo0iX06C4OVWzpm1m8qOw3JikSl\n02+6HvXx+k5U9AqIJc2qjiUI+Chacc3ixahAUibe+muJ264XwfM8/vKXv8BkMiEQCMDr9eIrX/kK\nDh06hFdeeSXrpa4B4LnnnkN5eTl6enpw0003pd1ewOqA4zh4PB7Mzc2hubkZ559/floExWQyKU4Y\nLMvCbDZjaWkJg4ODKC4uzkgvvZqgnVx2796dkpMLkcDQen0SKfV6vVhcXMTo6CgikQiKiopUkffi\n4uKMCaDX60V/fz9KSkryukorEJWO9Pf3o7i4GK2trapFWrrR91zLXPx+P/r6+lBSUrJmdpsWiwWV\nlZWqHSx6QUgSV8PhMHieh81mw8zMDKxWK5599lk888wzuPfee3HppZdmfWxn61x+ThF1UlwiE9CJ\nSF+8nwEQyd4gl5GJN3qi18azVTRqzxYyXTyk0m+yPiVRTNkFhrQn6lchvRp3mXgOLNrX0W2p/G1T\nOkcTxU8mk0rWj/54fhYa+dnRdvT19eG8885DdXU1uru78dBDD6G3txdlZWVwuVx4/vnn8aEPfSir\n1+3p6QEAHD58GG63Gz09PSof9GTtBeQGDMMokhRBEDA0NISpqSk0NjamHUEkEXSn0wmPxwNZllFU\nVIRQKASLxbJi2UCuEQgElMqn2XByoSOlRD4my7IqaXViYkJ5PnTkPZlDSDgchsvlAsdx6OjoyHq1\n8GyC4zgMDg4iEAigvb3d8D2QqnQmNP5XlJSUwC3G3HkytdY0giAIcLvdWFxcREdHR9w8jLUCvSCU\nJAlDQ0OYnp5GW1sbGIbB888/j9///veYmZlBQ0MDnnnmGXR2dmLTpk1ZG8PZPJefM0Q9npViMgQC\nAUXi0t7ejiuv7c/BKFeGeAQv3WqnK4HOzcTAmjHR9VIhtIRUJzsnXay035WOx2gXIN1dj2THY+35\nScTj4f/+eDcGBwfhcrmwbds22O12vPTSS7j11lvx0Y9+FNdddx0YhlEiNNnGM888g7e//e0AgObm\nZnR1dakm72TtBWQPWrvdcDiMqakpjI+Po76+HhdeeGFaicPaYkUbNmyA0+mEy+VCJBLBhg0bwPM8\nent7FS231kllLUEnX7a0tOTUyYVhGNhsNthsNlWhMNohxOPxqBJVaY2yLMsYHh7GzMxMWnKktQAp\nUX/mzBk0NTWlXQjIsNrqlkOKLz69S2G1WlXPym63p3UtIh9yu92or69XiG++gk4WPXjwIILBIO64\n4w6cPHkSP/3pT7F161YEAgGcOnUq64uNs3kuX/dEnSBd6UsoFFJW062trfjgJ90A+uNKILKtoT7b\nsNJFQaoEO5uLi3T6TedvmInEJt5rU8XZRsBTAZ0s2tHRgZmZGfz7v/87wuEwXnjhBdTV1Snn5qp0\n9OLioooAzc3NpdVeQPYhiiI4jkN3dzfq6+tx6NChtLTNRtVESTJbIBAwJL2kAJHX61WS0qC8AAAg\nAElEQVRVMiVabkLeV0O7LooihoeHMT09nTXf9kxhtVpRVaUubS8IguIQMjY2hvn5eXAcB4fDgY0b\nN8JsNse1wVxrzM/PY2BgAFVVVVnVzDMMA7vdDrvdrkpyJ7sURlaI5D3lcDgMxxEIBNDX1webzYZ9\n+/blbd4EENWt9/f3K8mixcXF+PWvf43bb78dn/zkJ/HAAw8oi2y73Y4LLrgg62M4m+fy/Puk5Aip\n2jOS7a7FxUW0tLTgw58aAjBoeG46OuV0CVg+Ev+zZcGRLlHWarlT7TfT8cQ/T22/mIpv/nqENlnU\nYrHghz/8Ib7zne/gq1/9Kt73vvet9RALWAMwDIP5+Xn09fWBZVls3bo1rUQzbTVRkjzqcrmwsLCA\npqamuLp2ugARgSzLCnmfm5uDx+MBz/Mq8l5aWpo1AkVsgEdHR1FXV5dz3/ZMYTabUV5eDkmSMDEx\ngdraWjQ0NCikdGpqSrH30yatrhXZDIVC6O+P7pavZi6CkRUivdA5c+aMyp2HRN3n5ubg9XrR3t6e\ndRvLbIJObCXJopOTk/j4xz8Om82G3/72t6pE1AKMse6JOpnIkkXUBUGAx+NRohSfvtUHYGh1BmmA\nbBP/lSBV3XMur7sS5NOzjD8Gvfd5svPWG+jKou3t7aisrERfXx+uv/567Nq1C3/6059WXdNaXl6O\n+fl5ANGICx05TKW9gOyCJMkNDw9DllP73BpVEyU62ampKTQ0NGQkGaCjpHTBGGJZt7CwgOHhYXAc\nB5vNphD3dPXJWieXfLaFBGJyUYZhsH37dpSUlACAkoxKQC905ufnVc+K3qVYqTd3IoiiqCQgp2Lh\nuRogCx2agJNkzPHxcQwNDcFisYBlWXg8HpV0ZiUJvtkGqSxKPrMmkwnf/e538YMf/AB33XUXLr/8\n8lUdz9k8l+fvpz3L0HrkEoiiiJGREYyPj2PLli244IILcPmHe9ZghPmLtSKy+UCg08F6JtG5xs+O\ntqO7u1vZcuY4DnfccQdeeuklPPzww0oJ6dXG1Vdfje7ubgCA2+3G4cOHAUQn8vLy8rjtBeQGJNKZ\nipTRqJoowzCK/riurg4XXHBBVqPSRpZ1siwjHA4rBYi0LiqJCOni4iJcLhdKSkpSdnJZK/A8D7fb\njaWlJbS1taGioiLh+fEWOuRZkYhyOBxWablJ0upKCKksy5iamoLH40FdXR0OHDiQl7sTBCQJ12q1\n4qKLLlKsDelnRRJ8SVXadHzMswmyCJ6ZmUFHRwfKy8tx6tQpHDlyBBdffDGOHTumLN5WE2fzXL7u\niTrtvUtHYEjCyOjoqFJd7l0feQ3AxBqNtIB8gLY4UYF85xb/76k9cLlcqmTRP/7xj/jCF76Aj3zk\nI/jTn/60ptHDvXv3oru7G11dXYqHNgBcdtllOHHiRNz2AnKDVO12tdVEGYbB1NQUhoeHUVtbu6pR\naYZhUFxcjOLiYmzYsAGA3kWFJqREAjI7OwuWZbPi5JJL0MmXmbju0DB6VkAsaZXkCASDQZhMpowI\nKbGGtNvtea/tpiP+7e3tqsVPvGdFfMx9Ph9GRkYUtzviYU7+zUUy9MLCAvr7+5XPWDgcxi233II/\n//nPeOSRR7Bz586sXzNVnM1zOZPq9uEyzq4QJ6KTSCQSAcuyeOWVV3DRRRdhYmICQ0NDqKmpQUND\nA979D6+v9TALSBFaHTd9jKBAsM8O/OihLXC73WhoaMB5552H+fl5fOlLX8Lc3By+/e1vo6GhYa2H\nmK/Ixt72WTeXA1HCJooiZmZm4PP50NraqmrXVhNlGEbRj1dWVqKxsXHN3VoSwefzKUmtNpsNgiDA\nYrEoUfdsRJOzBVqSs2HDBjQ0NKxqwSJBEBRC6vV6dYS0tLQUDodDWZBxHAeXy4VQKIT29va8toYE\noCQtb9q0CZs3b15RVFwURcXHnPxoC1utxDKS53kMDAwgEomgs7MTxcXF6Orqwq233oqPfexj+MQn\nPpEXxazyECk97HOGqDMMg5dffhlms1mZsE0mEy7/0AnD8vArKWWfaxiR1Wz2m6hvbcn4TPpNRKIT\nJVIWsD5AkkWtViva2tpgsVjw05/+FA899BC+9KUv4corr8wLIpLHOGeJOs/zEAQBi4uLmJqawvnn\nnw8AumqiLMtiYWEBbrcbDocDTU1NeS0bEQRBsS9samrChg0blM8AHU32+XyqaDKRziTzL882SF0R\nm82G1tbWvClYJIqikohJfqTl75JIJIK6ujrU19fnzXiNEAqF0NvbC4vFgra2tpyNlc6nID9amZHT\n6Uz43qKTRZuamlBbW4vp6Wl8/vOfB8/z+Na3vpVVL/R1iAJRB6CUqHW5XAgGg9izZw8cDoeiV//7\ncyiaXiDBBawlfv2TvRgbG8PExISSLDo4OIgjR46gtbUVd911V94V6shTnPNEPRAIwOPxYPv27TqC\n7vP5FD1vS0vLmuhhU4XWySXVyCktbyDRZOJfTsi73W7POnkPh8MYHBxEOBw+K6LSc3NzGBgYQFlZ\nGUpLSxUSz/M8iouLVUmr2SxAlAmI7ebMzMyaJrZGIhH4/X7VwpC4HtEEPhKJoLe3V6nYajKZ8MMf\n/hCPPfYYbr/9drznPe9Zk/GfZSgQdSBK1MfHx1FUVASXy4Xa2lpUVFTAbDZDlmWMjY1hfHwctz64\n7uX6BRSwZvjaDTJ8Ph+sVisGBgYQCoUwMjKCV199Fd/61rdw4YUX5uS6yUpCHz16FAAwODiIb3zj\nGzkZQw5wzhJ1QRDA8zzC4TDeeOMNbN26FUVFRWBZFsFgEIODg5AkCa2trXlNIrVOLo2NjSvWzGul\nIH6/P2VP7mQQRVFJEMz3gkUAEAwG0d/fD5Zl0dbWprNbJImYhIySaHJRUZEumrwa9zk7OwuXy4WN\nGzdiy5YteZfYKoqi6r01NzcHnufhcDjwwgsvoL6+Hs888wwOHTqE22+/PWc5FetwPi8QdYL+/n5U\nVVUpmdGBQEBxAyAyGKfTqftAvuPq42s04gIKWB8gyaKBQACdnZ2wWCz48Y9/jKeffhrBYBAMw8Bq\nteK2227DO97xjqxeu6enB263G1deeSWOHj2K/fv3qxKEurq60NzcjObmZlx11VW49tpr8yrTPwHO\nWaLu9XoxPT2NqqoqDA0Nwev1guM4SJIEhmGwefNm1NXV5bUOnXZyaW5uzqkkR0uw/H4/AKjIu9Pp\njEveyY708PBwWhH/tYIgCBgaGsrYbpFO8CXRZOKiQp5XNncqiH87wzBob2/Pa3kWEEsW3bBhA+rr\n6zEzM4NbbrkFp06dgs1mQzgcRnNzM37xi19kfYGzTufzlB7SORFGPnr0KF588UXYbDZUVlaiv78f\nX/nKV3DhhReC4zgMDQ0hEAjAbDYr24alpaX47U/3q95sBeJeQAGp48ffasDx48eVyqJLS0v4/Oc/\nj7GxMfzoRz9SqokGg8GUipGli2Qlod1uN9xuN6655ho0NzfD7XZnfQwFZBejo6P49Kc/jZmZGWzc\nuBE8z8NqteLOO+9EeXk5fD4fTp48qRQeoufztSbvgUAALpcLsiyvmpOLyWQy9OQm0obx8XGloI6W\nvJPE1rKyMuzfv3/Nn18i0FrpzZs3Z2y3aFSAiJYZDQ8Pq3Yq6KTVdKvjDg8PY2pqCm1tbXnl2W0E\nkiwaDoexY8cOlJSU4A9/+AO++MUv4h//8R/x5JNPKiqF6enpnOxCnMvz+TlB1O+9914MDg7iIx/5\nCKxWK/7hH/4BL7zwAu677z4UFxdj165d2L17N/bs2YOKigoEAgHMzMzoyPt/fm+rbiusQN4LKECN\nX/1gO/r6+jA9Pa1UFv35z3+Oe++9F5/73Ofw4Q9/WPUZypWGOFlJ6GuuuUb5f09PD66++uqcjKOA\n7GHbtm34/e9/j29+85s4evQoLr30UlgsFtx4442YnZ1FY2Mj9u7di927d6Ourg5msxmzs7Nwu90Q\nBAF2u13loLIa5DMSicDtdsPv96O1tTWpv3iuwbKs8p1GQArqeL1ejI2NYW5uDrIso7y8HEVFRfD5\nfDmz9FspiN2iw+HIyYLCYrGgsrJSNZeQpFVirUkvdmjpjNFYiG6+trY2b6vLEhgli87NzeH666/H\n0tISfvnLX2LLli3K+QzD5KzS6Lk8n58TRB0ANm7ciKeeegotLS3KMVmWsbi4iJ6eHnR3d+O+++5D\nX18f7HY7du3ahT179mD37t0KeZ+enkYwGITFYlEm+gJ5L6CAKH7z9D6Mjo6qKosODQ3hhhtuQF1d\nHf7nf/4nLyr/adHT04O9e/fmlW9uAYnxzne+E9ddd52KCEmSBJfLhePHj+Pll1/Ggw8+iPn5eTQ3\nN2P37t3Yu3cv6uvrwbKsIXknBD5bRE/r5NLZ2Zm3um6WZWGz2TA+Po5QKISdO3eivLxcqRxKrAJF\nUVQs/cjzWisfctpusaOjY1VzEkwmE8rKylTJ72Sx4/P5MD09jcHBQZUFos1mw9TUFFiWzfviVUB0\np7O3txc2mw379++H2WzGT37yEzz88MO45ZZb8IEPfCAv38/rcT4/Z4i63W5XkXQguvqrqKjAZZdd\nhssuuwxAjLyfOHEC3d3duOeee5TVOiHvO3fuNCTvZOIqkPcCzjXQlUUPHDgAWZbx4IMP4uc//znu\nv/9+/N3f/d2qjynVktBdXV1nS+JRAcvo7OzUHWNZFu3t7Whvb8dHPvIRANHI58DAALq7u/Hf//3f\nuO+++7CwsICWlhYlEEPIOyGjKyXvkiThzJkzGBsbw+bNm/M+akoXLGpoaFAVLHI4HCqJjizLCnmf\nm5vD0NAQOI5DcXGxaqcil/aHkiRhdHQU4+PjaG5uVllZriWI647T6VQsCWVZVhyKRkdHUVRUBFmW\n8eabb6pkRsXFxXlxD0BMljM9Pa1UFnW5XLj++uvR2dmJP/7xj2viznUuz+fnRDLpSiHLMhYWFnDi\nxAkcP34cPT096O/vR2lpKXbv3o3du3dj165dqKmpUbYPafJOfrQfxgJ5L+BshzZZ1G63o7u7Gzfe\neCPe/e534+abb14zz2KyU3bNNdfg7rvvxuHDh7F3716lZDQQzV8hW6ZdXV1nQ/IRcA4nk2YDoiii\nv78fx48fx4kTJ9DT04OlpSW0trYqEkiyECDaZBJJTkTec+HkkkvIsozZ2VkMDg6ipqZGqS2SST/E\nj5skYUYiEdhsNh15XykZJe4oa1FgKRPMz88ryZeNjY1gWVZXldbn8yEUCql26pP5l+cKi4uL6Ovr\nU56vIAh44IEH8Otf/xoPPfQQDh06tKrjobFO5/OC60suIcsy5ubmlMj7iRMn4HK5UFZWpkz2O3fu\nRHV1tSF5t9vtWFpagtfrVdldFch7AWcLfvytBgwODmLLli3YtGkTfD4fbrvtNvT19eGRRx5BR0fH\nWg8RR48eVRKLyAS+b98+nDhxAl1dXbjqqqtQWVmJ+fl5/OxnPzsbJnagQNSzDlEU0dvbi+7ubnR3\nd+O1116D1+tFW1ubEnnv7OyELMsq8m632+F0OsGyLCYnJ+FwONDS0pLXBXWA6AKkv78fRUVFaG1t\nzboMg9gf0mSU2B/S5N1ms6VE3mm7xbPBHSUcDmNgYACiKKKjo0NnD2kEUtiK/NDe+OQnU3vNZKCT\nRTs7O1FSUoJjx47h5ptvxlVXXYUjR47kRX7COpzPC0R9tUEiFHTkfXBwEOXl5arJ/uWXX0ZbW5uS\nzGO1WlWTVyHyXkA+4/knd6C3txdmsxnt7e2wWq14/vnn8fWvfx2f+cxn8M///M95s427TlEg6qsA\nQRB05N3v9yvkfc+ePTCZTHjzzTexbds2JXquTVjNp6h6JBJRdN3t7e2qhNLVgJa8h0KhhN9/giDA\n4/FgYWEBbW1ta56ImwxEljMxMYGWlhbU1NSsqD9BEFTFh/x+P2RZ1jn0ZPoeM0oWXVxcxJe//GVM\nTEzgkUceQVNT04ruoYCEKBD1fADZDv3rX/+KJ598El1dXcoKe9euXdi7dy927NiBiooK5QNJT17k\nw0hPXgXiXsBagCSL0pVFx8bG8NnPfhbl5eW47777VvzFVEBKKBD1NYIgCHjrrbfQ1dWFxx9/HHNz\nc2hpacGmTZsUt5n29nZIkqQQUkmSdJr31SbvpOrl9PQ0mpubUVNTkzeLaY7jFCJK7zyzLAu/34+6\nujo0NTXltc4fiHmME9lTrmQ5xF6Tjr7TSb5kwZMsyZdOFm1ra4PZbMZzzz2H+++/HzfffDM++MEP\n5s17ZB2jQNTzCX19ffj2t7+NL3/5y6iursb09LQSqTlx4gSGhoZQVVWlRGp27NiB8vLyuORdu21Y\nIO8F5BLPfbcDvb29qKqqQmNjIwDgsccew1NPPYV77rknJ1uMyarQEdx9990J29chCkR9jXHHHXfg\n/PPPx/vf/34IgoA333xTmc9PnjyJYDCIjo4ORQbZ3t6uFB9aTfJOR0w3bdqkJM7mM5aWltDb26tU\nCQ0EAggGgzCZTDkrPLQSRCIRDAwMgOd5dHR05MxuNhHoJF9C3jmOU+UJEOcZWZZ1yaJDQ0M4cuQI\n6uvrcffdd+dk56IwnxuiQNTPJsiyjKmpKXR3dyuymaGhIdTU1GDPnj3Yu3cvtm/fjrKyMuWDWCDv\nBeQat10vIhKJAIgljTkcDnzlK1/BpZdeii996Usp6S/TRbIqdAQkw//FF1/M+hjyGAWinufgeR6n\nT59WElZPnjyJUCiEzs5Ohby3tbVBEARlPjcqOrQS8r6wsICBgQGUlpaiubl5zWwUUwWR5UQiEbS3\nt+sKQtGFh7xer07DTQoPrRZ5p91yiMwlnyLQdJ4A/cw4joPT6cTw8DAaGxvxhz/8Ab/61a/wwAMP\n4JJLLsnJWArzeVwUKpOeTWAYBhs3bsQVV1yBK664AkCsfDOJ1PzkJz/ByMgIamtrFc37jh07UFpa\nCp/Ph/HxcSVhx+l04kcPbSmQ9wIyBkkWbWlpQWlpKV555RU89NBD6O3tRUVFBQYGBvCrX/0KH/zg\nB7N+7WRV6AooIJ9hsVgUR7CPf/zjAKISj1OnTqG7uxu/+MUv8Prrr4PjOHR2dirzeU1NDXiex+Tk\nJAYGBlTknRD4ZJKKYDAIl8sFSZKwbds22O321bjljCFJEkZGRjA5OZlQlmNUeIjWcI+MjMDv94Nh\nGB15z7YMZXFxEf39/aisrMTBgwfz0n2GYRgUFxejuLhYma9lWcb27dshCAKeffZZ3HPPPZidnUVz\nczOeeeYZNDU1YfPmzVkfS2E+XxkKRD2PwTAMNm3ahPe85z14z3veAyBK3sfHx5XI+1NPPYWRkRGc\nd955SqRm165dsNvtOvJeWlqqI++zs7P48Cc9a3ynBeQTSLIoqSxqtVrxm9/8Brfffjuuu+46fOxj\nH4Msy0pSWi6QrAodEI3SHD58eN155hawPmG1WnWFWDiOw9/+9jccP34czz33HE6ePAmO47B161Zl\nPt+wYQM4jsPExAT6+/vjknee5+HxeLC4uIjW1ta8LC6mBbFbrK2txYEDB9ImvGazGeXl5Yo9H6Cv\nGurz+QBAqRqa6oLHCBzHYWBgAJFI5KxYBJGdeo/Hg8bGRmzcuBE+nw933nknXC4Xfv7zn6O9vR1e\nrxcnT57MWXJxYT5fGdYFUSeVqIyQqi7qbAHDMKirq0NdXR3e+973Aoh+GM+cOYPjx4+ju7sbP/zh\nDzE2Noa6ujpVwmpJSQn8fj/Gx8cRCAQgCAKKiorwvfvqUVVVVYi8n+P47U/3Y3R0FK+//jra2tpQ\nVVWFiYkJ3HTTTbBYLPjNb36DjRs3Kuevtf0iKX5RwPrBuTSXA1Hyvm/fPuzbt085FolEFPL+zDPP\n4OTJkxAEAVu3blUi74S8j4+Pw+v1guM4iKKI6upqlaNYviIQCKC/vx9msznrVTrjVQ0l5F274KHJ\nezypkSzLGBsbw9jYWF4VWUoEo8qizz//PO666y4cOXIEjz76qHIPpaWlOZO9pIrCfB4fZz1R7+rq\nwrXXXovBwUFdW09PDwDg8OHDcLvdCb8EzmYwDIPNmzdj8+bNeP/73w8gZhNFfN6///3vY3x8HFVV\nVeB5HmVlZbjjjjtQUVEBn8+H3t5eVeT9x99q0BWpKJD39YvnvtuB48ePo7KyEgcOHADDMPjud7+L\n73//+7jzzjvx7ne/e1XHk6wKHYm+FLB+UJjLoygqKsL+/fuxf/9+5Vg4HMYbb7yhSCDfeOMNCIKA\n6upquN1u3HjjjXjHO94BQRAwMTGhyGZoCUimUeRsQhAEuN1uLC4uor29XRUJzyVYllV2IAgkSUIg\nEIDP58PU1BRcLpeqsBV5bsFgEH19faioqMhbmQsNurJoe3s7KioqMDo6is9+9rOorKzESy+9hOrq\n6lUdU2E+XxnOeqJ++PBhNDc3G7ady7oolmXR0NCAhoYGfOADHwAQLRbwzW9+E+9617tgMpnwuc99\nDuPj46ivr8fu3buVyHtRUZGybVgg7+sb/++pPRgcHER/fz+2bt0Kh8OB06dP4/rrr8dFF12EV155\nZU22d6+++mp0d3cDANxutzKJkyp0brcbbrcb8/PzmJ+fX9fE7VxBYS6PD5vNhoMHD+LgwYMAotHH\nq6++GhaLBR/96Efxl7/8BY8//jgAYNu2bSrZTDgcxvj4OHw+H2RZVhH31SLvJN9qeHgY9fX1aGtr\nW/OINJ2IumnTJmWcNHk/deoURFFEWVkZzGYzFhYWUrI+XCvQlUUPHDgAWZbx7W9/G08//TTuvfde\nXHrppWsyrsJ8vjKc9UQ9EVLRRZ1LuOSSS/BP//RPqqp5ZPVNZDOPPfYYpqamUF9fr/gC79ixA1ar\nFV6vF2NjY0p5aKfTWSDvZyl+98wBTE9P4/jx49iyZQva29sRDodx66234tixY3jkkUewa9euNRvf\n3r170d3dja6uLpSXlyuT9mWXXYYTJ07gyiuvBBBdfC4uLq7ZOAtYHRTmcjVI3YKdO3cqx4gr08mT\nJ5Vd1FOnToFhGGzfvl0lm4lEIgp5B6DSvGc7+XJpaQn9/f0oLS3F/v3786LCZTwwDAO73Y7FxUUs\nLS2ho6MDGzZsQCgUgs/nw/z8PIaHh8FxHIqLi1WR97WsRsvzvJIzRGSur732Gj772c/i8OHDOHbs\n2JpWcy3M5yvDurBnfPvb325o53Pttdfi2muvxd69e9HV1YUXX3yxkKiQAiRJgsfjUdxmenp6MD09\njS1btijkffv27bBarYrtEyHvdIITPTEUyHv+4Pknd6Cvrw8mk0mpLPrSSy/hlltuwUc/+lFcd911\neb+9e45j3dozFuby7IL4a7/++utKzY7Tp0+DZVns2LFDibw3NDQoZDRb5J34i3McZ2i3mI/wer3o\n6+tDWVkZmpubE2rWw+EwvF6v4l1OB7DIc6MDWLmAUbJoIBDA1772Nbzxxht45JFHsHXr1pxdv4AV\no2DPmEwXVYAxWJZFS0sLWlpacPXVVwOIkne3243jx4/j1VdfxcMPP4zZ2Vk0NjYqRZrq6+thsViw\ntLSE0dFRFXl/6uFGXdShQN5XF0bJotPT07j55pvBcRxeeOEF1NXVrfUwCyhAh8JcnhlIhPjiiy/G\nxRdfDCAm7yDk/ejRozh9+jRMJhN27typRN5ra2sRCoUwNjYGv98PIDXynqrdYj6BRKSDwSDOP//8\npIsK2vqwtrYWQPS5RiIRhbzT0lGavNOmDStBKBTCW2+9pSSLWiwW/PrXv8Ztt92GT33qU3jwwQfz\noiBUASvHuiTqRPcUTxdVQPpgWRatra1obW3Fhz70IQDRCdnlcqG7uxsvv/wyvvnNb2Jubg5NTU0K\ned+yZQvMZnNc8m6z2TAyMoJwOIyOjg584F/fWuM7XZ/QJouyLIsnn3wS3/nOd3DbbbcpDkIFFJBP\nKMzl2QfDMHA4HHjb296Gt73tbQCiJNPv9+O1117DiRMn8J3vfAdvvvkmzGYzdu3apUTejcg7nbAa\nDofh8XhQW1uLgwcP5j1RpLXzjY2N6OzszJhEMwwDm80Gm82GDRs2KMcJeff5fJiYmFAKFdK5AiUl\nJSlflyyEpqamlGTRiYkJ3HjjjbDZbPjd736nLB4KWB8466Uvzz33HD7+8Y/ju9/9rqJz2rdvH06c\nOAEgqnlqbm6G2+3GNddck5PrJ7IMI+25un6+QRRFuFwupSJfT08P5ufn0dLSokz227ZtgyiKOHny\nJEpKSmCxWFSRmkLkPXsgyaI+nw+dnZ1wOBzo6+vD9ddfj927d+P222+H0+lc62EWkB7WpfSlMJfn\nFwh57+npUSSQb775JqxWK3bt2qVE3jdv3oz+/n74fD5YrVaVBHK1q4WmA5/Ph76+PjidTjQ3N6+q\ndp7jOIW8e71ehEIhmEwmlXTUbrfryDtJFq2pqUFjYyNkWcYTTzyBJ598El//+tdx+eWXr9o9FJAV\npDSX///27j2mqfv9A/j7FAQpFMtlGgTFFQERBexPvE3djPUWo4sGs2XRxUVxf7moc1Y2dcl0YyJx\nm3FbVpY5Y2RfxjG7ODVKXYjz8kdLvTGHlRZE8YZcrK2IFM/vj3rOWq0rCKXt6fNKjAImfrT45uE5\nn8/zCfhC3Zc8XYvLjxTj91XGxsYG5Unmrq4uGI1GYc/78ePHcfPmTYwbNw6vvvoqsrOzkZmZCYZh\nhODydFiHinfPvv0sFrdv38bw4cORmJiIjo4OFBcX488//8Tu3btdxr/1JU8FD///BoBQkJEeEWWh\n7kuU5d3DcRwsFgvOnj0LvV6PM2fO4OTJkxg4cCBUKhVyc3Mxbtw4JCQkoL29HRaLxaXz7g/Fe2dn\nJ0wmE6xWK9LT0/2mUdHZ2Sl8/bNYLHjw4AFCQkKEjntrayvsdjsyMjIglUpx8eJFrFu3DlOnTsXH\nH38MqVTqlXVRnnsV7VH3tu6MDFOr1aioqAjqx7UhISHIyMhARkYG2tvb0dTUhLGJuYMAABCQSURB\nVEOHDsFmswknwYuKimCxWJCamip03l9++WUAji5CQ0ODS/Fe+vXLz4zJouLd4X/fpuDKlSu4desW\nBgwYgA0bNuD27dtobGzEnDlzwLKsV66JBro377qwsBDl5eUoKiqiMVzEL1CWdw/DMBg0aBBee+01\nTJs2Db///js+++wzLF68GOfOnYNOp8POnTtx+fJlREREICcnR/gxZMgQ2Gw2XLt2Dffv34dEInlm\nz7s3i3eO43Dr1i3U19cjOTkZ6enpfrV3fsCAAYiNjXWZbtTZ2YmGhgbU1dUhIiIC169fx7vvvotB\ngwahqakJ27Ztw6JFi7w2cYby3D9Qod4LnkaGKZVKKBQKxMTEoKSkpL+X55dWrlzp8th49OjRePvt\ntwE4LsO4fPkydDodjh49isLCQlitVowcOVKYNsMX762trc+MyQr24p0/LGo0GoXDos3NzYiOjkZX\nVxeWLl2K69evY8WKFXjvvfcwf/78Pl+Dp4KHZVnk5uYCgGhulySBj7K850JCQlBZWSkU1zNmzMCM\nGTMAOIritrY2YdtMcXExjEYjIiMjXbbN8MV7Q0MDrFary2zzvizerVYrampqEBUV5fcjInnt7e2o\nqalBWFgYJk+ejLCwMNy7dw8RERGYNm0akpKScOzYMezduxdHjhzxyhooz/0DFepexB+EKigoQH5+\nvhD2wey/Qjc0NBSZmZnIzMzE8uXLATiK93/++Qd6vR6HDx/Gp59+CpvNhrS0NCHsFQoFOI5zmXEr\nlUohk8mCpnh3d1i0tLQUu3btwubNm5GXl9cv3SNPBY9O5/i3NxgM0Gq1FO4kIFCWu/e8PGcYBjEx\nMZg5cyZmzpwJwFG8t7a2wmAwQKfTYceOHTAajYiKinqm8261Wp8p3vnOe2RkZLeLd7vdDpPJBIvF\ngvT0dJebSf2V89Sc9PR0xMTE4Pbt29i4cSPsdjsOHjwoXNDkbZTn/oEK9V7wNDJMo9GgoKAAcrkc\nCoUCLMvSJ3IPhYaGYuzYsRg7dizeeecdAI7wvXTpEnQ6Hf744w9s27YNNpsN6enpQvGekpKCrq4u\ntLS0oL6+Hp2dncLV0D99o4BMJhNF8e7uZtHa2lqsW7cOaWlp+OuvvzBo0CBfL9NFXFycsNeXZVna\n10h8jrLc+xiGQWxsLFQqlbB1iG+wVFVVQa/XY/v27bhy5QpkMpmwBTI7OxuDBw+GzWbD1atXu1W8\nO88X5y9086dtLs/jfFiUv4V2z5490Gg02Lp1KxYuXOjjFT6L8tz7qFDvBU/X4jrjDymR3gsNDUVW\nVhaysrKwYsUKAI69fH///Tf0ej1+++03fPLJJ2hvb3cp3hMSEtDV1YXm5mbU1dUJxbtMJsMXW6Sw\nWCxIS0sTvkj7c/F+rCwXTU1N0Ol0GDZsGNLS0tDZ2YmioiIcPnwYX331FSZPntzv6/JU8MTFxQmd\nSLlcDp1OR8FOfI6y3DcYhkFcXBxmz56N2bNnA3AU2c3NzULxfujQIdTW1kIul7t03l966SW3xXt4\neDiampogk8kCZpuL8xz3MWPGIDIyEjU1NVi7di2USiVOnTrlkwujKM/9AxXqveDpWtwNGzagqKgI\nCoUCLS0tPhkpFiwnsgcMGCAE+MqVKwE4wq+6uhp6vR6//vorzp07h46ODowaNUro1tTW1qKtrQ1Z\nWVkIDQ2F0Wh06bxHR0e7BL0/FO+/7x2L8+fPQyKRQKlUIjw8HGfOnIFarUZeXh5OnTrlsy9Ongqe\nvLw8sCwrvI/f3/iizGYzzGYzKioqkJubC7lcju+++w7l5eW9+4uQoOIPWQ5QngOO4j0+Ph5z5szB\nnDlzADiK97t376Kqqgo6nQ4HDx6EyWRCTEyMkOUpKSn45ZdfMGnSJMjlcmE6jfPksJ5sm+kPHMfh\nzp07MJvNwhz3jo4ObN26FZWVlfj66699ejiT8tw/0HjGAOZppBgALFmyRDiRrVKpgv5E9qNHj1Bd\nXY1jx46hpKQEHMdh8ODBUCgUQuc9PT0dnZ2dwpxb520z/A9fFO/8YdEbN24Ih0VbW1uxZcsWNDY2\n4ptvvvGLfbPu5l0/PQ87NjYWOp2u19fAa7VaqFQqsCyLsrIylJeXQ6PRiH3ONY1nFCHK857hOE54\nqrhv3z4cPXoUGRkZCAsLQ05ODpRKJbKyshAXFwer1QqLxQKbzQaJROKS5VKp1CfFu/Nh0dTUVISF\nhaGyshKbNm3CsmXLsHr1aoSG+r6XSnnuVTRHXezUajVmzZoFlUoFrVb7TBeGZVmYzWbaS+nG+vXr\noVKpMHfuXHR0dODixYvQ6/WoqqrC+fPnYbfbMXr0aKFbk5qaikePHglzbu12OyIjI126Nd4s3g98\nPwo1NTXCHlmJRIIDBw6guLgYarUab731VkDswfQWtVqN3Nxc0XYZn0KFughRnr+Y2tpafP755ygs\nLER8fDzu3LkDvV4PnU4Hg8GAuro6xMfHC7dlZ2dnIyYmxqV45+eVO+9591aeOh8WTUtLQ2xsLO7e\nvYuPPvoI9+7dw+7duzF8+HCv/NmBIojynOaoix2dyH5xxcXFwq/Dw8Mxfvx4lwuAOjo6cOHCBeh0\nOvz00084f/48urq6kJmZKXTe+UuE7t69C7PZjK6uLqHz/r9vUyCTyXpdvB8pVcJkMqGmpgYZGRmI\niopCfX093n//fSQmJqKystLlcyBYabVaFBQU+HoZhLwwyvMXM3LkSHz//ffC20OGDMH8+fOF8bP8\n/HT+wr2ysjJcvXoVgwcPdum8y+Vy3L9/H3V1dV4r3u/du4fLly8jPj4eEyZMAMMw2L9/P3bv3o0t\nW7Zg8eLFQd1w4VGeu6JCXeToRPaLCQ8PR25urrDnjuM4PHz4EBcuXIBer8e+fftQXV0NjuOQmZkp\ndN754r2pqQkmk+mFi3fnw6JJSUlITU2F3W7Hl19+iQMHDuCLL77A9OnT++Xfwl+ZzWZhr6TZbBYO\n/dHnOREryvOeYxgGCQkJWLBgARYsWADAkec3b94UivfS0lI0NDRgyJAhQiMmJycH0dHRuH//Psxm\ns8tNoT0t3vnbUG02GzIzMxEZGYna2lqsXbsWo0aNwokTJ/xuOld/ozx/PirUAxidyO4/DMMgIiIC\nEydOxMSJEwE4wr69vV3ovP/444+orq4GwzBui/c7d+64Ld6jo6Nd9iI+fPhQOCw6btw4hIeHQ6/X\n44MPPsD8+fNx+vRpr91EF0j4Pb1KpRLbt28XDjXR5zgJRJTn/YdhGAwdOhQLFy4URh5yHIcbN24I\n22b279+PhoYGDB06FNnZ2VAqlcjOzkZUVBSsVqtL8c4X7jKZzKV4dz4syt+G2tnZie3bt+PIkSPY\ntWsXJk2a5Mt/Cr9Bef58VKgHsP4+kf08niYV8IqKikT1uJZhGEilUkyaNEkIW754P3fuHPR6PX74\n4QdUV1dDIpFgzJgxQrcmKSkJDx8+fKZ4f/z4MaxWK1JSUpCQkACLxYIPP/wQRqMR+/btQ1pamtf+\nPp5eR/7jzoeKfIkCnIgJ5blvMQyDxMREJCYm4vXXXwfgyPPr168Lnfe9e/eisbERQ4cOFRoxOTk5\nkEqlsFqtaGpqEop3qdQx8nfgwIHIyclBREQETp8+jY0bN2LJkiVen85FeS4eVKgHME8jxRQKBeRy\nOViWRXNzs1dC1WAwAABUKhXMZjMMBoPbSQRarRYVFRWiCnZ3+OJ9ypQpmDJlCgBH2D948ABnz55F\nVVUVSkpKcOnSJUgkEmRlZQkhrtfrsWzZMsTFxWHr1q04ceIEHjx4gBkzZmDLli1ISEjw2ro9vY4G\ngwEKhUJ47P6815kQ8mIoz/0PwzAYNmwYhg0bhkWLFgFwHAa9du2aMCpyz549aGxsRFJSEnJycpCV\nlQWdTofU1FRMmDABbW1twlNYu92ONWvWYN68eQgJCfHauinPxYUK9QDn7jthfmyS88e99d1qWVkZ\nZs2aBQBQKBTQarX0H/4pDMMgMjISU6dOxdSpUwE4inebzYbTp0+jsLAQRqMRI0aMwNq1a5GamgqT\nyYRXXnkF+fn5MJvNYFkW165dw9KlS72yxu68jmq1GhUVFS7dPkJI36E8938SiQTJyclITk7G4sWL\nAfw7yaW0tBTr169HUlISjh8/joMHD0IqlSI8PBxr1qzBiBEjYDAYsHnzZuzduxcRERFeWSPlubhQ\noU56xdOkAsDx3btKper1jFUxYRgGUVFRePz4Md58803k5+eDYRhYrVacPHkStbW1WL16NQBg+vTp\nWL58uVfX4+l1VCqVUCgUiImJQUlJiVfXQgjxDcrzF8MX762traisrERaWhoeP36M+vp6lJaWYufO\nnUhOTgYAoYD2JspzcaFCnXgdf0CKPGvu3Lkub8tkMsybN89Hq3k+fp9sQUEB8vPzhaAnhAQXynP3\nGIbBjh07hLclEgkUCgU2bdrkw1W5R3keWKhQJ73iaVIB330h/s3T66jRaFBQUCBcuMSyrOj3pxIS\nbCjPxYHyXFz6/95cIipvvPEGzGYzgGcnFfDvY1kWGo0GLS0twiEX4l88vY7O8vLyhBm3hBDxoDwX\nB8pzcaFCnfQKf0DF3aQCwBEC/MEndyFB/IOn13HDhg3QaDTCF2l/GOdFCOlblOfiQHkuLgzHcT35\n/T36zYT0J09zYzUaDQDAZDLRQSgSyPrijnHKcuLXKM9JEOhWllNHnYiC89xYPtydabVaqFQqrFq1\nCmazGVqt1hfLJIQQ4gHlOSH/okKdiEJZWZmwz46fG+vMOcwVCoWwf48QQoh/oTwn5F9UqAcJlmWh\nVquFfYUGgwFqtdrHq+o7nubGrlq1StiHZzAYMH78+H5dn7f916EulmWh1WpRVFTUjysihHgL5bl4\n85yynDyNCvUgwLIs8vLyYDAYhJFNZWVlSElJ8fHK+h9/VbKYbtvTarVYsmSJ2495eoRMCAkslOf/\nElueU5YTd6hQDwJ5eXloa2uD2WwWLjXg9/iJhae5sTytViu6g0cqleq5l1V4eoRMCAkslOf/Elue\nU5YTd6hQDxI///yzMFYLgEvIi0F35sZqNBphekCwhFx3rgQnhAQWyvPgy3PK8uBFhXqQMJlMyM3N\nBeB4dCqm7gvgeW6sVquFWq1GSkoKYmJivLoWT/sIaZ8hIaQ3KM8pz0nw6OkcdRKgGIZRAHgXgO7J\nz+Ucx2l8uyrxYRhGCUDBcRzLMMwqAHqO4wzd/Xgv/twKjuNmuXn/dgAVHMdpGYbJe/Jn01cUQgIY\n5Xn/8EWeU5aTp1FHPUhwHGfmOE7NcRwLIBbAz75ek0i9AYB/PmsG8HSry9PH+wTDMPyd0GUA+Gfi\nCgDif0ZMiMhRnvcbn+c5ZTmhQj0IMAyjYBim/MmvVXB810/3P3uHHECL09tPn4Ly9PEee9JdGf/k\nZ95xAOC7O09e97a+6N4TQnyH8rxf9WueU5YTd0J9vQDSL1oAlDk9LnvX1wsifedJV4196n3/5/Rr\neiROiHhQnosUZTlxhwr1IPCk28J6/I2kL7TB8SgacHRbnj6a7+njhBDyXJTn/YrynPgcbX0hpG+5\n3UdI+wwJISTgUJ4Tn6NCnZA+9B/7CGmfISGEBBDKc+IPaDwjIYQQQgghfog66oQQQgghhPghKtQJ\nIYQQQgjxQ1SoE0IIIYQQ4of+H1oc1XrLDEUtAAAAAElFTkSuQmCC\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x7fa64296eef0>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "evaluation_points = np.column_stack((field50.x, field50.y))\n",
    "customModel.mlp.read_weights_from_disk(\"./customFunctionLearningAdaptive/baselineMLPFinal/best_weights.npy\")\n",
    "prediction = customModel.predict(evaluation_points)\n",
    "plot_field_and_error(field50.x, field50.y, prediction, field50.A, \"customFunctionBaselineFinal.pdf\",\n",
    "                     [r\"Custom function after $6\\cdot 10^3$ iterations\", \"Deviation from the numerical solution\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Initialized weights and biases for MLP with 20 parameters.\n",
      "\n",
      "MLP structure:\n",
      "--------------\n",
      "Input features:    2\n",
      "Hidden layers:     1\n",
      "Neurons per layer: 5\n",
      "Number of weights: 20\n",
      "Loaded weights from file ./customFunctionLearningAdaptive/baselineMLPFinal/best_weights.npy\n",
      "-------------------\n",
      "L2 norm:  0.00010439125648652941\n",
      "Lmax norm:  0.050723566039886314\n"
     ]
    }
   ],
   "source": [
    "baselineMLP = SimpleMLP(name='baselineMLPFinal', number_of_inputs=2, neurons_per_layer=5, hidden_layers=1)\n",
    "customModel = CustomFunctionLearning(name=\"customFunctionLearningAdaptive\", mlp=baselineMLP, training_data=field50.A, beta=25.0, d_v=0.025)\n",
    "customModel.mlp.read_weights_from_disk(\"./customFunctionLearningAdaptive/baselineMLPFinal/best_weights.npy\")\n",
    "evaluation_points = np.column_stack((field50.x, field50.y))\n",
    "l2_norm = customModel.l2_norm(evaluation_points, field50.A)\n",
    "lmax_norm = customModel.lmax_norm(evaluation_points, field50.A)\n",
    "print(\"-------------------\")\n",
    "print(\"L2 norm: \", l2_norm)\n",
    "print(\"Lmax norm: \", lmax_norm)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Pure network learning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Initialized weights and biases for MLP with 20 parameters.\n",
      "\n",
      "MLP structure:\n",
      "--------------\n",
      "Input features:    2\n",
      "Hidden layers:     1\n",
      "Neurons per layer: 5\n",
      "Number of weights: 20\n",
      "Selecting subset for training: 2 out of 2700 selected.\n",
      "\n",
      "The initial loss is 21.519028024986326\n",
      "\n",
      "Wrote weights to file ./pureNetworkLearningNoP/baselineMLPFinal/best_weights\n",
      "The loss after 100 iterations is E=0.3201399397845033\n",
      "Wrote weights to file ./pureNetworkLearningNoP/baselineMLPFinal/best_weights\n",
      "The loss after 200 iterations is E=0.3031473168140498\n",
      "Wrote weights to file ./pureNetworkLearningNoP/baselineMLPFinal/best_weights\n",
      "The loss after 300 iterations is E=0.28363602964543777\n",
      "Wrote weights to file ./pureNetworkLearningNoP/baselineMLPFinal/best_weights\n",
      "The loss after 400 iterations is E=0.2634707813074062\n",
      "Wrote weights to file ./pureNetworkLearningNoP/baselineMLPFinal/best_weights\n",
      "The loss after 500 iterations is E=0.24355516399053895\n",
      "Wrote weights to file ./pureNetworkLearningNoP/baselineMLPFinal/best_weights\n",
      "The loss after 600 iterations is E=0.22417587418857787\n",
      "Wrote weights to file ./pureNetworkLearningNoP/baselineMLPFinal/best_weights\n",
      "The loss after 700 iterations is E=0.20521242468136985\n",
      "Wrote weights to file ./pureNetworkLearningNoP/baselineMLPFinal/best_weights\n",
      "The loss after 800 iterations is E=0.18627759050835535\n",
      "Wrote weights to file ./pureNetworkLearningNoP/baselineMLPFinal/best_weights\n",
      "The loss after 900 iterations is E=0.16682831437332352\n",
      "Wrote weights to file ./pureNetworkLearningNoP/baselineMLPFinal/best_weights\n",
      "The loss after 1000 iterations is E=0.1462907977370781\n",
      "Wrote weights to file ./pureNetworkLearningNoP/baselineMLPFinal/best_weights\n",
      "The loss after 1100 iterations is E=0.12425197674218891\n",
      "Re-setting points: using every 1000th point for training.\n",
      "Selecting subset for training: 2 out of 2700 selected.\n",
      "Wrote weights to file ./pureNetworkLearningNoP/baselineMLPFinal/best_weights\n",
      "The loss after 1200 iterations is E=0.0009783916824938122\n",
      "Using every 900th point for                         training.\n",
      "Selecting subset for training: 3 out of 2700 selected.\n",
      "Wrote weights to file ./pureNetworkLearningNoP/baselineMLPFinal/best_weights\n",
      "The loss after 1300 iterations is E=0.03049641382078385\n",
      "Wrote weights to file ./pureNetworkLearningNoP/baselineMLPFinal/best_weights\n",
      "The loss after 1400 iterations is E=0.027824352783933973\n",
      "Wrote weights to file ./pureNetworkLearningNoP/baselineMLPFinal/best_weights\n",
      "The loss after 1500 iterations is E=0.025075236566868966\n",
      "Wrote weights to file ./pureNetworkLearningNoP/baselineMLPFinal/best_weights\n",
      "The loss after 1600 iterations is E=0.022295920353697093\n",
      "Wrote weights to file ./pureNetworkLearningNoP/baselineMLPFinal/best_weights\n",
      "The loss after 1700 iterations is E=0.019528435815930147\n",
      "Wrote weights to file ./pureNetworkLearningNoP/baselineMLPFinal/best_weights\n",
      "The loss after 1800 iterations is E=0.016805910630529778\n",
      "Wrote weights to file ./pureNetworkLearningNoP/baselineMLPFinal/best_weights\n",
      "The loss after 1900 iterations is E=0.014162451455426259\n",
      "Wrote weights to file ./pureNetworkLearningNoP/baselineMLPFinal/best_weights\n",
      "The loss after 2000 iterations is E=0.011639876269291284\n",
      "Wrote weights to file ./pureNetworkLearningNoP/baselineMLPFinal/best_weights\n",
      "The loss after 2100 iterations is E=0.009289709477456151\n",
      "Using every 810th point for                         training.\n",
      "Selecting subset for training: 3 out of 2700 selected.\n",
      "Wrote weights to file ./pureNetworkLearningNoP/baselineMLPFinal/best_weights\n",
      "The loss after 2200 iterations is E=0.0012721047415294813\n",
      "Using every 729th point for                         training.\n",
      "Selecting subset for training: 3 out of 2700 selected.\n",
      "Wrote weights to file ./pureNetworkLearningNoP/baselineMLPFinal/best_weights\n",
      "The loss after 2300 iterations is E=9.844818534659216e-05\n",
      "Using every 656th point for                         training.\n",
      "Selecting subset for training: 4 out of 2700 selected.\n",
      "Wrote weights to file ./pureNetworkLearningNoP/baselineMLPFinal/best_weights\n",
      "The loss after 2400 iterations is E=0.02582692722600013\n",
      "Wrote weights to file ./pureNetworkLearningNoP/baselineMLPFinal/best_weights\n",
      "The loss after 2500 iterations is E=0.017703728487882448\n",
      "Wrote weights to file ./pureNetworkLearningNoP/baselineMLPFinal/best_weights\n",
      "The loss after 2600 iterations is E=0.013488716173821969\n",
      "Wrote weights to file ./pureNetworkLearningNoP/baselineMLPFinal/best_weights\n",
      "The loss after 2700 iterations is E=0.011587873575157632\n",
      "Wrote weights to file ./pureNetworkLearningNoP/baselineMLPFinal/best_weights\n",
      "The loss after 2800 iterations is E=0.010854147566596441\n",
      "Wrote weights to file ./pureNetworkLearningNoP/baselineMLPFinal/best_weights\n",
      "The loss after 2900 iterations is E=0.010602328023037098\n",
      "Wrote weights to file ./pureNetworkLearningNoP/baselineMLPFinal/best_weights\n",
      "The loss after 3000 iterations is E=0.010518824055731402\n",
      "Wrote weights to file ./pureNetworkLearningNoP/baselineMLPFinal/best_weights\n",
      "The loss after 3100 iterations is E=0.010486127738774217\n",
      "Wrote weights to file ./pureNetworkLearningNoP/baselineMLPFinal/best_weights\n",
      "The loss after 3200 iterations is E=0.010466535135014962\n",
      "Wrote weights to file ./pureNetworkLearningNoP/baselineMLPFinal/best_weights\n",
      "The loss after 3300 iterations is E=0.010449389078069114\n",
      "Wrote weights to file ./pureNetworkLearningNoP/baselineMLPFinal/best_weights\n",
      "The loss after 3400 iterations is E=0.010431978760404944\n",
      "Re-setting points: using every 656th point for training.\n",
      "Selecting subset for training: 4 out of 2700 selected.\n",
      "Wrote weights to file ./pureNetworkLearningNoP/baselineMLPFinal/best_weights\n",
      "The loss after 3500 iterations is E=0.004150162573545325\n",
      "Using every 590th point for                         training.\n",
      "Selecting subset for training: 4 out of 2700 selected.\n",
      "Wrote weights to file ./pureNetworkLearningNoP/baselineMLPFinal/best_weights\n",
      "The loss after 3600 iterations is E=0.0006896385119693728\n",
      "Using every 531th point for                         training.\n",
      "Selecting subset for training: 5 out of 2700 selected.\n",
      "Wrote weights to file ./pureNetworkLearningNoP/baselineMLPFinal/best_weights\n",
      "The loss after 3700 iterations is E=0.0009302792629542126\n",
      "Using every 477th point for                         training.\n",
      "Selecting subset for training: 5 out of 2700 selected.\n",
      "Wrote weights to file ./pureNetworkLearningNoP/baselineMLPFinal/best_weights\n",
      "The loss after 3800 iterations is E=0.004730084115117922\n",
      "Using every 429th point for                         training.\n",
      "Selecting subset for training: 6 out of 2700 selected.\n",
      "Wrote weights to file ./pureNetworkLearningNoP/baselineMLPFinal/best_weights\n",
      "The loss after 3900 iterations is E=0.002461507909870438\n",
      "Using every 386th point for                         training.\n",
      "Selecting subset for training: 6 out of 2700 selected.\n",
      "Wrote weights to file ./pureNetworkLearningNoP/baselineMLPFinal/best_weights\n",
      "The loss after 4000 iterations is E=0.0014017455164762153\n",
      "Using every 347th point for                         training.\n",
      "Selecting subset for training: 7 out of 2700 selected.\n",
      "Wrote weights to file ./pureNetworkLearningNoP/baselineMLPFinal/best_weights\n",
      "The loss after 4100 iterations is E=0.0014908353757320458\n",
      "Using every 312th point for                         training.\n",
      "Selecting subset for training: 8 out of 2700 selected.\n",
      "Wrote weights to file ./pureNetworkLearningNoP/baselineMLPFinal/best_weights\n",
      "The loss after 4200 iterations is E=0.0015825537821417123\n",
      "Using every 280th point for                         training.\n",
      "Selecting subset for training: 9 out of 2700 selected.\n",
      "Wrote weights to file ./pureNetworkLearningNoP/baselineMLPFinal/best_weights\n",
      "The loss after 4300 iterations is E=0.006910903040667188\n",
      "Using every 252th point for                         training.\n",
      "Selecting subset for training: 10 out of 2700 selected.\n",
      "Wrote weights to file ./pureNetworkLearningNoP/baselineMLPFinal/best_weights\n",
      "The loss after 4400 iterations is E=0.0008199492749687344\n",
      "Using every 226th point for                         training.\n",
      "Selecting subset for training: 11 out of 2700 selected.\n",
      "Wrote weights to file ./pureNetworkLearningNoP/baselineMLPFinal/best_weights\n",
      "The loss after 4500 iterations is E=0.004188780915084564\n",
      "Using every 203th point for                         training.\n",
      "Selecting subset for training: 13 out of 2700 selected.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wrote weights to file ./pureNetworkLearningNoP/baselineMLPFinal/best_weights\n",
      "The loss after 4600 iterations is E=0.014177799740330985\n",
      "Wrote weights to file ./pureNetworkLearningNoP/baselineMLPFinal/best_weights\n",
      "The loss after 4700 iterations is E=0.013118440525076614\n",
      "Wrote weights to file ./pureNetworkLearningNoP/baselineMLPFinal/best_weights\n",
      "The loss after 4800 iterations is E=0.012385859707219919\n",
      "Wrote weights to file ./pureNetworkLearningNoP/baselineMLPFinal/best_weights\n",
      "The loss after 4900 iterations is E=0.011821328584896592\n",
      "Wrote weights to file ./pureNetworkLearningNoP/baselineMLPFinal/best_weights\n",
      "The loss after 5000 iterations is E=0.011346025958509481\n",
      "Wrote weights to file ./pureNetworkLearningNoP/baselineMLPFinal/best_weights\n",
      "The loss after 5100 iterations is E=0.01091504151657842\n",
      "Wrote weights to file ./pureNetworkLearningNoP/baselineMLPFinal/best_weights\n",
      "The loss after 5200 iterations is E=0.010506173085707317\n",
      "Wrote weights to file ./pureNetworkLearningNoP/baselineMLPFinal/best_weights\n",
      "The loss after 5300 iterations is E=0.010110622205027552\n",
      "Wrote weights to file ./pureNetworkLearningNoP/baselineMLPFinal/best_weights\n",
      "The loss after 5400 iterations is E=0.009726208355313331\n",
      "Using every 182th point for                         training.\n",
      "Selecting subset for training: 14 out of 2700 selected.\n",
      "Wrote weights to file ./pureNetworkLearningNoP/baselineMLPFinal/best_weights\n",
      "The loss after 5500 iterations is E=0.004709721910898535\n",
      "Using every 163th point for                         training.\n",
      "Selecting subset for training: 16 out of 2700 selected.\n",
      "Wrote weights to file ./pureNetworkLearningNoP/baselineMLPFinal/best_weights\n",
      "The loss after 5600 iterations is E=0.21929639729091654\n",
      "Wrote weights to file ./pureNetworkLearningNoP/baselineMLPFinal/best_weights\n",
      "The loss after 5700 iterations is E=0.1976177504431751\n",
      "Wrote weights to file ./pureNetworkLearningNoP/baselineMLPFinal/best_weights\n",
      "The loss after 5800 iterations is E=0.18327726248242474\n",
      "Wrote weights to file ./pureNetworkLearningNoP/baselineMLPFinal/best_weights\n",
      "The loss after 5900 iterations is E=0.16699977852880662\n",
      "Wrote weights to file ./pureNetworkLearningNoP/baselineMLPFinal/best_weights\n",
      "The loss after 6000 iterations is E=0.14699032594742084\n",
      "Wrote weights to file ./pureNetworkLearningNoP/baselineMLPFinal/best_weights\n",
      "The loss after 6100 iterations is E=0.12317451021429496\n",
      "Wrote weights to file ./pureNetworkLearningNoP/baselineMLPFinal/best_weights\n",
      "The loss after 6200 iterations is E=0.09643182245843553\n",
      "Wrote weights to file ./pureNetworkLearningNoP/baselineMLPFinal/best_weights\n",
      "The loss after 6300 iterations is E=0.06896411002267014\n",
      "Wrote weights to file ./pureNetworkLearningNoP/baselineMLPFinal/best_weights\n",
      "The loss after 6400 iterations is E=0.04417623549108864\n",
      "Wrote weights to file ./pureNetworkLearningNoP/baselineMLPFinal/best_weights\n",
      "The loss after 6500 iterations is E=0.02522482777815604\n",
      "Wrote weights to file ./pureNetworkLearningNoP/baselineMLPFinal/best_weights\n",
      "The loss after 6600 iterations is E=0.013168567917911534\n",
      "Re-setting points: using every 163th point for training.\n",
      "Selecting subset for training: 16 out of 2700 selected.\n",
      "Wrote weights to file ./pureNetworkLearningNoP/baselineMLPFinal/best_weights\n",
      "The loss after 6700 iterations is E=0.00824739846255646\n",
      "Using every 146th point for                         training.\n",
      "Selecting subset for training: 18 out of 2700 selected.\n",
      "Wrote weights to file ./pureNetworkLearningNoP/baselineMLPFinal/best_weights\n",
      "The loss after 6800 iterations is E=0.007866747172368089\n",
      "Using every 131th point for                         training.\n",
      "Selecting subset for training: 20 out of 2700 selected.\n",
      "Wrote weights to file ./pureNetworkLearningNoP/baselineMLPFinal/best_weights\n",
      "The loss after 6900 iterations is E=0.036482689658370586\n",
      "Wrote weights to file ./pureNetworkLearningNoP/baselineMLPFinal/best_weights\n",
      "The loss after 7000 iterations is E=0.027194244584261885\n",
      "Wrote weights to file ./pureNetworkLearningNoP/baselineMLPFinal/best_weights\n",
      "The loss after 7100 iterations is E=0.022742649501611513\n",
      "Wrote weights to file ./pureNetworkLearningNoP/baselineMLPFinal/best_weights\n",
      "The loss after 7200 iterations is E=0.019775802370588864\n",
      "Wrote weights to file ./pureNetworkLearningNoP/baselineMLPFinal/best_weights\n",
      "The loss after 7300 iterations is E=0.017490582866879344\n",
      "Wrote weights to file ./pureNetworkLearningNoP/baselineMLPFinal/best_weights\n",
      "The loss after 7400 iterations is E=0.01559499194809632\n",
      "Wrote weights to file ./pureNetworkLearningNoP/baselineMLPFinal/best_weights\n",
      "The loss after 7500 iterations is E=0.013955822812693515\n",
      "Wrote weights to file ./pureNetworkLearningNoP/baselineMLPFinal/best_weights\n",
      "The loss after 7600 iterations is E=0.012505737120496517\n",
      "Wrote weights to file ./pureNetworkLearningNoP/baselineMLPFinal/best_weights\n",
      "The loss after 7700 iterations is E=0.011209665313287662\n",
      "Wrote weights to file ./pureNetworkLearningNoP/baselineMLPFinal/best_weights\n",
      "The loss after 7800 iterations is E=0.01004958397361188\n",
      "Wrote weights to file ./pureNetworkLearningNoP/baselineMLPFinal/best_weights\n",
      "The loss after 7900 iterations is E=0.009016457272424802\n",
      "Using every 117th point for                         training.\n",
      "Selecting subset for training: 23 out of 2700 selected.\n",
      "Wrote weights to file ./pureNetworkLearningNoP/baselineMLPFinal/best_weights\n",
      "The loss after 8000 iterations is E=0.008730455686313348\n",
      "Using every 105th point for                         training.\n",
      "Selecting subset for training: 25 out of 2700 selected.\n",
      "Wrote weights to file ./pureNetworkLearningNoP/baselineMLPFinal/best_weights\n",
      "The loss after 8100 iterations is E=0.020511914261884964\n",
      "Wrote weights to file ./pureNetworkLearningNoP/baselineMLPFinal/best_weights\n",
      "The loss after 8200 iterations is E=0.017851162563215568\n",
      "Wrote weights to file ./pureNetworkLearningNoP/baselineMLPFinal/best_weights\n",
      "The loss after 8300 iterations is E=0.016562385638275053\n",
      "Wrote weights to file ./pureNetworkLearningNoP/baselineMLPFinal/best_weights\n",
      "The loss after 8400 iterations is E=0.01573088540857088\n",
      "Wrote weights to file ./pureNetworkLearningNoP/baselineMLPFinal/best_weights\n",
      "The loss after 8500 iterations is E=0.015044405250374026\n",
      "Wrote weights to file ./pureNetworkLearningNoP/baselineMLPFinal/best_weights\n",
      "The loss after 8600 iterations is E=0.014411671118347581\n",
      "Wrote weights to file ./pureNetworkLearningNoP/baselineMLPFinal/best_weights\n",
      "The loss after 8700 iterations is E=0.013807336923109727\n",
      "Wrote weights to file ./pureNetworkLearningNoP/baselineMLPFinal/best_weights\n",
      "The loss after 8800 iterations is E=0.013223412010247222\n",
      "Wrote weights to file ./pureNetworkLearningNoP/baselineMLPFinal/best_weights\n",
      "The loss after 8900 iterations is E=0.012656400644131717\n",
      "Wrote weights to file ./pureNetworkLearningNoP/baselineMLPFinal/best_weights\n",
      "The loss after 9000 iterations is E=0.01210416316010685\n",
      "Wrote weights to file ./pureNetworkLearningNoP/baselineMLPFinal/best_weights\n",
      "The loss after 9100 iterations is E=0.011565117771600304\n",
      "Re-setting points: using every 105th point for training.\n",
      "Selecting subset for training: 25 out of 2700 selected.\n",
      "Re-setting points: using every 105th point for training.\n",
      "Selecting subset for training: 25 out of 2700 selected.\n",
      "Re-setting points: using every 105th point for training.\n",
      "Selecting subset for training: 25 out of 2700 selected.\n",
      "Re-setting points: using every 105th point for training.\n",
      "Selecting subset for training: 25 out of 2700 selected.\n",
      "Wrote weights to file ./pureNetworkLearningNoP/baselineMLPFinal/best_weights\n",
      "The loss after 12500 iterations is E=0.010247618179836883\n",
      "Wrote weights to file ./pureNetworkLearningNoP/baselineMLPFinal/best_weights\n",
      "The loss after 12600 iterations is E=0.009407465590296087\n",
      "Using every 94th point for                         training.\n",
      "Selecting subset for training: 28 out of 2700 selected.\n",
      "Wrote weights to file ./pureNetworkLearningNoP/baselineMLPFinal/best_weights\n",
      "The loss after 12700 iterations is E=0.029286285391130606\n",
      "Wrote weights to file ./pureNetworkLearningNoP/baselineMLPFinal/best_weights\n",
      "The loss after 12800 iterations is E=0.024807784466667374\n",
      "Wrote weights to file ./pureNetworkLearningNoP/baselineMLPFinal/best_weights\n",
      "The loss after 12900 iterations is E=0.02229564396001016\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wrote weights to file ./pureNetworkLearningNoP/baselineMLPFinal/best_weights\n",
      "The loss after 13000 iterations is E=0.020836905590882502\n",
      "Wrote weights to file ./pureNetworkLearningNoP/baselineMLPFinal/best_weights\n",
      "The loss after 13100 iterations is E=0.01975033526735414\n",
      "Wrote weights to file ./pureNetworkLearningNoP/baselineMLPFinal/best_weights\n",
      "The loss after 13200 iterations is E=0.018628248550572494\n",
      "Wrote weights to file ./pureNetworkLearningNoP/baselineMLPFinal/best_weights\n",
      "The loss after 13300 iterations is E=0.017192665160371053\n",
      "Wrote weights to file ./pureNetworkLearningNoP/baselineMLPFinal/best_weights\n",
      "The loss after 13400 iterations is E=0.015314389012410553\n",
      "Wrote weights to file ./pureNetworkLearningNoP/baselineMLPFinal/best_weights\n",
      "The loss after 13500 iterations is E=0.01293658925761635\n",
      "Wrote weights to file ./pureNetworkLearningNoP/baselineMLPFinal/best_weights\n",
      "The loss after 13600 iterations is E=0.010531742060469109\n",
      "Wrote weights to file ./pureNetworkLearningNoP/baselineMLPFinal/best_weights\n",
      "The loss after 13700 iterations is E=0.008763633514235902\n",
      "Using every 84th point for                         training.\n",
      "Selecting subset for training: 32 out of 2700 selected.\n",
      "Wrote weights to file ./pureNetworkLearningNoP/baselineMLPFinal/best_weights\n",
      "The loss after 13800 iterations is E=0.11324423383498525\n",
      "Wrote weights to file ./pureNetworkLearningNoP/baselineMLPFinal/best_weights\n",
      "The loss after 13900 iterations is E=0.07857246464282536\n",
      "Wrote weights to file ./pureNetworkLearningNoP/baselineMLPFinal/best_weights\n",
      "The loss after 14000 iterations is E=0.06451483820801623\n",
      "Wrote weights to file ./pureNetworkLearningNoP/baselineMLPFinal/best_weights\n",
      "The loss after 14100 iterations is E=0.05654286000372075\n",
      "Wrote weights to file ./pureNetworkLearningNoP/baselineMLPFinal/best_weights\n",
      "The loss after 14200 iterations is E=0.05119271066338706\n",
      "Wrote weights to file ./pureNetworkLearningNoP/baselineMLPFinal/best_weights\n",
      "The loss after 14300 iterations is E=0.04716890303850763\n",
      "Wrote weights to file ./pureNetworkLearningNoP/baselineMLPFinal/best_weights\n",
      "The loss after 14400 iterations is E=0.04388939805582923\n",
      "Wrote weights to file ./pureNetworkLearningNoP/baselineMLPFinal/best_weights\n",
      "The loss after 14500 iterations is E=0.04106524578545879\n",
      "Wrote weights to file ./pureNetworkLearningNoP/baselineMLPFinal/best_weights\n",
      "The loss after 14600 iterations is E=0.03854324235322222\n",
      "Wrote weights to file ./pureNetworkLearningNoP/baselineMLPFinal/best_weights\n",
      "The loss after 14700 iterations is E=0.03623784596372595\n",
      "Wrote weights to file ./pureNetworkLearningNoP/baselineMLPFinal/best_weights\n",
      "The loss after 14800 iterations is E=0.034099002315418875\n",
      "Re-setting points: using every 84th point for training.\n",
      "Selecting subset for training: 32 out of 2700 selected.\n",
      "Wrote weights to file ./pureNetworkLearningNoP/baselineMLPFinal/best_weights\n",
      "The loss after 14900 iterations is E=0.023637836620278734\n",
      "Wrote weights to file ./pureNetworkLearningNoP/baselineMLPFinal/best_weights\n",
      "The loss after 15000 iterations is E=0.01713255370386388\n",
      "Wrote weights to file ./pureNetworkLearningNoP/baselineMLPFinal/best_weights\n",
      "The loss after 15100 iterations is E=0.015425867186883678\n",
      "Wrote weights to file ./pureNetworkLearningNoP/baselineMLPFinal/best_weights\n",
      "The loss after 15200 iterations is E=0.014310957239011391\n",
      "Wrote weights to file ./pureNetworkLearningNoP/baselineMLPFinal/best_weights\n",
      "The loss after 15300 iterations is E=0.01332871138255562\n",
      "Wrote weights to file ./pureNetworkLearningNoP/baselineMLPFinal/best_weights\n",
      "The loss after 15400 iterations is E=0.01240251051199964\n",
      "Wrote weights to file ./pureNetworkLearningNoP/baselineMLPFinal/best_weights\n",
      "The loss after 15500 iterations is E=0.011512655578427063\n",
      "Wrote weights to file ./pureNetworkLearningNoP/baselineMLPFinal/best_weights\n",
      "The loss after 15600 iterations is E=0.010652117754261644\n",
      "Wrote weights to file ./pureNetworkLearningNoP/baselineMLPFinal/best_weights\n",
      "The loss after 15700 iterations is E=0.00981794351395799\n",
      "Using every 75th point for                         training.\n",
      "Selecting subset for training: 36 out of 2700 selected.\n",
      "Wrote weights to file ./pureNetworkLearningNoP/baselineMLPFinal/best_weights\n",
      "The loss after 15800 iterations is E=0.017328447254170175\n",
      "Wrote weights to file ./pureNetworkLearningNoP/baselineMLPFinal/best_weights\n",
      "The loss after 15900 iterations is E=0.015869236975307503\n",
      "Wrote weights to file ./pureNetworkLearningNoP/baselineMLPFinal/best_weights\n",
      "The loss after 16000 iterations is E=0.014673941915563505\n",
      "Wrote weights to file ./pureNetworkLearningNoP/baselineMLPFinal/best_weights\n",
      "The loss after 16100 iterations is E=0.013665577523012042\n",
      "Wrote weights to file ./pureNetworkLearningNoP/baselineMLPFinal/best_weights\n",
      "The loss after 16200 iterations is E=0.012811156505216923\n",
      "Wrote weights to file ./pureNetworkLearningNoP/baselineMLPFinal/best_weights\n",
      "The loss after 16300 iterations is E=0.012085195276874964\n",
      "Wrote weights to file ./pureNetworkLearningNoP/baselineMLPFinal/best_weights\n",
      "The loss after 16400 iterations is E=0.011465629223784458\n",
      "Wrote weights to file ./pureNetworkLearningNoP/baselineMLPFinal/best_weights\n",
      "The loss after 16500 iterations is E=0.010933146713312043\n",
      "Wrote weights to file ./pureNetworkLearningNoP/baselineMLPFinal/best_weights\n",
      "The loss after 16600 iterations is E=0.010470981472742742\n",
      "Wrote weights to file ./pureNetworkLearningNoP/baselineMLPFinal/best_weights\n",
      "The loss after 16700 iterations is E=0.010064792959531672\n",
      "Wrote weights to file ./pureNetworkLearningNoP/baselineMLPFinal/best_weights\n",
      "The loss after 16800 iterations is E=0.009702558635941909\n",
      "Using every 67th point for                         training.\n",
      "Selecting subset for training: 40 out of 2700 selected.\n",
      "Wrote weights to file ./pureNetworkLearningNoP/baselineMLPFinal/best_weights\n",
      "The loss after 16900 iterations is E=0.011308193978330465\n",
      "Wrote weights to file ./pureNetworkLearningNoP/baselineMLPFinal/best_weights\n",
      "The loss after 17000 iterations is E=0.00963532246320333\n",
      "Using every 60th point for                         training.\n",
      "Selecting subset for training: 45 out of 2700 selected.\n",
      "Wrote weights to file ./pureNetworkLearningNoP/baselineMLPFinal/best_weights\n",
      "The loss after 17100 iterations is E=0.002498350621412683\n",
      "Using every 54th point for                         training.\n",
      "Selecting subset for training: 50 out of 2700 selected.\n",
      "Wrote weights to file ./pureNetworkLearningNoP/baselineMLPFinal/best_weights\n",
      "The loss after 17200 iterations is E=0.006921143553500995\n",
      "Using every 48th point for                         training.\n",
      "Selecting subset for training: 56 out of 2700 selected.\n",
      "Wrote weights to file ./pureNetworkLearningNoP/baselineMLPFinal/best_weights\n",
      "The loss after 17300 iterations is E=0.1085316834990128\n",
      "Wrote weights to file ./pureNetworkLearningNoP/baselineMLPFinal/best_weights\n",
      "The loss after 17400 iterations is E=0.09361479628630598\n",
      "Wrote weights to file ./pureNetworkLearningNoP/baselineMLPFinal/best_weights\n",
      "The loss after 17500 iterations is E=0.0802591060196482\n",
      "Wrote weights to file ./pureNetworkLearningNoP/baselineMLPFinal/best_weights\n",
      "The loss after 17600 iterations is E=0.06904221769739079\n",
      "Wrote weights to file ./pureNetworkLearningNoP/baselineMLPFinal/best_weights\n",
      "The loss after 17700 iterations is E=0.06030725916539847\n",
      "Wrote weights to file ./pureNetworkLearningNoP/baselineMLPFinal/best_weights\n",
      "The loss after 17800 iterations is E=0.0538529208875127\n",
      "Wrote weights to file ./pureNetworkLearningNoP/baselineMLPFinal/best_weights\n",
      "The loss after 17900 iterations is E=0.049056930839864434\n",
      "Wrote weights to file ./pureNetworkLearningNoP/baselineMLPFinal/best_weights\n",
      "The loss after 18000 iterations is E=0.04526157102926912\n",
      "Wrote weights to file ./pureNetworkLearningNoP/baselineMLPFinal/best_weights\n",
      "The loss after 18100 iterations is E=0.04201928466887727\n",
      "Wrote weights to file ./pureNetworkLearningNoP/baselineMLPFinal/best_weights\n",
      "The loss after 18200 iterations is E=0.03909992301284398\n",
      "Wrote weights to file ./pureNetworkLearningNoP/baselineMLPFinal/best_weights\n",
      "The loss after 18300 iterations is E=0.03640505134830317\n",
      "Re-setting points: using every 48th point for training.\n",
      "Selecting subset for training: 56 out of 2700 selected.\n",
      "Wrote weights to file ./pureNetworkLearningNoP/baselineMLPFinal/best_weights\n",
      "The loss after 18400 iterations is E=0.011500369874707591\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wrote weights to file ./pureNetworkLearningNoP/baselineMLPFinal/best_weights\n",
      "The loss after 18500 iterations is E=0.008153423725063847\n",
      "Using every 43th point for                         training.\n",
      "Selecting subset for training: 62 out of 2700 selected.\n",
      "Wrote weights to file ./pureNetworkLearningNoP/baselineMLPFinal/best_weights\n",
      "The loss after 18600 iterations is E=0.046522417999939024\n",
      "Wrote weights to file ./pureNetworkLearningNoP/baselineMLPFinal/best_weights\n",
      "The loss after 18700 iterations is E=0.03474380304179608\n",
      "Wrote weights to file ./pureNetworkLearningNoP/baselineMLPFinal/best_weights\n",
      "The loss after 18800 iterations is E=0.02807128642825682\n",
      "Wrote weights to file ./pureNetworkLearningNoP/baselineMLPFinal/best_weights\n",
      "The loss after 18900 iterations is E=0.02301815012791361\n",
      "Wrote weights to file ./pureNetworkLearningNoP/baselineMLPFinal/best_weights\n",
      "The loss after 19000 iterations is E=0.018999073366014634\n",
      "Wrote weights to file ./pureNetworkLearningNoP/baselineMLPFinal/best_weights\n",
      "The loss after 19100 iterations is E=0.01583196641228168\n",
      "Wrote weights to file ./pureNetworkLearningNoP/baselineMLPFinal/best_weights\n",
      "The loss after 19200 iterations is E=0.01366764124322305\n",
      "Wrote weights to file ./pureNetworkLearningNoP/baselineMLPFinal/best_weights\n",
      "The loss after 19300 iterations is E=0.011712388221537046\n",
      "Wrote weights to file ./pureNetworkLearningNoP/baselineMLPFinal/best_weights\n",
      "The loss after 19400 iterations is E=0.010330772154064435\n",
      "Wrote weights to file ./pureNetworkLearningNoP/baselineMLPFinal/best_weights\n",
      "The loss after 19500 iterations is E=0.009251711652725\n",
      "Using every 38th point for                         training.\n",
      "Selecting subset for training: 71 out of 2700 selected.\n",
      "Wrote weights to file ./pureNetworkLearningNoP/baselineMLPFinal/best_weights\n",
      "The loss after 19600 iterations is E=0.03256687095035518\n",
      "Wrote weights to file ./pureNetworkLearningNoP/baselineMLPFinal/best_weights\n",
      "The loss after 19700 iterations is E=0.023804213930395075\n",
      "Wrote weights to file ./pureNetworkLearningNoP/baselineMLPFinal/best_weights\n",
      "The loss after 19800 iterations is E=0.020320175401912374\n",
      "Wrote weights to file ./pureNetworkLearningNoP/baselineMLPFinal/best_weights\n",
      "The loss after 19900 iterations is E=0.018102293243583152\n",
      "Wrote weights to file ./pureNetworkLearningNoP/baselineMLPFinal/best_weights\n",
      "The loss after 20000 iterations is E=0.01633629898239255\n",
      "Wrote weights to file ./pureNetworkLearningNoP/baselineMLPFinal/best_weights\n",
      "The loss after 20100 iterations is E=0.014821710342311608\n",
      "Wrote weights to file ./pureNetworkLearningNoP/baselineMLPFinal/best_weights\n",
      "The loss after 20200 iterations is E=0.013519139199723945\n",
      "Wrote weights to file ./pureNetworkLearningNoP/baselineMLPFinal/best_weights\n",
      "The loss after 20300 iterations is E=0.012423413552168881\n",
      "Wrote weights to file ./pureNetworkLearningNoP/baselineMLPFinal/best_weights\n",
      "The loss after 20400 iterations is E=0.011524361973478989\n",
      "Wrote weights to file ./pureNetworkLearningNoP/baselineMLPFinal/best_weights\n",
      "The loss after 20500 iterations is E=0.010799993464303203\n",
      "Wrote weights to file ./pureNetworkLearningNoP/baselineMLPFinal/best_weights\n",
      "The loss after 20600 iterations is E=0.010221261925073081\n",
      "Re-setting points: using every 38th point for training.\n",
      "Selecting subset for training: 71 out of 2700 selected.\n",
      "Re-setting points: using every 38th point for training.\n",
      "Selecting subset for training: 71 out of 2700 selected.\n",
      "Re-setting points: using every 38th point for training.\n",
      "Selecting subset for training: 71 out of 2700 selected.\n",
      "Re-setting points: using every 38th point for training.\n",
      "Selecting subset for training: 71 out of 2700 selected.\n",
      "Re-setting points: using every 38th point for training.\n",
      "Selecting subset for training: 71 out of 2700 selected.\n",
      "Re-setting points: using every 38th point for training.\n",
      "Selecting subset for training: 71 out of 2700 selected.\n",
      "Wrote weights to file ./pureNetworkLearningNoP/baselineMLPFinal/best_weights\n",
      "The loss after 26200 iterations is E=0.004127310730741469\n",
      "Using every 34th point for                         training.\n",
      "Selecting subset for training: 79 out of 2700 selected.\n",
      "Wrote weights to file ./pureNetworkLearningNoP/baselineMLPFinal/best_weights\n",
      "The loss after 26300 iterations is E=0.010167121406434733\n",
      "Wrote weights to file ./pureNetworkLearningNoP/baselineMLPFinal/best_weights\n",
      "The loss after 26400 iterations is E=0.009626943633907852\n",
      "Using every 30th point for                         training.\n",
      "Selecting subset for training: 90 out of 2700 selected.\n",
      "Wrote weights to file ./pureNetworkLearningNoP/baselineMLPFinal/best_weights\n",
      "The loss after 26500 iterations is E=0.3525498131798122\n",
      "Wrote weights to file ./pureNetworkLearningNoP/baselineMLPFinal/best_weights\n",
      "The loss after 26600 iterations is E=0.3153255418761248\n",
      "Wrote weights to file ./pureNetworkLearningNoP/baselineMLPFinal/best_weights\n",
      "The loss after 26700 iterations is E=0.28683911720682276\n",
      "Wrote weights to file ./pureNetworkLearningNoP/baselineMLPFinal/best_weights\n",
      "The loss after 26800 iterations is E=0.26154666263342113\n",
      "Wrote weights to file ./pureNetworkLearningNoP/baselineMLPFinal/best_weights\n",
      "The loss after 26900 iterations is E=0.23841422219857727\n",
      "Wrote weights to file ./pureNetworkLearningNoP/baselineMLPFinal/best_weights\n",
      "The loss after 27000 iterations is E=0.21702049412359975\n",
      "Wrote weights to file ./pureNetworkLearningNoP/baselineMLPFinal/best_weights\n",
      "The loss after 27100 iterations is E=0.19718239287554992\n",
      "Wrote weights to file ./pureNetworkLearningNoP/baselineMLPFinal/best_weights\n",
      "The loss after 27200 iterations is E=0.17883032059781592\n",
      "Wrote weights to file ./pureNetworkLearningNoP/baselineMLPFinal/best_weights\n",
      "The loss after 27300 iterations is E=0.16194630180419117\n",
      "Wrote weights to file ./pureNetworkLearningNoP/baselineMLPFinal/best_weights\n",
      "The loss after 27400 iterations is E=0.14652907284511524\n",
      "Wrote weights to file ./pureNetworkLearningNoP/baselineMLPFinal/best_weights\n",
      "The loss after 27500 iterations is E=0.1325733544218169\n",
      "Re-setting points: using every 30th point for training.\n",
      "Selecting subset for training: 90 out of 2700 selected.\n",
      "Wrote weights to file ./pureNetworkLearningNoP/baselineMLPFinal/best_weights\n",
      "The loss after 27600 iterations is E=0.028252737668332313\n",
      "Wrote weights to file ./pureNetworkLearningNoP/baselineMLPFinal/best_weights\n",
      "The loss after 27700 iterations is E=0.02686559010695322\n",
      "Wrote weights to file ./pureNetworkLearningNoP/baselineMLPFinal/best_weights\n",
      "The loss after 27800 iterations is E=0.026013200124694963\n",
      "Wrote weights to file ./pureNetworkLearningNoP/baselineMLPFinal/best_weights\n",
      "The loss after 27900 iterations is E=0.025222687896244097\n",
      "Wrote weights to file ./pureNetworkLearningNoP/baselineMLPFinal/best_weights\n",
      "The loss after 28000 iterations is E=0.02447555913422711\n",
      "Wrote weights to file ./pureNetworkLearningNoP/baselineMLPFinal/best_weights\n",
      "The loss after 28100 iterations is E=0.023765989572905682\n",
      "Wrote weights to file ./pureNetworkLearningNoP/baselineMLPFinal/best_weights\n",
      "The loss after 28200 iterations is E=0.02308836043867071\n",
      "Wrote weights to file ./pureNetworkLearningNoP/baselineMLPFinal/best_weights\n",
      "The loss after 28300 iterations is E=0.022436136258490554\n",
      "Wrote weights to file ./pureNetworkLearningNoP/baselineMLPFinal/best_weights\n",
      "The loss after 28400 iterations is E=0.021802020031199583\n",
      "Wrote weights to file ./pureNetworkLearningNoP/baselineMLPFinal/best_weights\n",
      "The loss after 28500 iterations is E=0.021178378193826387\n",
      "Wrote weights to file ./pureNetworkLearningNoP/baselineMLPFinal/best_weights\n",
      "The loss after 28600 iterations is E=0.020557744154354813\n",
      "Re-setting points: using every 30th point for training.\n",
      "Selecting subset for training: 90 out of 2700 selected.\n",
      "Re-setting points: using every 30th point for training.\n",
      "Selecting subset for training: 90 out of 2700 selected.\n",
      "Wrote weights to file ./pureNetworkLearningNoP/baselineMLPFinal/best_weights\n",
      "The loss after 29800 iterations is E=0.01956396408616191\n",
      "Wrote weights to file ./pureNetworkLearningNoP/baselineMLPFinal/best_weights\n",
      "The loss after 29900 iterations is E=0.017690689238441498\n",
      "Wrote weights to file ./pureNetworkLearningNoP/baselineMLPFinal/best_weights\n",
      "The loss after 30000 iterations is E=0.017103466500473966\n",
      "Elapsed time: 12.12 min\n"
     ]
    }
   ],
   "source": [
    "baselineMLP = SimpleMLP(name='baselineMLPFinal', number_of_inputs=2, neurons_per_layer=5, hidden_layers=1)\n",
    "pureNetworkModel = PureNetworkLearning(name=\"pureNetworkLearningNoP\", mlp=baselineMLP, training_data=field50.A, beta=25.0, d_v=0.025, penalty=2.0)\n",
    "training_points = np.column_stack((field50.x, field50.y))\n",
    "pureNetworkModel.set_control_points(training_points)\n",
    "start = time.time()\n",
    "pureNetworkModel.solve(max_iter=30000, learning_rate=0.01, verbose=100, adaptive=True, tolerance=0.01, every=1000)\n",
    "print(\"Elapsed time: {:5.2f} min\".format((time.time() - start) / 60.0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded weights from file ./pureNetworkLearningNoP/baselineMLPFinal/best_weights.npy\n"
     ]
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAuoAAAD5CAYAAABxuhC6AAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\noIYo1AEAAIAa+g8bZ6gZngtXcwAAAABJRU5ErkJggg==\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x7f0f9cb9f860>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "evaluation_points = np.column_stack((field50.x, field50.y))\n",
    "pureNetworkModel.mlp.read_weights_from_disk(\"./pureNetworkLearningNoP/baselineMLPFinal/best_weights.npy\")\n",
    "prediction = pureNetworkModel.predict(evaluation_points)\n",
    "plot_field_and_error(field50.x, field50.y, prediction, field50.A, \"pureNetworkFinal.pdf\",\n",
    "                     [r\"Network output after $3\\cdot 10^4$ iterations\", \"Deviation from the numerical solution\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Keras/Tensorflow comparison"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "dense1 (Dense)               (None, 5)                 15        \n",
      "_________________________________________________________________\n",
      "dense2 (Dense)               (None, 1)                 6         \n",
      "=================================================================\n",
      "Total params: 21\n",
      "Trainable params: 21\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "from keras.models import Sequential\n",
    "from keras.layers import Dense\n",
    "from keras.callbacks import ModelCheckpoint\n",
    "\n",
    "x_train = np.column_stack((field50.x, field50.y))\n",
    "\n",
    "model = Sequential()\n",
    "model.add(Dense(units=5, activation='sigmoid', name='dense1', input_dim=2))\n",
    "model.add(Dense(units=1, name='dense2'))\n",
    "model.compile(loss='mse', optimizer='adam', metrics=['mae'])\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10000\n",
      "2700/2700 [==============================] - 0s 84us/step - loss: 0.0910 - mean_absolute_error: 0.2498\n",
      "\n",
      "Epoch 00001: loss improved from inf to 0.09104, saving model to keras/best_baseline_weights.hdf5\n",
      "Epoch 2/10000\n",
      "2700/2700 [==============================] - 0s 38us/step - loss: 0.0739 - mean_absolute_error: 0.2050\n",
      "\n",
      "Epoch 00002: loss improved from 0.09104 to 0.07392, saving model to keras/best_baseline_weights.hdf5\n",
      "Epoch 3/10000\n",
      "2700/2700 [==============================] - 0s 42us/step - loss: 0.0659 - mean_absolute_error: 0.1901\n",
      "\n",
      "Epoch 00003: loss improved from 0.07392 to 0.06589, saving model to keras/best_baseline_weights.hdf5\n",
      "Epoch 4/10000\n",
      "2700/2700 [==============================] - 0s 41us/step - loss: 0.0585 - mean_absolute_error: 0.1782\n",
      "\n",
      "Epoch 00004: loss improved from 0.06589 to 0.05850, saving model to keras/best_baseline_weights.hdf5\n",
      "Epoch 5/10000\n",
      "2700/2700 [==============================] - 0s 44us/step - loss: 0.0520 - mean_absolute_error: 0.1652\n",
      "\n",
      "Epoch 00005: loss improved from 0.05850 to 0.05195, saving model to keras/best_baseline_weights.hdf5\n",
      "Epoch 6/10000\n",
      "2700/2700 [==============================] - 0s 43us/step - loss: 0.0463 - mean_absolute_error: 0.1525\n",
      "\n",
      "Epoch 00006: loss improved from 0.05195 to 0.04627, saving model to keras/best_baseline_weights.hdf5\n",
      "Epoch 7/10000\n",
      "2700/2700 [==============================] - 0s 42us/step - loss: 0.0414 - mean_absolute_error: 0.1431\n",
      "\n",
      "Epoch 00007: loss improved from 0.04627 to 0.04141, saving model to keras/best_baseline_weights.hdf5\n",
      "Epoch 8/10000\n",
      "2700/2700 [==============================] - 0s 48us/step - loss: 0.0374 - mean_absolute_error: 0.1355\n",
      "\n",
      "Epoch 00008: loss improved from 0.04141 to 0.03738, saving model to keras/best_baseline_weights.hdf5\n",
      "Epoch 9/10000\n",
      "2700/2700 [==============================] - 0s 45us/step - loss: 0.0342 - mean_absolute_error: 0.1284\n",
      "\n",
      "Epoch 00009: loss improved from 0.03738 to 0.03423, saving model to keras/best_baseline_weights.hdf5\n",
      "Epoch 10/10000\n",
      "2700/2700 [==============================] - 0s 42us/step - loss: 0.0317 - mean_absolute_error: 0.1261\n",
      "\n",
      "Epoch 00010: loss improved from 0.03423 to 0.03168, saving model to keras/best_baseline_weights.hdf5\n",
      "Epoch 11/10000\n",
      "2700/2700 [==============================] - 0s 51us/step - loss: 0.0298 - mean_absolute_error: 0.1250: 0s - loss: 0.0313 - mean_absolute_error: 0.12\n",
      "\n",
      "Epoch 00011: loss improved from 0.03168 to 0.02978, saving model to keras/best_baseline_weights.hdf5\n",
      "Epoch 12/10000\n",
      "2700/2700 [==============================] - 0s 42us/step - loss: 0.0284 - mean_absolute_error: 0.1240\n",
      "\n",
      "Epoch 00012: loss improved from 0.02978 to 0.02837, saving model to keras/best_baseline_weights.hdf5\n",
      "Epoch 13/10000\n",
      "2700/2700 [==============================] - 0s 42us/step - loss: 0.0273 - mean_absolute_error: 0.1238\n",
      "\n",
      "Epoch 00013: loss improved from 0.02837 to 0.02732, saving model to keras/best_baseline_weights.hdf5\n",
      "Epoch 14/10000\n",
      "2700/2700 [==============================] - 0s 43us/step - loss: 0.0267 - mean_absolute_error: 0.1247\n",
      "\n",
      "Epoch 00014: loss improved from 0.02732 to 0.02671, saving model to keras/best_baseline_weights.hdf5\n",
      "Epoch 15/10000\n",
      "2700/2700 [==============================] - 0s 42us/step - loss: 0.0261 - mean_absolute_error: 0.1248\n",
      "\n",
      "Epoch 00015: loss improved from 0.02671 to 0.02615, saving model to keras/best_baseline_weights.hdf5\n",
      "Epoch 16/10000\n",
      "2700/2700 [==============================] - 0s 50us/step - loss: 0.0259 - mean_absolute_error: 0.1247\n",
      "\n",
      "Epoch 00016: loss improved from 0.02615 to 0.02586, saving model to keras/best_baseline_weights.hdf5\n",
      "Epoch 17/10000\n",
      "2700/2700 [==============================] - 0s 44us/step - loss: 0.0256 - mean_absolute_error: 0.1259\n",
      "\n",
      "Epoch 00017: loss improved from 0.02586 to 0.02563, saving model to keras/best_baseline_weights.hdf5\n",
      "Epoch 18/10000\n",
      "2700/2700 [==============================] - 0s 45us/step - loss: 0.0254 - mean_absolute_error: 0.1256\n",
      "\n",
      "Epoch 00018: loss improved from 0.02563 to 0.02542, saving model to keras/best_baseline_weights.hdf5\n",
      "Epoch 19/10000\n",
      "2700/2700 [==============================] - 0s 37us/step - loss: 0.0252 - mean_absolute_error: 0.1254\n",
      "\n",
      "Epoch 00019: loss improved from 0.02542 to 0.02522, saving model to keras/best_baseline_weights.hdf5\n",
      "Epoch 20/10000\n",
      "2700/2700 [==============================] - 0s 44us/step - loss: 0.0251 - mean_absolute_error: 0.1257\n",
      "\n",
      "Epoch 00020: loss improved from 0.02522 to 0.02511, saving model to keras/best_baseline_weights.hdf5\n",
      "Epoch 21/10000\n",
      "2700/2700 [==============================] - 0s 52us/step - loss: 0.0249 - mean_absolute_error: 0.1257\n",
      "\n",
      "Epoch 00021: loss improved from 0.02511 to 0.02492, saving model to keras/best_baseline_weights.hdf5\n",
      "Epoch 22/10000\n",
      "2700/2700 [==============================] - 0s 36us/step - loss: 0.0249 - mean_absolute_error: 0.1255\n",
      "\n",
      "Epoch 00022: loss improved from 0.02492 to 0.02487, saving model to keras/best_baseline_weights.hdf5\n",
      "Epoch 23/10000\n",
      "2700/2700 [==============================] - 0s 38us/step - loss: 0.0247 - mean_absolute_error: 0.1255\n",
      "\n",
      "Epoch 00023: loss improved from 0.02487 to 0.02466, saving model to keras/best_baseline_weights.hdf5\n",
      "Epoch 24/10000\n",
      "2700/2700 [==============================] - 0s 42us/step - loss: 0.0245 - mean_absolute_error: 0.1249\n",
      "\n",
      "Epoch 00024: loss improved from 0.02466 to 0.02452, saving model to keras/best_baseline_weights.hdf5\n",
      "Epoch 25/10000\n",
      "2700/2700 [==============================] - 0s 52us/step - loss: 0.0244 - mean_absolute_error: 0.1245\n",
      "\n",
      "Epoch 00025: loss improved from 0.02452 to 0.02439, saving model to keras/best_baseline_weights.hdf5\n",
      "Epoch 26/10000\n",
      "2700/2700 [==============================] - 0s 55us/step - loss: 0.0243 - mean_absolute_error: 0.1243\n",
      "\n",
      "Epoch 00026: loss improved from 0.02439 to 0.02425, saving model to keras/best_baseline_weights.hdf5\n",
      "Epoch 27/10000\n",
      "2700/2700 [==============================] - 0s 55us/step - loss: 0.0241 - mean_absolute_error: 0.1235\n",
      "\n",
      "Epoch 00027: loss improved from 0.02425 to 0.02406, saving model to keras/best_baseline_weights.hdf5\n",
      "Epoch 28/10000\n",
      "2700/2700 [==============================] - 0s 47us/step - loss: 0.0239 - mean_absolute_error: 0.1230\n",
      "\n",
      "Epoch 00028: loss improved from 0.02406 to 0.02387, saving model to keras/best_baseline_weights.hdf5\n",
      "Epoch 29/10000\n",
      "2700/2700 [==============================] - 0s 44us/step - loss: 0.0237 - mean_absolute_error: 0.1228\n",
      "\n",
      "Epoch 00029: loss improved from 0.02387 to 0.02367, saving model to keras/best_baseline_weights.hdf5\n",
      "Epoch 30/10000\n",
      "2700/2700 [==============================] - 0s 45us/step - loss: 0.0235 - mean_absolute_error: 0.1222\n",
      "\n",
      "Epoch 00030: loss improved from 0.02367 to 0.02349, saving model to keras/best_baseline_weights.hdf5\n",
      "Epoch 31/10000\n",
      "2700/2700 [==============================] - 0s 51us/step - loss: 0.0233 - mean_absolute_error: 0.1216\n",
      "\n",
      "Epoch 00031: loss improved from 0.02349 to 0.02327, saving model to keras/best_baseline_weights.hdf5\n",
      "Epoch 32/10000\n",
      "2700/2700 [==============================] - 0s 44us/step - loss: 0.0231 - mean_absolute_error: 0.1211\n",
      "\n",
      "Epoch 00032: loss improved from 0.02327 to 0.02313, saving model to keras/best_baseline_weights.hdf5\n",
      "Epoch 33/10000\n",
      "2700/2700 [==============================] - 0s 43us/step - loss: 0.0229 - mean_absolute_error: 0.1197\n",
      "\n",
      "Epoch 00033: loss improved from 0.02313 to 0.02285, saving model to keras/best_baseline_weights.hdf5\n",
      "Epoch 34/10000\n",
      "2700/2700 [==============================] - 0s 46us/step - loss: 0.0226 - mean_absolute_error: 0.1201\n",
      "\n",
      "Epoch 00034: loss improved from 0.02285 to 0.02262, saving model to keras/best_baseline_weights.hdf5\n",
      "Epoch 35/10000\n",
      "2700/2700 [==============================] - 0s 50us/step - loss: 0.0224 - mean_absolute_error: 0.1190\n",
      "\n",
      "Epoch 00035: loss improved from 0.02262 to 0.02243, saving model to keras/best_baseline_weights.hdf5\n",
      "Epoch 36/10000\n",
      "2700/2700 [==============================] - 0s 48us/step - loss: 0.0221 - mean_absolute_error: 0.1184\n",
      "\n",
      "Epoch 00036: loss improved from 0.02243 to 0.02209, saving model to keras/best_baseline_weights.hdf5\n",
      "Epoch 37/10000\n",
      "2700/2700 [==============================] - 0s 45us/step - loss: 0.0219 - mean_absolute_error: 0.1173\n",
      "\n",
      "Epoch 00037: loss improved from 0.02209 to 0.02189, saving model to keras/best_baseline_weights.hdf5\n",
      "Epoch 38/10000\n",
      "2700/2700 [==============================] - 0s 46us/step - loss: 0.0216 - mean_absolute_error: 0.1168\n",
      "\n",
      "Epoch 00038: loss improved from 0.02189 to 0.02160, saving model to keras/best_baseline_weights.hdf5\n",
      "Epoch 39/10000\n",
      "2700/2700 [==============================] - 0s 38us/step - loss: 0.0213 - mean_absolute_error: 0.1160\n",
      "\n",
      "Epoch 00039: loss improved from 0.02160 to 0.02133, saving model to keras/best_baseline_weights.hdf5\n",
      "Epoch 40/10000\n",
      "2700/2700 [==============================] - 0s 50us/step - loss: 0.0211 - mean_absolute_error: 0.1149\n",
      "\n",
      "Epoch 00040: loss improved from 0.02133 to 0.02106, saving model to keras/best_baseline_weights.hdf5\n",
      "Epoch 41/10000\n",
      "2700/2700 [==============================] - 0s 51us/step - loss: 0.0207 - mean_absolute_error: 0.1139\n",
      "\n",
      "Epoch 00041: loss improved from 0.02106 to 0.02073, saving model to keras/best_baseline_weights.hdf5\n",
      "Epoch 42/10000\n",
      "2700/2700 [==============================] - 0s 47us/step - loss: 0.0204 - mean_absolute_error: 0.1129\n",
      "\n",
      "Epoch 00042: loss improved from 0.02073 to 0.02042, saving model to keras/best_baseline_weights.hdf5\n",
      "Epoch 43/10000\n",
      "2700/2700 [==============================] - 0s 48us/step - loss: 0.0201 - mean_absolute_error: 0.1118\n",
      "\n",
      "Epoch 00043: loss improved from 0.02042 to 0.02006, saving model to keras/best_baseline_weights.hdf5\n",
      "Epoch 44/10000\n",
      "2700/2700 [==============================] - 0s 51us/step - loss: 0.0198 - mean_absolute_error: 0.1105\n",
      "\n",
      "Epoch 00044: loss improved from 0.02006 to 0.01977, saving model to keras/best_baseline_weights.hdf5\n",
      "Epoch 45/10000\n",
      "2700/2700 [==============================] - 0s 46us/step - loss: 0.0194 - mean_absolute_error: 0.1097\n",
      "\n",
      "Epoch 00045: loss improved from 0.01977 to 0.01939, saving model to keras/best_baseline_weights.hdf5\n",
      "Epoch 46/10000\n",
      "2700/2700 [==============================] - 0s 41us/step - loss: 0.0190 - mean_absolute_error: 0.1086\n",
      "\n",
      "Epoch 00046: loss improved from 0.01939 to 0.01902, saving model to keras/best_baseline_weights.hdf5\n",
      "Epoch 47/10000\n",
      "2700/2700 [==============================] - 0s 49us/step - loss: 0.0187 - mean_absolute_error: 0.1076\n",
      "\n",
      "Epoch 00047: loss improved from 0.01902 to 0.01874, saving model to keras/best_baseline_weights.hdf5\n",
      "Epoch 48/10000\n",
      "2700/2700 [==============================] - 0s 46us/step - loss: 0.0184 - mean_absolute_error: 0.1064\n",
      "\n",
      "Epoch 00048: loss improved from 0.01874 to 0.01838, saving model to keras/best_baseline_weights.hdf5\n",
      "Epoch 49/10000\n",
      "2700/2700 [==============================] - 0s 53us/step - loss: 0.0180 - mean_absolute_error: 0.1049\n",
      "\n",
      "Epoch 00049: loss improved from 0.01838 to 0.01797, saving model to keras/best_baseline_weights.hdf5\n",
      "Epoch 50/10000\n",
      "2700/2700 [==============================] - 0s 52us/step - loss: 0.0176 - mean_absolute_error: 0.1035\n",
      "\n",
      "Epoch 00050: loss improved from 0.01797 to 0.01758, saving model to keras/best_baseline_weights.hdf5\n",
      "Epoch 51/10000\n",
      "2700/2700 [==============================] - 0s 55us/step - loss: 0.0173 - mean_absolute_error: 0.1022\n",
      "\n",
      "Epoch 00051: loss improved from 0.01758 to 0.01727, saving model to keras/best_baseline_weights.hdf5\n",
      "Epoch 52/10000\n",
      "2700/2700 [==============================] - 0s 45us/step - loss: 0.0168 - mean_absolute_error: 0.1009\n",
      "\n",
      "Epoch 00052: loss improved from 0.01727 to 0.01685, saving model to keras/best_baseline_weights.hdf5\n",
      "Epoch 53/10000\n",
      "2700/2700 [==============================] - 0s 45us/step - loss: 0.0165 - mean_absolute_error: 0.0999\n",
      "\n",
      "Epoch 00053: loss improved from 0.01685 to 0.01645, saving model to keras/best_baseline_weights.hdf5\n",
      "Epoch 54/10000\n",
      "2700/2700 [==============================] - 0s 41us/step - loss: 0.0160 - mean_absolute_error: 0.0982\n",
      "\n",
      "Epoch 00054: loss improved from 0.01645 to 0.01601, saving model to keras/best_baseline_weights.hdf5\n",
      "Epoch 55/10000\n",
      "2700/2700 [==============================] - 0s 45us/step - loss: 0.0157 - mean_absolute_error: 0.0966\n",
      "\n",
      "Epoch 00055: loss improved from 0.01601 to 0.01566, saving model to keras/best_baseline_weights.hdf5\n",
      "Epoch 56/10000\n",
      "2700/2700 [==============================] - 0s 43us/step - loss: 0.0152 - mean_absolute_error: 0.0951\n",
      "\n",
      "Epoch 00056: loss improved from 0.01566 to 0.01524, saving model to keras/best_baseline_weights.hdf5\n",
      "Epoch 57/10000\n",
      "2700/2700 [==============================] - 0s 36us/step - loss: 0.0148 - mean_absolute_error: 0.0937\n",
      "\n",
      "Epoch 00057: loss improved from 0.01524 to 0.01482, saving model to keras/best_baseline_weights.hdf5\n",
      "Epoch 58/10000\n",
      "2700/2700 [==============================] - 0s 37us/step - loss: 0.0144 - mean_absolute_error: 0.0917\n",
      "\n",
      "Epoch 00058: loss improved from 0.01482 to 0.01442, saving model to keras/best_baseline_weights.hdf5\n",
      "Epoch 59/10000\n",
      "2700/2700 [==============================] - 0s 41us/step - loss: 0.0141 - mean_absolute_error: 0.0907\n",
      "\n",
      "Epoch 00059: loss improved from 0.01442 to 0.01405, saving model to keras/best_baseline_weights.hdf5\n",
      "Epoch 60/10000\n",
      "2700/2700 [==============================] - 0s 42us/step - loss: 0.0136 - mean_absolute_error: 0.0887\n",
      "\n",
      "Epoch 00060: loss improved from 0.01405 to 0.01363, saving model to keras/best_baseline_weights.hdf5\n",
      "Epoch 61/10000\n",
      "2700/2700 [==============================] - 0s 38us/step - loss: 0.0132 - mean_absolute_error: 0.0875\n",
      "\n",
      "Epoch 00061: loss improved from 0.01363 to 0.01325, saving model to keras/best_baseline_weights.hdf5\n",
      "Epoch 62/10000\n",
      "2700/2700 [==============================] - 0s 40us/step - loss: 0.0128 - mean_absolute_error: 0.0857\n",
      "\n",
      "Epoch 00062: loss improved from 0.01325 to 0.01283, saving model to keras/best_baseline_weights.hdf5\n",
      "Epoch 63/10000\n",
      "2700/2700 [==============================] - 0s 52us/step - loss: 0.0124 - mean_absolute_error: 0.0838\n",
      "\n",
      "Epoch 00063: loss improved from 0.01283 to 0.01242, saving model to keras/best_baseline_weights.hdf5\n",
      "Epoch 64/10000\n",
      "2700/2700 [==============================] - 0s 57us/step - loss: 0.0121 - mean_absolute_error: 0.0825\n",
      "\n",
      "Epoch 00064: loss improved from 0.01242 to 0.01206, saving model to keras/best_baseline_weights.hdf5\n",
      "Epoch 65/10000\n",
      "2700/2700 [==============================] - 0s 51us/step - loss: 0.0117 - mean_absolute_error: 0.0804\n",
      "\n",
      "Epoch 00065: loss improved from 0.01206 to 0.01166, saving model to keras/best_baseline_weights.hdf5\n",
      "Epoch 66/10000\n",
      "2700/2700 [==============================] - 0s 36us/step - loss: 0.0113 - mean_absolute_error: 0.0789\n",
      "\n",
      "Epoch 00066: loss improved from 0.01166 to 0.01131, saving model to keras/best_baseline_weights.hdf5\n",
      "Epoch 67/10000\n",
      "2700/2700 [==============================] - 0s 37us/step - loss: 0.0109 - mean_absolute_error: 0.0771\n",
      "\n",
      "Epoch 00067: loss improved from 0.01131 to 0.01091, saving model to keras/best_baseline_weights.hdf5\n",
      "Epoch 68/10000\n",
      "2700/2700 [==============================] - 0s 37us/step - loss: 0.0105 - mean_absolute_error: 0.0752\n",
      "\n",
      "Epoch 00068: loss improved from 0.01091 to 0.01053, saving model to keras/best_baseline_weights.hdf5\n",
      "Epoch 69/10000\n",
      "2700/2700 [==============================] - 0s 40us/step - loss: 0.0102 - mean_absolute_error: 0.0735\n",
      "\n",
      "Epoch 00069: loss improved from 0.01053 to 0.01018, saving model to keras/best_baseline_weights.hdf5\n",
      "Epoch 70/10000\n",
      "2700/2700 [==============================] - 0s 53us/step - loss: 0.0098 - mean_absolute_error: 0.0715\n",
      "\n",
      "Epoch 00070: loss improved from 0.01018 to 0.00979, saving model to keras/best_baseline_weights.hdf5\n",
      "Epoch 71/10000\n",
      "2700/2700 [==============================] - 0s 50us/step - loss: 0.0095 - mean_absolute_error: 0.0697\n",
      "\n",
      "Epoch 00071: loss improved from 0.00979 to 0.00947, saving model to keras/best_baseline_weights.hdf5\n",
      "Epoch 72/10000\n",
      "2700/2700 [==============================] - 0s 51us/step - loss: 0.0092 - mean_absolute_error: 0.0684\n",
      "\n",
      "Epoch 00072: loss improved from 0.00947 to 0.00918, saving model to keras/best_baseline_weights.hdf5\n",
      "Epoch 73/10000\n",
      "2700/2700 [==============================] - 0s 52us/step - loss: 0.0088 - mean_absolute_error: 0.0667\n",
      "\n",
      "Epoch 00073: loss improved from 0.00918 to 0.00881, saving model to keras/best_baseline_weights.hdf5\n",
      "Epoch 74/10000\n",
      "2700/2700 [==============================] - 0s 47us/step - loss: 0.0085 - mean_absolute_error: 0.0646\n",
      "\n",
      "Epoch 00074: loss improved from 0.00881 to 0.00849, saving model to keras/best_baseline_weights.hdf5\n",
      "Epoch 75/10000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2700/2700 [==============================] - 0s 44us/step - loss: 0.0082 - mean_absolute_error: 0.0627\n",
      "\n",
      "Epoch 00075: loss improved from 0.00849 to 0.00816, saving model to keras/best_baseline_weights.hdf5\n",
      "Epoch 76/10000\n",
      "2700/2700 [==============================] - 0s 47us/step - loss: 0.0079 - mean_absolute_error: 0.0608\n",
      "\n",
      "Epoch 00076: loss improved from 0.00816 to 0.00788, saving model to keras/best_baseline_weights.hdf5\n",
      "Epoch 77/10000\n",
      "2700/2700 [==============================] - 0s 51us/step - loss: 0.0076 - mean_absolute_error: 0.0591\n",
      "\n",
      "Epoch 00077: loss improved from 0.00788 to 0.00759, saving model to keras/best_baseline_weights.hdf5\n",
      "Epoch 78/10000\n",
      "2700/2700 [==============================] - 0s 42us/step - loss: 0.0073 - mean_absolute_error: 0.0576\n",
      "\n",
      "Epoch 00078: loss improved from 0.00759 to 0.00731, saving model to keras/best_baseline_weights.hdf5\n",
      "Epoch 79/10000\n",
      "2700/2700 [==============================] - 0s 48us/step - loss: 0.0071 - mean_absolute_error: 0.0560\n",
      "\n",
      "Epoch 00079: loss improved from 0.00731 to 0.00708, saving model to keras/best_baseline_weights.hdf5\n",
      "Epoch 80/10000\n",
      "2700/2700 [==============================] - 0s 46us/step - loss: 0.0068 - mean_absolute_error: 0.0540\n",
      "\n",
      "Epoch 00080: loss improved from 0.00708 to 0.00681, saving model to keras/best_baseline_weights.hdf5\n",
      "Epoch 81/10000\n",
      "2700/2700 [==============================] - 0s 51us/step - loss: 0.0066 - mean_absolute_error: 0.0523\n",
      "\n",
      "Epoch 00081: loss improved from 0.00681 to 0.00656, saving model to keras/best_baseline_weights.hdf5\n",
      "Epoch 82/10000\n",
      "2700/2700 [==============================] - 0s 48us/step - loss: 0.0063 - mean_absolute_error: 0.0510\n",
      "\n",
      "Epoch 00082: loss improved from 0.00656 to 0.00635, saving model to keras/best_baseline_weights.hdf5\n",
      "Epoch 83/10000\n",
      "2700/2700 [==============================] - 0s 35us/step - loss: 0.0061 - mean_absolute_error: 0.0491\n",
      "\n",
      "Epoch 00083: loss improved from 0.00635 to 0.00612, saving model to keras/best_baseline_weights.hdf5\n",
      "Epoch 84/10000\n",
      "2700/2700 [==============================] - 0s 41us/step - loss: 0.0059 - mean_absolute_error: 0.0477\n",
      "\n",
      "Epoch 00084: loss improved from 0.00612 to 0.00593, saving model to keras/best_baseline_weights.hdf5\n",
      "Epoch 85/10000\n",
      "2700/2700 [==============================] - 0s 45us/step - loss: 0.0057 - mean_absolute_error: 0.0462\n",
      "\n",
      "Epoch 00085: loss improved from 0.00593 to 0.00572, saving model to keras/best_baseline_weights.hdf5\n",
      "Epoch 86/10000\n",
      "2700/2700 [==============================] - 0s 42us/step - loss: 0.0056 - mean_absolute_error: 0.0451\n",
      "\n",
      "Epoch 00086: loss improved from 0.00572 to 0.00557, saving model to keras/best_baseline_weights.hdf5\n",
      "Epoch 87/10000\n",
      "2700/2700 [==============================] - 0s 44us/step - loss: 0.0054 - mean_absolute_error: 0.0440\n",
      "\n",
      "Epoch 00087: loss improved from 0.00557 to 0.00539, saving model to keras/best_baseline_weights.hdf5\n",
      "Epoch 88/10000\n",
      "2700/2700 [==============================] - 0s 42us/step - loss: 0.0052 - mean_absolute_error: 0.0426\n",
      "\n",
      "Epoch 00088: loss improved from 0.00539 to 0.00522, saving model to keras/best_baseline_weights.hdf5\n",
      "Epoch 89/10000\n",
      "2700/2700 [==============================] - 0s 47us/step - loss: 0.0051 - mean_absolute_error: 0.0420\n",
      "\n",
      "Epoch 00089: loss improved from 0.00522 to 0.00510, saving model to keras/best_baseline_weights.hdf5\n",
      "Epoch 90/10000\n",
      "2700/2700 [==============================] - 0s 48us/step - loss: 0.0050 - mean_absolute_error: 0.0406\n",
      "\n",
      "Epoch 00090: loss improved from 0.00510 to 0.00495, saving model to keras/best_baseline_weights.hdf5\n",
      "Epoch 91/10000\n",
      "2700/2700 [==============================] - 0s 53us/step - loss: 0.0048 - mean_absolute_error: 0.0398\n",
      "\n",
      "Epoch 00091: loss improved from 0.00495 to 0.00483, saving model to keras/best_baseline_weights.hdf5\n",
      "Epoch 92/10000\n",
      "2700/2700 [==============================] - 0s 48us/step - loss: 0.0047 - mean_absolute_error: 0.0390\n",
      "\n",
      "Epoch 00092: loss improved from 0.00483 to 0.00473, saving model to keras/best_baseline_weights.hdf5\n",
      "Epoch 93/10000\n",
      "2700/2700 [==============================] - 0s 52us/step - loss: 0.0046 - mean_absolute_error: 0.0388\n",
      "\n",
      "Epoch 00093: loss improved from 0.00473 to 0.00463, saving model to keras/best_baseline_weights.hdf5\n",
      "Epoch 94/10000\n",
      "2700/2700 [==============================] - 0s 52us/step - loss: 0.0045 - mean_absolute_error: 0.0380\n",
      "\n",
      "Epoch 00094: loss improved from 0.00463 to 0.00451, saving model to keras/best_baseline_weights.hdf5\n",
      "Epoch 95/10000\n",
      "2700/2700 [==============================] - 0s 47us/step - loss: 0.0044 - mean_absolute_error: 0.0372\n",
      "\n",
      "Epoch 00095: loss improved from 0.00451 to 0.00443, saving model to keras/best_baseline_weights.hdf5\n",
      "Epoch 96/10000\n",
      "2700/2700 [==============================] - 0s 47us/step - loss: 0.0044 - mean_absolute_error: 0.0373\n",
      "\n",
      "Epoch 00096: loss improved from 0.00443 to 0.00436, saving model to keras/best_baseline_weights.hdf5\n",
      "Epoch 97/10000\n",
      "2700/2700 [==============================] - 0s 46us/step - loss: 0.0043 - mean_absolute_error: 0.0367\n",
      "\n",
      "Epoch 00097: loss improved from 0.00436 to 0.00427, saving model to keras/best_baseline_weights.hdf5\n",
      "Epoch 98/10000\n",
      "2700/2700 [==============================] - 0s 49us/step - loss: 0.0042 - mean_absolute_error: 0.0370\n",
      "\n",
      "Epoch 00098: loss improved from 0.00427 to 0.00423, saving model to keras/best_baseline_weights.hdf5\n",
      "Epoch 99/10000\n",
      "2700/2700 [==============================] - 0s 46us/step - loss: 0.0041 - mean_absolute_error: 0.0360\n",
      "\n",
      "Epoch 00099: loss improved from 0.00423 to 0.00414, saving model to keras/best_baseline_weights.hdf5\n",
      "Epoch 100/10000\n",
      "2700/2700 [==============================] - 0s 48us/step - loss: 0.0041 - mean_absolute_error: 0.0360\n",
      "\n",
      "Epoch 00100: loss improved from 0.00414 to 0.00410, saving model to keras/best_baseline_weights.hdf5\n",
      "Epoch 101/10000\n",
      "2700/2700 [==============================] - 0s 53us/step - loss: 0.0041 - mean_absolute_error: 0.0359\n",
      "\n",
      "Epoch 00101: loss improved from 0.00410 to 0.00405, saving model to keras/best_baseline_weights.hdf5\n",
      "Epoch 102/10000\n",
      "2700/2700 [==============================] - 0s 50us/step - loss: 0.0040 - mean_absolute_error: 0.0355\n",
      "\n",
      "Epoch 00102: loss improved from 0.00405 to 0.00401, saving model to keras/best_baseline_weights.hdf5\n",
      "Epoch 103/10000\n",
      "2700/2700 [==============================] - 0s 42us/step - loss: 0.0040 - mean_absolute_error: 0.0355\n",
      "\n",
      "Epoch 00103: loss improved from 0.00401 to 0.00396, saving model to keras/best_baseline_weights.hdf5\n",
      "Epoch 104/10000\n",
      "2700/2700 [==============================] - 0s 42us/step - loss: 0.0039 - mean_absolute_error: 0.0353\n",
      "\n",
      "Epoch 00104: loss improved from 0.00396 to 0.00391, saving model to keras/best_baseline_weights.hdf5\n",
      "Epoch 105/10000\n",
      "2700/2700 [==============================] - 0s 42us/step - loss: 0.0039 - mean_absolute_error: 0.0350\n",
      "\n",
      "Epoch 00105: loss improved from 0.00391 to 0.00387, saving model to keras/best_baseline_weights.hdf5\n",
      "Epoch 106/10000\n",
      "2700/2700 [==============================] - 0s 42us/step - loss: 0.0038 - mean_absolute_error: 0.0349\n",
      "\n",
      "Epoch 00106: loss improved from 0.00387 to 0.00383, saving model to keras/best_baseline_weights.hdf5\n",
      "Epoch 107/10000\n",
      "2700/2700 [==============================] - 0s 49us/step - loss: 0.0038 - mean_absolute_error: 0.0350\n",
      "\n",
      "Epoch 00107: loss improved from 0.00383 to 0.00381, saving model to keras/best_baseline_weights.hdf5\n",
      "Epoch 108/10000\n",
      "2700/2700 [==============================] - 0s 51us/step - loss: 0.0038 - mean_absolute_error: 0.0350\n",
      "\n",
      "Epoch 00108: loss improved from 0.00381 to 0.00380, saving model to keras/best_baseline_weights.hdf5\n",
      "Epoch 109/10000\n",
      "2700/2700 [==============================] - 0s 45us/step - loss: 0.0037 - mean_absolute_error: 0.0346\n",
      "\n",
      "Epoch 00109: loss improved from 0.00380 to 0.00375, saving model to keras/best_baseline_weights.hdf5\n",
      "Epoch 110/10000\n",
      "2700/2700 [==============================] - 0s 38us/step - loss: 0.0037 - mean_absolute_error: 0.0343\n",
      "\n",
      "Epoch 00110: loss improved from 0.00375 to 0.00371, saving model to keras/best_baseline_weights.hdf5\n",
      "Epoch 111/10000\n",
      "2700/2700 [==============================] - 0s 53us/step - loss: 0.0037 - mean_absolute_error: 0.0342\n",
      "\n",
      "Epoch 00111: loss improved from 0.00371 to 0.00368, saving model to keras/best_baseline_weights.hdf5\n",
      "Epoch 112/10000\n",
      "2700/2700 [==============================] - 0s 41us/step - loss: 0.0037 - mean_absolute_error: 0.0342\n",
      "\n",
      "Epoch 00112: loss improved from 0.00368 to 0.00365, saving model to keras/best_baseline_weights.hdf5\n",
      "Epoch 113/10000\n",
      "2700/2700 [==============================] - 0s 40us/step - loss: 0.0036 - mean_absolute_error: 0.0338: 0s - loss: 0.0036 - mean_absolute_error: 0.03\n",
      "\n",
      "Epoch 00113: loss improved from 0.00365 to 0.00362, saving model to keras/best_baseline_weights.hdf5\n",
      "Epoch 114/10000\n",
      "2700/2700 [==============================] - 0s 38us/step - loss: 0.0036 - mean_absolute_error: 0.0341\n",
      "\n",
      "Epoch 00114: loss improved from 0.00362 to 0.00362, saving model to keras/best_baseline_weights.hdf5\n",
      "Epoch 115/10000\n",
      "2700/2700 [==============================] - 0s 42us/step - loss: 0.0036 - mean_absolute_error: 0.0337\n",
      "\n",
      "Epoch 00115: loss improved from 0.00362 to 0.00359, saving model to keras/best_baseline_weights.hdf5\n",
      "Epoch 116/10000\n",
      "2700/2700 [==============================] - 0s 38us/step - loss: 0.0036 - mean_absolute_error: 0.0337\n",
      "\n",
      "Epoch 00116: loss improved from 0.00359 to 0.00355, saving model to keras/best_baseline_weights.hdf5\n",
      "Epoch 117/10000\n",
      "2700/2700 [==============================] - 0s 35us/step - loss: 0.0035 - mean_absolute_error: 0.0335\n",
      "\n",
      "Epoch 00117: loss improved from 0.00355 to 0.00353, saving model to keras/best_baseline_weights.hdf5\n",
      "Epoch 118/10000\n",
      "2700/2700 [==============================] - 0s 52us/step - loss: 0.0035 - mean_absolute_error: 0.0333\n",
      "\n",
      "Epoch 00118: loss improved from 0.00353 to 0.00351, saving model to keras/best_baseline_weights.hdf5\n",
      "Epoch 119/10000\n",
      "2700/2700 [==============================] - 0s 40us/step - loss: 0.0035 - mean_absolute_error: 0.0331\n",
      "\n",
      "Epoch 00119: loss improved from 0.00351 to 0.00350, saving model to keras/best_baseline_weights.hdf5\n",
      "Epoch 120/10000\n",
      "2700/2700 [==============================] - 0s 37us/step - loss: 0.0035 - mean_absolute_error: 0.0330\n",
      "\n",
      "Epoch 00120: loss improved from 0.00350 to 0.00346, saving model to keras/best_baseline_weights.hdf5\n",
      "Epoch 121/10000\n",
      "2700/2700 [==============================] - 0s 48us/step - loss: 0.0034 - mean_absolute_error: 0.0328\n",
      "\n",
      "Epoch 00121: loss improved from 0.00346 to 0.00344, saving model to keras/best_baseline_weights.hdf5\n",
      "Epoch 122/10000\n",
      "2700/2700 [==============================] - 0s 46us/step - loss: 0.0034 - mean_absolute_error: 0.0328\n",
      "\n",
      "Epoch 00122: loss improved from 0.00344 to 0.00342, saving model to keras/best_baseline_weights.hdf5\n",
      "Epoch 123/10000\n",
      "2700/2700 [==============================] - 0s 48us/step - loss: 0.0034 - mean_absolute_error: 0.0328\n",
      "\n",
      "Epoch 00123: loss improved from 0.00342 to 0.00342, saving model to keras/best_baseline_weights.hdf5\n",
      "Epoch 124/10000\n",
      "2700/2700 [==============================] - 0s 48us/step - loss: 0.0034 - mean_absolute_error: 0.0325\n",
      "\n",
      "Epoch 00124: loss improved from 0.00342 to 0.00339, saving model to keras/best_baseline_weights.hdf5\n",
      "Epoch 125/10000\n",
      "2700/2700 [==============================] - 0s 45us/step - loss: 0.0034 - mean_absolute_error: 0.0325\n",
      "\n",
      "Epoch 00125: loss improved from 0.00339 to 0.00338, saving model to keras/best_baseline_weights.hdf5\n",
      "Epoch 126/10000\n",
      "2700/2700 [==============================] - 0s 47us/step - loss: 0.0033 - mean_absolute_error: 0.0326\n",
      "\n",
      "Epoch 00126: loss improved from 0.00338 to 0.00335, saving model to keras/best_baseline_weights.hdf5\n",
      "Epoch 127/10000\n",
      "2700/2700 [==============================] - 0s 42us/step - loss: 0.0034 - mean_absolute_error: 0.0329\n",
      "\n",
      "Epoch 00127: loss did not improve\n",
      "Epoch 128/10000\n",
      "2700/2700 [==============================] - 0s 36us/step - loss: 0.0033 - mean_absolute_error: 0.0324\n",
      "\n",
      "Epoch 00128: loss improved from 0.00335 to 0.00332, saving model to keras/best_baseline_weights.hdf5\n",
      "Epoch 129/10000\n",
      "2700/2700 [==============================] - 0s 43us/step - loss: 0.0033 - mean_absolute_error: 0.0320\n",
      "\n",
      "Epoch 00129: loss improved from 0.00332 to 0.00330, saving model to keras/best_baseline_weights.hdf5\n",
      "Epoch 130/10000\n",
      "2700/2700 [==============================] - 0s 46us/step - loss: 0.0033 - mean_absolute_error: 0.0322\n",
      "\n",
      "Epoch 00130: loss improved from 0.00330 to 0.00329, saving model to keras/best_baseline_weights.hdf5\n",
      "Epoch 131/10000\n",
      "2700/2700 [==============================] - 0s 49us/step - loss: 0.0032 - mean_absolute_error: 0.0316\n",
      "\n",
      "Epoch 00131: loss improved from 0.00329 to 0.00324, saving model to keras/best_baseline_weights.hdf5\n",
      "Epoch 132/10000\n",
      "2700/2700 [==============================] - 0s 50us/step - loss: 0.0032 - mean_absolute_error: 0.0318\n",
      "\n",
      "Epoch 00132: loss improved from 0.00324 to 0.00324, saving model to keras/best_baseline_weights.hdf5\n",
      "Epoch 133/10000\n",
      "2700/2700 [==============================] - 0s 46us/step - loss: 0.0032 - mean_absolute_error: 0.0314\n",
      "\n",
      "Epoch 00133: loss improved from 0.00324 to 0.00321, saving model to keras/best_baseline_weights.hdf5\n",
      "Epoch 134/10000\n",
      "2700/2700 [==============================] - 0s 52us/step - loss: 0.0032 - mean_absolute_error: 0.0314\n",
      "\n",
      "Epoch 00134: loss improved from 0.00321 to 0.00319, saving model to keras/best_baseline_weights.hdf5\n",
      "Epoch 135/10000\n",
      "2700/2700 [==============================] - 0s 43us/step - loss: 0.0032 - mean_absolute_error: 0.0314\n",
      "\n",
      "Epoch 00135: loss improved from 0.00319 to 0.00319, saving model to keras/best_baseline_weights.hdf5\n",
      "Epoch 136/10000\n",
      "2700/2700 [==============================] - 0s 42us/step - loss: 0.0032 - mean_absolute_error: 0.0311\n",
      "\n",
      "Epoch 00136: loss improved from 0.00319 to 0.00316, saving model to keras/best_baseline_weights.hdf5\n",
      "Epoch 137/10000\n",
      "2700/2700 [==============================] - 0s 43us/step - loss: 0.0032 - mean_absolute_error: 0.0310\n",
      "\n",
      "Epoch 00137: loss improved from 0.00316 to 0.00315, saving model to keras/best_baseline_weights.hdf5\n",
      "Epoch 138/10000\n",
      "2700/2700 [==============================] - 0s 41us/step - loss: 0.0031 - mean_absolute_error: 0.0310\n",
      "\n",
      "Epoch 00138: loss improved from 0.00315 to 0.00312, saving model to keras/best_baseline_weights.hdf5\n",
      "Epoch 139/10000\n",
      "2700/2700 [==============================] - 0s 45us/step - loss: 0.0031 - mean_absolute_error: 0.0310\n",
      "\n",
      "Epoch 00139: loss improved from 0.00312 to 0.00311, saving model to keras/best_baseline_weights.hdf5\n",
      "Epoch 140/10000\n",
      "2700/2700 [==============================] - 0s 45us/step - loss: 0.0031 - mean_absolute_error: 0.0311\n",
      "\n",
      "Epoch 00140: loss improved from 0.00311 to 0.00310, saving model to keras/best_baseline_weights.hdf5\n",
      "Epoch 141/10000\n",
      "2700/2700 [==============================] - 0s 36us/step - loss: 0.0031 - mean_absolute_error: 0.0308\n",
      "\n",
      "Epoch 00141: loss improved from 0.00310 to 0.00307, saving model to keras/best_baseline_weights.hdf5\n",
      "Epoch 142/10000\n",
      "2700/2700 [==============================] - 0s 41us/step - loss: 0.0031 - mean_absolute_error: 0.0314\n",
      "\n",
      "Epoch 00142: loss did not improve\n",
      "Epoch 143/10000\n",
      "2700/2700 [==============================] - 0s 38us/step - loss: 0.0030 - mean_absolute_error: 0.0305\n",
      "\n",
      "Epoch 00143: loss improved from 0.00307 to 0.00304, saving model to keras/best_baseline_weights.hdf5\n",
      "Epoch 144/10000\n",
      "2700/2700 [==============================] - 0s 40us/step - loss: 0.0030 - mean_absolute_error: 0.0306\n",
      "\n",
      "Epoch 00144: loss improved from 0.00304 to 0.00304, saving model to keras/best_baseline_weights.hdf5\n",
      "Epoch 145/10000\n",
      "2700/2700 [==============================] - 0s 38us/step - loss: 0.0030 - mean_absolute_error: 0.0302\n",
      "\n",
      "Epoch 00145: loss improved from 0.00304 to 0.00301, saving model to keras/best_baseline_weights.hdf5\n",
      "Epoch 146/10000\n",
      "2700/2700 [==============================] - 0s 44us/step - loss: 0.0030 - mean_absolute_error: 0.0308\n",
      "\n",
      "Epoch 00146: loss did not improve\n",
      "Epoch 147/10000\n",
      "2700/2700 [==============================] - 0s 41us/step - loss: 0.0030 - mean_absolute_error: 0.0306\n",
      "\n",
      "Epoch 00147: loss improved from 0.00301 to 0.00298, saving model to keras/best_baseline_weights.hdf5\n",
      "Epoch 148/10000\n",
      "2700/2700 [==============================] - 0s 41us/step - loss: 0.0030 - mean_absolute_error: 0.0303\n",
      "\n",
      "Epoch 00148: loss improved from 0.00298 to 0.00297, saving model to keras/best_baseline_weights.hdf5\n",
      "Epoch 149/10000\n",
      "2700/2700 [==============================] - 0s 49us/step - loss: 0.0030 - mean_absolute_error: 0.0304\n",
      "\n",
      "Epoch 00149: loss improved from 0.00297 to 0.00296, saving model to keras/best_baseline_weights.hdf5\n",
      "Epoch 150/10000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2700/2700 [==============================] - 0s 48us/step - loss: 0.0030 - mean_absolute_error: 0.0305\n",
      "\n",
      "Epoch 00150: loss improved from 0.00296 to 0.00296, saving model to keras/best_baseline_weights.hdf5\n",
      "Epoch 151/10000\n",
      "2700/2700 [==============================] - 0s 45us/step - loss: 0.0029 - mean_absolute_error: 0.0302\n",
      "\n",
      "Epoch 00151: loss improved from 0.00296 to 0.00293, saving model to keras/best_baseline_weights.hdf5\n",
      "Epoch 152/10000\n",
      "2700/2700 [==============================] - 0s 47us/step - loss: 0.0029 - mean_absolute_error: 0.0307\n",
      "\n",
      "Epoch 00152: loss did not improve\n",
      "Epoch 153/10000\n",
      "2700/2700 [==============================] - 0s 54us/step - loss: 0.0029 - mean_absolute_error: 0.0299\n",
      "\n",
      "Epoch 00153: loss improved from 0.00293 to 0.00290, saving model to keras/best_baseline_weights.hdf5\n",
      "Epoch 154/10000\n",
      "2700/2700 [==============================] - 0s 44us/step - loss: 0.0029 - mean_absolute_error: 0.0304\n",
      "\n",
      "Epoch 00154: loss did not improve\n",
      "Epoch 155/10000\n",
      "2700/2700 [==============================] - 0s 37us/step - loss: 0.0029 - mean_absolute_error: 0.0302\n",
      "\n",
      "Epoch 00155: loss improved from 0.00290 to 0.00288, saving model to keras/best_baseline_weights.hdf5\n",
      "Epoch 156/10000\n",
      "2700/2700 [==============================] - 0s 37us/step - loss: 0.0029 - mean_absolute_error: 0.0298\n",
      "\n",
      "Epoch 00156: loss improved from 0.00288 to 0.00287, saving model to keras/best_baseline_weights.hdf5\n",
      "Epoch 157/10000\n",
      "2700/2700 [==============================] - 0s 39us/step - loss: 0.0029 - mean_absolute_error: 0.0303\n",
      "\n",
      "Epoch 00157: loss did not improve\n",
      "Epoch 158/10000\n",
      "2700/2700 [==============================] - 0s 36us/step - loss: 0.0029 - mean_absolute_error: 0.0302\n",
      "\n",
      "Epoch 00158: loss improved from 0.00287 to 0.00285, saving model to keras/best_baseline_weights.hdf5\n",
      "Epoch 159/10000\n",
      "2700/2700 [==============================] - 0s 37us/step - loss: 0.0028 - mean_absolute_error: 0.0298\n",
      "\n",
      "Epoch 00159: loss improved from 0.00285 to 0.00283, saving model to keras/best_baseline_weights.hdf5\n",
      "Epoch 160/10000\n",
      "2700/2700 [==============================] - 0s 37us/step - loss: 0.0028 - mean_absolute_error: 0.0300\n",
      "\n",
      "Epoch 00160: loss improved from 0.00283 to 0.00282, saving model to keras/best_baseline_weights.hdf5\n",
      "Epoch 161/10000\n",
      "2700/2700 [==============================] - 0s 42us/step - loss: 0.0028 - mean_absolute_error: 0.0298\n",
      "\n",
      "Epoch 00161: loss improved from 0.00282 to 0.00281, saving model to keras/best_baseline_weights.hdf5\n",
      "Epoch 162/10000\n",
      "2700/2700 [==============================] - 0s 37us/step - loss: 0.0028 - mean_absolute_error: 0.0299\n",
      "\n",
      "Epoch 00162: loss improved from 0.00281 to 0.00279, saving model to keras/best_baseline_weights.hdf5\n",
      "Epoch 163/10000\n",
      "2700/2700 [==============================] - 0s 42us/step - loss: 0.0028 - mean_absolute_error: 0.0297\n",
      "\n",
      "Epoch 00163: loss improved from 0.00279 to 0.00278, saving model to keras/best_baseline_weights.hdf5\n",
      "Epoch 164/10000\n",
      "2700/2700 [==============================] - 0s 39us/step - loss: 0.0028 - mean_absolute_error: 0.0301\n",
      "\n",
      "Epoch 00164: loss did not improve\n",
      "Epoch 165/10000\n",
      "2700/2700 [==============================] - 0s 37us/step - loss: 0.0028 - mean_absolute_error: 0.0298\n",
      "\n",
      "Epoch 00165: loss improved from 0.00278 to 0.00277, saving model to keras/best_baseline_weights.hdf5\n",
      "Epoch 166/10000\n",
      "2700/2700 [==============================] - 0s 41us/step - loss: 0.0028 - mean_absolute_error: 0.0296\n",
      "\n",
      "Epoch 00166: loss improved from 0.00277 to 0.00275, saving model to keras/best_baseline_weights.hdf5\n",
      "Epoch 167/10000\n",
      "2700/2700 [==============================] - 0s 42us/step - loss: 0.0027 - mean_absolute_error: 0.0295\n",
      "\n",
      "Epoch 00167: loss improved from 0.00275 to 0.00273, saving model to keras/best_baseline_weights.hdf5\n",
      "Epoch 168/10000\n",
      "2700/2700 [==============================] - 0s 42us/step - loss: 0.0028 - mean_absolute_error: 0.0297\n",
      "\n",
      "Epoch 00168: loss did not improve\n",
      "Epoch 169/10000\n",
      "2700/2700 [==============================] - 0s 32us/step - loss: 0.0027 - mean_absolute_error: 0.0295\n",
      "\n",
      "Epoch 00169: loss improved from 0.00273 to 0.00272, saving model to keras/best_baseline_weights.hdf5\n",
      "Epoch 170/10000\n",
      "2700/2700 [==============================] - 0s 41us/step - loss: 0.0027 - mean_absolute_error: 0.0296\n",
      "\n",
      "Epoch 00170: loss improved from 0.00272 to 0.00270, saving model to keras/best_baseline_weights.hdf5\n",
      "Epoch 171/10000\n",
      "2700/2700 [==============================] - 0s 37us/step - loss: 0.0027 - mean_absolute_error: 0.0297\n",
      "\n",
      "Epoch 00171: loss did not improve\n",
      "Epoch 172/10000\n",
      "2700/2700 [==============================] - 0s 37us/step - loss: 0.0027 - mean_absolute_error: 0.0294\n",
      "\n",
      "Epoch 00172: loss improved from 0.00270 to 0.00269, saving model to keras/best_baseline_weights.hdf5\n",
      "Epoch 173/10000\n",
      "2700/2700 [==============================] - 0s 36us/step - loss: 0.0027 - mean_absolute_error: 0.0294\n",
      "\n",
      "Epoch 00173: loss improved from 0.00269 to 0.00268, saving model to keras/best_baseline_weights.hdf5\n",
      "Epoch 174/10000\n",
      "2700/2700 [==============================] - 0s 40us/step - loss: 0.0027 - mean_absolute_error: 0.0297\n",
      "\n",
      "Epoch 00174: loss improved from 0.00268 to 0.00268, saving model to keras/best_baseline_weights.hdf5\n",
      "Epoch 175/10000\n",
      "2700/2700 [==============================] - 0s 35us/step - loss: 0.0026 - mean_absolute_error: 0.0293\n",
      "\n",
      "Epoch 00175: loss improved from 0.00268 to 0.00265, saving model to keras/best_baseline_weights.hdf5\n",
      "Epoch 176/10000\n",
      "2700/2700 [==============================] - 0s 39us/step - loss: 0.0026 - mean_absolute_error: 0.0293\n",
      "\n",
      "Epoch 00176: loss improved from 0.00265 to 0.00264, saving model to keras/best_baseline_weights.hdf5\n",
      "Epoch 177/10000\n",
      "2700/2700 [==============================] - 0s 46us/step - loss: 0.0026 - mean_absolute_error: 0.0293\n",
      "\n",
      "Epoch 00177: loss improved from 0.00264 to 0.00264, saving model to keras/best_baseline_weights.hdf5\n",
      "Epoch 178/10000\n",
      "2700/2700 [==============================] - 0s 47us/step - loss: 0.0026 - mean_absolute_error: 0.0295\n",
      "\n",
      "Epoch 00178: loss improved from 0.00264 to 0.00263, saving model to keras/best_baseline_weights.hdf5\n",
      "Epoch 179/10000\n",
      "2700/2700 [==============================] - 0s 52us/step - loss: 0.0026 - mean_absolute_error: 0.0295\n",
      "\n",
      "Epoch 00179: loss did not improve\n",
      "Epoch 180/10000\n",
      "2700/2700 [==============================] - 0s 48us/step - loss: 0.0026 - mean_absolute_error: 0.0294\n",
      "\n",
      "Epoch 00180: loss improved from 0.00263 to 0.00263, saving model to keras/best_baseline_weights.hdf5\n",
      "Epoch 181/10000\n",
      "2700/2700 [==============================] - 0s 50us/step - loss: 0.0026 - mean_absolute_error: 0.0292\n",
      "\n",
      "Epoch 00181: loss improved from 0.00263 to 0.00261, saving model to keras/best_baseline_weights.hdf5\n",
      "Epoch 182/10000\n",
      "2700/2700 [==============================] - 0s 45us/step - loss: 0.0026 - mean_absolute_error: 0.0293\n",
      "\n",
      "Epoch 00182: loss improved from 0.00261 to 0.00260, saving model to keras/best_baseline_weights.hdf5\n",
      "Epoch 183/10000\n",
      "2700/2700 [==============================] - 0s 38us/step - loss: 0.0026 - mean_absolute_error: 0.0292\n",
      "\n",
      "Epoch 00183: loss improved from 0.00260 to 0.00260, saving model to keras/best_baseline_weights.hdf5\n",
      "Epoch 184/10000\n",
      "2700/2700 [==============================] - 0s 40us/step - loss: 0.0026 - mean_absolute_error: 0.0290\n",
      "\n",
      "Epoch 00184: loss improved from 0.00260 to 0.00258, saving model to keras/best_baseline_weights.hdf5\n",
      "Epoch 185/10000\n",
      "2700/2700 [==============================] - 0s 42us/step - loss: 0.0026 - mean_absolute_error: 0.0290\n",
      "\n",
      "Epoch 00185: loss improved from 0.00258 to 0.00257, saving model to keras/best_baseline_weights.hdf5\n",
      "Epoch 186/10000\n",
      "2700/2700 [==============================] - 0s 38us/step - loss: 0.0026 - mean_absolute_error: 0.0292\n",
      "\n",
      "Epoch 00186: loss improved from 0.00257 to 0.00256, saving model to keras/best_baseline_weights.hdf5\n",
      "Epoch 187/10000\n",
      "2700/2700 [==============================] - 0s 44us/step - loss: 0.0025 - mean_absolute_error: 0.0289\n",
      "\n",
      "Epoch 00187: loss improved from 0.00256 to 0.00254, saving model to keras/best_baseline_weights.hdf5\n",
      "Epoch 188/10000\n",
      "2700/2700 [==============================] - 0s 38us/step - loss: 0.0026 - mean_absolute_error: 0.0292\n",
      "\n",
      "Epoch 00188: loss did not improve\n",
      "Epoch 189/10000\n",
      "2700/2700 [==============================] - 0s 34us/step - loss: 0.0025 - mean_absolute_error: 0.0287\n",
      "\n",
      "Epoch 00189: loss improved from 0.00254 to 0.00253, saving model to keras/best_baseline_weights.hdf5\n",
      "Epoch 190/10000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2700/2700 [==============================] - 0s 41us/step - loss: 0.0025 - mean_absolute_error: 0.0291\n",
      "\n",
      "Epoch 00190: loss did not improve\n",
      "Epoch 191/10000\n",
      "2700/2700 [==============================] - 0s 38us/step - loss: 0.0025 - mean_absolute_error: 0.0291\n",
      "\n",
      "Epoch 00191: loss did not improve\n",
      "Epoch 192/10000\n",
      "2700/2700 [==============================] - 0s 37us/step - loss: 0.0025 - mean_absolute_error: 0.0290\n",
      "\n",
      "Epoch 00192: loss improved from 0.00253 to 0.00251, saving model to keras/best_baseline_weights.hdf5\n",
      "Epoch 193/10000\n",
      "2700/2700 [==============================] - 0s 36us/step - loss: 0.0025 - mean_absolute_error: 0.0288\n",
      "\n",
      "Epoch 00193: loss improved from 0.00251 to 0.00250, saving model to keras/best_baseline_weights.hdf5\n",
      "Epoch 194/10000\n",
      "2700/2700 [==============================] - 0s 39us/step - loss: 0.0025 - mean_absolute_error: 0.0289\n",
      "\n",
      "Epoch 00194: loss improved from 0.00250 to 0.00249, saving model to keras/best_baseline_weights.hdf5\n",
      "Epoch 195/10000\n",
      "2700/2700 [==============================] - 0s 37us/step - loss: 0.0025 - mean_absolute_error: 0.0289\n",
      "\n",
      "Epoch 00195: loss improved from 0.00249 to 0.00248, saving model to keras/best_baseline_weights.hdf5\n",
      "Epoch 196/10000\n",
      "2700/2700 [==============================] - 0s 49us/step - loss: 0.0025 - mean_absolute_error: 0.0289\n",
      "\n",
      "Epoch 00196: loss improved from 0.00248 to 0.00248, saving model to keras/best_baseline_weights.hdf5\n",
      "Epoch 197/10000\n",
      "2700/2700 [==============================] - 0s 56us/step - loss: 0.0025 - mean_absolute_error: 0.0290\n",
      "\n",
      "Epoch 00197: loss did not improve\n",
      "Epoch 198/10000\n",
      "2700/2700 [==============================] - 0s 50us/step - loss: 0.0025 - mean_absolute_error: 0.0287\n",
      "\n",
      "Epoch 00198: loss improved from 0.00248 to 0.00247, saving model to keras/best_baseline_weights.hdf5\n",
      "Epoch 199/10000\n",
      "2700/2700 [==============================] - 0s 50us/step - loss: 0.0025 - mean_absolute_error: 0.0289\n",
      "\n",
      "Epoch 00199: loss improved from 0.00247 to 0.00246, saving model to keras/best_baseline_weights.hdf5\n",
      "Epoch 200/10000\n",
      "2700/2700 [==============================] - 0s 52us/step - loss: 0.0025 - mean_absolute_error: 0.0287\n",
      "\n",
      "Epoch 00200: loss improved from 0.00246 to 0.00246, saving model to keras/best_baseline_weights.hdf5\n",
      "Epoch 201/10000\n",
      "2700/2700 [==============================] - 0s 43us/step - loss: 0.0025 - mean_absolute_error: 0.0290\n",
      "\n",
      "Epoch 00201: loss improved from 0.00246 to 0.00246, saving model to keras/best_baseline_weights.hdf5\n",
      "Epoch 202/10000\n",
      "2700/2700 [==============================] - 0s 49us/step - loss: 0.0024 - mean_absolute_error: 0.0286\n",
      "\n",
      "Epoch 00202: loss improved from 0.00246 to 0.00244, saving model to keras/best_baseline_weights.hdf5\n",
      "Epoch 203/10000\n",
      "2700/2700 [==============================] - 0s 47us/step - loss: 0.0024 - mean_absolute_error: 0.0286\n",
      "\n",
      "Epoch 00203: loss improved from 0.00244 to 0.00243, saving model to keras/best_baseline_weights.hdf5\n",
      "Epoch 204/10000\n",
      "2700/2700 [==============================] - 0s 52us/step - loss: 0.0024 - mean_absolute_error: 0.0288\n",
      "\n",
      "Epoch 00204: loss improved from 0.00243 to 0.00242, saving model to keras/best_baseline_weights.hdf5\n",
      "Epoch 205/10000\n",
      "2700/2700 [==============================] - 0s 55us/step - loss: 0.0024 - mean_absolute_error: 0.0284\n",
      "\n",
      "Epoch 00205: loss improved from 0.00242 to 0.00241, saving model to keras/best_baseline_weights.hdf5\n",
      "Epoch 206/10000\n",
      "2700/2700 [==============================] - 0s 50us/step - loss: 0.0024 - mean_absolute_error: 0.0286\n",
      "\n",
      "Epoch 00206: loss improved from 0.00241 to 0.00240, saving model to keras/best_baseline_weights.hdf5\n",
      "Epoch 207/10000\n",
      "2700/2700 [==============================] - 0s 47us/step - loss: 0.0024 - mean_absolute_error: 0.0283\n",
      "\n",
      "Epoch 00207: loss improved from 0.00240 to 0.00240, saving model to keras/best_baseline_weights.hdf5\n",
      "Epoch 208/10000\n",
      "2700/2700 [==============================] - 0s 49us/step - loss: 0.0024 - mean_absolute_error: 0.0284\n",
      "\n",
      "Epoch 00208: loss improved from 0.00240 to 0.00239, saving model to keras/best_baseline_weights.hdf5\n",
      "Epoch 209/10000\n",
      "2700/2700 [==============================] - 0s 50us/step - loss: 0.0024 - mean_absolute_error: 0.0287\n",
      "\n",
      "Epoch 00209: loss improved from 0.00239 to 0.00239, saving model to keras/best_baseline_weights.hdf5\n",
      "Epoch 210/10000\n",
      "2700/2700 [==============================] - 0s 44us/step - loss: 0.0024 - mean_absolute_error: 0.0287\n",
      "\n",
      "Epoch 00210: loss did not improve\n",
      "Epoch 211/10000\n",
      "2700/2700 [==============================] - 0s 42us/step - loss: 0.0024 - mean_absolute_error: 0.0284\n",
      "\n",
      "Epoch 00211: loss improved from 0.00239 to 0.00237, saving model to keras/best_baseline_weights.hdf5\n",
      "Epoch 212/10000\n",
      "2700/2700 [==============================] - 0s 43us/step - loss: 0.0024 - mean_absolute_error: 0.0281\n",
      "\n",
      "Epoch 00212: loss improved from 0.00237 to 0.00235, saving model to keras/best_baseline_weights.hdf5\n",
      "Epoch 213/10000\n",
      "2700/2700 [==============================] - 0s 49us/step - loss: 0.0024 - mean_absolute_error: 0.0286\n",
      "\n",
      "Epoch 00213: loss did not improve\n",
      "Epoch 214/10000\n",
      "2700/2700 [==============================] - 0s 54us/step - loss: 0.0023 - mean_absolute_error: 0.0282\n",
      "\n",
      "Epoch 00214: loss improved from 0.00235 to 0.00234, saving model to keras/best_baseline_weights.hdf5\n",
      "Epoch 215/10000\n",
      "2700/2700 [==============================] - 0s 44us/step - loss: 0.0024 - mean_absolute_error: 0.0285\n",
      "\n",
      "Epoch 00215: loss did not improve\n",
      "Epoch 216/10000\n",
      "2700/2700 [==============================] - 0s 41us/step - loss: 0.0023 - mean_absolute_error: 0.0280\n",
      "\n",
      "Epoch 00216: loss improved from 0.00234 to 0.00233, saving model to keras/best_baseline_weights.hdf5\n",
      "Epoch 217/10000\n",
      "2700/2700 [==============================] - 0s 51us/step - loss: 0.0023 - mean_absolute_error: 0.0283\n",
      "\n",
      "Epoch 00217: loss improved from 0.00233 to 0.00233, saving model to keras/best_baseline_weights.hdf5\n",
      "Epoch 218/10000\n",
      "2700/2700 [==============================] - 0s 47us/step - loss: 0.0023 - mean_absolute_error: 0.0279\n",
      "\n",
      "Epoch 00218: loss improved from 0.00233 to 0.00231, saving model to keras/best_baseline_weights.hdf5\n",
      "Epoch 219/10000\n",
      "2700/2700 [==============================] - 0s 42us/step - loss: 0.0023 - mean_absolute_error: 0.0278\n",
      "\n",
      "Epoch 00219: loss improved from 0.00231 to 0.00231, saving model to keras/best_baseline_weights.hdf5\n",
      "Epoch 220/10000\n",
      "2700/2700 [==============================] - 0s 41us/step - loss: 0.0023 - mean_absolute_error: 0.0278\n",
      "\n",
      "Epoch 00220: loss improved from 0.00231 to 0.00230, saving model to keras/best_baseline_weights.hdf5\n",
      "Epoch 221/10000\n",
      "2700/2700 [==============================] - 0s 42us/step - loss: 0.0023 - mean_absolute_error: 0.0280\n",
      "\n",
      "Epoch 00221: loss improved from 0.00230 to 0.00229, saving model to keras/best_baseline_weights.hdf5\n",
      "Epoch 222/10000\n",
      "2700/2700 [==============================] - 0s 52us/step - loss: 0.0023 - mean_absolute_error: 0.0279\n",
      "\n",
      "Epoch 00222: loss improved from 0.00229 to 0.00228, saving model to keras/best_baseline_weights.hdf5\n",
      "Epoch 223/10000\n",
      "2700/2700 [==============================] - 0s 53us/step - loss: 0.0023 - mean_absolute_error: 0.0277\n",
      "\n",
      "Epoch 00223: loss improved from 0.00228 to 0.00228, saving model to keras/best_baseline_weights.hdf5\n",
      "Epoch 224/10000\n",
      "2700/2700 [==============================] - 0s 47us/step - loss: 0.0023 - mean_absolute_error: 0.0277\n",
      "\n",
      "Epoch 00224: loss improved from 0.00228 to 0.00227, saving model to keras/best_baseline_weights.hdf5\n",
      "Epoch 225/10000\n",
      "2700/2700 [==============================] - 0s 49us/step - loss: 0.0023 - mean_absolute_error: 0.0277\n",
      "\n",
      "Epoch 00225: loss improved from 0.00227 to 0.00226, saving model to keras/best_baseline_weights.hdf5\n",
      "Epoch 226/10000\n",
      "2700/2700 [==============================] - 0s 46us/step - loss: 0.0022 - mean_absolute_error: 0.0276\n",
      "\n",
      "Epoch 00226: loss improved from 0.00226 to 0.00225, saving model to keras/best_baseline_weights.hdf5\n",
      "Epoch 227/10000\n",
      "2700/2700 [==============================] - 0s 51us/step - loss: 0.0023 - mean_absolute_error: 0.0277\n",
      "\n",
      "Epoch 00227: loss did not improve\n",
      "Epoch 228/10000\n",
      "2700/2700 [==============================] - 0s 43us/step - loss: 0.0022 - mean_absolute_error: 0.0273\n",
      "\n",
      "Epoch 00228: loss improved from 0.00225 to 0.00224, saving model to keras/best_baseline_weights.hdf5\n",
      "Epoch 229/10000\n",
      "2700/2700 [==============================] - 0s 40us/step - loss: 0.0022 - mean_absolute_error: 0.0280\n",
      "\n",
      "Epoch 00229: loss did not improve\n",
      "Epoch 230/10000\n",
      "2700/2700 [==============================] - 0s 38us/step - loss: 0.0022 - mean_absolute_error: 0.0275\n",
      "\n",
      "Epoch 00230: loss improved from 0.00224 to 0.00224, saving model to keras/best_baseline_weights.hdf5\n",
      "Epoch 231/10000\n",
      "2700/2700 [==============================] - 0s 37us/step - loss: 0.0022 - mean_absolute_error: 0.0273\n",
      "\n",
      "Epoch 00231: loss improved from 0.00224 to 0.00221, saving model to keras/best_baseline_weights.hdf5\n",
      "Epoch 232/10000\n",
      "2700/2700 [==============================] - 0s 36us/step - loss: 0.0022 - mean_absolute_error: 0.0279\n",
      "\n",
      "Epoch 00232: loss did not improve\n",
      "Epoch 233/10000\n",
      "2700/2700 [==============================] - 0s 35us/step - loss: 0.0022 - mean_absolute_error: 0.0273\n",
      "\n",
      "Epoch 00233: loss improved from 0.00221 to 0.00220, saving model to keras/best_baseline_weights.hdf5\n",
      "Epoch 234/10000\n",
      "2700/2700 [==============================] - 0s 35us/step - loss: 0.0022 - mean_absolute_error: 0.0272\n",
      "\n",
      "Epoch 00234: loss did not improve\n",
      "Epoch 235/10000\n",
      "2700/2700 [==============================] - 0s 37us/step - loss: 0.0022 - mean_absolute_error: 0.0272\n",
      "\n",
      "Epoch 00235: loss improved from 0.00220 to 0.00218, saving model to keras/best_baseline_weights.hdf5\n",
      "Epoch 236/10000\n",
      "2700/2700 [==============================] - 0s 39us/step - loss: 0.0022 - mean_absolute_error: 0.0273\n",
      "\n",
      "Epoch 00236: loss improved from 0.00218 to 0.00218, saving model to keras/best_baseline_weights.hdf5\n",
      "Epoch 237/10000\n",
      "2700/2700 [==============================] - 0s 36us/step - loss: 0.0022 - mean_absolute_error: 0.0272\n",
      "\n",
      "Epoch 00237: loss improved from 0.00218 to 0.00218, saving model to keras/best_baseline_weights.hdf5\n",
      "Epoch 238/10000\n",
      "2700/2700 [==============================] - 0s 40us/step - loss: 0.0022 - mean_absolute_error: 0.0269\n",
      "\n",
      "Epoch 00238: loss improved from 0.00218 to 0.00216, saving model to keras/best_baseline_weights.hdf5\n",
      "Epoch 239/10000\n",
      "2700/2700 [==============================] - 0s 37us/step - loss: 0.0022 - mean_absolute_error: 0.0271\n",
      "\n",
      "Epoch 00239: loss improved from 0.00216 to 0.00215, saving model to keras/best_baseline_weights.hdf5\n",
      "Epoch 240/10000\n",
      "2700/2700 [==============================] - 0s 37us/step - loss: 0.0022 - mean_absolute_error: 0.0270\n",
      "\n",
      "Epoch 00240: loss improved from 0.00215 to 0.00215, saving model to keras/best_baseline_weights.hdf5\n",
      "Epoch 241/10000\n",
      "2700/2700 [==============================] - 0s 38us/step - loss: 0.0021 - mean_absolute_error: 0.0267\n",
      "\n",
      "Epoch 00241: loss improved from 0.00215 to 0.00213, saving model to keras/best_baseline_weights.hdf5\n",
      "Epoch 242/10000\n",
      "2700/2700 [==============================] - 0s 38us/step - loss: 0.0021 - mean_absolute_error: 0.0270\n",
      "\n",
      "Epoch 00242: loss did not improve\n",
      "Epoch 243/10000\n",
      "2700/2700 [==============================] - 0s 42us/step - loss: 0.0021 - mean_absolute_error: 0.0269\n",
      "\n",
      "Epoch 00243: loss did not improve\n",
      "Epoch 244/10000\n",
      "2700/2700 [==============================] - 0s 40us/step - loss: 0.0021 - mean_absolute_error: 0.0266\n",
      "\n",
      "Epoch 00244: loss improved from 0.00213 to 0.00211, saving model to keras/best_baseline_weights.hdf5\n",
      "Epoch 245/10000\n",
      "2700/2700 [==============================] - 0s 37us/step - loss: 0.0021 - mean_absolute_error: 0.0271\n",
      "\n",
      "Epoch 00245: loss did not improve\n",
      "Epoch 246/10000\n",
      "2700/2700 [==============================] - 0s 35us/step - loss: 0.0021 - mean_absolute_error: 0.0266\n",
      "\n",
      "Epoch 00246: loss improved from 0.00211 to 0.00210, saving model to keras/best_baseline_weights.hdf5\n",
      "Epoch 247/10000\n",
      "2700/2700 [==============================] - 0s 34us/step - loss: 0.0021 - mean_absolute_error: 0.0265\n",
      "\n",
      "Epoch 00247: loss improved from 0.00210 to 0.00208, saving model to keras/best_baseline_weights.hdf5\n",
      "Epoch 248/10000\n",
      "2700/2700 [==============================] - 0s 35us/step - loss: 0.0021 - mean_absolute_error: 0.0264\n",
      "\n",
      "Epoch 00248: loss improved from 0.00208 to 0.00207, saving model to keras/best_baseline_weights.hdf5\n",
      "Epoch 249/10000\n",
      "2700/2700 [==============================] - 0s 38us/step - loss: 0.0021 - mean_absolute_error: 0.0266\n",
      "\n",
      "Epoch 00249: loss did not improve\n",
      "Epoch 250/10000\n",
      "2700/2700 [==============================] - 0s 36us/step - loss: 0.0021 - mean_absolute_error: 0.0262\n",
      "\n",
      "Epoch 00250: loss improved from 0.00207 to 0.00206, saving model to keras/best_baseline_weights.hdf5\n",
      "Epoch 251/10000\n",
      "2700/2700 [==============================] - 0s 34us/step - loss: 0.0021 - mean_absolute_error: 0.0269\n",
      "\n",
      "Epoch 00251: loss did not improve\n",
      "Epoch 252/10000\n",
      "2700/2700 [==============================] - 0s 37us/step - loss: 0.0020 - mean_absolute_error: 0.0261\n",
      "\n",
      "Epoch 00252: loss improved from 0.00206 to 0.00205, saving model to keras/best_baseline_weights.hdf5\n",
      "Epoch 253/10000\n",
      "2700/2700 [==============================] - 0s 37us/step - loss: 0.0020 - mean_absolute_error: 0.0262\n",
      "\n",
      "Epoch 00253: loss improved from 0.00205 to 0.00203, saving model to keras/best_baseline_weights.hdf5\n",
      "Epoch 254/10000\n",
      "2700/2700 [==============================] - 0s 37us/step - loss: 0.0020 - mean_absolute_error: 0.0260\n",
      "\n",
      "Epoch 00254: loss improved from 0.00203 to 0.00202, saving model to keras/best_baseline_weights.hdf5\n",
      "Epoch 255/10000\n",
      "2700/2700 [==============================] - 0s 38us/step - loss: 0.0020 - mean_absolute_error: 0.0261\n",
      "\n",
      "Epoch 00255: loss did not improve\n",
      "Epoch 256/10000\n",
      "2700/2700 [==============================] - 0s 35us/step - loss: 0.0020 - mean_absolute_error: 0.0262\n",
      "\n",
      "Epoch 00256: loss improved from 0.00202 to 0.00202, saving model to keras/best_baseline_weights.hdf5\n",
      "Epoch 257/10000\n",
      "2700/2700 [==============================] - 0s 37us/step - loss: 0.0020 - mean_absolute_error: 0.0259\n",
      "\n",
      "Epoch 00257: loss improved from 0.00202 to 0.00200, saving model to keras/best_baseline_weights.hdf5\n",
      "Epoch 258/10000\n",
      "2700/2700 [==============================] - 0s 40us/step - loss: 0.0020 - mean_absolute_error: 0.0262\n",
      "\n",
      "Epoch 00258: loss did not improve\n",
      "Epoch 259/10000\n",
      "2700/2700 [==============================] - 0s 39us/step - loss: 0.0020 - mean_absolute_error: 0.0257\n",
      "\n",
      "Epoch 00259: loss improved from 0.00200 to 0.00199, saving model to keras/best_baseline_weights.hdf5\n",
      "Epoch 260/10000\n",
      "2700/2700 [==============================] - 0s 39us/step - loss: 0.0020 - mean_absolute_error: 0.0254\n",
      "\n",
      "Epoch 00260: loss improved from 0.00199 to 0.00197, saving model to keras/best_baseline_weights.hdf5\n",
      "Epoch 261/10000\n",
      "2700/2700 [==============================] - 0s 46us/step - loss: 0.0020 - mean_absolute_error: 0.0253\n",
      "\n",
      "Epoch 00261: loss improved from 0.00197 to 0.00196, saving model to keras/best_baseline_weights.hdf5\n",
      "Epoch 262/10000\n",
      "2700/2700 [==============================] - 0s 40us/step - loss: 0.0020 - mean_absolute_error: 0.0259\n",
      "\n",
      "Epoch 00262: loss did not improve\n",
      "Epoch 263/10000\n",
      "2700/2700 [==============================] - 0s 42us/step - loss: 0.0020 - mean_absolute_error: 0.0256\n",
      "\n",
      "Epoch 00263: loss improved from 0.00196 to 0.00195, saving model to keras/best_baseline_weights.hdf5\n",
      "Epoch 264/10000\n",
      "2700/2700 [==============================] - 0s 42us/step - loss: 0.0019 - mean_absolute_error: 0.0252\n",
      "\n",
      "Epoch 00264: loss improved from 0.00195 to 0.00193, saving model to keras/best_baseline_weights.hdf5\n",
      "Epoch 265/10000\n",
      "2700/2700 [==============================] - 0s 45us/step - loss: 0.0019 - mean_absolute_error: 0.0250\n",
      "\n",
      "Epoch 00265: loss improved from 0.00193 to 0.00192, saving model to keras/best_baseline_weights.hdf5\n",
      "Epoch 266/10000\n",
      "2700/2700 [==============================] - 0s 41us/step - loss: 0.0019 - mean_absolute_error: 0.0254\n",
      "\n",
      "Epoch 00266: loss improved from 0.00192 to 0.00192, saving model to keras/best_baseline_weights.hdf5\n",
      "Epoch 267/10000\n",
      "2700/2700 [==============================] - 0s 38us/step - loss: 0.0019 - mean_absolute_error: 0.0253\n",
      "\n",
      "Epoch 00267: loss improved from 0.00192 to 0.00191, saving model to keras/best_baseline_weights.hdf5\n",
      "Epoch 268/10000\n",
      "2700/2700 [==============================] - 0s 47us/step - loss: 0.0019 - mean_absolute_error: 0.0247\n",
      "\n",
      "Epoch 00268: loss improved from 0.00191 to 0.00189, saving model to keras/best_baseline_weights.hdf5\n",
      "Epoch 269/10000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2700/2700 [==============================] - 0s 55us/step - loss: 0.0019 - mean_absolute_error: 0.0248\n",
      "\n",
      "Epoch 00269: loss improved from 0.00189 to 0.00189, saving model to keras/best_baseline_weights.hdf5\n",
      "Epoch 270/10000\n",
      "2700/2700 [==============================] - 0s 40us/step - loss: 0.0019 - mean_absolute_error: 0.0249\n",
      "\n",
      "Epoch 00270: loss improved from 0.00189 to 0.00188, saving model to keras/best_baseline_weights.hdf5\n",
      "Epoch 271/10000\n",
      "2700/2700 [==============================] - 0s 39us/step - loss: 0.0019 - mean_absolute_error: 0.0249\n",
      "\n",
      "Epoch 00271: loss improved from 0.00188 to 0.00188, saving model to keras/best_baseline_weights.hdf5\n",
      "Epoch 272/10000\n",
      "2700/2700 [==============================] - 0s 41us/step - loss: 0.0019 - mean_absolute_error: 0.0249\n",
      "\n",
      "Epoch 00272: loss improved from 0.00188 to 0.00187, saving model to keras/best_baseline_weights.hdf5\n",
      "Epoch 273/10000\n",
      "2700/2700 [==============================] - 0s 53us/step - loss: 0.0019 - mean_absolute_error: 0.0244\n",
      "\n",
      "Epoch 00273: loss improved from 0.00187 to 0.00186, saving model to keras/best_baseline_weights.hdf5\n",
      "Epoch 274/10000\n",
      "2700/2700 [==============================] - 0s 39us/step - loss: 0.0019 - mean_absolute_error: 0.0248\n",
      "\n",
      "Epoch 00274: loss improved from 0.00186 to 0.00185, saving model to keras/best_baseline_weights.hdf5\n",
      "Epoch 275/10000\n",
      "2700/2700 [==============================] - 0s 34us/step - loss: 0.0018 - mean_absolute_error: 0.0245\n",
      "\n",
      "Epoch 00275: loss improved from 0.00185 to 0.00184, saving model to keras/best_baseline_weights.hdf5\n",
      "Epoch 276/10000\n",
      "2700/2700 [==============================] - 0s 34us/step - loss: 0.0018 - mean_absolute_error: 0.0244\n",
      "\n",
      "Epoch 00276: loss improved from 0.00184 to 0.00183, saving model to keras/best_baseline_weights.hdf5\n",
      "Epoch 277/10000\n",
      "2700/2700 [==============================] - 0s 38us/step - loss: 0.0018 - mean_absolute_error: 0.0241\n",
      "\n",
      "Epoch 00277: loss improved from 0.00183 to 0.00182, saving model to keras/best_baseline_weights.hdf5\n",
      "Epoch 278/10000\n",
      "2700/2700 [==============================] - 0s 34us/step - loss: 0.0018 - mean_absolute_error: 0.0241\n",
      "\n",
      "Epoch 00278: loss improved from 0.00182 to 0.00181, saving model to keras/best_baseline_weights.hdf5\n",
      "Epoch 279/10000\n",
      "2700/2700 [==============================] - 0s 35us/step - loss: 0.0018 - mean_absolute_error: 0.0241\n",
      "\n",
      "Epoch 00279: loss improved from 0.00181 to 0.00180, saving model to keras/best_baseline_weights.hdf5\n",
      "Epoch 280/10000\n",
      "2700/2700 [==============================] - 0s 33us/step - loss: 0.0018 - mean_absolute_error: 0.0245\n",
      "\n",
      "Epoch 00280: loss did not improve\n",
      "Epoch 281/10000\n",
      "2700/2700 [==============================] - 0s 45us/step - loss: 0.0018 - mean_absolute_error: 0.0242\n",
      "\n",
      "Epoch 00281: loss improved from 0.00180 to 0.00179, saving model to keras/best_baseline_weights.hdf5\n",
      "Epoch 282/10000\n",
      "2700/2700 [==============================] - 0s 49us/step - loss: 0.0018 - mean_absolute_error: 0.0238\n",
      "\n",
      "Epoch 00282: loss improved from 0.00179 to 0.00178, saving model to keras/best_baseline_weights.hdf5\n",
      "Epoch 283/10000\n",
      "2700/2700 [==============================] - 0s 49us/step - loss: 0.0018 - mean_absolute_error: 0.0237\n",
      "\n",
      "Epoch 00283: loss improved from 0.00178 to 0.00176, saving model to keras/best_baseline_weights.hdf5\n",
      "Epoch 284/10000\n",
      "2700/2700 [==============================] - 0s 40us/step - loss: 0.0018 - mean_absolute_error: 0.0239\n",
      "\n",
      "Epoch 00284: loss improved from 0.00176 to 0.00176, saving model to keras/best_baseline_weights.hdf5\n",
      "Epoch 285/10000\n",
      "2700/2700 [==============================] - 0s 38us/step - loss: 0.0017 - mean_absolute_error: 0.0238\n",
      "\n",
      "Epoch 00285: loss improved from 0.00176 to 0.00175, saving model to keras/best_baseline_weights.hdf5\n",
      "Epoch 286/10000\n",
      "2700/2700 [==============================] - 0s 39us/step - loss: 0.0017 - mean_absolute_error: 0.0237\n",
      "\n",
      "Epoch 00286: loss improved from 0.00175 to 0.00174, saving model to keras/best_baseline_weights.hdf5\n",
      "Epoch 287/10000\n",
      "2700/2700 [==============================] - 0s 35us/step - loss: 0.0018 - mean_absolute_error: 0.0244\n",
      "\n",
      "Epoch 00287: loss did not improve\n",
      "Epoch 288/10000\n",
      "2700/2700 [==============================] - 0s 38us/step - loss: 0.0017 - mean_absolute_error: 0.0236\n",
      "\n",
      "Epoch 00288: loss improved from 0.00174 to 0.00172, saving model to keras/best_baseline_weights.hdf5\n",
      "Epoch 289/10000\n",
      "2700/2700 [==============================] - 0s 36us/step - loss: 0.0017 - mean_absolute_error: 0.0234\n",
      "\n",
      "Epoch 00289: loss improved from 0.00172 to 0.00171, saving model to keras/best_baseline_weights.hdf5\n",
      "Epoch 290/10000\n",
      "2700/2700 [==============================] - 0s 40us/step - loss: 0.0017 - mean_absolute_error: 0.0233\n",
      "\n",
      "Epoch 00290: loss improved from 0.00171 to 0.00170, saving model to keras/best_baseline_weights.hdf5\n",
      "Epoch 291/10000\n",
      "2700/2700 [==============================] - 0s 42us/step - loss: 0.0017 - mean_absolute_error: 0.0231\n",
      "\n",
      "Epoch 00291: loss improved from 0.00170 to 0.00169, saving model to keras/best_baseline_weights.hdf5\n",
      "Epoch 292/10000\n",
      "2700/2700 [==============================] - 0s 46us/step - loss: 0.0017 - mean_absolute_error: 0.0234\n",
      "\n",
      "Epoch 00292: loss did not improve\n",
      "Epoch 293/10000\n",
      "2700/2700 [==============================] - 0s 38us/step - loss: 0.0017 - mean_absolute_error: 0.0234\n",
      "\n",
      "Epoch 00293: loss did not improve\n",
      "Epoch 294/10000\n",
      "2700/2700 [==============================] - 0s 43us/step - loss: 0.0017 - mean_absolute_error: 0.0232\n",
      "\n",
      "Epoch 00294: loss improved from 0.00169 to 0.00167, saving model to keras/best_baseline_weights.hdf5\n",
      "Epoch 295/10000\n",
      "2700/2700 [==============================] - 0s 36us/step - loss: 0.0017 - mean_absolute_error: 0.0233\n",
      "\n",
      "Epoch 00295: loss improved from 0.00167 to 0.00167, saving model to keras/best_baseline_weights.hdf5\n",
      "Epoch 296/10000\n",
      "2700/2700 [==============================] - 0s 48us/step - loss: 0.0017 - mean_absolute_error: 0.0231\n",
      "\n",
      "Epoch 00296: loss improved from 0.00167 to 0.00167, saving model to keras/best_baseline_weights.hdf5\n",
      "Epoch 297/10000\n",
      "2700/2700 [==============================] - 0s 65us/step - loss: 0.0016 - mean_absolute_error: 0.0229\n",
      "\n",
      "Epoch 00297: loss improved from 0.00167 to 0.00164, saving model to keras/best_baseline_weights.hdf5\n",
      "Epoch 298/10000\n",
      "2700/2700 [==============================] - 0s 42us/step - loss: 0.0016 - mean_absolute_error: 0.0227\n",
      "\n",
      "Epoch 00298: loss did not improve\n",
      "Epoch 299/10000\n",
      "2700/2700 [==============================] - 0s 43us/step - loss: 0.0016 - mean_absolute_error: 0.0226\n",
      "\n",
      "Epoch 00299: loss improved from 0.00164 to 0.00163, saving model to keras/best_baseline_weights.hdf5\n",
      "Epoch 300/10000\n",
      "2700/2700 [==============================] - 0s 41us/step - loss: 0.0016 - mean_absolute_error: 0.0230\n",
      "\n",
      "Epoch 00300: loss improved from 0.00163 to 0.00162, saving model to keras/best_baseline_weights.hdf5\n",
      "Epoch 301/10000\n",
      "2700/2700 [==============================] - 0s 53us/step - loss: 0.0016 - mean_absolute_error: 0.0224\n",
      "\n",
      "Epoch 00301: loss improved from 0.00162 to 0.00161, saving model to keras/best_baseline_weights.hdf5\n",
      "Epoch 302/10000\n",
      "2700/2700 [==============================] - 0s 46us/step - loss: 0.0016 - mean_absolute_error: 0.0226\n",
      "\n",
      "Epoch 00302: loss improved from 0.00161 to 0.00160, saving model to keras/best_baseline_weights.hdf5\n",
      "Epoch 303/10000\n",
      "2700/2700 [==============================] - 0s 37us/step - loss: 0.0016 - mean_absolute_error: 0.0221\n",
      "\n",
      "Epoch 00303: loss improved from 0.00160 to 0.00159, saving model to keras/best_baseline_weights.hdf5\n",
      "Epoch 304/10000\n",
      "2700/2700 [==============================] - 0s 39us/step - loss: 0.0016 - mean_absolute_error: 0.0228\n",
      "\n",
      "Epoch 00304: loss did not improve\n",
      "Epoch 305/10000\n",
      "2700/2700 [==============================] - 0s 43us/step - loss: 0.0016 - mean_absolute_error: 0.0224\n",
      "\n",
      "Epoch 00305: loss improved from 0.00159 to 0.00158, saving model to keras/best_baseline_weights.hdf5\n",
      "Epoch 306/10000\n",
      "2700/2700 [==============================] - 0s 61us/step - loss: 0.0016 - mean_absolute_error: 0.0227\n",
      "\n",
      "Epoch 00306: loss improved from 0.00158 to 0.00158, saving model to keras/best_baseline_weights.hdf5\n",
      "Epoch 307/10000\n",
      "2700/2700 [==============================] - 0s 56us/step - loss: 0.0016 - mean_absolute_error: 0.0224\n",
      "\n",
      "Epoch 00307: loss improved from 0.00158 to 0.00157, saving model to keras/best_baseline_weights.hdf5\n",
      "Epoch 308/10000\n",
      "2700/2700 [==============================] - 0s 41us/step - loss: 0.0015 - mean_absolute_error: 0.0220\n",
      "\n",
      "Epoch 00308: loss improved from 0.00157 to 0.00155, saving model to keras/best_baseline_weights.hdf5\n",
      "Epoch 309/10000\n",
      "2700/2700 [==============================] - 0s 37us/step - loss: 0.0015 - mean_absolute_error: 0.0222\n",
      "\n",
      "Epoch 00309: loss improved from 0.00155 to 0.00154, saving model to keras/best_baseline_weights.hdf5\n",
      "Epoch 310/10000\n",
      "2700/2700 [==============================] - 0s 37us/step - loss: 0.0015 - mean_absolute_error: 0.0220\n",
      "\n",
      "Epoch 00310: loss improved from 0.00154 to 0.00154, saving model to keras/best_baseline_weights.hdf5\n",
      "Epoch 311/10000\n",
      "2700/2700 [==============================] - 0s 39us/step - loss: 0.0015 - mean_absolute_error: 0.0220\n",
      "\n",
      "Epoch 00311: loss improved from 0.00154 to 0.00153, saving model to keras/best_baseline_weights.hdf5\n",
      "Epoch 312/10000\n",
      "2700/2700 [==============================] - 0s 40us/step - loss: 0.0015 - mean_absolute_error: 0.0219\n",
      "\n",
      "Epoch 00312: loss improved from 0.00153 to 0.00152, saving model to keras/best_baseline_weights.hdf5\n",
      "Epoch 313/10000\n",
      "2700/2700 [==============================] - 0s 40us/step - loss: 0.0015 - mean_absolute_error: 0.0220\n",
      "\n",
      "Epoch 00313: loss improved from 0.00152 to 0.00151, saving model to keras/best_baseline_weights.hdf5\n",
      "Epoch 314/10000\n",
      "2700/2700 [==============================] - 0s 33us/step - loss: 0.0015 - mean_absolute_error: 0.0216\n",
      "\n",
      "Epoch 00314: loss improved from 0.00151 to 0.00150, saving model to keras/best_baseline_weights.hdf5\n",
      "Epoch 315/10000\n",
      "2700/2700 [==============================] - 0s 35us/step - loss: 0.0015 - mean_absolute_error: 0.0218\n",
      "\n",
      "Epoch 00315: loss improved from 0.00150 to 0.00150, saving model to keras/best_baseline_weights.hdf5\n",
      "Epoch 316/10000\n",
      "2700/2700 [==============================] - 0s 40us/step - loss: 0.0015 - mean_absolute_error: 0.0216\n",
      "\n",
      "Epoch 00316: loss improved from 0.00150 to 0.00149, saving model to keras/best_baseline_weights.hdf5\n",
      "Epoch 317/10000\n",
      "2700/2700 [==============================] - 0s 51us/step - loss: 0.0015 - mean_absolute_error: 0.0216\n",
      "\n",
      "Epoch 00317: loss improved from 0.00149 to 0.00149, saving model to keras/best_baseline_weights.hdf5\n",
      "Epoch 318/10000\n",
      "2700/2700 [==============================] - 0s 38us/step - loss: 0.0015 - mean_absolute_error: 0.0213\n",
      "\n",
      "Epoch 00318: loss improved from 0.00149 to 0.00147, saving model to keras/best_baseline_weights.hdf5\n",
      "Epoch 319/10000\n",
      "2700/2700 [==============================] - 0s 36us/step - loss: 0.0015 - mean_absolute_error: 0.0213\n",
      "\n",
      "Epoch 00319: loss improved from 0.00147 to 0.00146, saving model to keras/best_baseline_weights.hdf5\n",
      "Epoch 320/10000\n",
      "2700/2700 [==============================] - 0s 39us/step - loss: 0.0015 - mean_absolute_error: 0.0216\n",
      "\n",
      "Epoch 00320: loss did not improve\n",
      "Epoch 321/10000\n",
      "2700/2700 [==============================] - 0s 42us/step - loss: 0.0015 - mean_absolute_error: 0.0215\n",
      "\n",
      "Epoch 00321: loss did not improve\n",
      "Epoch 322/10000\n",
      "2700/2700 [==============================] - 0s 41us/step - loss: 0.0014 - mean_absolute_error: 0.0212\n",
      "\n",
      "Epoch 00322: loss improved from 0.00146 to 0.00143, saving model to keras/best_baseline_weights.hdf5\n",
      "Epoch 323/10000\n",
      "2700/2700 [==============================] - 0s 40us/step - loss: 0.0014 - mean_absolute_error: 0.0212\n",
      "\n",
      "Epoch 00323: loss did not improve\n",
      "Epoch 324/10000\n",
      "2700/2700 [==============================] - 0s 36us/step - loss: 0.0014 - mean_absolute_error: 0.0208\n",
      "\n",
      "Epoch 00324: loss improved from 0.00143 to 0.00141, saving model to keras/best_baseline_weights.hdf5\n",
      "Epoch 325/10000\n",
      "2700/2700 [==============================] - 0s 35us/step - loss: 0.0014 - mean_absolute_error: 0.0211\n",
      "\n",
      "Epoch 00325: loss did not improve\n",
      "Epoch 326/10000\n",
      "2700/2700 [==============================] - 0s 36us/step - loss: 0.0014 - mean_absolute_error: 0.0211\n",
      "\n",
      "Epoch 00326: loss improved from 0.00141 to 0.00141, saving model to keras/best_baseline_weights.hdf5\n",
      "Epoch 327/10000\n",
      "2700/2700 [==============================] - 0s 51us/step - loss: 0.0014 - mean_absolute_error: 0.0206\n",
      "\n",
      "Epoch 00327: loss improved from 0.00141 to 0.00139, saving model to keras/best_baseline_weights.hdf5\n",
      "Epoch 328/10000\n",
      "2700/2700 [==============================] - 0s 50us/step - loss: 0.0014 - mean_absolute_error: 0.0209\n",
      "\n",
      "Epoch 00328: loss improved from 0.00139 to 0.00139, saving model to keras/best_baseline_weights.hdf5\n",
      "Epoch 329/10000\n",
      "2700/2700 [==============================] - 0s 43us/step - loss: 0.0014 - mean_absolute_error: 0.0208\n",
      "\n",
      "Epoch 00329: loss improved from 0.00139 to 0.00139, saving model to keras/best_baseline_weights.hdf5\n",
      "Epoch 330/10000\n",
      "2700/2700 [==============================] - 0s 45us/step - loss: 0.0014 - mean_absolute_error: 0.0208\n",
      "\n",
      "Epoch 00330: loss improved from 0.00139 to 0.00138, saving model to keras/best_baseline_weights.hdf5\n",
      "Epoch 331/10000\n",
      "2700/2700 [==============================] - 0s 44us/step - loss: 0.0014 - mean_absolute_error: 0.0209\n",
      "\n",
      "Epoch 00331: loss did not improve\n",
      "Epoch 332/10000\n",
      "2700/2700 [==============================] - 0s 35us/step - loss: 0.0014 - mean_absolute_error: 0.0206\n",
      "\n",
      "Epoch 00332: loss improved from 0.00138 to 0.00136, saving model to keras/best_baseline_weights.hdf5\n",
      "Epoch 333/10000\n",
      "2700/2700 [==============================] - 0s 39us/step - loss: 0.0014 - mean_absolute_error: 0.0208\n",
      "\n",
      "Epoch 00333: loss improved from 0.00136 to 0.00136, saving model to keras/best_baseline_weights.hdf5\n",
      "Epoch 334/10000\n",
      "2700/2700 [==============================] - 0s 60us/step - loss: 0.0013 - mean_absolute_error: 0.0203\n",
      "\n",
      "Epoch 00334: loss improved from 0.00136 to 0.00134, saving model to keras/best_baseline_weights.hdf5\n",
      "Epoch 335/10000\n",
      "2700/2700 [==============================] - 0s 55us/step - loss: 0.0013 - mean_absolute_error: 0.0203\n",
      "\n",
      "Epoch 00335: loss improved from 0.00134 to 0.00133, saving model to keras/best_baseline_weights.hdf5\n",
      "Epoch 336/10000\n",
      "2700/2700 [==============================] - 0s 44us/step - loss: 0.0013 - mean_absolute_error: 0.0202\n",
      "\n",
      "Epoch 00336: loss improved from 0.00133 to 0.00133, saving model to keras/best_baseline_weights.hdf5\n",
      "Epoch 337/10000\n",
      "2700/2700 [==============================] - 0s 42us/step - loss: 0.0013 - mean_absolute_error: 0.0202\n",
      "\n",
      "Epoch 00337: loss improved from 0.00133 to 0.00132, saving model to keras/best_baseline_weights.hdf5\n",
      "Epoch 338/10000\n",
      "2700/2700 [==============================] - 0s 43us/step - loss: 0.0013 - mean_absolute_error: 0.0200\n",
      "\n",
      "Epoch 00338: loss improved from 0.00132 to 0.00130, saving model to keras/best_baseline_weights.hdf5\n",
      "Epoch 339/10000\n",
      "2700/2700 [==============================] - 0s 41us/step - loss: 0.0013 - mean_absolute_error: 0.0200\n",
      "\n",
      "Epoch 00339: loss improved from 0.00130 to 0.00130, saving model to keras/best_baseline_weights.hdf5\n",
      "Epoch 340/10000\n",
      "2700/2700 [==============================] - 0s 40us/step - loss: 0.0013 - mean_absolute_error: 0.0198\n",
      "\n",
      "Epoch 00340: loss improved from 0.00130 to 0.00129, saving model to keras/best_baseline_weights.hdf5\n",
      "Epoch 341/10000\n",
      "2700/2700 [==============================] - 0s 48us/step - loss: 0.0013 - mean_absolute_error: 0.0202\n",
      "\n",
      "Epoch 00341: loss did not improve\n",
      "Epoch 342/10000\n",
      "2700/2700 [==============================] - 0s 50us/step - loss: 0.0013 - mean_absolute_error: 0.0199\n",
      "\n",
      "Epoch 00342: loss improved from 0.00129 to 0.00127, saving model to keras/best_baseline_weights.hdf5\n",
      "Epoch 343/10000\n",
      "2700/2700 [==============================] - 0s 42us/step - loss: 0.0013 - mean_absolute_error: 0.0196\n",
      "\n",
      "Epoch 00343: loss did not improve\n",
      "Epoch 344/10000\n",
      "2700/2700 [==============================] - 0s 42us/step - loss: 0.0013 - mean_absolute_error: 0.0197\n",
      "\n",
      "Epoch 00344: loss improved from 0.00127 to 0.00126, saving model to keras/best_baseline_weights.hdf5\n",
      "Epoch 345/10000\n",
      "2700/2700 [==============================] - 0s 47us/step - loss: 0.0013 - mean_absolute_error: 0.0199\n",
      "\n",
      "Epoch 00345: loss did not improve\n",
      "Epoch 346/10000\n",
      "2700/2700 [==============================] - 0s 47us/step - loss: 0.0013 - mean_absolute_error: 0.0198\n",
      "\n",
      "Epoch 00346: loss improved from 0.00126 to 0.00126, saving model to keras/best_baseline_weights.hdf5\n",
      "Epoch 347/10000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2700/2700 [==============================] - 0s 46us/step - loss: 0.0013 - mean_absolute_error: 0.0197\n",
      "\n",
      "Epoch 00347: loss improved from 0.00126 to 0.00126, saving model to keras/best_baseline_weights.hdf5\n",
      "Epoch 348/10000\n",
      "2700/2700 [==============================] - 0s 41us/step - loss: 0.0012 - mean_absolute_error: 0.0196\n",
      "\n",
      "Epoch 00348: loss improved from 0.00126 to 0.00124, saving model to keras/best_baseline_weights.hdf5\n",
      "Epoch 349/10000\n",
      "2700/2700 [==============================] - 0s 37us/step - loss: 0.0012 - mean_absolute_error: 0.0194\n",
      "\n",
      "Epoch 00349: loss improved from 0.00124 to 0.00123, saving model to keras/best_baseline_weights.hdf5\n",
      "Epoch 350/10000\n",
      "2700/2700 [==============================] - 0s 44us/step - loss: 0.0012 - mean_absolute_error: 0.0192\n",
      "\n",
      "Epoch 00350: loss improved from 0.00123 to 0.00122, saving model to keras/best_baseline_weights.hdf5\n",
      "Epoch 351/10000\n",
      "2700/2700 [==============================] - 0s 33us/step - loss: 0.0012 - mean_absolute_error: 0.0193\n",
      "\n",
      "Epoch 00351: loss improved from 0.00122 to 0.00121, saving model to keras/best_baseline_weights.hdf5\n",
      "Epoch 352/10000\n",
      "2700/2700 [==============================] - 0s 42us/step - loss: 0.0012 - mean_absolute_error: 0.0194\n",
      "\n",
      "Epoch 00352: loss improved from 0.00121 to 0.00121, saving model to keras/best_baseline_weights.hdf5\n",
      "Epoch 353/10000\n",
      "2700/2700 [==============================] - 0s 34us/step - loss: 0.0012 - mean_absolute_error: 0.0189\n",
      "\n",
      "Epoch 00353: loss improved from 0.00121 to 0.00119, saving model to keras/best_baseline_weights.hdf5\n",
      "Epoch 354/10000\n",
      "2700/2700 [==============================] - 0s 27us/step - loss: 0.0012 - mean_absolute_error: 0.0193\n",
      "\n",
      "Epoch 00354: loss improved from 0.00119 to 0.00119, saving model to keras/best_baseline_weights.hdf5\n",
      "Epoch 355/10000\n",
      "2700/2700 [==============================] - 0s 26us/step - loss: 0.0012 - mean_absolute_error: 0.0190\n",
      "\n",
      "Epoch 00355: loss improved from 0.00119 to 0.00119, saving model to keras/best_baseline_weights.hdf5\n",
      "Epoch 356/10000\n",
      "2700/2700 [==============================] - 0s 25us/step - loss: 0.0012 - mean_absolute_error: 0.0189\n",
      "\n",
      "Epoch 00356: loss improved from 0.00119 to 0.00117, saving model to keras/best_baseline_weights.hdf5\n",
      "Epoch 357/10000\n",
      "2700/2700 [==============================] - 0s 24us/step - loss: 0.0012 - mean_absolute_error: 0.0188\n",
      "\n",
      "Epoch 00357: loss improved from 0.00117 to 0.00117, saving model to keras/best_baseline_weights.hdf5\n",
      "Epoch 358/10000\n",
      "2700/2700 [==============================] - 0s 25us/step - loss: 0.0012 - mean_absolute_error: 0.0191\n",
      "\n",
      "Epoch 00358: loss did not improve\n",
      "Epoch 359/10000\n",
      "2700/2700 [==============================] - 0s 23us/step - loss: 0.0011 - mean_absolute_error: 0.0185\n",
      "\n",
      "Epoch 00359: loss improved from 0.00117 to 0.00115, saving model to keras/best_baseline_weights.hdf5\n",
      "Epoch 360/10000\n",
      "2700/2700 [==============================] - 0s 23us/step - loss: 0.0011 - mean_absolute_error: 0.0188\n",
      "\n",
      "Epoch 00360: loss improved from 0.00115 to 0.00114, saving model to keras/best_baseline_weights.hdf5\n",
      "Epoch 361/10000\n",
      "2700/2700 [==============================] - 0s 23us/step - loss: 0.0011 - mean_absolute_error: 0.0185\n",
      "\n",
      "Epoch 00361: loss improved from 0.00114 to 0.00114, saving model to keras/best_baseline_weights.hdf5\n",
      "Epoch 362/10000\n",
      "2700/2700 [==============================] - 0s 22us/step - loss: 0.0012 - mean_absolute_error: 0.0191\n",
      "\n",
      "Epoch 00362: loss did not improve\n",
      "Epoch 363/10000\n",
      "2700/2700 [==============================] - 0s 22us/step - loss: 0.0011 - mean_absolute_error: 0.0186\n",
      "\n",
      "Epoch 00363: loss improved from 0.00114 to 0.00113, saving model to keras/best_baseline_weights.hdf5\n",
      "Epoch 364/10000\n",
      "2700/2700 [==============================] - 0s 25us/step - loss: 0.0011 - mean_absolute_error: 0.0184\n",
      "\n",
      "Epoch 00364: loss improved from 0.00113 to 0.00112, saving model to keras/best_baseline_weights.hdf5\n",
      "Epoch 365/10000\n",
      "2700/2700 [==============================] - 0s 25us/step - loss: 0.0011 - mean_absolute_error: 0.0184\n",
      "\n",
      "Epoch 00365: loss improved from 0.00112 to 0.00111, saving model to keras/best_baseline_weights.hdf5\n",
      "Epoch 366/10000\n",
      "2700/2700 [==============================] - 0s 24us/step - loss: 0.0011 - mean_absolute_error: 0.0181\n",
      "\n",
      "Epoch 00366: loss improved from 0.00111 to 0.00110, saving model to keras/best_baseline_weights.hdf5\n",
      "Epoch 367/10000\n",
      "2700/2700 [==============================] - 0s 22us/step - loss: 0.0011 - mean_absolute_error: 0.0182\n",
      "\n",
      "Epoch 00367: loss improved from 0.00110 to 0.00110, saving model to keras/best_baseline_weights.hdf5\n",
      "Epoch 368/10000\n",
      "2700/2700 [==============================] - 0s 25us/step - loss: 0.0011 - mean_absolute_error: 0.0185\n",
      "\n",
      "Epoch 00368: loss did not improve\n",
      "Epoch 369/10000\n",
      "2700/2700 [==============================] - 0s 24us/step - loss: 0.0011 - mean_absolute_error: 0.0184\n",
      "\n",
      "Epoch 00369: loss improved from 0.00110 to 0.00110, saving model to keras/best_baseline_weights.hdf5\n",
      "Epoch 370/10000\n",
      "2700/2700 [==============================] - 0s 24us/step - loss: 0.0011 - mean_absolute_error: 0.0180\n",
      "\n",
      "Epoch 00370: loss improved from 0.00110 to 0.00108, saving model to keras/best_baseline_weights.hdf5\n",
      "Epoch 371/10000\n",
      "2700/2700 [==============================] - 0s 22us/step - loss: 0.0011 - mean_absolute_error: 0.0180\n",
      "\n",
      "Epoch 00371: loss improved from 0.00108 to 0.00107, saving model to keras/best_baseline_weights.hdf5\n",
      "Epoch 372/10000\n",
      "2700/2700 [==============================] - 0s 22us/step - loss: 0.0011 - mean_absolute_error: 0.0180\n",
      "\n",
      "Epoch 00372: loss improved from 0.00107 to 0.00107, saving model to keras/best_baseline_weights.hdf5\n",
      "Epoch 373/10000\n",
      "2700/2700 [==============================] - 0s 22us/step - loss: 0.0011 - mean_absolute_error: 0.0178\n",
      "\n",
      "Epoch 00373: loss improved from 0.00107 to 0.00105, saving model to keras/best_baseline_weights.hdf5\n",
      "Epoch 374/10000\n",
      "2700/2700 [==============================] - 0s 22us/step - loss: 0.0011 - mean_absolute_error: 0.0181\n",
      "\n",
      "Epoch 00374: loss did not improve\n",
      "Epoch 375/10000\n",
      "2700/2700 [==============================] - ETA: 0s - loss: 0.0010 - mean_absolute_error: 0.0182    - 0s 22us/step - loss: 0.0011 - mean_absolute_error: 0.0179\n",
      "\n",
      "Epoch 00375: loss improved from 0.00105 to 0.00105, saving model to keras/best_baseline_weights.hdf5\n",
      "Epoch 376/10000\n",
      "2700/2700 [==============================] - 0s 24us/step - loss: 0.0010 - mean_absolute_error: 0.0181\n",
      "\n",
      "Epoch 00376: loss improved from 0.00105 to 0.00105, saving model to keras/best_baseline_weights.hdf5\n",
      "Epoch 377/10000\n",
      "2700/2700 [==============================] - 0s 36us/step - loss: 0.0010 - mean_absolute_error: 0.0177\n",
      "\n",
      "Epoch 00377: loss improved from 0.00105 to 0.00103, saving model to keras/best_baseline_weights.hdf5\n",
      "Epoch 378/10000\n",
      "2700/2700 [==============================] - 0s 46us/step - loss: 0.0010 - mean_absolute_error: 0.0178\n",
      "\n",
      "Epoch 00378: loss improved from 0.00103 to 0.00103, saving model to keras/best_baseline_weights.hdf5\n",
      "Epoch 379/10000\n",
      "2700/2700 [==============================] - 0s 38us/step - loss: 0.0010 - mean_absolute_error: 0.0174\n",
      "\n",
      "Epoch 00379: loss improved from 0.00103 to 0.00102, saving model to keras/best_baseline_weights.hdf5\n",
      "Epoch 380/10000\n",
      "2700/2700 [==============================] - 0s 25us/step - loss: 0.0010 - mean_absolute_error: 0.0175\n",
      "\n",
      "Epoch 00380: loss improved from 0.00102 to 0.00101, saving model to keras/best_baseline_weights.hdf5\n",
      "Epoch 381/10000\n",
      "2700/2700 [==============================] - 0s 23us/step - loss: 0.0010 - mean_absolute_error: 0.0175\n",
      "\n",
      "Epoch 00381: loss improved from 0.00101 to 0.00101, saving model to keras/best_baseline_weights.hdf5\n",
      "Epoch 382/10000\n",
      "2700/2700 [==============================] - 0s 23us/step - loss: 0.0010 - mean_absolute_error: 0.0174\n",
      "\n",
      "Epoch 00382: loss improved from 0.00101 to 0.00100, saving model to keras/best_baseline_weights.hdf5\n",
      "Epoch 383/10000\n",
      "2700/2700 [==============================] - 0s 21us/step - loss: 9.9796e-04 - mean_absolute_error: 0.0175\n",
      "\n",
      "Epoch 00383: loss improved from 0.00100 to 0.00100, saving model to keras/best_baseline_weights.hdf5\n",
      "Epoch 384/10000\n",
      "2700/2700 [==============================] - 0s 22us/step - loss: 9.9301e-04 - mean_absolute_error: 0.0177\n",
      "\n",
      "Epoch 00384: loss improved from 0.00100 to 0.00099, saving model to keras/best_baseline_weights.hdf5\n",
      "Epoch 385/10000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2700/2700 [==============================] - 0s 24us/step - loss: 9.8497e-04 - mean_absolute_error: 0.0171\n",
      "\n",
      "Epoch 00385: loss improved from 0.00099 to 0.00098, saving model to keras/best_baseline_weights.hdf5\n",
      "Epoch 386/10000\n",
      "2700/2700 [==============================] - 0s 25us/step - loss: 9.8842e-04 - mean_absolute_error: 0.0175\n",
      "\n",
      "Epoch 00386: loss did not improve\n",
      "Epoch 387/10000\n",
      "2700/2700 [==============================] - 0s 24us/step - loss: 9.7960e-04 - mean_absolute_error: 0.0173\n",
      "\n",
      "Epoch 00387: loss improved from 0.00098 to 0.00098, saving model to keras/best_baseline_weights.hdf5\n",
      "Epoch 388/10000\n",
      "2700/2700 [==============================] - 0s 22us/step - loss: 9.7366e-04 - mean_absolute_error: 0.0173\n",
      "\n",
      "Epoch 00388: loss improved from 0.00098 to 0.00097, saving model to keras/best_baseline_weights.hdf5\n",
      "Epoch 389/10000\n",
      "2700/2700 [==============================] - 0s 22us/step - loss: 9.5893e-04 - mean_absolute_error: 0.0171\n",
      "\n",
      "Epoch 00389: loss improved from 0.00097 to 0.00096, saving model to keras/best_baseline_weights.hdf5\n",
      "Epoch 390/10000\n",
      "2700/2700 [==============================] - 0s 21us/step - loss: 9.5819e-04 - mean_absolute_error: 0.0170\n",
      "\n",
      "Epoch 00390: loss improved from 0.00096 to 0.00096, saving model to keras/best_baseline_weights.hdf5\n",
      "Epoch 391/10000\n",
      "2700/2700 [==============================] - 0s 22us/step - loss: 9.6240e-04 - mean_absolute_error: 0.0175\n",
      "\n",
      "Epoch 00391: loss did not improve\n",
      "Epoch 392/10000\n",
      "2700/2700 [==============================] - 0s 21us/step - loss: 9.4593e-04 - mean_absolute_error: 0.0170\n",
      "\n",
      "Epoch 00392: loss improved from 0.00096 to 0.00095, saving model to keras/best_baseline_weights.hdf5\n",
      "Epoch 393/10000\n",
      "2700/2700 [==============================] - 0s 23us/step - loss: 9.3330e-04 - mean_absolute_error: 0.0167\n",
      "\n",
      "Epoch 00393: loss improved from 0.00095 to 0.00093, saving model to keras/best_baseline_weights.hdf5\n",
      "Epoch 394/10000\n",
      "2700/2700 [==============================] - 0s 26us/step - loss: 9.3601e-04 - mean_absolute_error: 0.0168\n",
      "\n",
      "Epoch 00394: loss did not improve\n",
      "Epoch 395/10000\n",
      "2700/2700 [==============================] - 0s 31us/step - loss: 9.2843e-04 - mean_absolute_error: 0.0167\n",
      "\n",
      "Epoch 00395: loss improved from 0.00093 to 0.00093, saving model to keras/best_baseline_weights.hdf5\n",
      "Epoch 396/10000\n",
      "2700/2700 [==============================] - 0s 29us/step - loss: 9.3142e-04 - mean_absolute_error: 0.0170\n",
      "\n",
      "Epoch 00396: loss did not improve\n",
      "Epoch 397/10000\n",
      "2700/2700 [==============================] - 0s 30us/step - loss: 9.2447e-04 - mean_absolute_error: 0.0169\n",
      "\n",
      "Epoch 00397: loss improved from 0.00093 to 0.00092, saving model to keras/best_baseline_weights.hdf5\n",
      "Epoch 398/10000\n",
      "2700/2700 [==============================] - 0s 29us/step - loss: 9.1922e-04 - mean_absolute_error: 0.0168\n",
      "\n",
      "Epoch 00398: loss improved from 0.00092 to 0.00092, saving model to keras/best_baseline_weights.hdf5\n",
      "Epoch 399/10000\n",
      "2700/2700 [==============================] - 0s 23us/step - loss: 9.1951e-04 - mean_absolute_error: 0.0171\n",
      "\n",
      "Epoch 00399: loss did not improve\n",
      "Epoch 400/10000\n",
      "2700/2700 [==============================] - 0s 22us/step - loss: 9.0069e-04 - mean_absolute_error: 0.0165\n",
      "\n",
      "Epoch 00400: loss improved from 0.00092 to 0.00090, saving model to keras/best_baseline_weights.hdf5\n",
      "Epoch 401/10000\n",
      "2700/2700 [==============================] - 0s 21us/step - loss: 8.9419e-04 - mean_absolute_error: 0.0164\n",
      "\n",
      "Epoch 00401: loss improved from 0.00090 to 0.00089, saving model to keras/best_baseline_weights.hdf5\n",
      "Epoch 402/10000\n",
      "2700/2700 [==============================] - 0s 21us/step - loss: 8.9300e-04 - mean_absolute_error: 0.0166\n",
      "\n",
      "Epoch 00402: loss improved from 0.00089 to 0.00089, saving model to keras/best_baseline_weights.hdf5\n",
      "Epoch 403/10000\n",
      "2700/2700 [==============================] - 0s 22us/step - loss: 8.9014e-04 - mean_absolute_error: 0.0165\n",
      "\n",
      "Epoch 00403: loss improved from 0.00089 to 0.00089, saving model to keras/best_baseline_weights.hdf5\n",
      "Epoch 404/10000\n",
      "2700/2700 [==============================] - 0s 21us/step - loss: 8.7675e-04 - mean_absolute_error: 0.0162\n",
      "\n",
      "Epoch 00404: loss improved from 0.00089 to 0.00088, saving model to keras/best_baseline_weights.hdf5\n",
      "Epoch 405/10000\n",
      "2700/2700 [==============================] - 0s 21us/step - loss: 8.7454e-04 - mean_absolute_error: 0.0162\n",
      "\n",
      "Epoch 00405: loss improved from 0.00088 to 0.00087, saving model to keras/best_baseline_weights.hdf5\n",
      "Epoch 406/10000\n",
      "2700/2700 [==============================] - 0s 21us/step - loss: 8.7546e-04 - mean_absolute_error: 0.0162\n",
      "\n",
      "Epoch 00406: loss did not improve\n",
      "Epoch 407/10000\n",
      "2700/2700 [==============================] - 0s 21us/step - loss: 8.6337e-04 - mean_absolute_error: 0.0162\n",
      "\n",
      "Epoch 00407: loss improved from 0.00087 to 0.00086, saving model to keras/best_baseline_weights.hdf5\n",
      "Epoch 408/10000\n",
      "2700/2700 [==============================] - 0s 21us/step - loss: 8.6930e-04 - mean_absolute_error: 0.0164\n",
      "\n",
      "Epoch 00408: loss did not improve\n",
      "Epoch 409/10000\n",
      "2700/2700 [==============================] - 0s 22us/step - loss: 8.5452e-04 - mean_absolute_error: 0.0160\n",
      "\n",
      "Epoch 00409: loss improved from 0.00086 to 0.00085, saving model to keras/best_baseline_weights.hdf5\n",
      "Epoch 410/10000\n",
      "2700/2700 [==============================] - 0s 22us/step - loss: 8.5351e-04 - mean_absolute_error: 0.0161\n",
      "\n",
      "Epoch 00410: loss improved from 0.00085 to 0.00085, saving model to keras/best_baseline_weights.hdf5\n",
      "Epoch 411/10000\n",
      "2700/2700 [==============================] - 0s 21us/step - loss: 8.5503e-04 - mean_absolute_error: 0.0163\n",
      "\n",
      "Epoch 00411: loss did not improve\n",
      "Epoch 412/10000\n",
      "2700/2700 [==============================] - 0s 22us/step - loss: 8.5486e-04 - mean_absolute_error: 0.0165\n",
      "\n",
      "Epoch 00412: loss did not improve\n",
      "Epoch 413/10000\n",
      "2700/2700 [==============================] - 0s 21us/step - loss: 8.4715e-04 - mean_absolute_error: 0.0161\n",
      "\n",
      "Epoch 00413: loss improved from 0.00085 to 0.00085, saving model to keras/best_baseline_weights.hdf5\n",
      "Epoch 414/10000\n",
      "2700/2700 [==============================] - 0s 22us/step - loss: 8.3483e-04 - mean_absolute_error: 0.0159\n",
      "\n",
      "Epoch 00414: loss improved from 0.00085 to 0.00083, saving model to keras/best_baseline_weights.hdf5\n",
      "Epoch 415/10000\n",
      "2700/2700 [==============================] - 0s 21us/step - loss: 8.3170e-04 - mean_absolute_error: 0.0159\n",
      "\n",
      "Epoch 00415: loss improved from 0.00083 to 0.00083, saving model to keras/best_baseline_weights.hdf5\n",
      "Epoch 416/10000\n",
      "2700/2700 [==============================] - 0s 24us/step - loss: 8.4109e-04 - mean_absolute_error: 0.0161\n",
      "\n",
      "Epoch 00416: loss did not improve\n",
      "Epoch 417/10000\n",
      "2700/2700 [==============================] - 0s 24us/step - loss: 8.2187e-04 - mean_absolute_error: 0.0157\n",
      "\n",
      "Epoch 00417: loss improved from 0.00083 to 0.00082, saving model to keras/best_baseline_weights.hdf5\n",
      "Epoch 418/10000\n",
      "2700/2700 [==============================] - 0s 23us/step - loss: 8.1950e-04 - mean_absolute_error: 0.0159\n",
      "\n",
      "Epoch 00418: loss improved from 0.00082 to 0.00082, saving model to keras/best_baseline_weights.hdf5\n",
      "Epoch 419/10000\n",
      "2700/2700 [==============================] - 0s 22us/step - loss: 8.2835e-04 - mean_absolute_error: 0.0164\n",
      "\n",
      "Epoch 00419: loss did not improve\n",
      "Epoch 420/10000\n",
      "2700/2700 [==============================] - 0s 21us/step - loss: 8.0929e-04 - mean_absolute_error: 0.0156\n",
      "\n",
      "Epoch 00420: loss improved from 0.00082 to 0.00081, saving model to keras/best_baseline_weights.hdf5\n",
      "Epoch 421/10000\n",
      "2700/2700 [==============================] - 0s 22us/step - loss: 8.0313e-04 - mean_absolute_error: 0.0157\n",
      "\n",
      "Epoch 00421: loss improved from 0.00081 to 0.00080, saving model to keras/best_baseline_weights.hdf5\n",
      "Epoch 422/10000\n",
      "2700/2700 [==============================] - 0s 22us/step - loss: 8.1449e-04 - mean_absolute_error: 0.0161\n",
      "\n",
      "Epoch 00422: loss did not improve\n",
      "Epoch 423/10000\n",
      "2700/2700 [==============================] - 0s 22us/step - loss: 7.9911e-04 - mean_absolute_error: 0.0158\n",
      "\n",
      "Epoch 00423: loss improved from 0.00080 to 0.00080, saving model to keras/best_baseline_weights.hdf5\n",
      "Epoch 424/10000\n",
      "2700/2700 [==============================] - 0s 22us/step - loss: 7.9416e-04 - mean_absolute_error: 0.0157\n",
      "\n",
      "Epoch 00424: loss improved from 0.00080 to 0.00079, saving model to keras/best_baseline_weights.hdf5\n",
      "Epoch 425/10000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2700/2700 [==============================] - 0s 23us/step - loss: 7.8742e-04 - mean_absolute_error: 0.0153\n",
      "\n",
      "Epoch 00425: loss improved from 0.00079 to 0.00079, saving model to keras/best_baseline_weights.hdf5\n",
      "Epoch 426/10000\n",
      "2700/2700 [==============================] - 0s 22us/step - loss: 7.8819e-04 - mean_absolute_error: 0.0157\n",
      "\n",
      "Epoch 00426: loss did not improve\n",
      "Epoch 427/10000\n",
      "2700/2700 [==============================] - 0s 21us/step - loss: 7.8694e-04 - mean_absolute_error: 0.0157\n",
      "\n",
      "Epoch 00427: loss improved from 0.00079 to 0.00079, saving model to keras/best_baseline_weights.hdf5\n",
      "Epoch 428/10000\n",
      "2700/2700 [==============================] - 0s 22us/step - loss: 7.7801e-04 - mean_absolute_error: 0.0154\n",
      "\n",
      "Epoch 00428: loss improved from 0.00079 to 0.00078, saving model to keras/best_baseline_weights.hdf5\n",
      "Epoch 429/10000\n",
      "2700/2700 [==============================] - 0s 22us/step - loss: 7.8038e-04 - mean_absolute_error: 0.0155\n",
      "\n",
      "Epoch 00429: loss did not improve\n",
      "Epoch 430/10000\n",
      "2700/2700 [==============================] - 0s 21us/step - loss: 7.7672e-04 - mean_absolute_error: 0.0156\n",
      "\n",
      "Epoch 00430: loss improved from 0.00078 to 0.00078, saving model to keras/best_baseline_weights.hdf5\n",
      "Epoch 431/10000\n",
      "2700/2700 [==============================] - 0s 23us/step - loss: 7.7062e-04 - mean_absolute_error: 0.0157\n",
      "\n",
      "Epoch 00431: loss improved from 0.00078 to 0.00077, saving model to keras/best_baseline_weights.hdf5\n",
      "Epoch 432/10000\n",
      "2700/2700 [==============================] - 0s 22us/step - loss: 7.6810e-04 - mean_absolute_error: 0.0154\n",
      "\n",
      "Epoch 00432: loss improved from 0.00077 to 0.00077, saving model to keras/best_baseline_weights.hdf5\n",
      "Epoch 433/10000\n",
      "2700/2700 [==============================] - 0s 21us/step - loss: 7.6348e-04 - mean_absolute_error: 0.0152\n",
      "\n",
      "Epoch 00433: loss improved from 0.00077 to 0.00076, saving model to keras/best_baseline_weights.hdf5\n",
      "Epoch 434/10000\n",
      "2700/2700 [==============================] - 0s 22us/step - loss: 7.5873e-04 - mean_absolute_error: 0.0153\n",
      "\n",
      "Epoch 00434: loss improved from 0.00076 to 0.00076, saving model to keras/best_baseline_weights.hdf5\n",
      "Epoch 435/10000\n",
      "2700/2700 [==============================] - 0s 21us/step - loss: 7.5543e-04 - mean_absolute_error: 0.0154\n",
      "\n",
      "Epoch 00435: loss improved from 0.00076 to 0.00076, saving model to keras/best_baseline_weights.hdf5\n",
      "Epoch 436/10000\n",
      "2700/2700 [==============================] - 0s 21us/step - loss: 7.4865e-04 - mean_absolute_error: 0.0151\n",
      "\n",
      "Epoch 00436: loss improved from 0.00076 to 0.00075, saving model to keras/best_baseline_weights.hdf5\n",
      "Epoch 437/10000\n",
      "2700/2700 [==============================] - 0s 22us/step - loss: 7.4814e-04 - mean_absolute_error: 0.0152\n",
      "\n",
      "Epoch 00437: loss improved from 0.00075 to 0.00075, saving model to keras/best_baseline_weights.hdf5\n",
      "Epoch 438/10000\n",
      "2700/2700 [==============================] - 0s 23us/step - loss: 7.4058e-04 - mean_absolute_error: 0.0150\n",
      "\n",
      "Epoch 00438: loss improved from 0.00075 to 0.00074, saving model to keras/best_baseline_weights.hdf5\n",
      "Epoch 439/10000\n",
      "2700/2700 [==============================] - 0s 21us/step - loss: 7.5013e-04 - mean_absolute_error: 0.0153\n",
      "\n",
      "Epoch 00439: loss did not improve\n",
      "Epoch 440/10000\n",
      "2700/2700 [==============================] - 0s 21us/step - loss: 7.5037e-04 - mean_absolute_error: 0.0155\n",
      "\n",
      "Epoch 00440: loss did not improve\n",
      "Epoch 441/10000\n",
      "2700/2700 [==============================] - 0s 21us/step - loss: 7.3843e-04 - mean_absolute_error: 0.0152\n",
      "\n",
      "Epoch 00441: loss improved from 0.00074 to 0.00074, saving model to keras/best_baseline_weights.hdf5\n",
      "Epoch 442/10000\n",
      "2700/2700 [==============================] - 0s 21us/step - loss: 7.2894e-04 - mean_absolute_error: 0.0149\n",
      "\n",
      "Epoch 00442: loss improved from 0.00074 to 0.00073, saving model to keras/best_baseline_weights.hdf5\n",
      "Epoch 443/10000\n",
      "2700/2700 [==============================] - 0s 22us/step - loss: 7.3128e-04 - mean_absolute_error: 0.0150\n",
      "\n",
      "Epoch 00443: loss did not improve\n",
      "Epoch 444/10000\n",
      "2700/2700 [==============================] - 0s 21us/step - loss: 7.2684e-04 - mean_absolute_error: 0.0150\n",
      "\n",
      "Epoch 00444: loss improved from 0.00073 to 0.00073, saving model to keras/best_baseline_weights.hdf5\n",
      "Epoch 445/10000\n",
      "2700/2700 [==============================] - 0s 22us/step - loss: 7.2225e-04 - mean_absolute_error: 0.0150\n",
      "\n",
      "Epoch 00445: loss improved from 0.00073 to 0.00072, saving model to keras/best_baseline_weights.hdf5\n",
      "Epoch 446/10000\n",
      "2700/2700 [==============================] - 0s 23us/step - loss: 7.1625e-04 - mean_absolute_error: 0.0149\n",
      "\n",
      "Epoch 00446: loss improved from 0.00072 to 0.00072, saving model to keras/best_baseline_weights.hdf5\n",
      "Epoch 447/10000\n",
      "2700/2700 [==============================] - 0s 22us/step - loss: 7.2091e-04 - mean_absolute_error: 0.0151\n",
      "\n",
      "Epoch 00447: loss did not improve\n",
      "Epoch 448/10000\n",
      "2700/2700 [==============================] - 0s 21us/step - loss: 7.1209e-04 - mean_absolute_error: 0.0149\n",
      "\n",
      "Epoch 00448: loss improved from 0.00072 to 0.00071, saving model to keras/best_baseline_weights.hdf5\n",
      "Epoch 449/10000\n",
      "2700/2700 [==============================] - 0s 21us/step - loss: 7.1345e-04 - mean_absolute_error: 0.0150\n",
      "\n",
      "Epoch 00449: loss did not improve\n",
      "Epoch 450/10000\n",
      "2700/2700 [==============================] - 0s 21us/step - loss: 7.0736e-04 - mean_absolute_error: 0.0149\n",
      "\n",
      "Epoch 00450: loss improved from 0.00071 to 0.00071, saving model to keras/best_baseline_weights.hdf5\n",
      "Epoch 451/10000\n",
      "2700/2700 [==============================] - 0s 22us/step - loss: 7.1382e-04 - mean_absolute_error: 0.0150\n",
      "\n",
      "Epoch 00451: loss did not improve\n",
      "Epoch 452/10000\n",
      "2700/2700 [==============================] - 0s 22us/step - loss: 7.0608e-04 - mean_absolute_error: 0.0147\n",
      "\n",
      "Epoch 00452: loss improved from 0.00071 to 0.00071, saving model to keras/best_baseline_weights.hdf5\n",
      "Epoch 453/10000\n",
      "2700/2700 [==============================] - 0s 30us/step - loss: 7.0558e-04 - mean_absolute_error: 0.0148\n",
      "\n",
      "Epoch 00453: loss improved from 0.00071 to 0.00071, saving model to keras/best_baseline_weights.hdf5\n",
      "Epoch 454/10000\n",
      "2700/2700 [==============================] - 0s 22us/step - loss: 7.0490e-04 - mean_absolute_error: 0.0151\n",
      "\n",
      "Epoch 00454: loss improved from 0.00071 to 0.00070, saving model to keras/best_baseline_weights.hdf5\n",
      "Epoch 455/10000\n",
      "2700/2700 [==============================] - 0s 21us/step - loss: 6.9416e-04 - mean_absolute_error: 0.0147\n",
      "\n",
      "Epoch 00455: loss improved from 0.00070 to 0.00069, saving model to keras/best_baseline_weights.hdf5\n",
      "Epoch 456/10000\n",
      "2700/2700 [==============================] - 0s 21us/step - loss: 6.9420e-04 - mean_absolute_error: 0.0151 - loss: 7.0686e-04 - mean_absolute_error: 0.015\n",
      "\n",
      "Epoch 00456: loss did not improve\n",
      "Epoch 457/10000\n",
      "2700/2700 [==============================] - 0s 21us/step - loss: 6.8903e-04 - mean_absolute_error: 0.0147\n",
      "\n",
      "Epoch 00457: loss improved from 0.00069 to 0.00069, saving model to keras/best_baseline_weights.hdf5\n",
      "Epoch 458/10000\n",
      "2700/2700 [==============================] - 0s 22us/step - loss: 6.8198e-04 - mean_absolute_error: 0.0144\n",
      "\n",
      "Epoch 00458: loss improved from 0.00069 to 0.00068, saving model to keras/best_baseline_weights.hdf5\n",
      "Epoch 459/10000\n",
      "2700/2700 [==============================] - 0s 21us/step - loss: 6.9431e-04 - mean_absolute_error: 0.0150\n",
      "\n",
      "Epoch 00459: loss did not improve\n",
      "Epoch 460/10000\n",
      "2700/2700 [==============================] - 0s 21us/step - loss: 6.8003e-04 - mean_absolute_error: 0.0144\n",
      "\n",
      "Epoch 00460: loss improved from 0.00068 to 0.00068, saving model to keras/best_baseline_weights.hdf5\n",
      "Epoch 461/10000\n",
      "2700/2700 [==============================] - 0s 21us/step - loss: 6.8127e-04 - mean_absolute_error: 0.0148\n",
      "\n",
      "Epoch 00461: loss did not improve\n",
      "Epoch 462/10000\n",
      "2700/2700 [==============================] - 0s 21us/step - loss: 6.7692e-04 - mean_absolute_error: 0.0146\n",
      "\n",
      "Epoch 00462: loss improved from 0.00068 to 0.00068, saving model to keras/best_baseline_weights.hdf5\n",
      "Epoch 463/10000\n",
      "2700/2700 [==============================] - 0s 22us/step - loss: 6.7036e-04 - mean_absolute_error: 0.0144\n",
      "\n",
      "Epoch 00463: loss improved from 0.00068 to 0.00067, saving model to keras/best_baseline_weights.hdf5\n",
      "Epoch 464/10000\n",
      "2700/2700 [==============================] - 0s 21us/step - loss: 6.7256e-04 - mean_absolute_error: 0.0147\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 00464: loss did not improve\n",
      "Epoch 465/10000\n",
      "2700/2700 [==============================] - 0s 22us/step - loss: 6.6678e-04 - mean_absolute_error: 0.0144\n",
      "\n",
      "Epoch 00465: loss improved from 0.00067 to 0.00067, saving model to keras/best_baseline_weights.hdf5\n",
      "Epoch 466/10000\n",
      "2700/2700 [==============================] - 0s 23us/step - loss: 6.6985e-04 - mean_absolute_error: 0.0147\n",
      "\n",
      "Epoch 00466: loss did not improve\n",
      "Epoch 467/10000\n",
      "2700/2700 [==============================] - 0s 22us/step - loss: 6.6436e-04 - mean_absolute_error: 0.0146\n",
      "\n",
      "Epoch 00467: loss improved from 0.00067 to 0.00066, saving model to keras/best_baseline_weights.hdf5\n",
      "Epoch 468/10000\n",
      "2700/2700 [==============================] - 0s 21us/step - loss: 6.5984e-04 - mean_absolute_error: 0.0144\n",
      "\n",
      "Epoch 00468: loss improved from 0.00066 to 0.00066, saving model to keras/best_baseline_weights.hdf5\n",
      "Epoch 469/10000\n",
      "2700/2700 [==============================] - 0s 21us/step - loss: 6.5867e-04 - mean_absolute_error: 0.0144\n",
      "\n",
      "Epoch 00469: loss improved from 0.00066 to 0.00066, saving model to keras/best_baseline_weights.hdf5\n",
      "Epoch 470/10000\n",
      "2700/2700 [==============================] - 0s 22us/step - loss: 6.5498e-04 - mean_absolute_error: 0.0143\n",
      "\n",
      "Epoch 00470: loss improved from 0.00066 to 0.00065, saving model to keras/best_baseline_weights.hdf5\n",
      "Epoch 471/10000\n",
      "2700/2700 [==============================] - 0s 22us/step - loss: 6.5063e-04 - mean_absolute_error: 0.0141\n",
      "\n",
      "Epoch 00471: loss improved from 0.00065 to 0.00065, saving model to keras/best_baseline_weights.hdf5\n",
      "Epoch 472/10000\n",
      "2700/2700 [==============================] - 0s 22us/step - loss: 6.4718e-04 - mean_absolute_error: 0.0141\n",
      "\n",
      "Epoch 00472: loss improved from 0.00065 to 0.00065, saving model to keras/best_baseline_weights.hdf5\n",
      "Epoch 473/10000\n",
      "2700/2700 [==============================] - 0s 22us/step - loss: 6.5645e-04 - mean_absolute_error: 0.0145\n",
      "\n",
      "Epoch 00473: loss did not improve\n",
      "Epoch 474/10000\n",
      "2700/2700 [==============================] - 0s 21us/step - loss: 6.4983e-04 - mean_absolute_error: 0.0143\n",
      "\n",
      "Epoch 00474: loss did not improve\n",
      "Epoch 475/10000\n",
      "2700/2700 [==============================] - 0s 21us/step - loss: 6.4316e-04 - mean_absolute_error: 0.0142\n",
      "\n",
      "Epoch 00475: loss improved from 0.00065 to 0.00064, saving model to keras/best_baseline_weights.hdf5\n",
      "Epoch 476/10000\n",
      "2700/2700 [==============================] - 0s 22us/step - loss: 6.4407e-04 - mean_absolute_error: 0.0143\n",
      "\n",
      "Epoch 00476: loss did not improve\n",
      "Epoch 477/10000\n",
      "2700/2700 [==============================] - 0s 21us/step - loss: 6.4030e-04 - mean_absolute_error: 0.0142\n",
      "\n",
      "Epoch 00477: loss improved from 0.00064 to 0.00064, saving model to keras/best_baseline_weights.hdf5\n",
      "Epoch 478/10000\n",
      "2700/2700 [==============================] - 0s 21us/step - loss: 6.3792e-04 - mean_absolute_error: 0.0141\n",
      "\n",
      "Epoch 00478: loss improved from 0.00064 to 0.00064, saving model to keras/best_baseline_weights.hdf5\n",
      "Epoch 479/10000\n",
      "2700/2700 [==============================] - 0s 22us/step - loss: 6.3866e-04 - mean_absolute_error: 0.0143\n",
      "\n",
      "Epoch 00479: loss did not improve\n",
      "Epoch 480/10000\n",
      "2700/2700 [==============================] - 0s 21us/step - loss: 6.3468e-04 - mean_absolute_error: 0.0141\n",
      "\n",
      "Epoch 00480: loss improved from 0.00064 to 0.00063, saving model to keras/best_baseline_weights.hdf5\n",
      "Epoch 481/10000\n",
      "2700/2700 [==============================] - 0s 27us/step - loss: 6.4236e-04 - mean_absolute_error: 0.0144\n",
      "\n",
      "Epoch 00481: loss did not improve\n",
      "Epoch 482/10000\n",
      "2700/2700 [==============================] - 0s 25us/step - loss: 6.3318e-04 - mean_absolute_error: 0.0143\n",
      "\n",
      "Epoch 00482: loss improved from 0.00063 to 0.00063, saving model to keras/best_baseline_weights.hdf5\n",
      "Epoch 483/10000\n",
      "2700/2700 [==============================] - 0s 21us/step - loss: 6.2893e-04 - mean_absolute_error: 0.0141\n",
      "\n",
      "Epoch 00483: loss improved from 0.00063 to 0.00063, saving model to keras/best_baseline_weights.hdf5\n",
      "Epoch 484/10000\n",
      "2700/2700 [==============================] - 0s 23us/step - loss: 6.3091e-04 - mean_absolute_error: 0.0142\n",
      "\n",
      "Epoch 00484: loss did not improve\n",
      "Epoch 485/10000\n",
      "2700/2700 [==============================] - 0s 22us/step - loss: 6.4549e-04 - mean_absolute_error: 0.0147\n",
      "\n",
      "Epoch 00485: loss did not improve\n",
      "Epoch 486/10000\n",
      "2700/2700 [==============================] - 0s 21us/step - loss: 6.2370e-04 - mean_absolute_error: 0.0141\n",
      "\n",
      "Epoch 00486: loss improved from 0.00063 to 0.00062, saving model to keras/best_baseline_weights.hdf5\n",
      "Epoch 487/10000\n",
      "2700/2700 [==============================] - 0s 21us/step - loss: 6.2348e-04 - mean_absolute_error: 0.0142\n",
      "\n",
      "Epoch 00487: loss improved from 0.00062 to 0.00062, saving model to keras/best_baseline_weights.hdf5\n",
      "Epoch 488/10000\n",
      "2700/2700 [==============================] - 0s 22us/step - loss: 6.1344e-04 - mean_absolute_error: 0.0138\n",
      "\n",
      "Epoch 00488: loss improved from 0.00062 to 0.00061, saving model to keras/best_baseline_weights.hdf5\n",
      "Epoch 489/10000\n",
      "2700/2700 [==============================] - 0s 22us/step - loss: 6.2161e-04 - mean_absolute_error: 0.0142\n",
      "\n",
      "Epoch 00489: loss did not improve\n",
      "Epoch 490/10000\n",
      "2700/2700 [==============================] - 0s 21us/step - loss: 6.2221e-04 - mean_absolute_error: 0.0142\n",
      "\n",
      "Epoch 00490: loss did not improve\n",
      "Epoch 491/10000\n",
      "2700/2700 [==============================] - 0s 21us/step - loss: 6.1908e-04 - mean_absolute_error: 0.0143\n",
      "\n",
      "Epoch 00491: loss did not improve\n",
      "Epoch 492/10000\n",
      "2700/2700 [==============================] - 0s 22us/step - loss: 6.0801e-04 - mean_absolute_error: 0.0138\n",
      "\n",
      "Epoch 00492: loss improved from 0.00061 to 0.00061, saving model to keras/best_baseline_weights.hdf5\n",
      "Epoch 493/10000\n",
      "2700/2700 [==============================] - 0s 21us/step - loss: 6.1327e-04 - mean_absolute_error: 0.0140\n",
      "\n",
      "Epoch 00493: loss did not improve\n",
      "Epoch 494/10000\n",
      "2700/2700 [==============================] - 0s 21us/step - loss: 6.0578e-04 - mean_absolute_error: 0.0138\n",
      "\n",
      "Epoch 00494: loss improved from 0.00061 to 0.00061, saving model to keras/best_baseline_weights.hdf5\n",
      "Epoch 495/10000\n",
      "2700/2700 [==============================] - 0s 22us/step - loss: 6.0605e-04 - mean_absolute_error: 0.0139\n",
      "\n",
      "Epoch 00495: loss did not improve\n",
      "Epoch 496/10000\n",
      "2700/2700 [==============================] - 0s 21us/step - loss: 6.0239e-04 - mean_absolute_error: 0.0139\n",
      "\n",
      "Epoch 00496: loss improved from 0.00061 to 0.00060, saving model to keras/best_baseline_weights.hdf5\n",
      "Epoch 497/10000\n",
      "2700/2700 [==============================] - 0s 21us/step - loss: 6.0316e-04 - mean_absolute_error: 0.0140\n",
      "\n",
      "Epoch 00497: loss did not improve\n",
      "Epoch 498/10000\n",
      "2700/2700 [==============================] - 0s 21us/step - loss: 5.9775e-04 - mean_absolute_error: 0.0137\n",
      "\n",
      "Epoch 00498: loss improved from 0.00060 to 0.00060, saving model to keras/best_baseline_weights.hdf5\n",
      "Epoch 499/10000\n",
      "2700/2700 [==============================] - 0s 22us/step - loss: 5.9733e-04 - mean_absolute_error: 0.0137\n",
      "\n",
      "Epoch 00499: loss improved from 0.00060 to 0.00060, saving model to keras/best_baseline_weights.hdf5\n",
      "Epoch 500/10000\n",
      "2700/2700 [==============================] - 0s 21us/step - loss: 5.9552e-04 - mean_absolute_error: 0.0136\n",
      "\n",
      "Epoch 00500: loss improved from 0.00060 to 0.00060, saving model to keras/best_baseline_weights.hdf5\n",
      "Epoch 501/10000\n",
      "2700/2700 [==============================] - 0s 22us/step - loss: 6.0302e-04 - mean_absolute_error: 0.0140\n",
      "\n",
      "Epoch 00501: loss did not improve\n",
      "Epoch 502/10000\n",
      "2700/2700 [==============================] - 0s 21us/step - loss: 6.0011e-04 - mean_absolute_error: 0.0140\n",
      "\n",
      "Epoch 00502: loss did not improve\n",
      "Epoch 503/10000\n",
      "2700/2700 [==============================] - 0s 21us/step - loss: 5.9899e-04 - mean_absolute_error: 0.0139\n",
      "\n",
      "Epoch 00503: loss did not improve\n",
      "Epoch 504/10000\n",
      "2700/2700 [==============================] - 0s 21us/step - loss: 5.9527e-04 - mean_absolute_error: 0.0140\n",
      "\n",
      "Epoch 00504: loss improved from 0.00060 to 0.00060, saving model to keras/best_baseline_weights.hdf5\n",
      "Epoch 505/10000\n",
      "2700/2700 [==============================] - 0s 21us/step - loss: 5.9198e-04 - mean_absolute_error: 0.0138\n",
      "\n",
      "Epoch 00505: loss improved from 0.00060 to 0.00059, saving model to keras/best_baseline_weights.hdf5\n",
      "Epoch 506/10000\n",
      "2700/2700 [==============================] - 0s 21us/step - loss: 5.9371e-04 - mean_absolute_error: 0.0139\n",
      "\n",
      "Epoch 00506: loss did not improve\n",
      "Epoch 507/10000\n",
      "2700/2700 [==============================] - 0s 21us/step - loss: 5.9141e-04 - mean_absolute_error: 0.0139\n",
      "\n",
      "Epoch 00507: loss improved from 0.00059 to 0.00059, saving model to keras/best_baseline_weights.hdf5\n",
      "Epoch 508/10000\n",
      "2700/2700 [==============================] - 0s 21us/step - loss: 5.9508e-04 - mean_absolute_error: 0.0140\n",
      "\n",
      "Epoch 00508: loss did not improve\n",
      "Epoch 509/10000\n",
      "2700/2700 [==============================] - 0s 23us/step - loss: 5.8045e-04 - mean_absolute_error: 0.0135\n",
      "\n",
      "Epoch 00509: loss improved from 0.00059 to 0.00058, saving model to keras/best_baseline_weights.hdf5\n",
      "Epoch 510/10000\n",
      "2700/2700 [==============================] - 0s 23us/step - loss: 5.8431e-04 - mean_absolute_error: 0.0137\n",
      "\n",
      "Epoch 00510: loss did not improve\n",
      "Epoch 511/10000\n",
      "2700/2700 [==============================] - 0s 23us/step - loss: 5.7936e-04 - mean_absolute_error: 0.0134\n",
      "\n",
      "Epoch 00511: loss improved from 0.00058 to 0.00058, saving model to keras/best_baseline_weights.hdf5\n",
      "Epoch 512/10000\n",
      "2700/2700 [==============================] - 0s 23us/step - loss: 5.7875e-04 - mean_absolute_error: 0.0136\n",
      "\n",
      "Epoch 00512: loss improved from 0.00058 to 0.00058, saving model to keras/best_baseline_weights.hdf5\n",
      "Epoch 513/10000\n",
      "2700/2700 [==============================] - 0s 21us/step - loss: 5.7865e-04 - mean_absolute_error: 0.0136\n",
      "\n",
      "Epoch 00513: loss improved from 0.00058 to 0.00058, saving model to keras/best_baseline_weights.hdf5\n",
      "Epoch 514/10000\n",
      "2700/2700 [==============================] - 0s 21us/step - loss: 5.8032e-04 - mean_absolute_error: 0.0138\n",
      "\n",
      "Epoch 00514: loss did not improve\n",
      "Epoch 515/10000\n",
      "2700/2700 [==============================] - 0s 22us/step - loss: 5.7717e-04 - mean_absolute_error: 0.0136\n",
      "\n",
      "Epoch 00515: loss improved from 0.00058 to 0.00058, saving model to keras/best_baseline_weights.hdf5\n",
      "Epoch 516/10000\n",
      "2700/2700 [==============================] - 0s 22us/step - loss: 5.7381e-04 - mean_absolute_error: 0.0136\n",
      "\n",
      "Epoch 00516: loss improved from 0.00058 to 0.00057, saving model to keras/best_baseline_weights.hdf5\n",
      "Epoch 517/10000\n",
      "2700/2700 [==============================] - 0s 21us/step - loss: 5.7417e-04 - mean_absolute_error: 0.0137\n",
      "\n",
      "Epoch 00517: loss did not improve\n",
      "Epoch 518/10000\n",
      "2700/2700 [==============================] - 0s 22us/step - loss: 5.7033e-04 - mean_absolute_error: 0.0136\n",
      "\n",
      "Epoch 00518: loss improved from 0.00057 to 0.00057, saving model to keras/best_baseline_weights.hdf5\n",
      "Epoch 519/10000\n",
      "2700/2700 [==============================] - 0s 24us/step - loss: 5.6702e-04 - mean_absolute_error: 0.0134\n",
      "\n",
      "Epoch 00519: loss improved from 0.00057 to 0.00057, saving model to keras/best_baseline_weights.hdf5\n",
      "Epoch 520/10000\n",
      "2700/2700 [==============================] - 0s 21us/step - loss: 5.7143e-04 - mean_absolute_error: 0.0136\n",
      "\n",
      "Epoch 00520: loss did not improve\n",
      "Epoch 521/10000\n",
      "2700/2700 [==============================] - 0s 21us/step - loss: 5.6847e-04 - mean_absolute_error: 0.0136\n",
      "\n",
      "Epoch 00521: loss did not improve\n",
      "Epoch 522/10000\n",
      "2700/2700 [==============================] - 0s 22us/step - loss: 5.6588e-04 - mean_absolute_error: 0.0135\n",
      "\n",
      "Epoch 00522: loss improved from 0.00057 to 0.00057, saving model to keras/best_baseline_weights.hdf5\n",
      "Epoch 523/10000\n",
      "2700/2700 [==============================] - 0s 24us/step - loss: 5.6537e-04 - mean_absolute_error: 0.0135\n",
      "\n",
      "Epoch 00523: loss improved from 0.00057 to 0.00057, saving model to keras/best_baseline_weights.hdf5\n",
      "Epoch 524/10000\n",
      "2700/2700 [==============================] - 0s 24us/step - loss: 5.6114e-04 - mean_absolute_error: 0.0134\n",
      "\n",
      "Epoch 00524: loss improved from 0.00057 to 0.00056, saving model to keras/best_baseline_weights.hdf5\n",
      "Epoch 525/10000\n",
      "2700/2700 [==============================] - 0s 22us/step - loss: 5.6252e-04 - mean_absolute_error: 0.0134\n",
      "\n",
      "Epoch 00525: loss did not improve\n",
      "Epoch 526/10000\n",
      "2700/2700 [==============================] - 0s 22us/step - loss: 5.6252e-04 - mean_absolute_error: 0.0135\n",
      "\n",
      "Epoch 00526: loss did not improve\n",
      "Epoch 527/10000\n",
      "2700/2700 [==============================] - 0s 22us/step - loss: 5.5820e-04 - mean_absolute_error: 0.0134\n",
      "\n",
      "Epoch 00527: loss improved from 0.00056 to 0.00056, saving model to keras/best_baseline_weights.hdf5\n",
      "Epoch 528/10000\n",
      "2700/2700 [==============================] - 0s 22us/step - loss: 5.6298e-04 - mean_absolute_error: 0.0136\n",
      "\n",
      "Epoch 00528: loss did not improve\n",
      "Epoch 529/10000\n",
      "2700/2700 [==============================] - 0s 22us/step - loss: 5.6428e-04 - mean_absolute_error: 0.0137\n",
      "\n",
      "Epoch 00529: loss did not improve\n",
      "Epoch 530/10000\n",
      "2700/2700 [==============================] - 0s 22us/step - loss: 5.5437e-04 - mean_absolute_error: 0.0133\n",
      "\n",
      "Epoch 00530: loss improved from 0.00056 to 0.00055, saving model to keras/best_baseline_weights.hdf5\n",
      "Epoch 531/10000\n",
      "2700/2700 [==============================] - 0s 22us/step - loss: 5.5662e-04 - mean_absolute_error: 0.0133\n",
      "\n",
      "Epoch 00531: loss did not improve\n",
      "Epoch 532/10000\n",
      "2700/2700 [==============================] - 0s 22us/step - loss: 5.5397e-04 - mean_absolute_error: 0.0132\n",
      "\n",
      "Epoch 00532: loss improved from 0.00055 to 0.00055, saving model to keras/best_baseline_weights.hdf5\n",
      "Epoch 533/10000\n",
      "2700/2700 [==============================] - 0s 23us/step - loss: 5.5402e-04 - mean_absolute_error: 0.0134\n",
      "\n",
      "Epoch 00533: loss did not improve\n",
      "Epoch 534/10000\n",
      "2700/2700 [==============================] - 0s 21us/step - loss: 5.6386e-04 - mean_absolute_error: 0.0138\n",
      "\n",
      "Epoch 00534: loss did not improve\n",
      "Epoch 535/10000\n",
      "2700/2700 [==============================] - 0s 21us/step - loss: 5.5266e-04 - mean_absolute_error: 0.0134\n",
      "\n",
      "Epoch 00535: loss improved from 0.00055 to 0.00055, saving model to keras/best_baseline_weights.hdf5\n",
      "Epoch 536/10000\n",
      "2700/2700 [==============================] - 0s 22us/step - loss: 5.4614e-04 - mean_absolute_error: 0.0134\n",
      "\n",
      "Epoch 00536: loss improved from 0.00055 to 0.00055, saving model to keras/best_baseline_weights.hdf5\n",
      "Epoch 537/10000\n",
      "2700/2700 [==============================] - 0s 23us/step - loss: 5.5047e-04 - mean_absolute_error: 0.0134\n",
      "\n",
      "Epoch 00537: loss did not improve\n",
      "Epoch 538/10000\n",
      "2700/2700 [==============================] - 0s 22us/step - loss: 5.5500e-04 - mean_absolute_error: 0.0136\n",
      "\n",
      "Epoch 00538: loss did not improve\n",
      "Epoch 539/10000\n",
      "2700/2700 [==============================] - 0s 22us/step - loss: 5.5679e-04 - mean_absolute_error: 0.0137\n",
      "\n",
      "Epoch 00539: loss did not improve\n",
      "Epoch 540/10000\n",
      "2700/2700 [==============================] - 0s 22us/step - loss: 5.4422e-04 - mean_absolute_error: 0.0132\n",
      "\n",
      "Epoch 00540: loss improved from 0.00055 to 0.00054, saving model to keras/best_baseline_weights.hdf5\n",
      "Epoch 541/10000\n",
      "2700/2700 [==============================] - 0s 22us/step - loss: 5.4154e-04 - mean_absolute_error: 0.0131\n",
      "\n",
      "Epoch 00541: loss improved from 0.00054 to 0.00054, saving model to keras/best_baseline_weights.hdf5\n",
      "Epoch 542/10000\n",
      "2700/2700 [==============================] - 0s 23us/step - loss: 5.4195e-04 - mean_absolute_error: 0.0133\n",
      "\n",
      "Epoch 00542: loss did not improve\n",
      "Epoch 543/10000\n",
      "2700/2700 [==============================] - 0s 21us/step - loss: 5.4108e-04 - mean_absolute_error: 0.0131\n",
      "\n",
      "Epoch 00543: loss improved from 0.00054 to 0.00054, saving model to keras/best_baseline_weights.hdf5\n",
      "Epoch 544/10000\n",
      "2700/2700 [==============================] - 0s 22us/step - loss: 5.4066e-04 - mean_absolute_error: 0.0132\n",
      "\n",
      "Epoch 00544: loss improved from 0.00054 to 0.00054, saving model to keras/best_baseline_weights.hdf5\n",
      "Epoch 545/10000\n",
      "2700/2700 [==============================] - 0s 22us/step - loss: 5.3853e-04 - mean_absolute_error: 0.0132\n",
      "\n",
      "Epoch 00545: loss improved from 0.00054 to 0.00054, saving model to keras/best_baseline_weights.hdf5\n",
      "Epoch 546/10000\n",
      "2700/2700 [==============================] - 0s 24us/step - loss: 5.3745e-04 - mean_absolute_error: 0.0132\n",
      "\n",
      "Epoch 00546: loss improved from 0.00054 to 0.00054, saving model to keras/best_baseline_weights.hdf5\n",
      "Epoch 547/10000\n",
      "2700/2700 [==============================] - 0s 23us/step - loss: 5.3813e-04 - mean_absolute_error: 0.0131\n",
      "\n",
      "Epoch 00547: loss did not improve\n",
      "Epoch 548/10000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2700/2700 [==============================] - 0s 23us/step - loss: 5.3528e-04 - mean_absolute_error: 0.0131\n",
      "\n",
      "Epoch 00548: loss improved from 0.00054 to 0.00054, saving model to keras/best_baseline_weights.hdf5\n",
      "Epoch 549/10000\n",
      "2700/2700 [==============================] - 0s 25us/step - loss: 5.3617e-04 - mean_absolute_error: 0.0131\n",
      "\n",
      "Epoch 00549: loss did not improve\n",
      "Epoch 550/10000\n",
      "2700/2700 [==============================] - 0s 23us/step - loss: 5.3917e-04 - mean_absolute_error: 0.0134\n",
      "\n",
      "Epoch 00550: loss did not improve\n",
      "Epoch 551/10000\n",
      "2700/2700 [==============================] - 0s 24us/step - loss: 5.3590e-04 - mean_absolute_error: 0.0132\n",
      "\n",
      "Epoch 00551: loss did not improve\n",
      "Epoch 552/10000\n",
      "2700/2700 [==============================] - 0s 24us/step - loss: 5.3312e-04 - mean_absolute_error: 0.0132\n",
      "\n",
      "Epoch 00552: loss improved from 0.00054 to 0.00053, saving model to keras/best_baseline_weights.hdf5\n",
      "Epoch 553/10000\n",
      "2700/2700 [==============================] - 0s 22us/step - loss: 5.3031e-04 - mean_absolute_error: 0.0131\n",
      "\n",
      "Epoch 00553: loss improved from 0.00053 to 0.00053, saving model to keras/best_baseline_weights.hdf5\n",
      "Epoch 554/10000\n",
      "2700/2700 [==============================] - 0s 21us/step - loss: 5.3287e-04 - mean_absolute_error: 0.0130\n",
      "\n",
      "Epoch 00554: loss did not improve\n",
      "Epoch 555/10000\n",
      "2700/2700 [==============================] - 0s 21us/step - loss: 5.3463e-04 - mean_absolute_error: 0.0132\n",
      "\n",
      "Epoch 00555: loss did not improve\n",
      "Epoch 556/10000\n",
      "2700/2700 [==============================] - 0s 22us/step - loss: 5.3084e-04 - mean_absolute_error: 0.0131\n",
      "\n",
      "Epoch 00556: loss did not improve\n",
      "Epoch 557/10000\n",
      "2700/2700 [==============================] - 0s 21us/step - loss: 5.3242e-04 - mean_absolute_error: 0.0132\n",
      "\n",
      "Epoch 00557: loss did not improve\n",
      "Epoch 558/10000\n",
      "2700/2700 [==============================] - 0s 21us/step - loss: 5.2777e-04 - mean_absolute_error: 0.0130\n",
      "\n",
      "Epoch 00558: loss improved from 0.00053 to 0.00053, saving model to keras/best_baseline_weights.hdf5\n",
      "Epoch 559/10000\n",
      "2700/2700 [==============================] - 0s 21us/step - loss: 5.3183e-04 - mean_absolute_error: 0.0132\n",
      "\n",
      "Epoch 00559: loss did not improve\n",
      "Epoch 560/10000\n",
      "2700/2700 [==============================] - 0s 22us/step - loss: 5.3078e-04 - mean_absolute_error: 0.0132\n",
      "\n",
      "Epoch 00560: loss did not improve\n",
      "Epoch 561/10000\n",
      "2700/2700 [==============================] - 0s 22us/step - loss: 5.2275e-04 - mean_absolute_error: 0.0130\n",
      "\n",
      "Epoch 00561: loss improved from 0.00053 to 0.00052, saving model to keras/best_baseline_weights.hdf5\n",
      "Epoch 562/10000\n",
      "2700/2700 [==============================] - 0s 21us/step - loss: 5.2374e-04 - mean_absolute_error: 0.0131\n",
      "\n",
      "Epoch 00562: loss did not improve\n",
      "Epoch 563/10000\n",
      "2700/2700 [==============================] - 0s 21us/step - loss: 5.2378e-04 - mean_absolute_error: 0.0131\n",
      "\n",
      "Epoch 00563: loss did not improve\n",
      "Epoch 564/10000\n",
      "2700/2700 [==============================] - 0s 21us/step - loss: 5.2439e-04 - mean_absolute_error: 0.0130\n",
      "\n",
      "Epoch 00564: loss did not improve\n",
      "Epoch 565/10000\n",
      "2700/2700 [==============================] - 0s 21us/step - loss: 5.3358e-04 - mean_absolute_error: 0.0136\n",
      "\n",
      "Epoch 00565: loss did not improve\n",
      "Epoch 566/10000\n",
      "2700/2700 [==============================] - 0s 21us/step - loss: 5.2273e-04 - mean_absolute_error: 0.0131\n",
      "\n",
      "Epoch 00566: loss improved from 0.00052 to 0.00052, saving model to keras/best_baseline_weights.hdf5\n",
      "Epoch 567/10000\n",
      "2700/2700 [==============================] - 0s 21us/step - loss: 5.1774e-04 - mean_absolute_error: 0.0129\n",
      "\n",
      "Epoch 00567: loss improved from 0.00052 to 0.00052, saving model to keras/best_baseline_weights.hdf5\n",
      "Epoch 568/10000\n",
      "2700/2700 [==============================] - 0s 22us/step - loss: 5.2349e-04 - mean_absolute_error: 0.0131\n",
      "\n",
      "Epoch 00568: loss did not improve\n",
      "Epoch 569/10000\n",
      "2700/2700 [==============================] - 0s 21us/step - loss: 5.2347e-04 - mean_absolute_error: 0.0131\n",
      "\n",
      "Epoch 00569: loss did not improve\n",
      "Epoch 570/10000\n",
      "2700/2700 [==============================] - 0s 21us/step - loss: 5.2239e-04 - mean_absolute_error: 0.0131\n",
      "\n",
      "Epoch 00570: loss did not improve\n",
      "Epoch 571/10000\n",
      "2700/2700 [==============================] - 0s 22us/step - loss: 5.1685e-04 - mean_absolute_error: 0.0130\n",
      "\n",
      "Epoch 00571: loss improved from 0.00052 to 0.00052, saving model to keras/best_baseline_weights.hdf5\n",
      "Epoch 572/10000\n",
      "2700/2700 [==============================] - 0s 21us/step - loss: 5.2066e-04 - mean_absolute_error: 0.0130\n",
      "\n",
      "Epoch 00572: loss did not improve\n",
      "Epoch 573/10000\n",
      "2700/2700 [==============================] - 0s 21us/step - loss: 5.2152e-04 - mean_absolute_error: 0.0132\n",
      "\n",
      "Epoch 00573: loss did not improve\n",
      "Epoch 574/10000\n",
      "2700/2700 [==============================] - 0s 21us/step - loss: 5.1456e-04 - mean_absolute_error: 0.0130\n",
      "\n",
      "Epoch 00574: loss improved from 0.00052 to 0.00051, saving model to keras/best_baseline_weights.hdf5\n",
      "Epoch 575/10000\n",
      "2700/2700 [==============================] - 0s 22us/step - loss: 5.1370e-04 - mean_absolute_error: 0.0130\n",
      "\n",
      "Epoch 00575: loss improved from 0.00051 to 0.00051, saving model to keras/best_baseline_weights.hdf5\n",
      "Epoch 576/10000\n",
      "2700/2700 [==============================] - 0s 22us/step - loss: 5.1685e-04 - mean_absolute_error: 0.0131\n",
      "\n",
      "Epoch 00576: loss did not improve\n",
      "Epoch 577/10000\n",
      "2700/2700 [==============================] - 0s 21us/step - loss: 5.1232e-04 - mean_absolute_error: 0.0131\n",
      "\n",
      "Epoch 00577: loss improved from 0.00051 to 0.00051, saving model to keras/best_baseline_weights.hdf5\n",
      "Epoch 578/10000\n",
      "2700/2700 [==============================] - 0s 22us/step - loss: 5.1967e-04 - mean_absolute_error: 0.0132\n",
      "\n",
      "Epoch 00578: loss did not improve\n",
      "Epoch 579/10000\n",
      "2700/2700 [==============================] - 0s 21us/step - loss: 5.1726e-04 - mean_absolute_error: 0.0132\n",
      "\n",
      "Epoch 00579: loss did not improve\n",
      "Epoch 580/10000\n",
      "2700/2700 [==============================] - 0s 22us/step - loss: 5.1464e-04 - mean_absolute_error: 0.0130\n",
      "\n",
      "Epoch 00580: loss did not improve\n",
      "Epoch 581/10000\n",
      "2700/2700 [==============================] - 0s 21us/step - loss: 5.1344e-04 - mean_absolute_error: 0.0129\n",
      "\n",
      "Epoch 00581: loss did not improve\n",
      "Epoch 582/10000\n",
      "2700/2700 [==============================] - 0s 21us/step - loss: 5.1211e-04 - mean_absolute_error: 0.0132\n",
      "\n",
      "Epoch 00582: loss improved from 0.00051 to 0.00051, saving model to keras/best_baseline_weights.hdf5\n",
      "Epoch 583/10000\n",
      "2700/2700 [==============================] - 0s 21us/step - loss: 5.1790e-04 - mean_absolute_error: 0.0132\n",
      "\n",
      "Epoch 00583: loss did not improve\n",
      "Epoch 584/10000\n",
      "2700/2700 [==============================] - 0s 22us/step - loss: 5.1624e-04 - mean_absolute_error: 0.0132\n",
      "\n",
      "Epoch 00584: loss did not improve\n",
      "Epoch 585/10000\n",
      "2700/2700 [==============================] - 0s 21us/step - loss: 5.0884e-04 - mean_absolute_error: 0.0129\n",
      "\n",
      "Epoch 00585: loss improved from 0.00051 to 0.00051, saving model to keras/best_baseline_weights.hdf5\n",
      "Epoch 586/10000\n",
      "2700/2700 [==============================] - 0s 22us/step - loss: 5.0755e-04 - mean_absolute_error: 0.0129\n",
      "\n",
      "Epoch 00586: loss improved from 0.00051 to 0.00051, saving model to keras/best_baseline_weights.hdf5\n",
      "Epoch 587/10000\n",
      "2700/2700 [==============================] - 0s 23us/step - loss: 5.1309e-04 - mean_absolute_error: 0.0130\n",
      "\n",
      "Epoch 00587: loss did not improve\n",
      "Epoch 588/10000\n",
      "2700/2700 [==============================] - 0s 21us/step - loss: 5.0845e-04 - mean_absolute_error: 0.0129\n",
      "\n",
      "Epoch 00588: loss did not improve\n",
      "Epoch 589/10000\n",
      "2700/2700 [==============================] - 0s 21us/step - loss: 5.0538e-04 - mean_absolute_error: 0.0129\n",
      "\n",
      "Epoch 00589: loss improved from 0.00051 to 0.00051, saving model to keras/best_baseline_weights.hdf5\n",
      "Epoch 590/10000\n",
      "2700/2700 [==============================] - 0s 22us/step - loss: 5.1314e-04 - mean_absolute_error: 0.0131\n",
      "\n",
      "Epoch 00590: loss did not improve\n",
      "Epoch 591/10000\n",
      "2700/2700 [==============================] - 0s 21us/step - loss: 5.0432e-04 - mean_absolute_error: 0.0128\n",
      "\n",
      "Epoch 00591: loss improved from 0.00051 to 0.00050, saving model to keras/best_baseline_weights.hdf5\n",
      "Epoch 592/10000\n",
      "2700/2700 [==============================] - 0s 22us/step - loss: 5.0323e-04 - mean_absolute_error: 0.0128\n",
      "\n",
      "Epoch 00592: loss improved from 0.00050 to 0.00050, saving model to keras/best_baseline_weights.hdf5\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 593/10000\n",
      "2700/2700 [==============================] - 0s 21us/step - loss: 5.0709e-04 - mean_absolute_error: 0.0129\n",
      "\n",
      "Epoch 00593: loss did not improve\n",
      "Epoch 594/10000\n",
      "2700/2700 [==============================] - 0s 21us/step - loss: 5.1470e-04 - mean_absolute_error: 0.0133\n",
      "\n",
      "Epoch 00594: loss did not improve\n",
      "Epoch 595/10000\n",
      "2700/2700 [==============================] - 0s 21us/step - loss: 5.0184e-04 - mean_absolute_error: 0.0129\n",
      "\n",
      "Epoch 00595: loss improved from 0.00050 to 0.00050, saving model to keras/best_baseline_weights.hdf5\n",
      "Epoch 596/10000\n",
      "2700/2700 [==============================] - 0s 21us/step - loss: 5.0809e-04 - mean_absolute_error: 0.0132\n",
      "\n",
      "Epoch 00596: loss did not improve\n",
      "Epoch 597/10000\n",
      "2700/2700 [==============================] - 0s 22us/step - loss: 5.1015e-04 - mean_absolute_error: 0.0132\n",
      "\n",
      "Epoch 00597: loss did not improve\n",
      "Epoch 598/10000\n",
      "2700/2700 [==============================] - 0s 21us/step - loss: 5.1675e-04 - mean_absolute_error: 0.0136\n",
      "\n",
      "Epoch 00598: loss did not improve\n",
      "Epoch 599/10000\n",
      "2700/2700 [==============================] - 0s 21us/step - loss: 5.0793e-04 - mean_absolute_error: 0.0132\n",
      "\n",
      "Epoch 00599: loss did not improve\n",
      "Epoch 600/10000\n",
      "2700/2700 [==============================] - 0s 21us/step - loss: 4.9712e-04 - mean_absolute_error: 0.0127\n",
      "\n",
      "Epoch 00600: loss improved from 0.00050 to 0.00050, saving model to keras/best_baseline_weights.hdf5\n",
      "Epoch 601/10000\n",
      "2700/2700 [==============================] - 0s 21us/step - loss: 4.9857e-04 - mean_absolute_error: 0.0127\n",
      "\n",
      "Epoch 00601: loss did not improve\n",
      "Epoch 602/10000\n",
      "2700/2700 [==============================] - 0s 21us/step - loss: 5.0011e-04 - mean_absolute_error: 0.0128\n",
      "\n",
      "Epoch 00602: loss did not improve\n",
      "Epoch 603/10000\n",
      "2700/2700 [==============================] - 0s 22us/step - loss: 5.0113e-04 - mean_absolute_error: 0.0130\n",
      "\n",
      "Epoch 00603: loss did not improve\n",
      "Epoch 604/10000\n",
      "2700/2700 [==============================] - 0s 21us/step - loss: 5.0052e-04 - mean_absolute_error: 0.0128\n",
      "\n",
      "Epoch 00604: loss did not improve\n",
      "Epoch 605/10000\n",
      "2700/2700 [==============================] - 0s 22us/step - loss: 4.9884e-04 - mean_absolute_error: 0.0128\n",
      "\n",
      "Epoch 00605: loss did not improve\n",
      "Epoch 606/10000\n",
      "2700/2700 [==============================] - 0s 21us/step - loss: 5.0394e-04 - mean_absolute_error: 0.0131\n",
      "\n",
      "Epoch 00606: loss did not improve\n",
      "Epoch 607/10000\n",
      "2700/2700 [==============================] - 0s 21us/step - loss: 4.9876e-04 - mean_absolute_error: 0.0129\n",
      "\n",
      "Epoch 00607: loss did not improve\n",
      "Epoch 608/10000\n",
      "2700/2700 [==============================] - 0s 26us/step - loss: 4.9776e-04 - mean_absolute_error: 0.0129\n",
      "\n",
      "Epoch 00608: loss did not improve\n",
      "Epoch 609/10000\n",
      "2700/2700 [==============================] - ETA: 0s - loss: 4.8040e-04 - mean_absolute_error: 0.012 - 0s 23us/step - loss: 4.9709e-04 - mean_absolute_error: 0.0129\n",
      "\n",
      "Epoch 00609: loss improved from 0.00050 to 0.00050, saving model to keras/best_baseline_weights.hdf5\n",
      "Epoch 610/10000\n",
      "2700/2700 [==============================] - 0s 22us/step - loss: 4.9940e-04 - mean_absolute_error: 0.0130\n",
      "\n",
      "Epoch 00610: loss did not improve\n",
      "Epoch 611/10000\n",
      "2700/2700 [==============================] - 0s 21us/step - loss: 4.9354e-04 - mean_absolute_error: 0.0127\n",
      "\n",
      "Epoch 00611: loss improved from 0.00050 to 0.00049, saving model to keras/best_baseline_weights.hdf5\n",
      "Epoch 612/10000\n",
      "2700/2700 [==============================] - 0s 22us/step - loss: 4.9496e-04 - mean_absolute_error: 0.0127\n",
      "\n",
      "Epoch 00612: loss did not improve\n",
      "Epoch 613/10000\n",
      "2700/2700 [==============================] - 0s 22us/step - loss: 5.0329e-04 - mean_absolute_error: 0.0133\n",
      "\n",
      "Epoch 00613: loss did not improve\n",
      "Epoch 614/10000\n",
      "2700/2700 [==============================] - 0s 20us/step - loss: 4.9145e-04 - mean_absolute_error: 0.0127\n",
      "\n",
      "Epoch 00614: loss improved from 0.00049 to 0.00049, saving model to keras/best_baseline_weights.hdf5\n",
      "Epoch 615/10000\n",
      "2700/2700 [==============================] - 0s 22us/step - loss: 4.9104e-04 - mean_absolute_error: 0.0128\n",
      "\n",
      "Epoch 00615: loss improved from 0.00049 to 0.00049, saving model to keras/best_baseline_weights.hdf5\n",
      "Epoch 616/10000\n",
      "2700/2700 [==============================] - 0s 22us/step - loss: 4.9518e-04 - mean_absolute_error: 0.0128\n",
      "\n",
      "Epoch 00616: loss did not improve\n",
      "Epoch 617/10000\n",
      "2700/2700 [==============================] - 0s 21us/step - loss: 4.9689e-04 - mean_absolute_error: 0.0129\n",
      "\n",
      "Epoch 00617: loss did not improve\n",
      "Epoch 618/10000\n",
      "2700/2700 [==============================] - 0s 25us/step - loss: 4.9547e-04 - mean_absolute_error: 0.0129\n",
      "\n",
      "Epoch 00618: loss did not improve\n",
      "Epoch 619/10000\n",
      "2700/2700 [==============================] - 0s 21us/step - loss: 4.9033e-04 - mean_absolute_error: 0.0126\n",
      "\n",
      "Epoch 00619: loss improved from 0.00049 to 0.00049, saving model to keras/best_baseline_weights.hdf5\n",
      "Epoch 620/10000\n",
      "2700/2700 [==============================] - 0s 22us/step - loss: 4.9014e-04 - mean_absolute_error: 0.0127\n",
      "\n",
      "Epoch 00620: loss improved from 0.00049 to 0.00049, saving model to keras/best_baseline_weights.hdf5\n",
      "Epoch 621/10000\n",
      "2700/2700 [==============================] - 0s 21us/step - loss: 4.9448e-04 - mean_absolute_error: 0.0127\n",
      "\n",
      "Epoch 00621: loss did not improve\n",
      "Epoch 622/10000\n",
      "2700/2700 [==============================] - 0s 21us/step - loss: 4.9104e-04 - mean_absolute_error: 0.0128\n",
      "\n",
      "Epoch 00622: loss did not improve\n",
      "Epoch 623/10000\n",
      "2700/2700 [==============================] - 0s 21us/step - loss: 4.8739e-04 - mean_absolute_error: 0.0127\n",
      "\n",
      "Epoch 00623: loss improved from 0.00049 to 0.00049, saving model to keras/best_baseline_weights.hdf5\n",
      "Epoch 624/10000\n",
      "2700/2700 [==============================] - 0s 21us/step - loss: 4.8890e-04 - mean_absolute_error: 0.0128\n",
      "\n",
      "Epoch 00624: loss did not improve\n",
      "Epoch 625/10000\n",
      "2700/2700 [==============================] - 0s 21us/step - loss: 4.8752e-04 - mean_absolute_error: 0.0127\n",
      "\n",
      "Epoch 00625: loss did not improve\n",
      "Epoch 626/10000\n",
      "2700/2700 [==============================] - 0s 21us/step - loss: 4.8660e-04 - mean_absolute_error: 0.0126\n",
      "\n",
      "Epoch 00626: loss improved from 0.00049 to 0.00049, saving model to keras/best_baseline_weights.hdf5\n",
      "Epoch 627/10000\n",
      "2700/2700 [==============================] - 0s 22us/step - loss: 4.8478e-04 - mean_absolute_error: 0.0127\n",
      "\n",
      "Epoch 00627: loss improved from 0.00049 to 0.00048, saving model to keras/best_baseline_weights.hdf5\n",
      "Epoch 628/10000\n",
      "2700/2700 [==============================] - 0s 22us/step - loss: 4.8364e-04 - mean_absolute_error: 0.0126\n",
      "\n",
      "Epoch 00628: loss improved from 0.00048 to 0.00048, saving model to keras/best_baseline_weights.hdf5\n",
      "Epoch 629/10000\n",
      "2700/2700 [==============================] - 0s 25us/step - loss: 4.8648e-04 - mean_absolute_error: 0.0129\n",
      "\n",
      "Epoch 00629: loss did not improve\n",
      "Epoch 630/10000\n",
      "2700/2700 [==============================] - 0s 22us/step - loss: 4.8562e-04 - mean_absolute_error: 0.0127\n",
      "\n",
      "Epoch 00630: loss did not improve\n",
      "Epoch 631/10000\n",
      "2700/2700 [==============================] - 0s 21us/step - loss: 4.8511e-04 - mean_absolute_error: 0.0127\n",
      "\n",
      "Epoch 00631: loss did not improve\n",
      "Epoch 632/10000\n",
      "2700/2700 [==============================] - 0s 22us/step - loss: 4.8779e-04 - mean_absolute_error: 0.0128\n",
      "\n",
      "Epoch 00632: loss did not improve\n",
      "Epoch 633/10000\n",
      "2700/2700 [==============================] - 0s 21us/step - loss: 4.8902e-04 - mean_absolute_error: 0.0131\n",
      "\n",
      "Epoch 00633: loss did not improve\n",
      "Epoch 634/10000\n",
      "2700/2700 [==============================] - 0s 21us/step - loss: 4.9000e-04 - mean_absolute_error: 0.0132\n",
      "\n",
      "Epoch 00634: loss did not improve\n",
      "Epoch 635/10000\n",
      "2700/2700 [==============================] - 0s 21us/step - loss: 4.8515e-04 - mean_absolute_error: 0.0127\n",
      "\n",
      "Epoch 00635: loss did not improve\n",
      "Epoch 636/10000\n",
      "2700/2700 [==============================] - 0s 21us/step - loss: 4.8301e-04 - mean_absolute_error: 0.0127\n",
      "\n",
      "Epoch 00636: loss improved from 0.00048 to 0.00048, saving model to keras/best_baseline_weights.hdf5\n",
      "Epoch 637/10000\n",
      "2700/2700 [==============================] - 0s 21us/step - loss: 4.8436e-04 - mean_absolute_error: 0.0127\n",
      "\n",
      "Epoch 00637: loss did not improve\n",
      "Epoch 638/10000\n",
      "2700/2700 [==============================] - 0s 21us/step - loss: 4.8904e-04 - mean_absolute_error: 0.0130\n",
      "\n",
      "Epoch 00638: loss did not improve\n",
      "Epoch 639/10000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2700/2700 [==============================] - 0s 22us/step - loss: 4.8562e-04 - mean_absolute_error: 0.0128\n",
      "\n",
      "Epoch 00639: loss did not improve\n",
      "Epoch 640/10000\n",
      "2700/2700 [==============================] - 0s 21us/step - loss: 4.8604e-04 - mean_absolute_error: 0.0128\n",
      "\n",
      "Epoch 00640: loss did not improve\n",
      "Epoch 641/10000\n",
      "2700/2700 [==============================] - 0s 21us/step - loss: 4.8520e-04 - mean_absolute_error: 0.0129\n",
      "\n",
      "Epoch 00641: loss did not improve\n",
      "Epoch 642/10000\n",
      "2700/2700 [==============================] - 0s 21us/step - loss: 4.8393e-04 - mean_absolute_error: 0.0128\n",
      "\n",
      "Epoch 00642: loss did not improve\n",
      "Epoch 643/10000\n",
      "2700/2700 [==============================] - 0s 21us/step - loss: 4.8321e-04 - mean_absolute_error: 0.0128\n",
      "\n",
      "Epoch 00643: loss did not improve\n",
      "Epoch 644/10000\n",
      "2700/2700 [==============================] - 0s 21us/step - loss: 4.8571e-04 - mean_absolute_error: 0.0129\n",
      "\n",
      "Epoch 00644: loss did not improve\n",
      "Epoch 645/10000\n",
      "2700/2700 [==============================] - 0s 21us/step - loss: 4.8219e-04 - mean_absolute_error: 0.0128\n",
      "\n",
      "Epoch 00645: loss improved from 0.00048 to 0.00048, saving model to keras/best_baseline_weights.hdf5\n",
      "Epoch 646/10000\n",
      "2700/2700 [==============================] - 0s 21us/step - loss: 4.8207e-04 - mean_absolute_error: 0.0128\n",
      "\n",
      "Epoch 00646: loss improved from 0.00048 to 0.00048, saving model to keras/best_baseline_weights.hdf5\n",
      "Epoch 647/10000\n",
      "2700/2700 [==============================] - 0s 22us/step - loss: 4.8676e-04 - mean_absolute_error: 0.0129\n",
      "\n",
      "Epoch 00647: loss did not improve\n",
      "Epoch 648/10000\n",
      "2700/2700 [==============================] - 0s 20us/step - loss: 4.8374e-04 - mean_absolute_error: 0.0130\n",
      "\n",
      "Epoch 00648: loss did not improve\n",
      "Epoch 649/10000\n",
      "2700/2700 [==============================] - 0s 21us/step - loss: 4.7856e-04 - mean_absolute_error: 0.0127\n",
      "\n",
      "Epoch 00649: loss improved from 0.00048 to 0.00048, saving model to keras/best_baseline_weights.hdf5\n",
      "Epoch 650/10000\n",
      "2700/2700 [==============================] - 0s 22us/step - loss: 4.8164e-04 - mean_absolute_error: 0.0128\n",
      "\n",
      "Epoch 00650: loss did not improve\n",
      "Epoch 651/10000\n",
      "2700/2700 [==============================] - 0s 21us/step - loss: 4.7738e-04 - mean_absolute_error: 0.0125\n",
      "\n",
      "Epoch 00651: loss improved from 0.00048 to 0.00048, saving model to keras/best_baseline_weights.hdf5\n",
      "Epoch 652/10000\n",
      "2700/2700 [==============================] - 0s 21us/step - loss: 4.8144e-04 - mean_absolute_error: 0.0129\n",
      "\n",
      "Epoch 00652: loss did not improve\n",
      "Epoch 653/10000\n",
      "2700/2700 [==============================] - 0s 21us/step - loss: 4.7621e-04 - mean_absolute_error: 0.0126\n",
      "\n",
      "Epoch 00653: loss improved from 0.00048 to 0.00048, saving model to keras/best_baseline_weights.hdf5\n",
      "Epoch 654/10000\n",
      "2700/2700 [==============================] - 0s 21us/step - loss: 4.7357e-04 - mean_absolute_error: 0.0127\n",
      "\n",
      "Epoch 00654: loss improved from 0.00048 to 0.00047, saving model to keras/best_baseline_weights.hdf5\n",
      "Epoch 655/10000\n",
      "2700/2700 [==============================] - 0s 22us/step - loss: 4.7674e-04 - mean_absolute_error: 0.0126\n",
      "\n",
      "Epoch 00655: loss did not improve\n",
      "Epoch 656/10000\n",
      "2700/2700 [==============================] - ETA: 0s - loss: 4.8066e-04 - mean_absolute_error: 0.013 - 0s 21us/step - loss: 4.8758e-04 - mean_absolute_error: 0.0132\n",
      "\n",
      "Epoch 00656: loss did not improve\n",
      "Epoch 657/10000\n",
      "2700/2700 [==============================] - 0s 21us/step - loss: 4.8233e-04 - mean_absolute_error: 0.0130\n",
      "\n",
      "Epoch 00657: loss did not improve\n",
      "Epoch 658/10000\n",
      "2700/2700 [==============================] - 0s 21us/step - loss: 4.7744e-04 - mean_absolute_error: 0.0127\n",
      "\n",
      "Epoch 00658: loss did not improve\n",
      "Epoch 659/10000\n",
      "2700/2700 [==============================] - 0s 21us/step - loss: 4.7449e-04 - mean_absolute_error: 0.0127\n",
      "\n",
      "Epoch 00659: loss did not improve\n",
      "Epoch 660/10000\n",
      "2700/2700 [==============================] - 0s 22us/step - loss: 4.7734e-04 - mean_absolute_error: 0.0127\n",
      "\n",
      "Epoch 00660: loss did not improve\n",
      "Epoch 661/10000\n",
      "2700/2700 [==============================] - 0s 21us/step - loss: 4.7706e-04 - mean_absolute_error: 0.0129\n",
      "\n",
      "Epoch 00661: loss did not improve\n",
      "Epoch 662/10000\n",
      "2700/2700 [==============================] - 0s 21us/step - loss: 4.8505e-04 - mean_absolute_error: 0.0131\n",
      "\n",
      "Epoch 00662: loss did not improve\n",
      "Epoch 663/10000\n",
      "2700/2700 [==============================] - 0s 21us/step - loss: 4.7768e-04 - mean_absolute_error: 0.0128\n",
      "\n",
      "Epoch 00663: loss did not improve\n",
      "Epoch 664/10000\n",
      "2700/2700 [==============================] - 0s 21us/step - loss: 4.7181e-04 - mean_absolute_error: 0.0126\n",
      "\n",
      "Epoch 00664: loss improved from 0.00047 to 0.00047, saving model to keras/best_baseline_weights.hdf5\n",
      "Epoch 665/10000\n",
      "2700/2700 [==============================] - 0s 21us/step - loss: 4.7718e-04 - mean_absolute_error: 0.0128\n",
      "\n",
      "Epoch 00665: loss did not improve\n",
      "Epoch 666/10000\n",
      "2700/2700 [==============================] - 0s 21us/step - loss: 4.7910e-04 - mean_absolute_error: 0.0128\n",
      "\n",
      "Epoch 00666: loss did not improve\n",
      "Epoch 667/10000\n",
      "2700/2700 [==============================] - 0s 21us/step - loss: 4.7239e-04 - mean_absolute_error: 0.0126\n",
      "\n",
      "Epoch 00667: loss did not improve\n",
      "Epoch 668/10000\n",
      "2700/2700 [==============================] - 0s 21us/step - loss: 4.7951e-04 - mean_absolute_error: 0.0127\n",
      "\n",
      "Epoch 00668: loss did not improve\n",
      "Epoch 669/10000\n",
      "2700/2700 [==============================] - 0s 21us/step - loss: 4.7622e-04 - mean_absolute_error: 0.0129\n",
      "\n",
      "Epoch 00669: loss did not improve\n",
      "Epoch 670/10000\n",
      "2700/2700 [==============================] - 0s 20us/step - loss: 4.7473e-04 - mean_absolute_error: 0.0126\n",
      "\n",
      "Epoch 00670: loss did not improve\n",
      "Epoch 671/10000\n",
      "2700/2700 [==============================] - 0s 21us/step - loss: 4.7509e-04 - mean_absolute_error: 0.0129\n",
      "\n",
      "Epoch 00671: loss did not improve\n",
      "Epoch 672/10000\n",
      "2700/2700 [==============================] - 0s 21us/step - loss: 4.8191e-04 - mean_absolute_error: 0.0131\n",
      "\n",
      "Epoch 00672: loss did not improve\n",
      "Epoch 673/10000\n",
      "2700/2700 [==============================] - 0s 21us/step - loss: 4.7920e-04 - mean_absolute_error: 0.0131\n",
      "\n",
      "Epoch 00673: loss did not improve\n",
      "Epoch 674/10000\n",
      "2700/2700 [==============================] - 0s 21us/step - loss: 4.7624e-04 - mean_absolute_error: 0.0129\n",
      "\n",
      "Epoch 00674: loss did not improve\n",
      "Epoch 675/10000\n",
      "2700/2700 [==============================] - 0s 21us/step - loss: 4.7894e-04 - mean_absolute_error: 0.0131\n",
      "\n",
      "Epoch 00675: loss did not improve\n",
      "Epoch 676/10000\n",
      "2700/2700 [==============================] - 0s 21us/step - loss: 4.6845e-04 - mean_absolute_error: 0.0126\n",
      "\n",
      "Epoch 00676: loss improved from 0.00047 to 0.00047, saving model to keras/best_baseline_weights.hdf5\n",
      "Epoch 677/10000\n",
      "2700/2700 [==============================] - 0s 21us/step - loss: 4.6987e-04 - mean_absolute_error: 0.0126\n",
      "\n",
      "Epoch 00677: loss did not improve\n",
      "Epoch 678/10000\n",
      "2700/2700 [==============================] - 0s 23us/step - loss: 4.7563e-04 - mean_absolute_error: 0.0129\n",
      "\n",
      "Epoch 00678: loss did not improve\n",
      "Epoch 679/10000\n",
      "2700/2700 [==============================] - 0s 21us/step - loss: 4.7103e-04 - mean_absolute_error: 0.0128\n",
      "\n",
      "Epoch 00679: loss did not improve\n",
      "Epoch 680/10000\n",
      "2700/2700 [==============================] - 0s 21us/step - loss: 4.6830e-04 - mean_absolute_error: 0.0126\n",
      "\n",
      "Epoch 00680: loss improved from 0.00047 to 0.00047, saving model to keras/best_baseline_weights.hdf5\n",
      "Epoch 681/10000\n",
      "2700/2700 [==============================] - 0s 21us/step - loss: 4.7705e-04 - mean_absolute_error: 0.0131\n",
      "\n",
      "Epoch 00681: loss did not improve\n",
      "Epoch 682/10000\n",
      "2700/2700 [==============================] - 0s 21us/step - loss: 4.7037e-04 - mean_absolute_error: 0.0126\n",
      "\n",
      "Epoch 00682: loss did not improve\n",
      "Epoch 683/10000\n",
      "2700/2700 [==============================] - 0s 21us/step - loss: 4.6836e-04 - mean_absolute_error: 0.0127\n",
      "\n",
      "Epoch 00683: loss did not improve\n",
      "Epoch 684/10000\n",
      "2700/2700 [==============================] - 0s 21us/step - loss: 4.7444e-04 - mean_absolute_error: 0.0129\n",
      "\n",
      "Epoch 00684: loss did not improve\n",
      "Epoch 685/10000\n",
      "2700/2700 [==============================] - 0s 21us/step - loss: 4.7318e-04 - mean_absolute_error: 0.0128\n",
      "\n",
      "Epoch 00685: loss did not improve\n",
      "Epoch 686/10000\n",
      "2700/2700 [==============================] - 0s 21us/step - loss: 4.7005e-04 - mean_absolute_error: 0.0128\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 00686: loss did not improve\n",
      "Epoch 687/10000\n",
      "2700/2700 [==============================] - 0s 21us/step - loss: 4.6838e-04 - mean_absolute_error: 0.0126\n",
      "\n",
      "Epoch 00687: loss did not improve\n",
      "Epoch 688/10000\n",
      "2700/2700 [==============================] - 0s 21us/step - loss: 4.7032e-04 - mean_absolute_error: 0.0129\n",
      "\n",
      "Epoch 00688: loss did not improve\n",
      "Epoch 689/10000\n",
      "2700/2700 [==============================] - 0s 21us/step - loss: 4.7495e-04 - mean_absolute_error: 0.0130\n",
      "\n",
      "Epoch 00689: loss did not improve\n",
      "Epoch 690/10000\n",
      "2700/2700 [==============================] - 0s 22us/step - loss: 4.6741e-04 - mean_absolute_error: 0.0126\n",
      "\n",
      "Epoch 00690: loss improved from 0.00047 to 0.00047, saving model to keras/best_baseline_weights.hdf5\n",
      "Epoch 691/10000\n",
      "2700/2700 [==============================] - 0s 21us/step - loss: 4.7383e-04 - mean_absolute_error: 0.0130\n",
      "\n",
      "Epoch 00691: loss did not improve\n",
      "Epoch 692/10000\n",
      "2700/2700 [==============================] - 0s 21us/step - loss: 4.6698e-04 - mean_absolute_error: 0.0127\n",
      "\n",
      "Epoch 00692: loss improved from 0.00047 to 0.00047, saving model to keras/best_baseline_weights.hdf5\n",
      "Epoch 693/10000\n",
      "2700/2700 [==============================] - 0s 21us/step - loss: 4.6591e-04 - mean_absolute_error: 0.0127\n",
      "\n",
      "Epoch 00693: loss improved from 0.00047 to 0.00047, saving model to keras/best_baseline_weights.hdf5\n",
      "Epoch 694/10000\n",
      "2700/2700 [==============================] - 0s 21us/step - loss: 4.6707e-04 - mean_absolute_error: 0.0126\n",
      "\n",
      "Epoch 00694: loss did not improve\n",
      "Epoch 695/10000\n",
      "2700/2700 [==============================] - 0s 21us/step - loss: 4.7171e-04 - mean_absolute_error: 0.0128\n",
      "\n",
      "Epoch 00695: loss did not improve\n",
      "Epoch 696/10000\n",
      "2700/2700 [==============================] - 0s 21us/step - loss: 4.6662e-04 - mean_absolute_error: 0.0127\n",
      "\n",
      "Epoch 00696: loss did not improve\n",
      "Epoch 697/10000\n",
      "2700/2700 [==============================] - 0s 21us/step - loss: 4.7034e-04 - mean_absolute_error: 0.0128\n",
      "\n",
      "Epoch 00697: loss did not improve\n",
      "Epoch 698/10000\n",
      "2700/2700 [==============================] - 0s 21us/step - loss: 4.6631e-04 - mean_absolute_error: 0.0127\n",
      "\n",
      "Epoch 00698: loss did not improve\n",
      "Epoch 699/10000\n",
      "2700/2700 [==============================] - 0s 21us/step - loss: 4.7368e-04 - mean_absolute_error: 0.0130\n",
      "\n",
      "Epoch 00699: loss did not improve\n",
      "Epoch 700/10000\n",
      "2700/2700 [==============================] - 0s 21us/step - loss: 4.7129e-04 - mean_absolute_error: 0.0128\n",
      "\n",
      "Epoch 00700: loss did not improve\n",
      "Epoch 701/10000\n",
      "2700/2700 [==============================] - 0s 21us/step - loss: 4.7115e-04 - mean_absolute_error: 0.0129\n",
      "\n",
      "Epoch 00701: loss did not improve\n",
      "Epoch 702/10000\n",
      "2700/2700 [==============================] - 0s 21us/step - loss: 4.6288e-04 - mean_absolute_error: 0.0126\n",
      "\n",
      "Epoch 00702: loss improved from 0.00047 to 0.00046, saving model to keras/best_baseline_weights.hdf5\n",
      "Epoch 703/10000\n",
      "2700/2700 [==============================] - 0s 21us/step - loss: 4.6405e-04 - mean_absolute_error: 0.0127\n",
      "\n",
      "Epoch 00703: loss did not improve\n",
      "Epoch 704/10000\n",
      "2700/2700 [==============================] - 0s 21us/step - loss: 4.6345e-04 - mean_absolute_error: 0.0126\n",
      "\n",
      "Epoch 00704: loss did not improve\n",
      "Epoch 705/10000\n",
      "2700/2700 [==============================] - 0s 21us/step - loss: 4.6446e-04 - mean_absolute_error: 0.0127\n",
      "\n",
      "Epoch 00705: loss did not improve\n",
      "Epoch 706/10000\n",
      "2700/2700 [==============================] - 0s 21us/step - loss: 4.6708e-04 - mean_absolute_error: 0.0130\n",
      "\n",
      "Epoch 00706: loss did not improve\n",
      "Epoch 707/10000\n",
      "2700/2700 [==============================] - 0s 21us/step - loss: 4.6463e-04 - mean_absolute_error: 0.0129\n",
      "\n",
      "Epoch 00707: loss did not improve\n",
      "Epoch 708/10000\n",
      "2700/2700 [==============================] - 0s 21us/step - loss: 4.6704e-04 - mean_absolute_error: 0.0127\n",
      "\n",
      "Epoch 00708: loss did not improve\n",
      "Epoch 709/10000\n",
      "2700/2700 [==============================] - 0s 21us/step - loss: 4.6241e-04 - mean_absolute_error: 0.0126\n",
      "\n",
      "Epoch 00709: loss improved from 0.00046 to 0.00046, saving model to keras/best_baseline_weights.hdf5\n",
      "Epoch 710/10000\n",
      "2700/2700 [==============================] - 0s 21us/step - loss: 4.6608e-04 - mean_absolute_error: 0.0128\n",
      "\n",
      "Epoch 00710: loss did not improve\n",
      "Epoch 711/10000\n",
      "2700/2700 [==============================] - 0s 21us/step - loss: 4.6290e-04 - mean_absolute_error: 0.0127\n",
      "\n",
      "Epoch 00711: loss did not improve\n",
      "Epoch 712/10000\n",
      "2700/2700 [==============================] - 0s 21us/step - loss: 4.6371e-04 - mean_absolute_error: 0.0127\n",
      "\n",
      "Epoch 00712: loss did not improve\n",
      "Epoch 713/10000\n",
      "2700/2700 [==============================] - 0s 21us/step - loss: 4.6534e-04 - mean_absolute_error: 0.0129\n",
      "\n",
      "Epoch 00713: loss did not improve\n",
      "Epoch 714/10000\n",
      "2700/2700 [==============================] - 0s 21us/step - loss: 4.6271e-04 - mean_absolute_error: 0.0127\n",
      "\n",
      "Epoch 00714: loss did not improve\n",
      "Epoch 715/10000\n",
      "2700/2700 [==============================] - 0s 21us/step - loss: 4.6234e-04 - mean_absolute_error: 0.0126\n",
      "\n",
      "Epoch 00715: loss improved from 0.00046 to 0.00046, saving model to keras/best_baseline_weights.hdf5\n",
      "Epoch 716/10000\n",
      "2700/2700 [==============================] - 0s 21us/step - loss: 4.6159e-04 - mean_absolute_error: 0.0127\n",
      "\n",
      "Epoch 00716: loss improved from 0.00046 to 0.00046, saving model to keras/best_baseline_weights.hdf5\n",
      "Epoch 717/10000\n",
      "2700/2700 [==============================] - 0s 21us/step - loss: 4.6647e-04 - mean_absolute_error: 0.0128\n",
      "\n",
      "Epoch 00717: loss did not improve\n",
      "Epoch 718/10000\n",
      "2700/2700 [==============================] - 0s 21us/step - loss: 4.6876e-04 - mean_absolute_error: 0.0131\n",
      "\n",
      "Epoch 00718: loss did not improve\n",
      "Epoch 719/10000\n",
      "2700/2700 [==============================] - 0s 21us/step - loss: 4.6307e-04 - mean_absolute_error: 0.0127\n",
      "\n",
      "Epoch 00719: loss did not improve\n",
      "Epoch 720/10000\n",
      "2700/2700 [==============================] - 0s 21us/step - loss: 4.6471e-04 - mean_absolute_error: 0.0127\n",
      "\n",
      "Epoch 00720: loss did not improve\n",
      "Epoch 721/10000\n",
      "2700/2700 [==============================] - 0s 21us/step - loss: 4.6345e-04 - mean_absolute_error: 0.0127\n",
      "\n",
      "Epoch 00721: loss did not improve\n",
      "Epoch 722/10000\n",
      "2700/2700 [==============================] - 0s 21us/step - loss: 4.5763e-04 - mean_absolute_error: 0.0125\n",
      "\n",
      "Epoch 00722: loss improved from 0.00046 to 0.00046, saving model to keras/best_baseline_weights.hdf5\n",
      "Epoch 723/10000\n",
      "2700/2700 [==============================] - 0s 21us/step - loss: 4.6123e-04 - mean_absolute_error: 0.0126\n",
      "\n",
      "Epoch 00723: loss did not improve\n",
      "Epoch 724/10000\n",
      "2700/2700 [==============================] - 0s 21us/step - loss: 4.6048e-04 - mean_absolute_error: 0.0126\n",
      "\n",
      "Epoch 00724: loss did not improve\n",
      "Epoch 725/10000\n",
      "2700/2700 [==============================] - 0s 21us/step - loss: 4.6268e-04 - mean_absolute_error: 0.0128\n",
      "\n",
      "Epoch 00725: loss did not improve\n",
      "Epoch 726/10000\n",
      "2700/2700 [==============================] - 0s 21us/step - loss: 4.6180e-04 - mean_absolute_error: 0.0127\n",
      "\n",
      "Epoch 00726: loss did not improve\n",
      "Epoch 727/10000\n",
      "2700/2700 [==============================] - 0s 21us/step - loss: 4.5878e-04 - mean_absolute_error: 0.0126\n",
      "\n",
      "Epoch 00727: loss did not improve\n",
      "Epoch 728/10000\n",
      "2700/2700 [==============================] - 0s 21us/step - loss: 4.6018e-04 - mean_absolute_error: 0.0126\n",
      "\n",
      "Epoch 00728: loss did not improve\n",
      "Epoch 729/10000\n",
      "2700/2700 [==============================] - 0s 21us/step - loss: 4.6352e-04 - mean_absolute_error: 0.0128\n",
      "\n",
      "Epoch 00729: loss did not improve\n",
      "Epoch 730/10000\n",
      "2700/2700 [==============================] - 0s 21us/step - loss: 4.6314e-04 - mean_absolute_error: 0.0129\n",
      "\n",
      "Epoch 00730: loss did not improve\n",
      "Epoch 731/10000\n",
      "2700/2700 [==============================] - 0s 21us/step - loss: 4.6192e-04 - mean_absolute_error: 0.0128\n",
      "\n",
      "Epoch 00731: loss did not improve\n",
      "Epoch 732/10000\n",
      "2700/2700 [==============================] - 0s 22us/step - loss: 4.6095e-04 - mean_absolute_error: 0.0128\n",
      "\n",
      "Epoch 00732: loss did not improve\n",
      "Epoch 733/10000\n",
      "2700/2700 [==============================] - 0s 21us/step - loss: 4.5871e-04 - mean_absolute_error: 0.0127\n",
      "\n",
      "Epoch 00733: loss did not improve\n",
      "Epoch 734/10000\n",
      "2700/2700 [==============================] - 0s 21us/step - loss: 4.6500e-04 - mean_absolute_error: 0.0130\n",
      "\n",
      "Epoch 00734: loss did not improve\n",
      "Epoch 735/10000\n",
      "2700/2700 [==============================] - 0s 21us/step - loss: 4.5689e-04 - mean_absolute_error: 0.0128\n",
      "\n",
      "Epoch 00735: loss improved from 0.00046 to 0.00046, saving model to keras/best_baseline_weights.hdf5\n",
      "Epoch 736/10000\n",
      "2700/2700 [==============================] - 0s 21us/step - loss: 4.5900e-04 - mean_absolute_error: 0.0127\n",
      "\n",
      "Epoch 00736: loss did not improve\n",
      "Epoch 737/10000\n",
      "2700/2700 [==============================] - 0s 21us/step - loss: 4.6097e-04 - mean_absolute_error: 0.0128\n",
      "\n",
      "Epoch 00737: loss did not improve\n",
      "Epoch 738/10000\n",
      "2700/2700 [==============================] - 0s 21us/step - loss: 4.5941e-04 - mean_absolute_error: 0.0128\n",
      "\n",
      "Epoch 00738: loss did not improve\n",
      "Epoch 739/10000\n",
      "2700/2700 [==============================] - 0s 21us/step - loss: 4.6430e-04 - mean_absolute_error: 0.0130\n",
      "\n",
      "Epoch 00739: loss did not improve\n",
      "Epoch 740/10000\n",
      "2700/2700 [==============================] - 0s 21us/step - loss: 4.5927e-04 - mean_absolute_error: 0.0128\n",
      "\n",
      "Epoch 00740: loss did not improve\n",
      "Epoch 741/10000\n",
      "2700/2700 [==============================] - 0s 22us/step - loss: 4.5994e-04 - mean_absolute_error: 0.0128\n",
      "\n",
      "Epoch 00741: loss did not improve\n",
      "Epoch 742/10000\n",
      "2700/2700 [==============================] - 0s 21us/step - loss: 4.5992e-04 - mean_absolute_error: 0.0127\n",
      "\n",
      "Epoch 00742: loss did not improve\n",
      "Epoch 743/10000\n",
      "2700/2700 [==============================] - 0s 39us/step - loss: 4.6100e-04 - mean_absolute_error: 0.0129\n",
      "\n",
      "Epoch 00743: loss did not improve\n",
      "Epoch 744/10000\n",
      "2700/2700 [==============================] - 0s 28us/step - loss: 4.5794e-04 - mean_absolute_error: 0.0127\n",
      "\n",
      "Epoch 00744: loss did not improve\n",
      "Epoch 745/10000\n",
      "2700/2700 [==============================] - 0s 26us/step - loss: 4.5803e-04 - mean_absolute_error: 0.0127\n",
      "\n",
      "Epoch 00745: loss did not improve\n",
      "Epoch 746/10000\n",
      "2700/2700 [==============================] - 0s 22us/step - loss: 4.6387e-04 - mean_absolute_error: 0.0130\n",
      "\n",
      "Epoch 00746: loss did not improve\n",
      "Epoch 747/10000\n",
      "2700/2700 [==============================] - 0s 21us/step - loss: 4.5885e-04 - mean_absolute_error: 0.0129\n",
      "\n",
      "Epoch 00747: loss did not improve\n",
      "Epoch 748/10000\n",
      "2700/2700 [==============================] - 0s 21us/step - loss: 4.5944e-04 - mean_absolute_error: 0.0128\n",
      "\n",
      "Epoch 00748: loss did not improve\n",
      "Epoch 749/10000\n",
      "2700/2700 [==============================] - 0s 21us/step - loss: 4.5618e-04 - mean_absolute_error: 0.0127\n",
      "\n",
      "Epoch 00749: loss improved from 0.00046 to 0.00046, saving model to keras/best_baseline_weights.hdf5\n",
      "Epoch 750/10000\n",
      "2700/2700 [==============================] - 0s 21us/step - loss: 4.5457e-04 - mean_absolute_error: 0.0127\n",
      "\n",
      "Epoch 00750: loss improved from 0.00046 to 0.00045, saving model to keras/best_baseline_weights.hdf5\n",
      "Epoch 751/10000\n",
      "2700/2700 [==============================] - 0s 21us/step - loss: 4.5835e-04 - mean_absolute_error: 0.0129\n",
      "\n",
      "Epoch 00751: loss did not improve\n",
      "Epoch 752/10000\n",
      "2700/2700 [==============================] - 0s 22us/step - loss: 4.5513e-04 - mean_absolute_error: 0.0127\n",
      "\n",
      "Epoch 00752: loss did not improve\n",
      "Epoch 753/10000\n",
      "2700/2700 [==============================] - 0s 21us/step - loss: 4.5987e-04 - mean_absolute_error: 0.0128\n",
      "\n",
      "Epoch 00753: loss did not improve\n",
      "Epoch 754/10000\n",
      "2700/2700 [==============================] - 0s 21us/step - loss: 4.6297e-04 - mean_absolute_error: 0.0131\n",
      "\n",
      "Epoch 00754: loss did not improve\n",
      "Epoch 755/10000\n",
      "2700/2700 [==============================] - 0s 21us/step - loss: 4.5431e-04 - mean_absolute_error: 0.0126\n",
      "\n",
      "Epoch 00755: loss improved from 0.00045 to 0.00045, saving model to keras/best_baseline_weights.hdf5\n",
      "Epoch 756/10000\n",
      "2700/2700 [==============================] - 0s 21us/step - loss: 4.5602e-04 - mean_absolute_error: 0.0127\n",
      "\n",
      "Epoch 00756: loss did not improve\n",
      "Epoch 757/10000\n",
      "2700/2700 [==============================] - 0s 21us/step - loss: 4.5497e-04 - mean_absolute_error: 0.0127\n",
      "\n",
      "Epoch 00757: loss did not improve\n",
      "Epoch 758/10000\n",
      "2700/2700 [==============================] - 0s 21us/step - loss: 4.5557e-04 - mean_absolute_error: 0.0127\n",
      "\n",
      "Epoch 00758: loss did not improve\n",
      "Epoch 759/10000\n",
      "2700/2700 [==============================] - 0s 22us/step - loss: 4.6234e-04 - mean_absolute_error: 0.0130\n",
      "\n",
      "Epoch 00759: loss did not improve\n",
      "Epoch 760/10000\n",
      "2700/2700 [==============================] - 0s 21us/step - loss: 4.5496e-04 - mean_absolute_error: 0.0127\n",
      "\n",
      "Epoch 00760: loss did not improve\n",
      "Epoch 761/10000\n",
      "2700/2700 [==============================] - 0s 21us/step - loss: 4.5591e-04 - mean_absolute_error: 0.0127\n",
      "\n",
      "Epoch 00761: loss did not improve\n",
      "Epoch 762/10000\n",
      "2700/2700 [==============================] - 0s 21us/step - loss: 4.5422e-04 - mean_absolute_error: 0.0127\n",
      "\n",
      "Epoch 00762: loss improved from 0.00045 to 0.00045, saving model to keras/best_baseline_weights.hdf5\n",
      "Epoch 763/10000\n",
      "2700/2700 [==============================] - 0s 21us/step - loss: 4.5620e-04 - mean_absolute_error: 0.0129\n",
      "\n",
      "Epoch 00763: loss did not improve\n",
      "Epoch 764/10000\n",
      "2700/2700 [==============================] - 0s 22us/step - loss: 4.5658e-04 - mean_absolute_error: 0.0128\n",
      "\n",
      "Epoch 00764: loss did not improve\n",
      "Epoch 765/10000\n",
      "2700/2700 [==============================] - 0s 21us/step - loss: 4.5590e-04 - mean_absolute_error: 0.0128\n",
      "\n",
      "Epoch 00765: loss did not improve\n",
      "Epoch 766/10000\n",
      "2700/2700 [==============================] - 0s 21us/step - loss: 4.6262e-04 - mean_absolute_error: 0.0130\n",
      "\n",
      "Epoch 00766: loss did not improve\n",
      "Epoch 767/10000\n",
      "2700/2700 [==============================] - 0s 21us/step - loss: 4.5723e-04 - mean_absolute_error: 0.0128\n",
      "\n",
      "Epoch 00767: loss did not improve\n",
      "Epoch 768/10000\n",
      "2700/2700 [==============================] - 0s 21us/step - loss: 4.6036e-04 - mean_absolute_error: 0.0131\n",
      "\n",
      "Epoch 00768: loss did not improve\n",
      "Epoch 769/10000\n",
      "2700/2700 [==============================] - 0s 21us/step - loss: 4.5431e-04 - mean_absolute_error: 0.0126\n",
      "\n",
      "Epoch 00769: loss did not improve\n",
      "Epoch 770/10000\n",
      "2700/2700 [==============================] - 0s 21us/step - loss: 4.5427e-04 - mean_absolute_error: 0.0128\n",
      "\n",
      "Epoch 00770: loss did not improve\n",
      "Epoch 771/10000\n",
      "2700/2700 [==============================] - 0s 22us/step - loss: 4.5335e-04 - mean_absolute_error: 0.0127\n",
      "\n",
      "Epoch 00771: loss improved from 0.00045 to 0.00045, saving model to keras/best_baseline_weights.hdf5\n",
      "Epoch 772/10000\n",
      "2700/2700 [==============================] - 0s 21us/step - loss: 4.5426e-04 - mean_absolute_error: 0.0127\n",
      "\n",
      "Epoch 00772: loss did not improve\n",
      "Epoch 773/10000\n",
      "2700/2700 [==============================] - 0s 21us/step - loss: 4.6881e-04 - mean_absolute_error: 0.0134\n",
      "\n",
      "Epoch 00773: loss did not improve\n",
      "Epoch 774/10000\n",
      "2700/2700 [==============================] - 0s 21us/step - loss: 4.5054e-04 - mean_absolute_error: 0.0126\n",
      "\n",
      "Epoch 00774: loss improved from 0.00045 to 0.00045, saving model to keras/best_baseline_weights.hdf5\n",
      "Epoch 775/10000\n",
      "2700/2700 [==============================] - 0s 22us/step - loss: 4.5264e-04 - mean_absolute_error: 0.0127\n",
      "\n",
      "Epoch 00775: loss did not improve\n",
      "Epoch 776/10000\n",
      "2700/2700 [==============================] - 0s 21us/step - loss: 4.6281e-04 - mean_absolute_error: 0.0132\n",
      "\n",
      "Epoch 00776: loss did not improve\n",
      "Epoch 777/10000\n",
      "2700/2700 [==============================] - 0s 21us/step - loss: 4.5545e-04 - mean_absolute_error: 0.0129\n",
      "\n",
      "Epoch 00777: loss did not improve\n",
      "Epoch 778/10000\n",
      "2700/2700 [==============================] - 0s 21us/step - loss: 4.5304e-04 - mean_absolute_error: 0.0126\n",
      "\n",
      "Epoch 00778: loss did not improve\n",
      "Epoch 779/10000\n",
      "2700/2700 [==============================] - 0s 21us/step - loss: 4.5132e-04 - mean_absolute_error: 0.0126\n",
      "\n",
      "Epoch 00779: loss did not improve\n",
      "Epoch 780/10000\n",
      "2700/2700 [==============================] - 0s 21us/step - loss: 4.5203e-04 - mean_absolute_error: 0.0127\n",
      "\n",
      "Epoch 00780: loss did not improve\n",
      "Epoch 781/10000\n",
      "2700/2700 [==============================] - 0s 21us/step - loss: 4.4938e-04 - mean_absolute_error: 0.0126\n",
      "\n",
      "Epoch 00781: loss improved from 0.00045 to 0.00045, saving model to keras/best_baseline_weights.hdf5\n",
      "Epoch 782/10000\n",
      "2700/2700 [==============================] - 0s 22us/step - loss: 4.5510e-04 - mean_absolute_error: 0.0128\n",
      "\n",
      "Epoch 00782: loss did not improve\n",
      "Epoch 783/10000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2700/2700 [==============================] - 0s 21us/step - loss: 4.5306e-04 - mean_absolute_error: 0.0128\n",
      "\n",
      "Epoch 00783: loss did not improve\n",
      "Epoch 784/10000\n",
      "2700/2700 [==============================] - 0s 21us/step - loss: 4.6166e-04 - mean_absolute_error: 0.0133\n",
      "\n",
      "Epoch 00784: loss did not improve\n",
      "Epoch 785/10000\n",
      "2700/2700 [==============================] - 0s 21us/step - loss: 4.5879e-04 - mean_absolute_error: 0.0130\n",
      "\n",
      "Epoch 00785: loss did not improve\n",
      "Epoch 786/10000\n",
      "2700/2700 [==============================] - 0s 21us/step - loss: 4.5265e-04 - mean_absolute_error: 0.0129\n",
      "\n",
      "Epoch 00786: loss did not improve\n",
      "Epoch 787/10000\n",
      "2700/2700 [==============================] - 0s 21us/step - loss: 4.5164e-04 - mean_absolute_error: 0.0128\n",
      "\n",
      "Epoch 00787: loss did not improve\n",
      "Epoch 788/10000\n",
      "2700/2700 [==============================] - 0s 21us/step - loss: 4.5452e-04 - mean_absolute_error: 0.0128\n",
      "\n",
      "Epoch 00788: loss did not improve\n",
      "Epoch 789/10000\n",
      "2700/2700 [==============================] - 0s 21us/step - loss: 4.5089e-04 - mean_absolute_error: 0.0127\n",
      "\n",
      "Epoch 00789: loss did not improve\n",
      "Epoch 790/10000\n",
      "2700/2700 [==============================] - 0s 22us/step - loss: 4.5088e-04 - mean_absolute_error: 0.0127\n",
      "\n",
      "Epoch 00790: loss did not improve\n",
      "Epoch 791/10000\n",
      "2700/2700 [==============================] - 0s 21us/step - loss: 4.5431e-04 - mean_absolute_error: 0.0130\n",
      "\n",
      "Epoch 00791: loss did not improve\n",
      "Epoch 792/10000\n",
      "2700/2700 [==============================] - 0s 21us/step - loss: 4.5548e-04 - mean_absolute_error: 0.0130\n",
      "\n",
      "Epoch 00792: loss did not improve\n",
      "Epoch 793/10000\n",
      "2700/2700 [==============================] - 0s 22us/step - loss: 4.5492e-04 - mean_absolute_error: 0.0131\n",
      "\n",
      "Epoch 00793: loss did not improve\n",
      "Epoch 794/10000\n",
      "2700/2700 [==============================] - 0s 21us/step - loss: 4.5132e-04 - mean_absolute_error: 0.0128\n",
      "\n",
      "Epoch 00794: loss did not improve\n",
      "Epoch 795/10000\n",
      "2700/2700 [==============================] - 0s 21us/step - loss: 4.4988e-04 - mean_absolute_error: 0.0127\n",
      "\n",
      "Epoch 00795: loss did not improve\n",
      "Epoch 796/10000\n",
      "2700/2700 [==============================] - 0s 21us/step - loss: 4.5132e-04 - mean_absolute_error: 0.0128\n",
      "\n",
      "Epoch 00796: loss did not improve\n",
      "Epoch 797/10000\n",
      "2700/2700 [==============================] - 0s 21us/step - loss: 4.5178e-04 - mean_absolute_error: 0.0128\n",
      "\n",
      "Epoch 00797: loss did not improve\n",
      "Epoch 798/10000\n",
      "2700/2700 [==============================] - 0s 21us/step - loss: 4.5432e-04 - mean_absolute_error: 0.0129\n",
      "\n",
      "Epoch 00798: loss did not improve\n",
      "Epoch 799/10000\n",
      "2700/2700 [==============================] - 0s 21us/step - loss: 4.5163e-04 - mean_absolute_error: 0.0128\n",
      "\n",
      "Epoch 00799: loss did not improve\n",
      "Epoch 800/10000\n",
      "2700/2700 [==============================] - 0s 21us/step - loss: 4.5510e-04 - mean_absolute_error: 0.0130\n",
      "\n",
      "Epoch 00800: loss did not improve\n",
      "Epoch 801/10000\n",
      "2700/2700 [==============================] - 0s 21us/step - loss: 4.4873e-04 - mean_absolute_error: 0.0126\n",
      "\n",
      "Epoch 00801: loss improved from 0.00045 to 0.00045, saving model to keras/best_baseline_weights.hdf5\n",
      "Epoch 802/10000\n",
      "2700/2700 [==============================] - 0s 21us/step - loss: 4.5000e-04 - mean_absolute_error: 0.0127\n",
      "\n",
      "Epoch 00802: loss did not improve\n",
      "Epoch 803/10000\n",
      "2700/2700 [==============================] - 0s 21us/step - loss: 4.5087e-04 - mean_absolute_error: 0.0127\n",
      "\n",
      "Epoch 00803: loss did not improve\n",
      "Epoch 804/10000\n",
      "2700/2700 [==============================] - 0s 21us/step - loss: 4.5742e-04 - mean_absolute_error: 0.0132\n",
      "\n",
      "Epoch 00804: loss did not improve\n",
      "Epoch 805/10000\n",
      "2700/2700 [==============================] - 0s 21us/step - loss: 4.5245e-04 - mean_absolute_error: 0.0129\n",
      "\n",
      "Epoch 00805: loss did not improve\n",
      "Epoch 806/10000\n",
      "2700/2700 [==============================] - 0s 21us/step - loss: 4.5008e-04 - mean_absolute_error: 0.0128\n",
      "\n",
      "Epoch 00806: loss did not improve\n",
      "Epoch 807/10000\n",
      "2700/2700 [==============================] - 0s 21us/step - loss: 4.5158e-04 - mean_absolute_error: 0.0129\n",
      "\n",
      "Epoch 00807: loss did not improve\n",
      "Epoch 808/10000\n",
      "2700/2700 [==============================] - 0s 21us/step - loss: 4.5193e-04 - mean_absolute_error: 0.0129\n",
      "\n",
      "Epoch 00808: loss did not improve\n",
      "Epoch 809/10000\n",
      "2700/2700 [==============================] - 0s 22us/step - loss: 4.5280e-04 - mean_absolute_error: 0.0129\n",
      "\n",
      "Epoch 00809: loss did not improve\n",
      "Epoch 810/10000\n",
      "2700/2700 [==============================] - 0s 21us/step - loss: 4.6152e-04 - mean_absolute_error: 0.0133\n",
      "\n",
      "Epoch 00810: loss did not improve\n",
      "Epoch 811/10000\n",
      "2700/2700 [==============================] - 0s 21us/step - loss: 4.5771e-04 - mean_absolute_error: 0.0131\n",
      "\n",
      "Epoch 00811: loss did not improve\n",
      "Epoch 812/10000\n",
      "2700/2700 [==============================] - 0s 22us/step - loss: 4.4768e-04 - mean_absolute_error: 0.0127\n",
      "\n",
      "Epoch 00812: loss improved from 0.00045 to 0.00045, saving model to keras/best_baseline_weights.hdf5\n",
      "Epoch 813/10000\n",
      "2700/2700 [==============================] - 0s 21us/step - loss: 4.5473e-04 - mean_absolute_error: 0.0131\n",
      "\n",
      "Epoch 00813: loss did not improve\n",
      "Epoch 814/10000\n",
      "2700/2700 [==============================] - 0s 21us/step - loss: 4.5263e-04 - mean_absolute_error: 0.0130\n",
      "\n",
      "Epoch 00814: loss did not improve\n",
      "Epoch 815/10000\n",
      "2700/2700 [==============================] - 0s 21us/step - loss: 4.5447e-04 - mean_absolute_error: 0.0131\n",
      "\n",
      "Epoch 00815: loss did not improve\n",
      "Epoch 816/10000\n",
      "2700/2700 [==============================] - 0s 21us/step - loss: 4.4876e-04 - mean_absolute_error: 0.0128\n",
      "\n",
      "Epoch 00816: loss did not improve\n",
      "Epoch 817/10000\n",
      "2700/2700 [==============================] - 0s 21us/step - loss: 4.5061e-04 - mean_absolute_error: 0.0129\n",
      "\n",
      "Epoch 00817: loss did not improve\n",
      "Epoch 818/10000\n",
      "2700/2700 [==============================] - 0s 21us/step - loss: 4.5124e-04 - mean_absolute_error: 0.0129\n",
      "\n",
      "Epoch 00818: loss did not improve\n",
      "Epoch 819/10000\n",
      "2700/2700 [==============================] - 0s 21us/step - loss: 4.4808e-04 - mean_absolute_error: 0.0129\n",
      "\n",
      "Epoch 00819: loss did not improve\n",
      "Epoch 820/10000\n",
      "2700/2700 [==============================] - 0s 21us/step - loss: 4.5038e-04 - mean_absolute_error: 0.0131\n",
      "\n",
      "Epoch 00820: loss did not improve\n",
      "Epoch 821/10000\n",
      "2700/2700 [==============================] - 0s 21us/step - loss: 4.4961e-04 - mean_absolute_error: 0.0128\n",
      "\n",
      "Epoch 00821: loss did not improve\n",
      "Epoch 822/10000\n",
      "2700/2700 [==============================] - 0s 21us/step - loss: 4.5064e-04 - mean_absolute_error: 0.0129\n",
      "\n",
      "Epoch 00822: loss did not improve\n",
      "Epoch 823/10000\n",
      "2700/2700 [==============================] - 0s 22us/step - loss: 4.5211e-04 - mean_absolute_error: 0.0130\n",
      "\n",
      "Epoch 00823: loss did not improve\n",
      "Epoch 824/10000\n",
      "2700/2700 [==============================] - 0s 21us/step - loss: 4.4997e-04 - mean_absolute_error: 0.0129\n",
      "\n",
      "Epoch 00824: loss did not improve\n",
      "Epoch 825/10000\n",
      "2700/2700 [==============================] - 0s 21us/step - loss: 4.5366e-04 - mean_absolute_error: 0.0130\n",
      "\n",
      "Epoch 00825: loss did not improve\n",
      "Epoch 826/10000\n",
      "2700/2700 [==============================] - 0s 21us/step - loss: 4.4836e-04 - mean_absolute_error: 0.0128\n",
      "\n",
      "Epoch 00826: loss did not improve\n",
      "Epoch 827/10000\n",
      "2700/2700 [==============================] - 0s 21us/step - loss: 4.4978e-04 - mean_absolute_error: 0.0128\n",
      "\n",
      "Epoch 00827: loss did not improve\n",
      "Epoch 828/10000\n",
      "2700/2700 [==============================] - 0s 22us/step - loss: 4.5274e-04 - mean_absolute_error: 0.0131\n",
      "\n",
      "Epoch 00828: loss did not improve\n",
      "Epoch 829/10000\n",
      "2700/2700 [==============================] - 0s 21us/step - loss: 4.5171e-04 - mean_absolute_error: 0.0130\n",
      "\n",
      "Epoch 00829: loss did not improve\n",
      "Epoch 830/10000\n",
      "2700/2700 [==============================] - 0s 21us/step - loss: 4.4796e-04 - mean_absolute_error: 0.0128\n",
      "\n",
      "Epoch 00830: loss did not improve\n",
      "Epoch 831/10000\n",
      "2700/2700 [==============================] - 0s 21us/step - loss: 4.5074e-04 - mean_absolute_error: 0.0129\n",
      "\n",
      "Epoch 00831: loss did not improve\n",
      "Epoch 832/10000\n",
      "2700/2700 [==============================] - 0s 21us/step - loss: 4.5262e-04 - mean_absolute_error: 0.0131\n",
      "\n",
      "Epoch 00832: loss did not improve\n",
      "Epoch 833/10000\n",
      "2700/2700 [==============================] - 0s 21us/step - loss: 4.4760e-04 - mean_absolute_error: 0.0128\n",
      "\n",
      "Epoch 00833: loss improved from 0.00045 to 0.00045, saving model to keras/best_baseline_weights.hdf5\n",
      "Epoch 834/10000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2700/2700 [==============================] - 0s 21us/step - loss: 4.5008e-04 - mean_absolute_error: 0.0129\n",
      "\n",
      "Epoch 00834: loss did not improve\n",
      "Epoch 835/10000\n",
      "2700/2700 [==============================] - 0s 21us/step - loss: 4.4378e-04 - mean_absolute_error: 0.0126\n",
      "\n",
      "Epoch 00835: loss improved from 0.00045 to 0.00044, saving model to keras/best_baseline_weights.hdf5\n",
      "Epoch 836/10000\n",
      "2700/2700 [==============================] - 0s 21us/step - loss: 4.4847e-04 - mean_absolute_error: 0.0130\n",
      "\n",
      "Epoch 00836: loss did not improve\n",
      "Epoch 837/10000\n",
      "2700/2700 [==============================] - 0s 21us/step - loss: 4.5137e-04 - mean_absolute_error: 0.0131\n",
      "\n",
      "Epoch 00837: loss did not improve\n",
      "Epoch 838/10000\n",
      "2700/2700 [==============================] - 0s 21us/step - loss: 4.4983e-04 - mean_absolute_error: 0.0130\n",
      "\n",
      "Epoch 00838: loss did not improve\n",
      "Epoch 839/10000\n",
      "2700/2700 [==============================] - 0s 21us/step - loss: 4.5012e-04 - mean_absolute_error: 0.0129\n",
      "\n",
      "Epoch 00839: loss did not improve\n",
      "Epoch 840/10000\n",
      "2700/2700 [==============================] - 0s 21us/step - loss: 4.4741e-04 - mean_absolute_error: 0.0128\n",
      "\n",
      "Epoch 00840: loss did not improve\n",
      "Epoch 841/10000\n",
      "2700/2700 [==============================] - 0s 22us/step - loss: 4.4935e-04 - mean_absolute_error: 0.0129\n",
      "\n",
      "Epoch 00841: loss did not improve\n",
      "Epoch 842/10000\n",
      "2700/2700 [==============================] - 0s 21us/step - loss: 4.5193e-04 - mean_absolute_error: 0.0130\n",
      "\n",
      "Epoch 00842: loss did not improve\n",
      "Epoch 843/10000\n",
      "2700/2700 [==============================] - 0s 21us/step - loss: 4.4780e-04 - mean_absolute_error: 0.0128\n",
      "\n",
      "Epoch 00843: loss did not improve\n",
      "Epoch 844/10000\n",
      "2700/2700 [==============================] - 0s 22us/step - loss: 4.4755e-04 - mean_absolute_error: 0.0129\n",
      "\n",
      "Epoch 00844: loss did not improve\n",
      "Epoch 845/10000\n",
      "2700/2700 [==============================] - 0s 21us/step - loss: 4.5153e-04 - mean_absolute_error: 0.0130\n",
      "\n",
      "Epoch 00845: loss did not improve\n",
      "Epoch 846/10000\n",
      "2700/2700 [==============================] - 0s 22us/step - loss: 4.4905e-04 - mean_absolute_error: 0.0128\n",
      "\n",
      "Epoch 00846: loss did not improve\n",
      "Epoch 847/10000\n",
      "2700/2700 [==============================] - 0s 21us/step - loss: 4.4737e-04 - mean_absolute_error: 0.0129\n",
      "\n",
      "Epoch 00847: loss did not improve\n",
      "Epoch 848/10000\n",
      "2700/2700 [==============================] - 0s 21us/step - loss: 4.4957e-04 - mean_absolute_error: 0.0130\n",
      "\n",
      "Epoch 00848: loss did not improve\n",
      "Epoch 849/10000\n",
      "2700/2700 [==============================] - 0s 21us/step - loss: 4.4979e-04 - mean_absolute_error: 0.0129\n",
      "\n",
      "Epoch 00849: loss did not improve\n",
      "Epoch 850/10000\n",
      "2700/2700 [==============================] - 0s 21us/step - loss: 4.5473e-04 - mean_absolute_error: 0.0132\n",
      "\n",
      "Epoch 00850: loss did not improve\n",
      "Epoch 851/10000\n",
      "2700/2700 [==============================] - 0s 21us/step - loss: 4.4652e-04 - mean_absolute_error: 0.0128\n",
      "\n",
      "Epoch 00851: loss did not improve\n",
      "Epoch 852/10000\n",
      "2700/2700 [==============================] - 0s 21us/step - loss: 4.4490e-04 - mean_absolute_error: 0.0127\n",
      "\n",
      "Epoch 00852: loss did not improve\n",
      "Epoch 853/10000\n",
      "2700/2700 [==============================] - 0s 21us/step - loss: 4.4210e-04 - mean_absolute_error: 0.0128\n",
      "\n",
      "Epoch 00853: loss improved from 0.00044 to 0.00044, saving model to keras/best_baseline_weights.hdf5\n",
      "Epoch 854/10000\n",
      "2700/2700 [==============================] - 0s 21us/step - loss: 4.4753e-04 - mean_absolute_error: 0.0129\n",
      "\n",
      "Epoch 00854: loss did not improve\n",
      "Epoch 855/10000\n",
      "2700/2700 [==============================] - 0s 21us/step - loss: 4.4212e-04 - mean_absolute_error: 0.0126\n",
      "\n",
      "Epoch 00855: loss did not improve\n",
      "Epoch 856/10000\n",
      "2700/2700 [==============================] - 0s 21us/step - loss: 4.4508e-04 - mean_absolute_error: 0.0128\n",
      "\n",
      "Epoch 00856: loss did not improve\n",
      "Epoch 857/10000\n",
      "2700/2700 [==============================] - 0s 21us/step - loss: 4.4607e-04 - mean_absolute_error: 0.0128\n",
      "\n",
      "Epoch 00857: loss did not improve\n",
      "Epoch 858/10000\n",
      "2700/2700 [==============================] - 0s 22us/step - loss: 4.4682e-04 - mean_absolute_error: 0.0128\n",
      "\n",
      "Epoch 00858: loss did not improve\n",
      "Epoch 859/10000\n",
      "2700/2700 [==============================] - 0s 21us/step - loss: 4.4669e-04 - mean_absolute_error: 0.0130\n",
      "\n",
      "Epoch 00859: loss did not improve\n",
      "Epoch 860/10000\n",
      "2700/2700 [==============================] - 0s 21us/step - loss: 4.5239e-04 - mean_absolute_error: 0.0131\n",
      "\n",
      "Epoch 00860: loss did not improve\n",
      "Epoch 861/10000\n",
      "2700/2700 [==============================] - 0s 21us/step - loss: 4.4712e-04 - mean_absolute_error: 0.0129\n",
      "\n",
      "Epoch 00861: loss did not improve\n",
      "Epoch 862/10000\n",
      "2700/2700 [==============================] - 0s 22us/step - loss: 4.4485e-04 - mean_absolute_error: 0.0130\n",
      "\n",
      "Epoch 00862: loss did not improve\n",
      "Epoch 863/10000\n",
      "2700/2700 [==============================] - 0s 21us/step - loss: 4.5142e-04 - mean_absolute_error: 0.0132\n",
      "\n",
      "Epoch 00863: loss did not improve\n",
      "Epoch 864/10000\n",
      "2700/2700 [==============================] - 0s 21us/step - loss: 4.4870e-04 - mean_absolute_error: 0.0129\n",
      "\n",
      "Epoch 00864: loss did not improve\n",
      "Epoch 865/10000\n",
      "2700/2700 [==============================] - 0s 21us/step - loss: 4.4654e-04 - mean_absolute_error: 0.0129\n",
      "\n",
      "Epoch 00865: loss did not improve\n",
      "Epoch 866/10000\n",
      "2700/2700 [==============================] - 0s 23us/step - loss: 4.4469e-04 - mean_absolute_error: 0.0129\n",
      "\n",
      "Epoch 00866: loss did not improve\n",
      "Epoch 867/10000\n",
      "2700/2700 [==============================] - 0s 21us/step - loss: 4.4422e-04 - mean_absolute_error: 0.0128\n",
      "\n",
      "Epoch 00867: loss did not improve\n",
      "Epoch 868/10000\n",
      "2700/2700 [==============================] - 0s 21us/step - loss: 4.4581e-04 - mean_absolute_error: 0.0130\n",
      "\n",
      "Epoch 00868: loss did not improve\n",
      "Epoch 869/10000\n",
      "2700/2700 [==============================] - 0s 21us/step - loss: 4.5039e-04 - mean_absolute_error: 0.0130\n",
      "\n",
      "Epoch 00869: loss did not improve\n",
      "Epoch 870/10000\n",
      "2700/2700 [==============================] - 0s 21us/step - loss: 4.4520e-04 - mean_absolute_error: 0.0129\n",
      "\n",
      "Epoch 00870: loss did not improve\n",
      "Epoch 871/10000\n",
      "2700/2700 [==============================] - 0s 21us/step - loss: 4.4245e-04 - mean_absolute_error: 0.0127\n",
      "\n",
      "Epoch 00871: loss did not improve\n",
      "Epoch 872/10000\n",
      "2700/2700 [==============================] - 0s 22us/step - loss: 4.4760e-04 - mean_absolute_error: 0.0130\n",
      "\n",
      "Epoch 00872: loss did not improve\n",
      "Epoch 873/10000\n",
      "2700/2700 [==============================] - 0s 21us/step - loss: 4.4401e-04 - mean_absolute_error: 0.0127\n",
      "\n",
      "Epoch 00873: loss did not improve\n",
      "Epoch 874/10000\n",
      "2700/2700 [==============================] - 0s 21us/step - loss: 4.4853e-04 - mean_absolute_error: 0.0130\n",
      "\n",
      "Epoch 00874: loss did not improve\n",
      "Epoch 875/10000\n",
      "2700/2700 [==============================] - 0s 21us/step - loss: 4.4491e-04 - mean_absolute_error: 0.0129\n",
      "\n",
      "Epoch 00875: loss did not improve\n",
      "Epoch 876/10000\n",
      "2700/2700 [==============================] - 0s 21us/step - loss: 4.4523e-04 - mean_absolute_error: 0.0131\n",
      "\n",
      "Epoch 00876: loss did not improve\n",
      "Epoch 877/10000\n",
      "2700/2700 [==============================] - 0s 22us/step - loss: 4.5145e-04 - mean_absolute_error: 0.0131\n",
      "\n",
      "Epoch 00877: loss did not improve\n",
      "Epoch 878/10000\n",
      "2700/2700 [==============================] - 0s 21us/step - loss: 4.5206e-04 - mean_absolute_error: 0.0132\n",
      "\n",
      "Epoch 00878: loss did not improve\n",
      "Epoch 879/10000\n",
      "2700/2700 [==============================] - 0s 21us/step - loss: 4.4530e-04 - mean_absolute_error: 0.0128\n",
      "\n",
      "Epoch 00879: loss did not improve\n",
      "Epoch 880/10000\n",
      "2700/2700 [==============================] - 0s 21us/step - loss: 4.4906e-04 - mean_absolute_error: 0.0130\n",
      "\n",
      "Epoch 00880: loss did not improve\n",
      "Epoch 881/10000\n",
      "2700/2700 [==============================] - 0s 24us/step - loss: 4.4353e-04 - mean_absolute_error: 0.0128\n",
      "\n",
      "Epoch 00881: loss did not improve\n",
      "Epoch 882/10000\n",
      "2700/2700 [==============================] - 0s 22us/step - loss: 4.4560e-04 - mean_absolute_error: 0.0129\n",
      "\n",
      "Epoch 00882: loss did not improve\n",
      "Epoch 883/10000\n",
      "2700/2700 [==============================] - 0s 21us/step - loss: 4.5108e-04 - mean_absolute_error: 0.0131\n",
      "\n",
      "Epoch 00883: loss did not improve\n",
      "Epoch 884/10000\n",
      "2700/2700 [==============================] - 0s 21us/step - loss: 4.4731e-04 - mean_absolute_error: 0.0131\n",
      "\n",
      "Epoch 00884: loss did not improve\n",
      "Epoch 885/10000\n",
      "2700/2700 [==============================] - 0s 23us/step - loss: 4.4606e-04 - mean_absolute_error: 0.0129\n",
      "\n",
      "Epoch 00885: loss did not improve\n",
      "Epoch 886/10000\n",
      "2700/2700 [==============================] - 0s 21us/step - loss: 4.4824e-04 - mean_absolute_error: 0.0129\n",
      "\n",
      "Epoch 00886: loss did not improve\n",
      "Epoch 887/10000\n",
      "2700/2700 [==============================] - 0s 21us/step - loss: 4.5152e-04 - mean_absolute_error: 0.0132\n",
      "\n",
      "Epoch 00887: loss did not improve\n",
      "Epoch 888/10000\n",
      "2700/2700 [==============================] - 0s 21us/step - loss: 4.4656e-04 - mean_absolute_error: 0.0131\n",
      "\n",
      "Epoch 00888: loss did not improve\n",
      "Epoch 889/10000\n",
      "2700/2700 [==============================] - 0s 21us/step - loss: 4.4065e-04 - mean_absolute_error: 0.0128\n",
      "\n",
      "Epoch 00889: loss improved from 0.00044 to 0.00044, saving model to keras/best_baseline_weights.hdf5\n",
      "Epoch 890/10000\n",
      "2700/2700 [==============================] - 0s 21us/step - loss: 4.3998e-04 - mean_absolute_error: 0.0127\n",
      "\n",
      "Epoch 00890: loss improved from 0.00044 to 0.00044, saving model to keras/best_baseline_weights.hdf5\n",
      "Epoch 891/10000\n",
      "2700/2700 [==============================] - 0s 22us/step - loss: 4.5611e-04 - mean_absolute_error: 0.0135\n",
      "\n",
      "Epoch 00891: loss did not improve\n",
      "Epoch 892/10000\n",
      "2700/2700 [==============================] - 0s 23us/step - loss: 4.4331e-04 - mean_absolute_error: 0.0128\n",
      "\n",
      "Epoch 00892: loss did not improve\n",
      "Epoch 893/10000\n",
      "2700/2700 [==============================] - 0s 26us/step - loss: 4.4300e-04 - mean_absolute_error: 0.0129\n",
      "\n",
      "Epoch 00893: loss did not improve\n",
      "Epoch 894/10000\n",
      "2700/2700 [==============================] - 0s 28us/step - loss: 4.4478e-04 - mean_absolute_error: 0.0130\n",
      "\n",
      "Epoch 00894: loss did not improve\n",
      "Epoch 895/10000\n",
      "2700/2700 [==============================] - 0s 24us/step - loss: 4.4195e-04 - mean_absolute_error: 0.0128\n",
      "\n",
      "Epoch 00895: loss did not improve\n",
      "Epoch 896/10000\n",
      "2700/2700 [==============================] - 0s 26us/step - loss: 4.4346e-04 - mean_absolute_error: 0.0128\n",
      "\n",
      "Epoch 00896: loss did not improve\n",
      "Epoch 897/10000\n",
      "2700/2700 [==============================] - 0s 26us/step - loss: 4.4192e-04 - mean_absolute_error: 0.0127\n",
      "\n",
      "Epoch 00897: loss did not improve\n",
      "Epoch 898/10000\n",
      "2700/2700 [==============================] - 0s 31us/step - loss: 4.4257e-04 - mean_absolute_error: 0.0127\n",
      "\n",
      "Epoch 00898: loss did not improve\n",
      "Epoch 899/10000\n",
      "2700/2700 [==============================] - 0s 30us/step - loss: 4.4491e-04 - mean_absolute_error: 0.0129\n",
      "\n",
      "Epoch 00899: loss did not improve\n",
      "Epoch 900/10000\n",
      "2700/2700 [==============================] - 0s 27us/step - loss: 4.4332e-04 - mean_absolute_error: 0.0130\n",
      "\n",
      "Epoch 00900: loss did not improve\n",
      "Epoch 901/10000\n",
      "2700/2700 [==============================] - 0s 27us/step - loss: 4.4420e-04 - mean_absolute_error: 0.0129\n",
      "\n",
      "Epoch 00901: loss did not improve\n",
      "Epoch 902/10000\n",
      "2700/2700 [==============================] - 0s 27us/step - loss: 4.4377e-04 - mean_absolute_error: 0.0130\n",
      "\n",
      "Epoch 00902: loss did not improve\n",
      "Epoch 903/10000\n",
      "2700/2700 [==============================] - 0s 26us/step - loss: 4.4781e-04 - mean_absolute_error: 0.0131\n",
      "\n",
      "Epoch 00903: loss did not improve\n",
      "Epoch 904/10000\n",
      "2700/2700 [==============================] - 0s 27us/step - loss: 4.4333e-04 - mean_absolute_error: 0.0129\n",
      "\n",
      "Epoch 00904: loss did not improve\n",
      "Epoch 905/10000\n",
      "2700/2700 [==============================] - 0s 29us/step - loss: 4.4692e-04 - mean_absolute_error: 0.0131\n",
      "\n",
      "Epoch 00905: loss did not improve\n",
      "Epoch 906/10000\n",
      "2700/2700 [==============================] - 0s 26us/step - loss: 4.4138e-04 - mean_absolute_error: 0.0127\n",
      "\n",
      "Epoch 00906: loss did not improve\n",
      "Epoch 907/10000\n",
      "2700/2700 [==============================] - 0s 26us/step - loss: 4.4624e-04 - mean_absolute_error: 0.0131\n",
      "\n",
      "Epoch 00907: loss did not improve\n",
      "Epoch 908/10000\n",
      "2700/2700 [==============================] - 0s 26us/step - loss: 4.4232e-04 - mean_absolute_error: 0.0129\n",
      "\n",
      "Epoch 00908: loss did not improve\n",
      "Epoch 909/10000\n",
      "2700/2700 [==============================] - 0s 22us/step - loss: 4.4625e-04 - mean_absolute_error: 0.0130\n",
      "\n",
      "Epoch 00909: loss did not improve\n",
      "Epoch 910/10000\n",
      "2700/2700 [==============================] - 0s 22us/step - loss: 4.4332e-04 - mean_absolute_error: 0.0129\n",
      "\n",
      "Epoch 00910: loss did not improve\n",
      "Epoch 911/10000\n",
      "2700/2700 [==============================] - 0s 21us/step - loss: 4.4280e-04 - mean_absolute_error: 0.0128\n",
      "\n",
      "Epoch 00911: loss did not improve\n",
      "Epoch 912/10000\n",
      "2700/2700 [==============================] - 0s 28us/step - loss: 4.5007e-04 - mean_absolute_error: 0.0132\n",
      "\n",
      "Epoch 00912: loss did not improve\n",
      "Epoch 913/10000\n",
      "2700/2700 [==============================] - 0s 36us/step - loss: 4.4488e-04 - mean_absolute_error: 0.0129\n",
      "\n",
      "Epoch 00913: loss did not improve\n",
      "Epoch 914/10000\n",
      "2700/2700 [==============================] - 0s 28us/step - loss: 4.4276e-04 - mean_absolute_error: 0.0129\n",
      "\n",
      "Epoch 00914: loss did not improve\n",
      "Epoch 915/10000\n",
      "2700/2700 [==============================] - 0s 27us/step - loss: 4.4134e-04 - mean_absolute_error: 0.0128\n",
      "\n",
      "Epoch 00915: loss did not improve\n",
      "Epoch 916/10000\n",
      "2700/2700 [==============================] - 0s 36us/step - loss: 4.4401e-04 - mean_absolute_error: 0.0129\n",
      "\n",
      "Epoch 00916: loss did not improve\n",
      "Epoch 917/10000\n",
      "2700/2700 [==============================] - 0s 30us/step - loss: 4.4357e-04 - mean_absolute_error: 0.0130\n",
      "\n",
      "Epoch 00917: loss did not improve\n",
      "Epoch 918/10000\n",
      "2700/2700 [==============================] - 0s 27us/step - loss: 4.4356e-04 - mean_absolute_error: 0.0130\n",
      "\n",
      "Epoch 00918: loss did not improve\n",
      "Epoch 919/10000\n",
      "2700/2700 [==============================] - 0s 27us/step - loss: 4.4448e-04 - mean_absolute_error: 0.0130\n",
      "\n",
      "Epoch 00919: loss did not improve\n",
      "Epoch 920/10000\n",
      "2700/2700 [==============================] - 0s 31us/step - loss: 4.4483e-04 - mean_absolute_error: 0.0130\n",
      "\n",
      "Epoch 00920: loss did not improve\n",
      "Epoch 921/10000\n",
      "2700/2700 [==============================] - 0s 27us/step - loss: 4.4394e-04 - mean_absolute_error: 0.0131\n",
      "\n",
      "Epoch 00921: loss did not improve\n",
      "Epoch 922/10000\n",
      "2700/2700 [==============================] - 0s 27us/step - loss: 4.4609e-04 - mean_absolute_error: 0.0131\n",
      "\n",
      "Epoch 00922: loss did not improve\n",
      "Epoch 923/10000\n",
      "2700/2700 [==============================] - 0s 27us/step - loss: 4.4623e-04 - mean_absolute_error: 0.0131\n",
      "\n",
      "Epoch 00923: loss did not improve\n",
      "Epoch 924/10000\n",
      "2700/2700 [==============================] - 0s 28us/step - loss: 4.4199e-04 - mean_absolute_error: 0.0129\n",
      "\n",
      "Epoch 00924: loss did not improve\n",
      "Epoch 925/10000\n",
      "2700/2700 [==============================] - 0s 28us/step - loss: 4.4201e-04 - mean_absolute_error: 0.0129\n",
      "\n",
      "Epoch 00925: loss did not improve\n",
      "Epoch 926/10000\n",
      "2700/2700 [==============================] - 0s 27us/step - loss: 4.4718e-04 - mean_absolute_error: 0.0132\n",
      "\n",
      "Epoch 00926: loss did not improve\n",
      "Epoch 927/10000\n",
      "2700/2700 [==============================] - 0s 29us/step - loss: 4.5835e-04 - mean_absolute_error: 0.0137\n",
      "\n",
      "Epoch 00927: loss did not improve\n",
      "Epoch 928/10000\n",
      "2700/2700 [==============================] - 0s 26us/step - loss: 4.4512e-04 - mean_absolute_error: 0.0130\n",
      "\n",
      "Epoch 00928: loss did not improve\n",
      "Epoch 929/10000\n",
      "2700/2700 [==============================] - 0s 28us/step - loss: 4.4846e-04 - mean_absolute_error: 0.0132\n",
      "\n",
      "Epoch 00929: loss did not improve\n",
      "Epoch 930/10000\n",
      "2700/2700 [==============================] - 0s 25us/step - loss: 4.3996e-04 - mean_absolute_error: 0.0127\n",
      "\n",
      "Epoch 00930: loss improved from 0.00044 to 0.00044, saving model to keras/best_baseline_weights.hdf5\n",
      "Epoch 931/10000\n",
      "2700/2700 [==============================] - 0s 27us/step - loss: 4.4541e-04 - mean_absolute_error: 0.0130\n",
      "\n",
      "Epoch 00931: loss did not improve\n",
      "Epoch 932/10000\n",
      "2700/2700 [==============================] - 0s 34us/step - loss: 4.4624e-04 - mean_absolute_error: 0.0131\n",
      "\n",
      "Epoch 00932: loss did not improve\n",
      "Epoch 933/10000\n",
      "2700/2700 [==============================] - 0s 34us/step - loss: 4.4227e-04 - mean_absolute_error: 0.0130\n",
      "\n",
      "Epoch 00933: loss did not improve\n",
      "Epoch 934/10000\n",
      "2700/2700 [==============================] - 0s 37us/step - loss: 4.4315e-04 - mean_absolute_error: 0.0130\n",
      "\n",
      "Epoch 00934: loss did not improve\n",
      "Epoch 935/10000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2700/2700 [==============================] - 0s 37us/step - loss: 4.4157e-04 - mean_absolute_error: 0.0130\n",
      "\n",
      "Epoch 00935: loss did not improve\n",
      "Epoch 936/10000\n",
      "2700/2700 [==============================] - 0s 37us/step - loss: 4.4147e-04 - mean_absolute_error: 0.0129\n",
      "\n",
      "Epoch 00936: loss did not improve\n",
      "Epoch 937/10000\n",
      "2700/2700 [==============================] - 0s 36us/step - loss: 4.3900e-04 - mean_absolute_error: 0.0129\n",
      "\n",
      "Epoch 00937: loss improved from 0.00044 to 0.00044, saving model to keras/best_baseline_weights.hdf5\n",
      "Epoch 938/10000\n",
      "2700/2700 [==============================] - 0s 36us/step - loss: 4.4264e-04 - mean_absolute_error: 0.0130\n",
      "\n",
      "Epoch 00938: loss did not improve\n",
      "Epoch 939/10000\n",
      "2700/2700 [==============================] - 0s 35us/step - loss: 4.4435e-04 - mean_absolute_error: 0.0132\n",
      "\n",
      "Epoch 00939: loss did not improve\n",
      "Epoch 940/10000\n",
      "2700/2700 [==============================] - 0s 32us/step - loss: 4.4975e-04 - mean_absolute_error: 0.0133\n",
      "\n",
      "Epoch 00940: loss did not improve\n",
      "Epoch 941/10000\n",
      "2700/2700 [==============================] - 0s 33us/step - loss: 4.4127e-04 - mean_absolute_error: 0.0129\n",
      "\n",
      "Epoch 00941: loss did not improve\n",
      "Epoch 942/10000\n",
      "2700/2700 [==============================] - 0s 32us/step - loss: 4.4021e-04 - mean_absolute_error: 0.0129\n",
      "\n",
      "Epoch 00942: loss did not improve\n",
      "Epoch 943/10000\n",
      "2700/2700 [==============================] - 0s 36us/step - loss: 4.4062e-04 - mean_absolute_error: 0.0129\n",
      "\n",
      "Epoch 00943: loss did not improve\n",
      "Epoch 944/10000\n",
      "2700/2700 [==============================] - 0s 34us/step - loss: 4.6794e-04 - mean_absolute_error: 0.0140\n",
      "\n",
      "Epoch 00944: loss did not improve\n",
      "Epoch 945/10000\n",
      "2700/2700 [==============================] - 0s 37us/step - loss: 4.3835e-04 - mean_absolute_error: 0.0128\n",
      "\n",
      "Epoch 00945: loss improved from 0.00044 to 0.00044, saving model to keras/best_baseline_weights.hdf5\n",
      "Epoch 946/10000\n",
      "2700/2700 [==============================] - 0s 35us/step - loss: 4.4070e-04 - mean_absolute_error: 0.0130\n",
      "\n",
      "Epoch 00946: loss did not improve\n",
      "Epoch 947/10000\n",
      "2700/2700 [==============================] - 0s 37us/step - loss: 4.4808e-04 - mean_absolute_error: 0.0131\n",
      "\n",
      "Epoch 00947: loss did not improve\n",
      "Epoch 948/10000\n",
      "2700/2700 [==============================] - 0s 34us/step - loss: 4.4326e-04 - mean_absolute_error: 0.0129\n",
      "\n",
      "Epoch 00948: loss did not improve\n",
      "Epoch 949/10000\n",
      "2700/2700 [==============================] - 0s 34us/step - loss: 4.4513e-04 - mean_absolute_error: 0.0131\n",
      "\n",
      "Epoch 00949: loss did not improve\n",
      "Epoch 950/10000\n",
      "2700/2700 [==============================] - 0s 33us/step - loss: 4.4343e-04 - mean_absolute_error: 0.0131\n",
      "\n",
      "Epoch 00950: loss did not improve\n",
      "Epoch 951/10000\n",
      "2700/2700 [==============================] - 0s 35us/step - loss: 4.4658e-04 - mean_absolute_error: 0.0131\n",
      "\n",
      "Epoch 00951: loss did not improve\n",
      "Epoch 952/10000\n",
      "2700/2700 [==============================] - 0s 35us/step - loss: 4.3995e-04 - mean_absolute_error: 0.0129\n",
      "\n",
      "Epoch 00952: loss did not improve\n",
      "Epoch 953/10000\n",
      "2700/2700 [==============================] - 0s 31us/step - loss: 4.4084e-04 - mean_absolute_error: 0.0129\n",
      "\n",
      "Epoch 00953: loss did not improve\n",
      "Epoch 954/10000\n",
      "2700/2700 [==============================] - 0s 35us/step - loss: 4.4080e-04 - mean_absolute_error: 0.0130\n",
      "\n",
      "Epoch 00954: loss did not improve\n",
      "Epoch 955/10000\n",
      "2700/2700 [==============================] - 0s 35us/step - loss: 4.4056e-04 - mean_absolute_error: 0.0129\n",
      "\n",
      "Epoch 00955: loss did not improve\n",
      "Epoch 956/10000\n",
      "2700/2700 [==============================] - 0s 31us/step - loss: 4.4063e-04 - mean_absolute_error: 0.0129\n",
      "\n",
      "Epoch 00956: loss did not improve\n",
      "Epoch 957/10000\n",
      "2700/2700 [==============================] - 0s 26us/step - loss: 4.4475e-04 - mean_absolute_error: 0.0131\n",
      "\n",
      "Epoch 00957: loss did not improve\n",
      "Epoch 958/10000\n",
      "2700/2700 [==============================] - 0s 28us/step - loss: 4.4839e-04 - mean_absolute_error: 0.0132\n",
      "\n",
      "Epoch 00958: loss did not improve\n",
      "Epoch 959/10000\n",
      "2700/2700 [==============================] - 0s 27us/step - loss: 4.4196e-04 - mean_absolute_error: 0.0130\n",
      "\n",
      "Epoch 00959: loss did not improve\n",
      "Epoch 960/10000\n",
      "2700/2700 [==============================] - 0s 30us/step - loss: 4.3834e-04 - mean_absolute_error: 0.0129\n",
      "\n",
      "Epoch 00960: loss improved from 0.00044 to 0.00044, saving model to keras/best_baseline_weights.hdf5\n",
      "Epoch 961/10000\n",
      "2700/2700 [==============================] - 0s 30us/step - loss: 4.4477e-04 - mean_absolute_error: 0.0130\n",
      "\n",
      "Epoch 00961: loss did not improve\n",
      "Epoch 962/10000\n",
      "2700/2700 [==============================] - 0s 27us/step - loss: 4.4549e-04 - mean_absolute_error: 0.0131\n",
      "\n",
      "Epoch 00962: loss did not improve\n",
      "Epoch 963/10000\n",
      "2700/2700 [==============================] - 0s 29us/step - loss: 4.5259e-04 - mean_absolute_error: 0.0134\n",
      "\n",
      "Epoch 00963: loss did not improve\n",
      "Epoch 964/10000\n",
      "2700/2700 [==============================] - 0s 26us/step - loss: 4.3694e-04 - mean_absolute_error: 0.0129\n",
      "\n",
      "Epoch 00964: loss improved from 0.00044 to 0.00044, saving model to keras/best_baseline_weights.hdf5\n",
      "Epoch 965/10000\n",
      "2700/2700 [==============================] - 0s 29us/step - loss: 4.4005e-04 - mean_absolute_error: 0.0129\n",
      "\n",
      "Epoch 00965: loss did not improve\n",
      "Epoch 966/10000\n",
      "2700/2700 [==============================] - 0s 31us/step - loss: 4.4547e-04 - mean_absolute_error: 0.0131\n",
      "\n",
      "Epoch 00966: loss did not improve\n",
      "Epoch 967/10000\n",
      "2700/2700 [==============================] - 0s 27us/step - loss: 4.4664e-04 - mean_absolute_error: 0.0132\n",
      "\n",
      "Epoch 00967: loss did not improve\n",
      "Epoch 968/10000\n",
      "2700/2700 [==============================] - 0s 28us/step - loss: 4.3955e-04 - mean_absolute_error: 0.0130\n",
      "\n",
      "Epoch 00968: loss did not improve\n",
      "Epoch 969/10000\n",
      "2700/2700 [==============================] - 0s 28us/step - loss: 4.4015e-04 - mean_absolute_error: 0.0129\n",
      "\n",
      "Epoch 00969: loss did not improve\n",
      "Epoch 970/10000\n",
      "2700/2700 [==============================] - 0s 28us/step - loss: 4.3945e-04 - mean_absolute_error: 0.0129\n",
      "\n",
      "Epoch 00970: loss did not improve\n",
      "Epoch 971/10000\n",
      "2700/2700 [==============================] - 0s 30us/step - loss: 4.3815e-04 - mean_absolute_error: 0.0129\n",
      "\n",
      "Epoch 00971: loss did not improve\n",
      "Epoch 972/10000\n",
      "2700/2700 [==============================] - 0s 27us/step - loss: 4.4143e-04 - mean_absolute_error: 0.0131\n",
      "\n",
      "Epoch 00972: loss did not improve\n",
      "Epoch 973/10000\n",
      "2700/2700 [==============================] - 0s 28us/step - loss: 4.3820e-04 - mean_absolute_error: 0.0129\n",
      "\n",
      "Epoch 00973: loss did not improve\n",
      "Epoch 974/10000\n",
      "2700/2700 [==============================] - 0s 28us/step - loss: 4.4134e-04 - mean_absolute_error: 0.0130\n",
      "\n",
      "Epoch 00974: loss did not improve\n",
      "Epoch 975/10000\n",
      "2700/2700 [==============================] - 0s 28us/step - loss: 4.4078e-04 - mean_absolute_error: 0.0129\n",
      "\n",
      "Epoch 00975: loss did not improve\n",
      "Epoch 976/10000\n",
      "2700/2700 [==============================] - 0s 30us/step - loss: 4.3834e-04 - mean_absolute_error: 0.0129\n",
      "\n",
      "Epoch 00976: loss did not improve\n",
      "Epoch 977/10000\n",
      "2700/2700 [==============================] - 0s 28us/step - loss: 4.3767e-04 - mean_absolute_error: 0.0129\n",
      "\n",
      "Epoch 00977: loss did not improve\n",
      "Epoch 978/10000\n",
      "2700/2700 [==============================] - 0s 26us/step - loss: 4.4251e-04 - mean_absolute_error: 0.0132\n",
      "\n",
      "Epoch 00978: loss did not improve\n",
      "Epoch 979/10000\n",
      "2700/2700 [==============================] - 0s 28us/step - loss: 4.3943e-04 - mean_absolute_error: 0.0129\n",
      "\n",
      "Epoch 00979: loss did not improve\n",
      "Epoch 980/10000\n",
      "2700/2700 [==============================] - 0s 28us/step - loss: 4.4405e-04 - mean_absolute_error: 0.0131\n",
      "\n",
      "Epoch 00980: loss did not improve\n",
      "Epoch 981/10000\n",
      "2700/2700 [==============================] - 0s 27us/step - loss: 4.4540e-04 - mean_absolute_error: 0.0132\n",
      "\n",
      "Epoch 00981: loss did not improve\n",
      "Epoch 982/10000\n",
      "2700/2700 [==============================] - 0s 26us/step - loss: 4.3965e-04 - mean_absolute_error: 0.0130\n",
      "\n",
      "Epoch 00982: loss did not improve\n",
      "Epoch 983/10000\n",
      "2700/2700 [==============================] - 0s 27us/step - loss: 4.4373e-04 - mean_absolute_error: 0.0132\n",
      "\n",
      "Epoch 00983: loss did not improve\n",
      "Epoch 984/10000\n",
      "2700/2700 [==============================] - 0s 27us/step - loss: 4.4052e-04 - mean_absolute_error: 0.0131\n",
      "\n",
      "Epoch 00984: loss did not improve\n",
      "Epoch 985/10000\n",
      "2700/2700 [==============================] - 0s 28us/step - loss: 4.4303e-04 - mean_absolute_error: 0.0132\n",
      "\n",
      "Epoch 00985: loss did not improve\n",
      "Epoch 986/10000\n",
      "2700/2700 [==============================] - 0s 29us/step - loss: 4.3672e-04 - mean_absolute_error: 0.0129\n",
      "\n",
      "Epoch 00986: loss improved from 0.00044 to 0.00044, saving model to keras/best_baseline_weights.hdf5\n",
      "Epoch 987/10000\n",
      "2700/2700 [==============================] - 0s 42us/step - loss: 4.4830e-04 - mean_absolute_error: 0.0133\n",
      "\n",
      "Epoch 00987: loss did not improve\n",
      "Epoch 988/10000\n",
      "2700/2700 [==============================] - 0s 33us/step - loss: 4.4166e-04 - mean_absolute_error: 0.0130\n",
      "\n",
      "Epoch 00988: loss did not improve\n",
      "Epoch 989/10000\n",
      "2700/2700 [==============================] - 0s 28us/step - loss: 4.3870e-04 - mean_absolute_error: 0.0129\n",
      "\n",
      "Epoch 00989: loss did not improve\n",
      "Epoch 990/10000\n",
      "2700/2700 [==============================] - 0s 27us/step - loss: 4.3713e-04 - mean_absolute_error: 0.0128\n",
      "\n",
      "Epoch 00990: loss did not improve\n",
      "Epoch 991/10000\n",
      "2700/2700 [==============================] - 0s 27us/step - loss: 4.4777e-04 - mean_absolute_error: 0.0133\n",
      "\n",
      "Epoch 00991: loss did not improve\n",
      "Epoch 992/10000\n",
      "2700/2700 [==============================] - 0s 33us/step - loss: 4.4458e-04 - mean_absolute_error: 0.0133: 0s - loss: 4.2209e-04 - mean_absolute_error: 0.012\n",
      "\n",
      "Epoch 00992: loss did not improve\n",
      "Epoch 993/10000\n",
      "2700/2700 [==============================] - 0s 32us/step - loss: 4.4262e-04 - mean_absolute_error: 0.0131\n",
      "\n",
      "Epoch 00993: loss did not improve\n",
      "Epoch 994/10000\n",
      "2700/2700 [==============================] - 0s 30us/step - loss: 4.3948e-04 - mean_absolute_error: 0.0130\n",
      "\n",
      "Epoch 00994: loss did not improve\n",
      "Epoch 995/10000\n",
      "2700/2700 [==============================] - 0s 27us/step - loss: 4.4084e-04 - mean_absolute_error: 0.0130\n",
      "\n",
      "Epoch 00995: loss did not improve\n",
      "Epoch 996/10000\n",
      "2700/2700 [==============================] - 0s 32us/step - loss: 4.3999e-04 - mean_absolute_error: 0.0130\n",
      "\n",
      "Epoch 00996: loss did not improve\n",
      "Epoch 997/10000\n",
      "2700/2700 [==============================] - 0s 27us/step - loss: 4.3996e-04 - mean_absolute_error: 0.0129\n",
      "\n",
      "Epoch 00997: loss did not improve\n",
      "Epoch 998/10000\n",
      "2700/2700 [==============================] - 0s 29us/step - loss: 4.4108e-04 - mean_absolute_error: 0.0130\n",
      "\n",
      "Epoch 00998: loss did not improve\n",
      "Epoch 999/10000\n",
      "2700/2700 [==============================] - 0s 28us/step - loss: 4.3505e-04 - mean_absolute_error: 0.0128\n",
      "\n",
      "Epoch 00999: loss improved from 0.00044 to 0.00044, saving model to keras/best_baseline_weights.hdf5\n",
      "Epoch 1000/10000\n",
      "2700/2700 [==============================] - 0s 28us/step - loss: 4.3788e-04 - mean_absolute_error: 0.0130\n",
      "\n",
      "Epoch 01000: loss did not improve\n",
      "Epoch 1001/10000\n",
      "2700/2700 [==============================] - 0s 35us/step - loss: 4.4283e-04 - mean_absolute_error: 0.0130\n",
      "\n",
      "Epoch 01001: loss did not improve\n",
      "Epoch 1002/10000\n",
      "2700/2700 [==============================] - 0s 30us/step - loss: 4.4715e-04 - mean_absolute_error: 0.0132\n",
      "\n",
      "Epoch 01002: loss did not improve\n",
      "Epoch 1003/10000\n",
      "2700/2700 [==============================] - 0s 38us/step - loss: 4.3678e-04 - mean_absolute_error: 0.0129\n",
      "\n",
      "Epoch 01003: loss did not improve\n",
      "Epoch 1004/10000\n",
      "2700/2700 [==============================] - 0s 29us/step - loss: 4.3900e-04 - mean_absolute_error: 0.0128\n",
      "\n",
      "Epoch 01004: loss did not improve\n",
      "Epoch 1005/10000\n",
      "2700/2700 [==============================] - 0s 37us/step - loss: 4.4203e-04 - mean_absolute_error: 0.0132\n",
      "\n",
      "Epoch 01005: loss did not improve\n",
      "Epoch 1006/10000\n",
      "2700/2700 [==============================] - 0s 34us/step - loss: 4.3465e-04 - mean_absolute_error: 0.0129\n",
      "\n",
      "Epoch 01006: loss improved from 0.00044 to 0.00043, saving model to keras/best_baseline_weights.hdf5\n",
      "Epoch 1007/10000\n",
      "2700/2700 [==============================] - 0s 31us/step - loss: 4.4352e-04 - mean_absolute_error: 0.0130\n",
      "\n",
      "Epoch 01007: loss did not improve\n",
      "Epoch 1008/10000\n",
      "2700/2700 [==============================] - 0s 31us/step - loss: 4.3825e-04 - mean_absolute_error: 0.0131\n",
      "\n",
      "Epoch 01008: loss did not improve\n",
      "Epoch 1009/10000\n",
      "2700/2700 [==============================] - 0s 30us/step - loss: 4.4042e-04 - mean_absolute_error: 0.0130\n",
      "\n",
      "Epoch 01009: loss did not improve\n",
      "Epoch 1010/10000\n",
      "2700/2700 [==============================] - 0s 31us/step - loss: 4.4955e-04 - mean_absolute_error: 0.0135\n",
      "\n",
      "Epoch 01010: loss did not improve\n",
      "Epoch 1011/10000\n",
      "2700/2700 [==============================] - 0s 30us/step - loss: 4.3786e-04 - mean_absolute_error: 0.0130\n",
      "\n",
      "Epoch 01011: loss did not improve\n",
      "Epoch 1012/10000\n",
      "2700/2700 [==============================] - 0s 31us/step - loss: 4.4118e-04 - mean_absolute_error: 0.0131\n",
      "\n",
      "Epoch 01012: loss did not improve\n",
      "Epoch 1013/10000\n",
      "2700/2700 [==============================] - 0s 30us/step - loss: 4.4145e-04 - mean_absolute_error: 0.0130\n",
      "\n",
      "Epoch 01013: loss did not improve\n",
      "Epoch 1014/10000\n",
      "2700/2700 [==============================] - 0s 29us/step - loss: 4.4041e-04 - mean_absolute_error: 0.0129\n",
      "\n",
      "Epoch 01014: loss did not improve\n",
      "Epoch 1015/10000\n",
      "2700/2700 [==============================] - 0s 28us/step - loss: 4.3781e-04 - mean_absolute_error: 0.0130\n",
      "\n",
      "Epoch 01015: loss did not improve\n",
      "Epoch 1016/10000\n",
      "2700/2700 [==============================] - 0s 31us/step - loss: 4.4075e-04 - mean_absolute_error: 0.0130\n",
      "\n",
      "Epoch 01016: loss did not improve\n",
      "Epoch 1017/10000\n",
      "2700/2700 [==============================] - 0s 35us/step - loss: 4.4102e-04 - mean_absolute_error: 0.0131\n",
      "\n",
      "Epoch 01017: loss did not improve\n",
      "Epoch 1018/10000\n",
      "2700/2700 [==============================] - 0s 29us/step - loss: 4.3716e-04 - mean_absolute_error: 0.0130\n",
      "\n",
      "Epoch 01018: loss did not improve\n",
      "Epoch 1019/10000\n",
      "2700/2700 [==============================] - 0s 27us/step - loss: 4.4057e-04 - mean_absolute_error: 0.0130\n",
      "\n",
      "Epoch 01019: loss did not improve\n",
      "Epoch 1020/10000\n",
      "2700/2700 [==============================] - 0s 28us/step - loss: 4.3993e-04 - mean_absolute_error: 0.0130\n",
      "\n",
      "Epoch 01020: loss did not improve\n",
      "Epoch 1021/10000\n",
      "2700/2700 [==============================] - 0s 32us/step - loss: 4.3901e-04 - mean_absolute_error: 0.0131\n",
      "\n",
      "Epoch 01021: loss did not improve\n",
      "Epoch 1022/10000\n",
      "2700/2700 [==============================] - 0s 29us/step - loss: 4.3612e-04 - mean_absolute_error: 0.0129\n",
      "\n",
      "Epoch 01022: loss did not improve\n",
      "Epoch 1023/10000\n",
      "2700/2700 [==============================] - 0s 28us/step - loss: 4.4413e-04 - mean_absolute_error: 0.0132\n",
      "\n",
      "Epoch 01023: loss did not improve\n",
      "Epoch 1024/10000\n",
      "2700/2700 [==============================] - 0s 31us/step - loss: 4.3500e-04 - mean_absolute_error: 0.0129\n",
      "\n",
      "Epoch 01024: loss did not improve\n",
      "Epoch 1025/10000\n",
      "2700/2700 [==============================] - 0s 29us/step - loss: 4.4439e-04 - mean_absolute_error: 0.0134\n",
      "\n",
      "Epoch 01025: loss did not improve\n",
      "Epoch 1026/10000\n",
      "2700/2700 [==============================] - 0s 27us/step - loss: 4.4431e-04 - mean_absolute_error: 0.0130\n",
      "\n",
      "Epoch 01026: loss did not improve\n",
      "Epoch 1027/10000\n",
      "2700/2700 [==============================] - 0s 30us/step - loss: 4.4227e-04 - mean_absolute_error: 0.0132\n",
      "\n",
      "Epoch 01027: loss did not improve\n",
      "Epoch 1028/10000\n",
      "2700/2700 [==============================] - 0s 30us/step - loss: 4.3449e-04 - mean_absolute_error: 0.0128\n",
      "\n",
      "Epoch 01028: loss improved from 0.00043 to 0.00043, saving model to keras/best_baseline_weights.hdf5\n",
      "Epoch 1029/10000\n",
      "2700/2700 [==============================] - 0s 35us/step - loss: 4.4429e-04 - mean_absolute_error: 0.0132\n",
      "\n",
      "Epoch 01029: loss did not improve\n",
      "Epoch 1030/10000\n",
      "2700/2700 [==============================] - 0s 40us/step - loss: 4.3870e-04 - mean_absolute_error: 0.0129\n",
      "\n",
      "Epoch 01030: loss did not improve\n",
      "Epoch 1031/10000\n",
      "2700/2700 [==============================] - 0s 30us/step - loss: 4.3710e-04 - mean_absolute_error: 0.0130\n",
      "\n",
      "Epoch 01031: loss did not improve\n",
      "Epoch 1032/10000\n",
      "2700/2700 [==============================] - 0s 32us/step - loss: 4.4171e-04 - mean_absolute_error: 0.0131\n",
      "\n",
      "Epoch 01032: loss did not improve\n",
      "Epoch 1033/10000\n",
      "2700/2700 [==============================] - 0s 31us/step - loss: 4.4760e-04 - mean_absolute_error: 0.0133\n",
      "\n",
      "Epoch 01033: loss did not improve\n",
      "Epoch 1034/10000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2700/2700 [==============================] - 0s 28us/step - loss: 4.3677e-04 - mean_absolute_error: 0.0129\n",
      "\n",
      "Epoch 01034: loss did not improve\n",
      "Epoch 1035/10000\n",
      "2700/2700 [==============================] - 0s 28us/step - loss: 4.4327e-04 - mean_absolute_error: 0.0131\n",
      "\n",
      "Epoch 01035: loss did not improve\n",
      "Epoch 1036/10000\n",
      "2700/2700 [==============================] - 0s 27us/step - loss: 4.4268e-04 - mean_absolute_error: 0.0131\n",
      "\n",
      "Epoch 01036: loss did not improve\n",
      "Epoch 1037/10000\n",
      "2700/2700 [==============================] - 0s 27us/step - loss: 4.3688e-04 - mean_absolute_error: 0.0130\n",
      "\n",
      "Epoch 01037: loss did not improve\n",
      "Epoch 1038/10000\n",
      "2700/2700 [==============================] - 0s 27us/step - loss: 4.4062e-04 - mean_absolute_error: 0.0131\n",
      "\n",
      "Epoch 01038: loss did not improve\n",
      "Epoch 1039/10000\n",
      "2700/2700 [==============================] - 0s 34us/step - loss: 4.3819e-04 - mean_absolute_error: 0.0131\n",
      "\n",
      "Epoch 01039: loss did not improve\n",
      "Epoch 1040/10000\n",
      "2700/2700 [==============================] - 0s 32us/step - loss: 4.3686e-04 - mean_absolute_error: 0.0130\n",
      "\n",
      "Epoch 01040: loss did not improve\n",
      "Epoch 1041/10000\n",
      "2700/2700 [==============================] - 0s 33us/step - loss: 4.3937e-04 - mean_absolute_error: 0.0130\n",
      "\n",
      "Epoch 01041: loss did not improve\n",
      "Epoch 1042/10000\n",
      "2700/2700 [==============================] - 0s 28us/step - loss: 4.4761e-04 - mean_absolute_error: 0.0133\n",
      "\n",
      "Epoch 01042: loss did not improve\n",
      "Epoch 1043/10000\n",
      "2700/2700 [==============================] - 0s 30us/step - loss: 4.4028e-04 - mean_absolute_error: 0.0133\n",
      "\n",
      "Epoch 01043: loss did not improve\n",
      "Epoch 1044/10000\n",
      "2700/2700 [==============================] - 0s 29us/step - loss: 4.3791e-04 - mean_absolute_error: 0.0130\n",
      "\n",
      "Epoch 01044: loss did not improve\n",
      "Epoch 1045/10000\n",
      "2700/2700 [==============================] - 0s 29us/step - loss: 4.3956e-04 - mean_absolute_error: 0.0131\n",
      "\n",
      "Epoch 01045: loss did not improve\n",
      "Epoch 1046/10000\n",
      "2700/2700 [==============================] - 0s 29us/step - loss: 4.3647e-04 - mean_absolute_error: 0.0130\n",
      "\n",
      "Epoch 01046: loss did not improve\n",
      "Epoch 1047/10000\n",
      "2700/2700 [==============================] - 0s 34us/step - loss: 4.3663e-04 - mean_absolute_error: 0.0130\n",
      "\n",
      "Epoch 01047: loss did not improve\n",
      "Epoch 1048/10000\n",
      "2700/2700 [==============================] - 0s 32us/step - loss: 4.3899e-04 - mean_absolute_error: 0.0130\n",
      "\n",
      "Epoch 01048: loss did not improve\n",
      "Epoch 1049/10000\n",
      "2700/2700 [==============================] - 0s 31us/step - loss: 4.4070e-04 - mean_absolute_error: 0.0130\n",
      "\n",
      "Epoch 01049: loss did not improve\n",
      "Epoch 1050/10000\n",
      "2700/2700 [==============================] - 0s 31us/step - loss: 4.3775e-04 - mean_absolute_error: 0.0129\n",
      "\n",
      "Epoch 01050: loss did not improve\n",
      "Epoch 1051/10000\n",
      "2700/2700 [==============================] - 0s 29us/step - loss: 4.3666e-04 - mean_absolute_error: 0.0130\n",
      "\n",
      "Epoch 01051: loss did not improve\n",
      "Epoch 1052/10000\n",
      "2700/2700 [==============================] - 0s 27us/step - loss: 4.4072e-04 - mean_absolute_error: 0.0132\n",
      "\n",
      "Epoch 01052: loss did not improve\n",
      "Epoch 1053/10000\n",
      "2700/2700 [==============================] - 0s 30us/step - loss: 4.3665e-04 - mean_absolute_error: 0.0129\n",
      "\n",
      "Epoch 01053: loss did not improve\n",
      "Epoch 1054/10000\n",
      "2700/2700 [==============================] - 0s 34us/step - loss: 4.3371e-04 - mean_absolute_error: 0.0128\n",
      "\n",
      "Epoch 01054: loss improved from 0.00043 to 0.00043, saving model to keras/best_baseline_weights.hdf5\n",
      "Epoch 1055/10000\n",
      "2700/2700 [==============================] - 0s 33us/step - loss: 4.3453e-04 - mean_absolute_error: 0.0130\n",
      "\n",
      "Epoch 01055: loss did not improve\n",
      "Epoch 1056/10000\n",
      "2700/2700 [==============================] - 0s 33us/step - loss: 4.4577e-04 - mean_absolute_error: 0.0134\n",
      "\n",
      "Epoch 01056: loss did not improve\n",
      "Epoch 1057/10000\n",
      "2700/2700 [==============================] - 0s 33us/step - loss: 4.3954e-04 - mean_absolute_error: 0.0130\n",
      "\n",
      "Epoch 01057: loss did not improve\n",
      "Epoch 1058/10000\n",
      "2700/2700 [==============================] - 0s 30us/step - loss: 4.4923e-04 - mean_absolute_error: 0.0137\n",
      "\n",
      "Epoch 01058: loss did not improve\n",
      "Epoch 1059/10000\n",
      "2700/2700 [==============================] - 0s 32us/step - loss: 4.3540e-04 - mean_absolute_error: 0.0130\n",
      "\n",
      "Epoch 01059: loss did not improve\n",
      "Epoch 1060/10000\n",
      "2700/2700 [==============================] - 0s 30us/step - loss: 4.3880e-04 - mean_absolute_error: 0.0131\n",
      "\n",
      "Epoch 01060: loss did not improve\n",
      "Epoch 1061/10000\n",
      "2700/2700 [==============================] - 0s 29us/step - loss: 4.4398e-04 - mean_absolute_error: 0.0132\n",
      "\n",
      "Epoch 01061: loss did not improve\n",
      "Epoch 1062/10000\n",
      "2700/2700 [==============================] - 0s 29us/step - loss: 4.3586e-04 - mean_absolute_error: 0.0130\n",
      "\n",
      "Epoch 01062: loss did not improve\n",
      "Epoch 1063/10000\n",
      "2700/2700 [==============================] - 0s 28us/step - loss: 4.4302e-04 - mean_absolute_error: 0.0133\n",
      "\n",
      "Epoch 01063: loss did not improve\n",
      "Epoch 1064/10000\n",
      "2700/2700 [==============================] - 0s 30us/step - loss: 4.3988e-04 - mean_absolute_error: 0.0132\n",
      "\n",
      "Epoch 01064: loss did not improve\n",
      "Epoch 1065/10000\n",
      "2700/2700 [==============================] - 0s 32us/step - loss: 4.4009e-04 - mean_absolute_error: 0.0131\n",
      "\n",
      "Epoch 01065: loss did not improve\n",
      "Epoch 1066/10000\n",
      "2700/2700 [==============================] - 0s 35us/step - loss: 4.3535e-04 - mean_absolute_error: 0.0130\n",
      "\n",
      "Epoch 01066: loss did not improve\n",
      "Epoch 1067/10000\n",
      "2700/2700 [==============================] - 0s 34us/step - loss: 4.3793e-04 - mean_absolute_error: 0.0131\n",
      "\n",
      "Epoch 01067: loss did not improve\n",
      "Epoch 1068/10000\n",
      "2700/2700 [==============================] - 0s 30us/step - loss: 4.3621e-04 - mean_absolute_error: 0.0130\n",
      "\n",
      "Epoch 01068: loss did not improve\n",
      "Epoch 1069/10000\n",
      "2700/2700 [==============================] - 0s 30us/step - loss: 4.3889e-04 - mean_absolute_error: 0.0131\n",
      "\n",
      "Epoch 01069: loss did not improve\n",
      "Epoch 1070/10000\n",
      "2700/2700 [==============================] - 0s 30us/step - loss: 4.3815e-04 - mean_absolute_error: 0.0131\n",
      "\n",
      "Epoch 01070: loss did not improve\n",
      "Epoch 1071/10000\n",
      "2700/2700 [==============================] - 0s 30us/step - loss: 4.3394e-04 - mean_absolute_error: 0.0129\n",
      "\n",
      "Epoch 01071: loss did not improve\n",
      "Epoch 1072/10000\n",
      "2700/2700 [==============================] - 0s 35us/step - loss: 4.3734e-04 - mean_absolute_error: 0.0132\n",
      "\n",
      "Epoch 01072: loss did not improve\n",
      "Epoch 1073/10000\n",
      "2700/2700 [==============================] - 0s 29us/step - loss: 4.3587e-04 - mean_absolute_error: 0.0130\n",
      "\n",
      "Epoch 01073: loss did not improve\n",
      "Epoch 1074/10000\n",
      "2700/2700 [==============================] - 0s 28us/step - loss: 4.4159e-04 - mean_absolute_error: 0.0133\n",
      "\n",
      "Epoch 01074: loss did not improve\n",
      "Epoch 1075/10000\n",
      "2700/2700 [==============================] - 0s 30us/step - loss: 4.4805e-04 - mean_absolute_error: 0.0134\n",
      "\n",
      "Epoch 01075: loss did not improve\n",
      "Epoch 1076/10000\n",
      "2700/2700 [==============================] - 0s 29us/step - loss: 4.4110e-04 - mean_absolute_error: 0.0132\n",
      "\n",
      "Epoch 01076: loss did not improve\n",
      "Epoch 1077/10000\n",
      "2700/2700 [==============================] - 0s 31us/step - loss: 4.4237e-04 - mean_absolute_error: 0.0133\n",
      "\n",
      "Epoch 01077: loss did not improve\n",
      "Epoch 1078/10000\n",
      "2700/2700 [==============================] - 0s 34us/step - loss: 4.3619e-04 - mean_absolute_error: 0.0130\n",
      "\n",
      "Epoch 01078: loss did not improve\n",
      "Epoch 1079/10000\n",
      "2700/2700 [==============================] - 0s 29us/step - loss: 4.3559e-04 - mean_absolute_error: 0.0130\n",
      "\n",
      "Epoch 01079: loss did not improve\n",
      "Epoch 1080/10000\n",
      "2700/2700 [==============================] - 0s 28us/step - loss: 4.3636e-04 - mean_absolute_error: 0.0130\n",
      "\n",
      "Epoch 01080: loss did not improve\n",
      "Epoch 1081/10000\n",
      "2700/2700 [==============================] - 0s 35us/step - loss: 4.3794e-04 - mean_absolute_error: 0.0130\n",
      "\n",
      "Epoch 01081: loss did not improve\n",
      "Epoch 1082/10000\n",
      "2700/2700 [==============================] - 0s 32us/step - loss: 4.3901e-04 - mean_absolute_error: 0.0132\n",
      "\n",
      "Epoch 01082: loss did not improve\n",
      "Epoch 1083/10000\n",
      "2700/2700 [==============================] - 0s 31us/step - loss: 4.4305e-04 - mean_absolute_error: 0.0133\n",
      "\n",
      "Epoch 01083: loss did not improve\n",
      "Epoch 1084/10000\n",
      "2700/2700 [==============================] - 0s 31us/step - loss: 4.4044e-04 - mean_absolute_error: 0.0130\n",
      "\n",
      "Epoch 01084: loss did not improve\n",
      "Epoch 1085/10000\n",
      "2700/2700 [==============================] - 0s 27us/step - loss: 4.3868e-04 - mean_absolute_error: 0.0132\n",
      "\n",
      "Epoch 01085: loss did not improve\n",
      "Epoch 1086/10000\n",
      "2700/2700 [==============================] - 0s 26us/step - loss: 4.3239e-04 - mean_absolute_error: 0.0130\n",
      "\n",
      "Epoch 01086: loss improved from 0.00043 to 0.00043, saving model to keras/best_baseline_weights.hdf5\n",
      "Epoch 1087/10000\n",
      "2700/2700 [==============================] - 0s 28us/step - loss: 4.3769e-04 - mean_absolute_error: 0.0131\n",
      "\n",
      "Epoch 01087: loss did not improve\n",
      "Epoch 1088/10000\n",
      "2700/2700 [==============================] - 0s 26us/step - loss: 4.3321e-04 - mean_absolute_error: 0.0129\n",
      "\n",
      "Epoch 01088: loss did not improve\n",
      "Epoch 1089/10000\n",
      "2700/2700 [==============================] - 0s 28us/step - loss: 4.3791e-04 - mean_absolute_error: 0.0131\n",
      "\n",
      "Epoch 01089: loss did not improve\n",
      "Epoch 1090/10000\n",
      "2700/2700 [==============================] - 0s 27us/step - loss: 4.3890e-04 - mean_absolute_error: 0.0132\n",
      "\n",
      "Epoch 01090: loss did not improve\n",
      "Epoch 1091/10000\n",
      "2700/2700 [==============================] - 0s 28us/step - loss: 4.3370e-04 - mean_absolute_error: 0.0130\n",
      "\n",
      "Epoch 01091: loss did not improve\n",
      "Epoch 1092/10000\n",
      "2700/2700 [==============================] - 0s 27us/step - loss: 4.3793e-04 - mean_absolute_error: 0.0130\n",
      "\n",
      "Epoch 01092: loss did not improve\n",
      "Epoch 1093/10000\n",
      "2700/2700 [==============================] - 0s 26us/step - loss: 4.4007e-04 - mean_absolute_error: 0.0132\n",
      "\n",
      "Epoch 01093: loss did not improve\n",
      "Epoch 1094/10000\n",
      "2700/2700 [==============================] - 0s 29us/step - loss: 4.3508e-04 - mean_absolute_error: 0.0130\n",
      "\n",
      "Epoch 01094: loss did not improve\n",
      "Epoch 1095/10000\n",
      "2700/2700 [==============================] - 0s 28us/step - loss: 4.3480e-04 - mean_absolute_error: 0.0130\n",
      "\n",
      "Epoch 01095: loss did not improve\n",
      "Epoch 1096/10000\n",
      "2700/2700 [==============================] - 0s 26us/step - loss: 4.4024e-04 - mean_absolute_error: 0.0132\n",
      "\n",
      "Epoch 01096: loss did not improve\n",
      "Epoch 1097/10000\n",
      "2700/2700 [==============================] - 0s 28us/step - loss: 4.3827e-04 - mean_absolute_error: 0.0131\n",
      "\n",
      "Epoch 01097: loss did not improve\n",
      "Epoch 1098/10000\n",
      "2700/2700 [==============================] - 0s 27us/step - loss: 4.4114e-04 - mean_absolute_error: 0.0131: 0s - loss: 4.5688e-04 - mean_absolute_error: 0.013\n",
      "\n",
      "Epoch 01098: loss did not improve\n",
      "Epoch 1099/10000\n",
      "2700/2700 [==============================] - 0s 27us/step - loss: 4.3318e-04 - mean_absolute_error: 0.0129\n",
      "\n",
      "Epoch 01099: loss did not improve\n",
      "Epoch 1100/10000\n",
      "2700/2700 [==============================] - 0s 28us/step - loss: 4.4232e-04 - mean_absolute_error: 0.0133\n",
      "\n",
      "Epoch 01100: loss did not improve\n",
      "Epoch 1101/10000\n",
      "2700/2700 [==============================] - 0s 30us/step - loss: 4.3800e-04 - mean_absolute_error: 0.0132\n",
      "\n",
      "Epoch 01101: loss did not improve\n",
      "Epoch 1102/10000\n",
      "2700/2700 [==============================] - 0s 28us/step - loss: 4.3409e-04 - mean_absolute_error: 0.0129\n",
      "\n",
      "Epoch 01102: loss did not improve\n",
      "Epoch 1103/10000\n",
      "2700/2700 [==============================] - 0s 29us/step - loss: 4.3525e-04 - mean_absolute_error: 0.0130\n",
      "\n",
      "Epoch 01103: loss did not improve\n",
      "Epoch 1104/10000\n",
      "2700/2700 [==============================] - 0s 28us/step - loss: 4.3875e-04 - mean_absolute_error: 0.0132\n",
      "\n",
      "Epoch 01104: loss did not improve\n",
      "Epoch 1105/10000\n",
      "2700/2700 [==============================] - 0s 27us/step - loss: 4.3808e-04 - mean_absolute_error: 0.0129\n",
      "\n",
      "Epoch 01105: loss did not improve\n",
      "Epoch 1106/10000\n",
      "2700/2700 [==============================] - 0s 29us/step - loss: 4.3311e-04 - mean_absolute_error: 0.0130\n",
      "\n",
      "Epoch 01106: loss did not improve\n",
      "Epoch 1107/10000\n",
      "2700/2700 [==============================] - 0s 30us/step - loss: 4.3354e-04 - mean_absolute_error: 0.0129\n",
      "\n",
      "Epoch 01107: loss did not improve\n",
      "Epoch 1108/10000\n",
      "2700/2700 [==============================] - 0s 27us/step - loss: 4.5376e-04 - mean_absolute_error: 0.0138\n",
      "\n",
      "Epoch 01108: loss did not improve\n",
      "Epoch 1109/10000\n",
      "2700/2700 [==============================] - 0s 28us/step - loss: 4.3595e-04 - mean_absolute_error: 0.0130\n",
      "\n",
      "Epoch 01109: loss did not improve\n",
      "Epoch 1110/10000\n",
      "2700/2700 [==============================] - 0s 28us/step - loss: 4.3407e-04 - mean_absolute_error: 0.0129\n",
      "\n",
      "Epoch 01110: loss did not improve\n",
      "Epoch 1111/10000\n",
      "2700/2700 [==============================] - 0s 29us/step - loss: 4.3396e-04 - mean_absolute_error: 0.0129\n",
      "\n",
      "Epoch 01111: loss did not improve\n",
      "Epoch 1112/10000\n",
      "2700/2700 [==============================] - 0s 28us/step - loss: 4.3470e-04 - mean_absolute_error: 0.0130\n",
      "\n",
      "Epoch 01112: loss did not improve\n",
      "Epoch 1113/10000\n",
      "2700/2700 [==============================] - 0s 26us/step - loss: 4.3454e-04 - mean_absolute_error: 0.0130\n",
      "\n",
      "Epoch 01113: loss did not improve\n",
      "Epoch 1114/10000\n",
      "2700/2700 [==============================] - 0s 27us/step - loss: 4.3587e-04 - mean_absolute_error: 0.0130\n",
      "\n",
      "Epoch 01114: loss did not improve\n",
      "Epoch 1115/10000\n",
      "2700/2700 [==============================] - 0s 25us/step - loss: 4.3507e-04 - mean_absolute_error: 0.0130\n",
      "\n",
      "Epoch 01115: loss did not improve\n",
      "Epoch 1116/10000\n",
      "2700/2700 [==============================] - 0s 27us/step - loss: 4.4017e-04 - mean_absolute_error: 0.0133\n",
      "\n",
      "Epoch 01116: loss did not improve\n",
      "Epoch 1117/10000\n",
      "2700/2700 [==============================] - 0s 27us/step - loss: 4.3453e-04 - mean_absolute_error: 0.0130\n",
      "\n",
      "Epoch 01117: loss did not improve\n",
      "Epoch 1118/10000\n",
      "2700/2700 [==============================] - 0s 29us/step - loss: 4.3511e-04 - mean_absolute_error: 0.0129: 0s - loss: 4.1764e-04 - mean_absolute_error: 0.012\n",
      "\n",
      "Epoch 01118: loss did not improve\n",
      "Epoch 1119/10000\n",
      "2700/2700 [==============================] - 0s 28us/step - loss: 4.3362e-04 - mean_absolute_error: 0.0130\n",
      "\n",
      "Epoch 01119: loss did not improve\n",
      "Epoch 1120/10000\n",
      "2700/2700 [==============================] - 0s 28us/step - loss: 4.3599e-04 - mean_absolute_error: 0.0131\n",
      "\n",
      "Epoch 01120: loss did not improve\n",
      "Epoch 1121/10000\n",
      "2700/2700 [==============================] - 0s 29us/step - loss: 4.3837e-04 - mean_absolute_error: 0.0131\n",
      "\n",
      "Epoch 01121: loss did not improve\n",
      "Epoch 1122/10000\n",
      "2700/2700 [==============================] - 0s 27us/step - loss: 4.3636e-04 - mean_absolute_error: 0.0131\n",
      "\n",
      "Epoch 01122: loss did not improve\n",
      "Epoch 1123/10000\n",
      "2700/2700 [==============================] - 0s 26us/step - loss: 4.3339e-04 - mean_absolute_error: 0.0130\n",
      "\n",
      "Epoch 01123: loss did not improve\n",
      "Epoch 1124/10000\n",
      "2700/2700 [==============================] - 0s 27us/step - loss: 4.3655e-04 - mean_absolute_error: 0.0130\n",
      "\n",
      "Epoch 01124: loss did not improve\n",
      "Epoch 1125/10000\n",
      "2700/2700 [==============================] - 0s 26us/step - loss: 4.3389e-04 - mean_absolute_error: 0.0129\n",
      "\n",
      "Epoch 01125: loss did not improve\n",
      "Epoch 1126/10000\n",
      "2700/2700 [==============================] - 0s 26us/step - loss: 4.3336e-04 - mean_absolute_error: 0.0129\n",
      "\n",
      "Epoch 01126: loss did not improve\n",
      "Epoch 1127/10000\n",
      "2700/2700 [==============================] - 0s 27us/step - loss: 4.3293e-04 - mean_absolute_error: 0.0129\n",
      "\n",
      "Epoch 01127: loss did not improve\n",
      "Epoch 1128/10000\n",
      "2700/2700 [==============================] - 0s 27us/step - loss: 4.3298e-04 - mean_absolute_error: 0.0130\n",
      "\n",
      "Epoch 01128: loss did not improve\n",
      "Epoch 1129/10000\n",
      "2700/2700 [==============================] - 0s 27us/step - loss: 4.4387e-04 - mean_absolute_error: 0.0133\n",
      "\n",
      "Epoch 01129: loss did not improve\n",
      "Epoch 1130/10000\n",
      "2700/2700 [==============================] - 0s 27us/step - loss: 4.3450e-04 - mean_absolute_error: 0.0133\n",
      "\n",
      "Epoch 01130: loss did not improve\n",
      "Epoch 1131/10000\n",
      "2700/2700 [==============================] - 0s 30us/step - loss: 4.3843e-04 - mean_absolute_error: 0.0131\n",
      "\n",
      "Epoch 01131: loss did not improve\n",
      "Epoch 1132/10000\n",
      "2700/2700 [==============================] - 0s 28us/step - loss: 4.3637e-04 - mean_absolute_error: 0.0132\n",
      "\n",
      "Epoch 01132: loss did not improve\n",
      "Epoch 1133/10000\n",
      "2700/2700 [==============================] - 0s 27us/step - loss: 4.3861e-04 - mean_absolute_error: 0.0131\n",
      "\n",
      "Epoch 01133: loss did not improve\n",
      "Epoch 1134/10000\n",
      "2700/2700 [==============================] - 0s 30us/step - loss: 4.3660e-04 - mean_absolute_error: 0.0131\n",
      "\n",
      "Epoch 01134: loss did not improve\n",
      "Epoch 1135/10000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2700/2700 [==============================] - 0s 29us/step - loss: 4.3568e-04 - mean_absolute_error: 0.0130\n",
      "\n",
      "Epoch 01135: loss did not improve\n",
      "Epoch 1136/10000\n",
      "2700/2700 [==============================] - 0s 27us/step - loss: 4.3297e-04 - mean_absolute_error: 0.0130\n",
      "\n",
      "Epoch 01136: loss did not improve\n",
      "Epoch 1137/10000\n",
      "2700/2700 [==============================] - 0s 28us/step - loss: 4.3922e-04 - mean_absolute_error: 0.0132\n",
      "\n",
      "Epoch 01137: loss did not improve\n",
      "Epoch 1138/10000\n",
      "2700/2700 [==============================] - 0s 27us/step - loss: 4.3576e-04 - mean_absolute_error: 0.0131\n",
      "\n",
      "Epoch 01138: loss did not improve\n",
      "Epoch 1139/10000\n",
      "2700/2700 [==============================] - 0s 30us/step - loss: 4.3282e-04 - mean_absolute_error: 0.0129\n",
      "\n",
      "Epoch 01139: loss did not improve\n",
      "Epoch 1140/10000\n",
      "2700/2700 [==============================] - 0s 29us/step - loss: 4.3687e-04 - mean_absolute_error: 0.0131\n",
      "\n",
      "Epoch 01140: loss did not improve\n",
      "Epoch 1141/10000\n",
      "2700/2700 [==============================] - 0s 44us/step - loss: 4.3284e-04 - mean_absolute_error: 0.0129\n",
      "\n",
      "Epoch 01141: loss did not improve\n",
      "Epoch 1142/10000\n",
      "2700/2700 [==============================] - 0s 35us/step - loss: 4.4745e-04 - mean_absolute_error: 0.0135\n",
      "\n",
      "Epoch 01142: loss did not improve\n",
      "Epoch 1143/10000\n",
      "2700/2700 [==============================] - 0s 32us/step - loss: 4.3679e-04 - mean_absolute_error: 0.0132\n",
      "\n",
      "Epoch 01143: loss did not improve\n",
      "Epoch 1144/10000\n",
      "2700/2700 [==============================] - 0s 34us/step - loss: 4.3187e-04 - mean_absolute_error: 0.0129\n",
      "\n",
      "Epoch 01144: loss improved from 0.00043 to 0.00043, saving model to keras/best_baseline_weights.hdf5\n",
      "Epoch 1145/10000\n",
      "2700/2700 [==============================] - 0s 30us/step - loss: 4.3343e-04 - mean_absolute_error: 0.0130\n",
      "\n",
      "Epoch 01145: loss did not improve\n",
      "Epoch 1146/10000\n",
      "2700/2700 [==============================] - 0s 29us/step - loss: 4.3930e-04 - mean_absolute_error: 0.0132\n",
      "\n",
      "Epoch 01146: loss did not improve\n",
      "Epoch 1147/10000\n",
      "2700/2700 [==============================] - 0s 30us/step - loss: 4.3444e-04 - mean_absolute_error: 0.0130\n",
      "\n",
      "Epoch 01147: loss did not improve\n",
      "Epoch 1148/10000\n",
      "2700/2700 [==============================] - 0s 33us/step - loss: 4.3886e-04 - mean_absolute_error: 0.0132\n",
      "\n",
      "Epoch 01148: loss did not improve\n",
      "Epoch 1149/10000\n",
      "2700/2700 [==============================] - 0s 27us/step - loss: 4.4451e-04 - mean_absolute_error: 0.0135\n",
      "\n",
      "Epoch 01149: loss did not improve\n",
      "Epoch 1150/10000\n",
      "2700/2700 [==============================] - 0s 34us/step - loss: 4.4016e-04 - mean_absolute_error: 0.0133\n",
      "\n",
      "Epoch 01150: loss did not improve\n",
      "Epoch 1151/10000\n",
      "2700/2700 [==============================] - 0s 32us/step - loss: 4.3802e-04 - mean_absolute_error: 0.0131\n",
      "\n",
      "Epoch 01151: loss did not improve\n",
      "Epoch 1152/10000\n",
      "2700/2700 [==============================] - 0s 36us/step - loss: 4.3380e-04 - mean_absolute_error: 0.0131\n",
      "\n",
      "Epoch 01152: loss did not improve\n",
      "Epoch 1153/10000\n",
      "2700/2700 [==============================] - 0s 29us/step - loss: 4.3360e-04 - mean_absolute_error: 0.0130\n",
      "\n",
      "Epoch 01153: loss did not improve\n",
      "Epoch 1154/10000\n",
      "2700/2700 [==============================] - 0s 34us/step - loss: 4.3692e-04 - mean_absolute_error: 0.0132\n",
      "\n",
      "Epoch 01154: loss did not improve\n",
      "Epoch 1155/10000\n",
      "2700/2700 [==============================] - 0s 36us/step - loss: 4.3831e-04 - mean_absolute_error: 0.0131\n",
      "\n",
      "Epoch 01155: loss did not improve\n",
      "Epoch 1156/10000\n",
      "2700/2700 [==============================] - 0s 32us/step - loss: 4.3773e-04 - mean_absolute_error: 0.0131\n",
      "\n",
      "Epoch 01156: loss did not improve\n",
      "Epoch 1157/10000\n",
      "2700/2700 [==============================] - 0s 34us/step - loss: 4.3813e-04 - mean_absolute_error: 0.0132\n",
      "\n",
      "Epoch 01157: loss did not improve\n",
      "Epoch 1158/10000\n",
      "2700/2700 [==============================] - 0s 32us/step - loss: 4.3726e-04 - mean_absolute_error: 0.0131\n",
      "\n",
      "Epoch 01158: loss did not improve\n",
      "Epoch 1159/10000\n",
      "2700/2700 [==============================] - 0s 36us/step - loss: 4.3613e-04 - mean_absolute_error: 0.0131\n",
      "\n",
      "Epoch 01159: loss did not improve\n",
      "Epoch 1160/10000\n",
      "2700/2700 [==============================] - 0s 30us/step - loss: 4.3586e-04 - mean_absolute_error: 0.0130\n",
      "\n",
      "Epoch 01160: loss did not improve\n",
      "Epoch 1161/10000\n",
      "2700/2700 [==============================] - 0s 26us/step - loss: 4.3500e-04 - mean_absolute_error: 0.0131\n",
      "\n",
      "Epoch 01161: loss did not improve\n",
      "Epoch 1162/10000\n",
      "2700/2700 [==============================] - 0s 31us/step - loss: 4.3286e-04 - mean_absolute_error: 0.0130\n",
      "\n",
      "Epoch 01162: loss did not improve\n",
      "Epoch 1163/10000\n",
      "2700/2700 [==============================] - 0s 32us/step - loss: 4.3470e-04 - mean_absolute_error: 0.0131\n",
      "\n",
      "Epoch 01163: loss did not improve\n",
      "Epoch 1164/10000\n",
      "2700/2700 [==============================] - 0s 31us/step - loss: 4.3645e-04 - mean_absolute_error: 0.0133\n",
      "\n",
      "Epoch 01164: loss did not improve\n",
      "Epoch 1165/10000\n",
      "2700/2700 [==============================] - 0s 28us/step - loss: 4.3041e-04 - mean_absolute_error: 0.0129\n",
      "\n",
      "Epoch 01165: loss improved from 0.00043 to 0.00043, saving model to keras/best_baseline_weights.hdf5\n",
      "Epoch 1166/10000\n",
      "2700/2700 [==============================] - 0s 39us/step - loss: 4.3823e-04 - mean_absolute_error: 0.0131\n",
      "\n",
      "Epoch 01166: loss did not improve\n",
      "Epoch 1167/10000\n",
      "2700/2700 [==============================] - 0s 34us/step - loss: 4.3802e-04 - mean_absolute_error: 0.0131\n",
      "\n",
      "Epoch 01167: loss did not improve\n",
      "Epoch 1168/10000\n",
      "2700/2700 [==============================] - 0s 40us/step - loss: 4.3389e-04 - mean_absolute_error: 0.0130\n",
      "\n",
      "Epoch 01168: loss did not improve\n",
      "Epoch 1169/10000\n",
      "2700/2700 [==============================] - 0s 43us/step - loss: 4.3557e-04 - mean_absolute_error: 0.0131\n",
      "\n",
      "Epoch 01169: loss did not improve\n",
      "Epoch 1170/10000\n",
      "2700/2700 [==============================] - 0s 30us/step - loss: 4.3549e-04 - mean_absolute_error: 0.0130\n",
      "\n",
      "Epoch 01170: loss did not improve\n",
      "Epoch 1171/10000\n",
      "2700/2700 [==============================] - 0s 30us/step - loss: 4.3439e-04 - mean_absolute_error: 0.0132\n",
      "\n",
      "Epoch 01171: loss did not improve\n",
      "Epoch 1172/10000\n",
      "2700/2700 [==============================] - 0s 40us/step - loss: 4.3339e-04 - mean_absolute_error: 0.0129\n",
      "\n",
      "Epoch 01172: loss did not improve\n",
      "Epoch 1173/10000\n",
      "2700/2700 [==============================] - 0s 32us/step - loss: 4.3815e-04 - mean_absolute_error: 0.0132\n",
      "\n",
      "Epoch 01173: loss did not improve\n",
      "Epoch 1174/10000\n",
      "2700/2700 [==============================] - 0s 25us/step - loss: 4.4081e-04 - mean_absolute_error: 0.0133\n",
      "\n",
      "Epoch 01174: loss did not improve\n",
      "Epoch 1175/10000\n",
      "2700/2700 [==============================] - 0s 25us/step - loss: 4.3286e-04 - mean_absolute_error: 0.0130\n",
      "\n",
      "Epoch 01175: loss did not improve\n",
      "Epoch 1176/10000\n",
      "2700/2700 [==============================] - 0s 23us/step - loss: 4.3580e-04 - mean_absolute_error: 0.0131\n",
      "\n",
      "Epoch 01176: loss did not improve\n",
      "Epoch 1177/10000\n",
      "2700/2700 [==============================] - 0s 23us/step - loss: 4.4042e-04 - mean_absolute_error: 0.0135\n",
      "\n",
      "Epoch 01177: loss did not improve\n",
      "Epoch 1178/10000\n",
      "2700/2700 [==============================] - 0s 25us/step - loss: 4.4045e-04 - mean_absolute_error: 0.0133\n",
      "\n",
      "Epoch 01178: loss did not improve\n",
      "Epoch 1179/10000\n",
      "2700/2700 [==============================] - 0s 25us/step - loss: 4.3748e-04 - mean_absolute_error: 0.0132\n",
      "\n",
      "Epoch 01179: loss did not improve\n",
      "Epoch 1180/10000\n",
      "2700/2700 [==============================] - 0s 25us/step - loss: 4.3137e-04 - mean_absolute_error: 0.0129\n",
      "\n",
      "Epoch 01180: loss did not improve\n",
      "Epoch 1181/10000\n",
      "2700/2700 [==============================] - 0s 25us/step - loss: 4.3573e-04 - mean_absolute_error: 0.0131\n",
      "\n",
      "Epoch 01181: loss did not improve\n",
      "Epoch 1182/10000\n",
      "2700/2700 [==============================] - 0s 29us/step - loss: 4.3341e-04 - mean_absolute_error: 0.0131\n",
      "\n",
      "Epoch 01182: loss did not improve\n",
      "Epoch 1183/10000\n",
      "2700/2700 [==============================] - 0s 25us/step - loss: 4.3499e-04 - mean_absolute_error: 0.0131\n",
      "\n",
      "Epoch 01183: loss did not improve\n",
      "Epoch 1184/10000\n",
      "2700/2700 [==============================] - 0s 23us/step - loss: 4.3173e-04 - mean_absolute_error: 0.0131\n",
      "\n",
      "Epoch 01184: loss did not improve\n",
      "Epoch 1185/10000\n",
      "2700/2700 [==============================] - 0s 26us/step - loss: 4.5068e-04 - mean_absolute_error: 0.0138\n",
      "\n",
      "Epoch 01185: loss did not improve\n",
      "Epoch 1186/10000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2700/2700 [==============================] - 0s 32us/step - loss: 4.3778e-04 - mean_absolute_error: 0.0131\n",
      "\n",
      "Epoch 01186: loss did not improve\n",
      "Epoch 1187/10000\n",
      "2700/2700 [==============================] - 0s 31us/step - loss: 4.3698e-04 - mean_absolute_error: 0.0132\n",
      "\n",
      "Epoch 01187: loss did not improve\n",
      "Epoch 1188/10000\n",
      "2700/2700 [==============================] - 0s 29us/step - loss: 4.3178e-04 - mean_absolute_error: 0.0130\n",
      "\n",
      "Epoch 01188: loss did not improve\n",
      "Epoch 1189/10000\n",
      "2700/2700 [==============================] - 0s 27us/step - loss: 4.3248e-04 - mean_absolute_error: 0.0130\n",
      "\n",
      "Epoch 01189: loss did not improve\n",
      "Epoch 1190/10000\n",
      "2700/2700 [==============================] - 0s 24us/step - loss: 4.3211e-04 - mean_absolute_error: 0.0129\n",
      "\n",
      "Epoch 01190: loss did not improve\n",
      "Epoch 1191/10000\n",
      "2700/2700 [==============================] - 0s 24us/step - loss: 4.3297e-04 - mean_absolute_error: 0.0131\n",
      "\n",
      "Epoch 01191: loss did not improve\n",
      "Epoch 1192/10000\n",
      "2700/2700 [==============================] - 0s 25us/step - loss: 4.3898e-04 - mean_absolute_error: 0.0133\n",
      "\n",
      "Epoch 01192: loss did not improve\n",
      "Epoch 1193/10000\n",
      "2700/2700 [==============================] - 0s 25us/step - loss: 4.3412e-04 - mean_absolute_error: 0.0130\n",
      "\n",
      "Epoch 01193: loss did not improve\n",
      "Epoch 1194/10000\n",
      "2700/2700 [==============================] - 0s 24us/step - loss: 4.3013e-04 - mean_absolute_error: 0.0130\n",
      "\n",
      "Epoch 01194: loss improved from 0.00043 to 0.00043, saving model to keras/best_baseline_weights.hdf5\n",
      "Epoch 1195/10000\n",
      "2700/2700 [==============================] - 0s 24us/step - loss: 4.5064e-04 - mean_absolute_error: 0.0137\n",
      "\n",
      "Epoch 01195: loss did not improve\n",
      "Epoch 1196/10000\n",
      "2700/2700 [==============================] - 0s 24us/step - loss: 4.3627e-04 - mean_absolute_error: 0.0131\n",
      "\n",
      "Epoch 01196: loss did not improve\n",
      "Epoch 1197/10000\n",
      "2700/2700 [==============================] - 0s 22us/step - loss: 4.3785e-04 - mean_absolute_error: 0.0132\n",
      "\n",
      "Epoch 01197: loss did not improve\n",
      "Epoch 1198/10000\n",
      "2700/2700 [==============================] - 0s 21us/step - loss: 4.3757e-04 - mean_absolute_error: 0.0132\n",
      "\n",
      "Epoch 01198: loss did not improve\n",
      "Epoch 1199/10000\n",
      "2700/2700 [==============================] - 0s 22us/step - loss: 4.3632e-04 - mean_absolute_error: 0.0131\n",
      "\n",
      "Epoch 01199: loss did not improve\n",
      "Epoch 1200/10000\n",
      "2700/2700 [==============================] - 0s 23us/step - loss: 4.3203e-04 - mean_absolute_error: 0.0130\n",
      "\n",
      "Epoch 01200: loss did not improve\n",
      "Epoch 1201/10000\n",
      "2700/2700 [==============================] - 0s 25us/step - loss: 4.3390e-04 - mean_absolute_error: 0.0131\n",
      "\n",
      "Epoch 01201: loss did not improve\n",
      "Epoch 1202/10000\n",
      "2700/2700 [==============================] - 0s 25us/step - loss: 4.2914e-04 - mean_absolute_error: 0.0129\n",
      "\n",
      "Epoch 01202: loss improved from 0.00043 to 0.00043, saving model to keras/best_baseline_weights.hdf5\n",
      "Epoch 1203/10000\n",
      "2700/2700 [==============================] - 0s 22us/step - loss: 4.3006e-04 - mean_absolute_error: 0.0130\n",
      "\n",
      "Epoch 01203: loss did not improve\n",
      "Epoch 1204/10000\n",
      "2700/2700 [==============================] - 0s 21us/step - loss: 4.3106e-04 - mean_absolute_error: 0.0130\n",
      "\n",
      "Epoch 01204: loss did not improve\n",
      "Epoch 1205/10000\n",
      "2700/2700 [==============================] - 0s 22us/step - loss: 4.3175e-04 - mean_absolute_error: 0.0131\n",
      "\n",
      "Epoch 01205: loss did not improve\n",
      "Epoch 1206/10000\n",
      "2700/2700 [==============================] - 0s 21us/step - loss: 4.3290e-04 - mean_absolute_error: 0.0131\n",
      "\n",
      "Epoch 01206: loss did not improve\n",
      "Epoch 1207/10000\n",
      "2700/2700 [==============================] - 0s 22us/step - loss: 4.3930e-04 - mean_absolute_error: 0.0133\n",
      "\n",
      "Epoch 01207: loss did not improve\n",
      "Epoch 1208/10000\n",
      "2700/2700 [==============================] - 0s 22us/step - loss: 4.3115e-04 - mean_absolute_error: 0.0130\n",
      "\n",
      "Epoch 01208: loss did not improve\n",
      "Epoch 1209/10000\n",
      "2700/2700 [==============================] - 0s 25us/step - loss: 4.3613e-04 - mean_absolute_error: 0.0132\n",
      "\n",
      "Epoch 01209: loss did not improve\n",
      "Epoch 1210/10000\n",
      "2700/2700 [==============================] - 0s 38us/step - loss: 4.3358e-04 - mean_absolute_error: 0.0130\n",
      "\n",
      "Epoch 01210: loss did not improve\n",
      "Epoch 1211/10000\n",
      "2700/2700 [==============================] - 0s 38us/step - loss: 4.3334e-04 - mean_absolute_error: 0.0131\n",
      "\n",
      "Epoch 01211: loss did not improve\n",
      "Epoch 1212/10000\n",
      "2700/2700 [==============================] - 0s 26us/step - loss: 4.3197e-04 - mean_absolute_error: 0.0129\n",
      "\n",
      "Epoch 01212: loss did not improve\n",
      "Epoch 1213/10000\n",
      "2700/2700 [==============================] - 0s 23us/step - loss: 4.3367e-04 - mean_absolute_error: 0.0130\n",
      "\n",
      "Epoch 01213: loss did not improve\n",
      "Epoch 1214/10000\n",
      "2700/2700 [==============================] - 0s 22us/step - loss: 4.3852e-04 - mean_absolute_error: 0.0132\n",
      "\n",
      "Epoch 01214: loss did not improve\n",
      "Epoch 1215/10000\n",
      "2700/2700 [==============================] - 0s 22us/step - loss: 4.3077e-04 - mean_absolute_error: 0.0130\n",
      "\n",
      "Epoch 01215: loss did not improve\n",
      "Epoch 1216/10000\n",
      "2700/2700 [==============================] - 0s 22us/step - loss: 4.3636e-04 - mean_absolute_error: 0.0133\n",
      "\n",
      "Epoch 01216: loss did not improve\n",
      "Epoch 1217/10000\n",
      "2700/2700 [==============================] - 0s 22us/step - loss: 4.3795e-04 - mean_absolute_error: 0.0131\n",
      "\n",
      "Epoch 01217: loss did not improve\n",
      "Epoch 1218/10000\n",
      "2700/2700 [==============================] - 0s 22us/step - loss: 4.3300e-04 - mean_absolute_error: 0.0132\n",
      "\n",
      "Epoch 01218: loss did not improve\n",
      "Epoch 1219/10000\n",
      "2700/2700 [==============================] - 0s 22us/step - loss: 4.3661e-04 - mean_absolute_error: 0.0131\n",
      "\n",
      "Epoch 01219: loss did not improve\n",
      "Epoch 1220/10000\n",
      "2700/2700 [==============================] - 0s 21us/step - loss: 4.3121e-04 - mean_absolute_error: 0.0130\n",
      "\n",
      "Epoch 01220: loss did not improve\n",
      "Epoch 1221/10000\n",
      "2700/2700 [==============================] - 0s 22us/step - loss: 4.3246e-04 - mean_absolute_error: 0.0130\n",
      "\n",
      "Epoch 01221: loss did not improve\n",
      "Epoch 1222/10000\n",
      "2700/2700 [==============================] - 0s 22us/step - loss: 4.3230e-04 - mean_absolute_error: 0.0131\n",
      "\n",
      "Epoch 01222: loss did not improve\n",
      "Epoch 1223/10000\n",
      "2700/2700 [==============================] - 0s 22us/step - loss: 4.3802e-04 - mean_absolute_error: 0.0131\n",
      "\n",
      "Epoch 01223: loss did not improve\n",
      "Epoch 1224/10000\n",
      "2700/2700 [==============================] - 0s 21us/step - loss: 4.3509e-04 - mean_absolute_error: 0.0132\n",
      "\n",
      "Epoch 01224: loss did not improve\n",
      "Epoch 1225/10000\n",
      "2700/2700 [==============================] - 0s 21us/step - loss: 4.3028e-04 - mean_absolute_error: 0.0131\n",
      "\n",
      "Epoch 01225: loss did not improve\n",
      "Epoch 1226/10000\n",
      "2700/2700 [==============================] - 0s 21us/step - loss: 4.3556e-04 - mean_absolute_error: 0.0131\n",
      "\n",
      "Epoch 01226: loss did not improve\n",
      "Epoch 1227/10000\n",
      "2700/2700 [==============================] - 0s 21us/step - loss: 4.3198e-04 - mean_absolute_error: 0.0130\n",
      "\n",
      "Epoch 01227: loss did not improve\n",
      "Epoch 1228/10000\n",
      "2700/2700 [==============================] - 0s 22us/step - loss: 4.3429e-04 - mean_absolute_error: 0.0131\n",
      "\n",
      "Epoch 01228: loss did not improve\n",
      "Epoch 1229/10000\n",
      "2700/2700 [==============================] - 0s 22us/step - loss: 4.3296e-04 - mean_absolute_error: 0.0130\n",
      "\n",
      "Epoch 01229: loss did not improve\n",
      "Epoch 1230/10000\n",
      "2700/2700 [==============================] - 0s 22us/step - loss: 4.3661e-04 - mean_absolute_error: 0.0132\n",
      "\n",
      "Epoch 01230: loss did not improve\n",
      "Epoch 1231/10000\n",
      "2700/2700 [==============================] - 0s 21us/step - loss: 4.3255e-04 - mean_absolute_error: 0.0131\n",
      "\n",
      "Epoch 01231: loss did not improve\n",
      "Epoch 1232/10000\n",
      "2700/2700 [==============================] - 0s 22us/step - loss: 4.3186e-04 - mean_absolute_error: 0.0131\n",
      "\n",
      "Epoch 01232: loss did not improve\n",
      "Epoch 1233/10000\n",
      "2700/2700 [==============================] - 0s 22us/step - loss: 4.3213e-04 - mean_absolute_error: 0.0131\n",
      "\n",
      "Epoch 01233: loss did not improve\n",
      "Epoch 1234/10000\n",
      "2700/2700 [==============================] - 0s 22us/step - loss: 4.3549e-04 - mean_absolute_error: 0.0131\n",
      "\n",
      "Epoch 01234: loss did not improve\n",
      "Epoch 1235/10000\n",
      "2700/2700 [==============================] - 0s 22us/step - loss: 4.3224e-04 - mean_absolute_error: 0.0130\n",
      "\n",
      "Epoch 01235: loss did not improve\n",
      "Epoch 1236/10000\n",
      "2700/2700 [==============================] - 0s 22us/step - loss: 4.3197e-04 - mean_absolute_error: 0.0131\n",
      "\n",
      "Epoch 01236: loss did not improve\n",
      "Epoch 1237/10000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2700/2700 [==============================] - 0s 22us/step - loss: 4.3415e-04 - mean_absolute_error: 0.0131\n",
      "\n",
      "Epoch 01237: loss did not improve\n",
      "Epoch 1238/10000\n",
      "2700/2700 [==============================] - 0s 21us/step - loss: 4.3255e-04 - mean_absolute_error: 0.0130\n",
      "\n",
      "Epoch 01238: loss did not improve\n",
      "Epoch 1239/10000\n",
      "2700/2700 [==============================] - 0s 21us/step - loss: 4.3642e-04 - mean_absolute_error: 0.0132\n",
      "\n",
      "Epoch 01239: loss did not improve\n",
      "Epoch 1240/10000\n",
      "2700/2700 [==============================] - 0s 22us/step - loss: 4.3250e-04 - mean_absolute_error: 0.0130\n",
      "\n",
      "Epoch 01240: loss did not improve\n",
      "Epoch 1241/10000\n",
      "2700/2700 [==============================] - 0s 21us/step - loss: 4.3290e-04 - mean_absolute_error: 0.0131\n",
      "\n",
      "Epoch 01241: loss did not improve\n",
      "Epoch 1242/10000\n",
      "2700/2700 [==============================] - 0s 23us/step - loss: 4.3271e-04 - mean_absolute_error: 0.0131\n",
      "\n",
      "Epoch 01242: loss did not improve\n",
      "Epoch 1243/10000\n",
      "2700/2700 [==============================] - 0s 21us/step - loss: 4.3690e-04 - mean_absolute_error: 0.0133\n",
      "\n",
      "Epoch 01243: loss did not improve\n",
      "Epoch 1244/10000\n",
      "2700/2700 [==============================] - 0s 21us/step - loss: 4.3650e-04 - mean_absolute_error: 0.0132\n",
      "\n",
      "Epoch 01244: loss did not improve\n",
      "Epoch 1245/10000\n",
      "2700/2700 [==============================] - 0s 22us/step - loss: 4.3361e-04 - mean_absolute_error: 0.0132\n",
      "\n",
      "Epoch 01245: loss did not improve\n",
      "Epoch 1246/10000\n",
      "2700/2700 [==============================] - 0s 21us/step - loss: 4.3145e-04 - mean_absolute_error: 0.0130\n",
      "\n",
      "Epoch 01246: loss did not improve\n",
      "Epoch 1247/10000\n",
      "2700/2700 [==============================] - 0s 21us/step - loss: 4.3728e-04 - mean_absolute_error: 0.0131\n",
      "\n",
      "Epoch 01247: loss did not improve\n",
      "Epoch 1248/10000\n",
      "2700/2700 [==============================] - 0s 21us/step - loss: 4.3537e-04 - mean_absolute_error: 0.0131\n",
      "\n",
      "Epoch 01248: loss did not improve\n",
      "Epoch 1249/10000\n",
      "2700/2700 [==============================] - 0s 21us/step - loss: 4.3561e-04 - mean_absolute_error: 0.0131\n",
      "\n",
      "Epoch 01249: loss did not improve\n",
      "Epoch 1250/10000\n",
      "2700/2700 [==============================] - 0s 22us/step - loss: 4.3300e-04 - mean_absolute_error: 0.0130\n",
      "\n",
      "Epoch 01250: loss did not improve\n",
      "Epoch 1251/10000\n",
      "2700/2700 [==============================] - 0s 21us/step - loss: 4.3358e-04 - mean_absolute_error: 0.0131\n",
      "\n",
      "Epoch 01251: loss did not improve\n",
      "Epoch 1252/10000\n",
      "2700/2700 [==============================] - 0s 22us/step - loss: 4.3144e-04 - mean_absolute_error: 0.0131\n",
      "\n",
      "Epoch 01252: loss did not improve\n",
      "Epoch 1253/10000\n",
      "2700/2700 [==============================] - 0s 21us/step - loss: 4.3278e-04 - mean_absolute_error: 0.0132\n",
      "\n",
      "Epoch 01253: loss did not improve\n",
      "Epoch 1254/10000\n",
      "2700/2700 [==============================] - 0s 22us/step - loss: 4.3295e-04 - mean_absolute_error: 0.0131\n",
      "\n",
      "Epoch 01254: loss did not improve\n",
      "Epoch 1255/10000\n",
      "2700/2700 [==============================] - 0s 23us/step - loss: 4.3601e-04 - mean_absolute_error: 0.0132\n",
      "\n",
      "Epoch 01255: loss did not improve\n",
      "Epoch 1256/10000\n",
      "2700/2700 [==============================] - 0s 23us/step - loss: 4.3407e-04 - mean_absolute_error: 0.0132\n",
      "\n",
      "Epoch 01256: loss did not improve\n",
      "Epoch 1257/10000\n",
      "2700/2700 [==============================] - 0s 22us/step - loss: 4.3246e-04 - mean_absolute_error: 0.0131\n",
      "\n",
      "Epoch 01257: loss did not improve\n",
      "Epoch 1258/10000\n",
      "2700/2700 [==============================] - 0s 24us/step - loss: 4.3497e-04 - mean_absolute_error: 0.0131\n",
      "\n",
      "Epoch 01258: loss did not improve\n",
      "Epoch 1259/10000\n",
      "2700/2700 [==============================] - 0s 31us/step - loss: 4.3496e-04 - mean_absolute_error: 0.0132\n",
      "\n",
      "Epoch 01259: loss did not improve\n",
      "Epoch 1260/10000\n",
      "2700/2700 [==============================] - 0s 34us/step - loss: 4.3327e-04 - mean_absolute_error: 0.0130\n",
      "\n",
      "Epoch 01260: loss did not improve\n",
      "Epoch 1261/10000\n",
      "2700/2700 [==============================] - 0s 31us/step - loss: 4.2929e-04 - mean_absolute_error: 0.0129\n",
      "\n",
      "Epoch 01261: loss did not improve\n",
      "Epoch 1262/10000\n",
      "2700/2700 [==============================] - 0s 31us/step - loss: 4.3057e-04 - mean_absolute_error: 0.0131\n",
      "\n",
      "Epoch 01262: loss did not improve\n",
      "Epoch 1263/10000\n",
      "2700/2700 [==============================] - 0s 23us/step - loss: 4.3211e-04 - mean_absolute_error: 0.0131\n",
      "\n",
      "Epoch 01263: loss did not improve\n",
      "Epoch 1264/10000\n",
      "2700/2700 [==============================] - 0s 22us/step - loss: 4.3007e-04 - mean_absolute_error: 0.0131\n",
      "\n",
      "Epoch 01264: loss did not improve\n",
      "Epoch 1265/10000\n",
      "2700/2700 [==============================] - 0s 21us/step - loss: 4.3112e-04 - mean_absolute_error: 0.0129\n",
      "\n",
      "Epoch 01265: loss did not improve\n",
      "Epoch 1266/10000\n",
      "2700/2700 [==============================] - 0s 22us/step - loss: 4.3900e-04 - mean_absolute_error: 0.0134\n",
      "\n",
      "Epoch 01266: loss did not improve\n",
      "Epoch 1267/10000\n",
      "2700/2700 [==============================] - 0s 22us/step - loss: 4.3068e-04 - mean_absolute_error: 0.0130\n",
      "\n",
      "Epoch 01267: loss did not improve\n",
      "Epoch 1268/10000\n",
      "2700/2700 [==============================] - 0s 22us/step - loss: 4.3135e-04 - mean_absolute_error: 0.0131\n",
      "\n",
      "Epoch 01268: loss did not improve\n",
      "Epoch 1269/10000\n",
      "2700/2700 [==============================] - 0s 22us/step - loss: 4.3145e-04 - mean_absolute_error: 0.0130\n",
      "\n",
      "Epoch 01269: loss did not improve\n",
      "Epoch 1270/10000\n",
      "2700/2700 [==============================] - 0s 21us/step - loss: 4.3374e-04 - mean_absolute_error: 0.0132\n",
      "\n",
      "Epoch 01270: loss did not improve\n",
      "Epoch 1271/10000\n",
      "2700/2700 [==============================] - 0s 21us/step - loss: 4.3133e-04 - mean_absolute_error: 0.0130\n",
      "\n",
      "Epoch 01271: loss did not improve\n",
      "Epoch 1272/10000\n",
      "2700/2700 [==============================] - 0s 22us/step - loss: 4.3187e-04 - mean_absolute_error: 0.0130\n",
      "\n",
      "Epoch 01272: loss did not improve\n",
      "Epoch 1273/10000\n",
      "2700/2700 [==============================] - 0s 22us/step - loss: 4.3029e-04 - mean_absolute_error: 0.0130\n",
      "\n",
      "Epoch 01273: loss did not improve\n",
      "Epoch 1274/10000\n",
      "2700/2700 [==============================] - 0s 21us/step - loss: 4.3562e-04 - mean_absolute_error: 0.0132\n",
      "\n",
      "Epoch 01274: loss did not improve\n",
      "Epoch 1275/10000\n",
      "2700/2700 [==============================] - 0s 21us/step - loss: 4.3373e-04 - mean_absolute_error: 0.0132\n",
      "\n",
      "Epoch 01275: loss did not improve\n",
      "Epoch 1276/10000\n",
      "2700/2700 [==============================] - 0s 22us/step - loss: 4.3199e-04 - mean_absolute_error: 0.0130\n",
      "\n",
      "Epoch 01276: loss did not improve\n",
      "Epoch 1277/10000\n",
      "2700/2700 [==============================] - 0s 21us/step - loss: 4.3337e-04 - mean_absolute_error: 0.0131\n",
      "\n",
      "Epoch 01277: loss did not improve\n",
      "Epoch 1278/10000\n",
      "2700/2700 [==============================] - 0s 22us/step - loss: 4.3025e-04 - mean_absolute_error: 0.0130\n",
      "\n",
      "Epoch 01278: loss did not improve\n",
      "Epoch 1279/10000\n",
      "2700/2700 [==============================] - 0s 21us/step - loss: 4.3544e-04 - mean_absolute_error: 0.0132\n",
      "\n",
      "Epoch 01279: loss did not improve\n",
      "Epoch 1280/10000\n",
      "2700/2700 [==============================] - 0s 21us/step - loss: 4.3978e-04 - mean_absolute_error: 0.0134\n",
      "\n",
      "Epoch 01280: loss did not improve\n",
      "Epoch 1281/10000\n",
      "2700/2700 [==============================] - 0s 21us/step - loss: 4.3045e-04 - mean_absolute_error: 0.0132\n",
      "\n",
      "Epoch 01281: loss did not improve\n",
      "Epoch 1282/10000\n",
      "2700/2700 [==============================] - 0s 21us/step - loss: 4.3275e-04 - mean_absolute_error: 0.0130\n",
      "\n",
      "Epoch 01282: loss did not improve\n",
      "Epoch 1283/10000\n",
      "2700/2700 [==============================] - 0s 21us/step - loss: 4.3576e-04 - mean_absolute_error: 0.0131\n",
      "\n",
      "Epoch 01283: loss did not improve\n",
      "Epoch 1284/10000\n",
      "2700/2700 [==============================] - 0s 21us/step - loss: 4.2827e-04 - mean_absolute_error: 0.0129\n",
      "\n",
      "Epoch 01284: loss improved from 0.00043 to 0.00043, saving model to keras/best_baseline_weights.hdf5\n",
      "Epoch 1285/10000\n",
      "2700/2700 [==============================] - 0s 24us/step - loss: 4.3247e-04 - mean_absolute_error: 0.0131\n",
      "\n",
      "Epoch 01285: loss did not improve\n",
      "Epoch 1286/10000\n",
      "2700/2700 [==============================] - 0s 31us/step - loss: 4.3030e-04 - mean_absolute_error: 0.0130\n",
      "\n",
      "Epoch 01286: loss did not improve\n",
      "Epoch 1287/10000\n",
      "2700/2700 [==============================] - 0s 23us/step - loss: 4.2930e-04 - mean_absolute_error: 0.0129\n",
      "\n",
      "Epoch 01287: loss did not improve\n",
      "Epoch 1288/10000\n",
      "2700/2700 [==============================] - 0s 22us/step - loss: 4.3693e-04 - mean_absolute_error: 0.0133\n",
      "\n",
      "Epoch 01288: loss did not improve\n",
      "Epoch 1289/10000\n",
      "2700/2700 [==============================] - 0s 22us/step - loss: 4.3508e-04 - mean_absolute_error: 0.0131\n",
      "\n",
      "Epoch 01289: loss did not improve\n",
      "Epoch 1290/10000\n",
      "2700/2700 [==============================] - 0s 22us/step - loss: 4.2817e-04 - mean_absolute_error: 0.0130\n",
      "\n",
      "Epoch 01290: loss improved from 0.00043 to 0.00043, saving model to keras/best_baseline_weights.hdf5\n",
      "Epoch 1291/10000\n",
      "2700/2700 [==============================] - 0s 23us/step - loss: 4.3709e-04 - mean_absolute_error: 0.0132\n",
      "\n",
      "Epoch 01291: loss did not improve\n",
      "Epoch 1292/10000\n",
      "2700/2700 [==============================] - 0s 22us/step - loss: 4.3205e-04 - mean_absolute_error: 0.0131\n",
      "\n",
      "Epoch 01292: loss did not improve\n",
      "Epoch 1293/10000\n",
      "2700/2700 [==============================] - 0s 22us/step - loss: 4.3535e-04 - mean_absolute_error: 0.0133\n",
      "\n",
      "Epoch 01293: loss did not improve\n",
      "Epoch 1294/10000\n",
      "2700/2700 [==============================] - 0s 23us/step - loss: 4.2994e-04 - mean_absolute_error: 0.0130\n",
      "\n",
      "Epoch 01294: loss did not improve\n",
      "Epoch 1295/10000\n",
      "2700/2700 [==============================] - 0s 23us/step - loss: 4.3212e-04 - mean_absolute_error: 0.0132\n",
      "\n",
      "Epoch 01295: loss did not improve\n",
      "Epoch 1296/10000\n",
      "2700/2700 [==============================] - 0s 23us/step - loss: 4.3621e-04 - mean_absolute_error: 0.0134\n",
      "\n",
      "Epoch 01296: loss did not improve\n",
      "Epoch 1297/10000\n",
      "2700/2700 [==============================] - 0s 21us/step - loss: 4.3124e-04 - mean_absolute_error: 0.0131\n",
      "\n",
      "Epoch 01297: loss did not improve\n",
      "Epoch 1298/10000\n",
      "2700/2700 [==============================] - 0s 21us/step - loss: 4.2802e-04 - mean_absolute_error: 0.0129\n",
      "\n",
      "Epoch 01298: loss improved from 0.00043 to 0.00043, saving model to keras/best_baseline_weights.hdf5\n",
      "Epoch 1299/10000\n",
      "2700/2700 [==============================] - 0s 22us/step - loss: 4.3033e-04 - mean_absolute_error: 0.0130\n",
      "\n",
      "Epoch 01299: loss did not improve\n",
      "Epoch 1300/10000\n",
      "2700/2700 [==============================] - 0s 21us/step - loss: 4.3027e-04 - mean_absolute_error: 0.0130\n",
      "\n",
      "Epoch 01300: loss did not improve\n",
      "Epoch 1301/10000\n",
      "2700/2700 [==============================] - 0s 22us/step - loss: 4.3063e-04 - mean_absolute_error: 0.0131\n",
      "\n",
      "Epoch 01301: loss did not improve\n",
      "Epoch 1302/10000\n",
      "2700/2700 [==============================] - 0s 22us/step - loss: 4.3018e-04 - mean_absolute_error: 0.0131\n",
      "\n",
      "Epoch 01302: loss did not improve\n",
      "Epoch 1303/10000\n",
      "2700/2700 [==============================] - 0s 24us/step - loss: 4.3867e-04 - mean_absolute_error: 0.0134\n",
      "\n",
      "Epoch 01303: loss did not improve\n",
      "Epoch 1304/10000\n",
      "2700/2700 [==============================] - 0s 22us/step - loss: 4.2987e-04 - mean_absolute_error: 0.0130\n",
      "\n",
      "Epoch 01304: loss did not improve\n",
      "Epoch 1305/10000\n",
      "2700/2700 [==============================] - 0s 22us/step - loss: 4.3107e-04 - mean_absolute_error: 0.0130\n",
      "\n",
      "Epoch 01305: loss did not improve\n",
      "Epoch 1306/10000\n",
      "2700/2700 [==============================] - 0s 22us/step - loss: 4.4447e-04 - mean_absolute_error: 0.0136\n",
      "\n",
      "Epoch 01306: loss did not improve\n",
      "Epoch 1307/10000\n",
      "2700/2700 [==============================] - 0s 22us/step - loss: 4.3018e-04 - mean_absolute_error: 0.0131\n",
      "\n",
      "Epoch 01307: loss did not improve\n",
      "Epoch 1308/10000\n",
      "2700/2700 [==============================] - 0s 22us/step - loss: 4.3314e-04 - mean_absolute_error: 0.0132\n",
      "\n",
      "Epoch 01308: loss did not improve\n",
      "Epoch 1309/10000\n",
      "2700/2700 [==============================] - 0s 21us/step - loss: 4.3648e-04 - mean_absolute_error: 0.0133\n",
      "\n",
      "Epoch 01309: loss did not improve\n",
      "Epoch 1310/10000\n",
      "2700/2700 [==============================] - 0s 21us/step - loss: 4.3419e-04 - mean_absolute_error: 0.0132\n",
      "\n",
      "Epoch 01310: loss did not improve\n",
      "Epoch 1311/10000\n",
      "2700/2700 [==============================] - 0s 22us/step - loss: 4.3388e-04 - mean_absolute_error: 0.0132\n",
      "\n",
      "Epoch 01311: loss did not improve\n",
      "Epoch 1312/10000\n",
      "2700/2700 [==============================] - 0s 24us/step - loss: 4.3128e-04 - mean_absolute_error: 0.0131\n",
      "\n",
      "Epoch 01312: loss did not improve\n",
      "Epoch 1313/10000\n",
      "2700/2700 [==============================] - 0s 23us/step - loss: 4.3298e-04 - mean_absolute_error: 0.0130\n",
      "\n",
      "Epoch 01313: loss did not improve\n",
      "Epoch 1314/10000\n",
      "2700/2700 [==============================] - 0s 23us/step - loss: 4.2702e-04 - mean_absolute_error: 0.0129\n",
      "\n",
      "Epoch 01314: loss improved from 0.00043 to 0.00043, saving model to keras/best_baseline_weights.hdf5\n",
      "Epoch 1315/10000\n",
      "2700/2700 [==============================] - 0s 22us/step - loss: 4.2884e-04 - mean_absolute_error: 0.0130\n",
      "\n",
      "Epoch 01315: loss did not improve\n",
      "Epoch 1316/10000\n",
      "2700/2700 [==============================] - 0s 21us/step - loss: 4.2851e-04 - mean_absolute_error: 0.0130\n",
      "\n",
      "Epoch 01316: loss did not improve\n",
      "Epoch 1317/10000\n",
      "2700/2700 [==============================] - 0s 21us/step - loss: 4.3463e-04 - mean_absolute_error: 0.0133\n",
      "\n",
      "Epoch 01317: loss did not improve\n",
      "Epoch 1318/10000\n",
      "2700/2700 [==============================] - 0s 22us/step - loss: 4.3161e-04 - mean_absolute_error: 0.0130\n",
      "\n",
      "Epoch 01318: loss did not improve\n",
      "Epoch 1319/10000\n",
      "2700/2700 [==============================] - 0s 22us/step - loss: 4.4038e-04 - mean_absolute_error: 0.0134\n",
      "\n",
      "Epoch 01319: loss did not improve\n",
      "Epoch 1320/10000\n",
      "2700/2700 [==============================] - 0s 26us/step - loss: 4.3085e-04 - mean_absolute_error: 0.0131\n",
      "\n",
      "Epoch 01320: loss did not improve\n",
      "Epoch 1321/10000\n",
      "2700/2700 [==============================] - 0s 23us/step - loss: 4.2703e-04 - mean_absolute_error: 0.0129\n",
      "\n",
      "Epoch 01321: loss did not improve\n",
      "Epoch 1322/10000\n",
      "2700/2700 [==============================] - 0s 21us/step - loss: 4.3461e-04 - mean_absolute_error: 0.0134\n",
      "\n",
      "Epoch 01322: loss did not improve\n",
      "Epoch 1323/10000\n",
      "2700/2700 [==============================] - 0s 22us/step - loss: 4.3485e-04 - mean_absolute_error: 0.0133\n",
      "\n",
      "Epoch 01323: loss did not improve\n",
      "Epoch 1324/10000\n",
      "2700/2700 [==============================] - 0s 23us/step - loss: 4.2845e-04 - mean_absolute_error: 0.0130\n",
      "\n",
      "Epoch 01324: loss did not improve\n",
      "Epoch 1325/10000\n",
      "2700/2700 [==============================] - 0s 25us/step - loss: 4.3296e-04 - mean_absolute_error: 0.0132\n",
      "\n",
      "Epoch 01325: loss did not improve\n",
      "Epoch 1326/10000\n",
      "2700/2700 [==============================] - 0s 24us/step - loss: 4.3782e-04 - mean_absolute_error: 0.0133\n",
      "\n",
      "Epoch 01326: loss did not improve\n",
      "Epoch 1327/10000\n",
      "2700/2700 [==============================] - 0s 24us/step - loss: 4.3206e-04 - mean_absolute_error: 0.0130\n",
      "\n",
      "Epoch 01327: loss did not improve\n",
      "Epoch 1328/10000\n",
      "2700/2700 [==============================] - 0s 25us/step - loss: 4.3330e-04 - mean_absolute_error: 0.0131\n",
      "\n",
      "Epoch 01328: loss did not improve\n",
      "Epoch 1329/10000\n",
      "2700/2700 [==============================] - 0s 22us/step - loss: 4.3172e-04 - mean_absolute_error: 0.0131\n",
      "\n",
      "Epoch 01329: loss did not improve\n",
      "Epoch 1330/10000\n",
      "2700/2700 [==============================] - 0s 21us/step - loss: 4.2933e-04 - mean_absolute_error: 0.0130\n",
      "\n",
      "Epoch 01330: loss did not improve\n",
      "Epoch 1331/10000\n",
      "2700/2700 [==============================] - 0s 22us/step - loss: 4.3452e-04 - mean_absolute_error: 0.0132\n",
      "\n",
      "Epoch 01331: loss did not improve\n",
      "Epoch 1332/10000\n",
      "2700/2700 [==============================] - 0s 21us/step - loss: 4.3067e-04 - mean_absolute_error: 0.0131\n",
      "\n",
      "Epoch 01332: loss did not improve\n",
      "Epoch 1333/10000\n",
      "2700/2700 [==============================] - 0s 23us/step - loss: 4.3521e-04 - mean_absolute_error: 0.0133\n",
      "\n",
      "Epoch 01333: loss did not improve\n",
      "Epoch 1334/10000\n",
      "2700/2700 [==============================] - 0s 25us/step - loss: 4.3757e-04 - mean_absolute_error: 0.0134\n",
      "\n",
      "Epoch 01334: loss did not improve\n",
      "Epoch 1335/10000\n",
      "2700/2700 [==============================] - 0s 25us/step - loss: 4.2932e-04 - mean_absolute_error: 0.0130\n",
      "\n",
      "Epoch 01335: loss did not improve\n",
      "Epoch 1336/10000\n",
      "2700/2700 [==============================] - 0s 24us/step - loss: 4.3030e-04 - mean_absolute_error: 0.0131\n",
      "\n",
      "Epoch 01336: loss did not improve\n",
      "Epoch 1337/10000\n",
      "2700/2700 [==============================] - 0s 28us/step - loss: 4.2913e-04 - mean_absolute_error: 0.0130\n",
      "\n",
      "Epoch 01337: loss did not improve\n",
      "Epoch 1338/10000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2700/2700 [==============================] - 0s 23us/step - loss: 4.3096e-04 - mean_absolute_error: 0.0131\n",
      "\n",
      "Epoch 01338: loss did not improve\n",
      "Epoch 1339/10000\n",
      "2700/2700 [==============================] - 0s 22us/step - loss: 4.3008e-04 - mean_absolute_error: 0.0131\n",
      "\n",
      "Epoch 01339: loss did not improve\n",
      "Epoch 1340/10000\n",
      "2700/2700 [==============================] - 0s 22us/step - loss: 4.3068e-04 - mean_absolute_error: 0.0131\n",
      "\n",
      "Epoch 01340: loss did not improve\n",
      "Epoch 1341/10000\n",
      "2700/2700 [==============================] - 0s 21us/step - loss: 4.3421e-04 - mean_absolute_error: 0.0132\n",
      "\n",
      "Epoch 01341: loss did not improve\n",
      "Epoch 1342/10000\n",
      "2700/2700 [==============================] - 0s 21us/step - loss: 4.3518e-04 - mean_absolute_error: 0.0132\n",
      "\n",
      "Epoch 01342: loss did not improve\n",
      "Epoch 1343/10000\n",
      "2700/2700 [==============================] - 0s 22us/step - loss: 4.3380e-04 - mean_absolute_error: 0.0132\n",
      "\n",
      "Epoch 01343: loss did not improve\n",
      "Epoch 1344/10000\n",
      "2700/2700 [==============================] - 0s 24us/step - loss: 4.3481e-04 - mean_absolute_error: 0.0133\n",
      "\n",
      "Epoch 01344: loss did not improve\n",
      "Epoch 1345/10000\n",
      "2700/2700 [==============================] - 0s 22us/step - loss: 4.2708e-04 - mean_absolute_error: 0.0130\n",
      "\n",
      "Epoch 01345: loss did not improve\n",
      "Epoch 1346/10000\n",
      "2700/2700 [==============================] - 0s 24us/step - loss: 4.3758e-04 - mean_absolute_error: 0.0133\n",
      "\n",
      "Epoch 01346: loss did not improve\n",
      "Epoch 1347/10000\n",
      "2700/2700 [==============================] - 0s 22us/step - loss: 4.3555e-04 - mean_absolute_error: 0.0132\n",
      "\n",
      "Epoch 01347: loss did not improve\n",
      "Epoch 1348/10000\n",
      "2700/2700 [==============================] - 0s 22us/step - loss: 4.3565e-04 - mean_absolute_error: 0.0132\n",
      "\n",
      "Epoch 01348: loss did not improve\n",
      "Epoch 1349/10000\n",
      "2700/2700 [==============================] - 0s 21us/step - loss: 4.3224e-04 - mean_absolute_error: 0.0131\n",
      "\n",
      "Epoch 01349: loss did not improve\n",
      "Epoch 1350/10000\n",
      "2700/2700 [==============================] - 0s 22us/step - loss: 4.3344e-04 - mean_absolute_error: 0.0132\n",
      "\n",
      "Epoch 01350: loss did not improve\n",
      "Epoch 1351/10000\n",
      "2700/2700 [==============================] - 0s 22us/step - loss: 4.2589e-04 - mean_absolute_error: 0.0129\n",
      "\n",
      "Epoch 01351: loss improved from 0.00043 to 0.00043, saving model to keras/best_baseline_weights.hdf5\n",
      "Epoch 1352/10000\n",
      "2700/2700 [==============================] - 0s 22us/step - loss: 4.3557e-04 - mean_absolute_error: 0.0133\n",
      "\n",
      "Epoch 01352: loss did not improve\n",
      "Epoch 1353/10000\n",
      "2700/2700 [==============================] - 0s 21us/step - loss: 4.3157e-04 - mean_absolute_error: 0.0131\n",
      "\n",
      "Epoch 01353: loss did not improve\n",
      "Epoch 1354/10000\n",
      "2700/2700 [==============================] - 0s 24us/step - loss: 4.3144e-04 - mean_absolute_error: 0.0131\n",
      "\n",
      "Epoch 01354: loss did not improve\n",
      "Epoch 1355/10000\n",
      "2700/2700 [==============================] - 0s 22us/step - loss: 4.3072e-04 - mean_absolute_error: 0.0132\n",
      "\n",
      "Epoch 01355: loss did not improve\n",
      "Epoch 1356/10000\n",
      "2700/2700 [==============================] - 0s 28us/step - loss: 4.3079e-04 - mean_absolute_error: 0.0131\n",
      "\n",
      "Epoch 01356: loss did not improve\n",
      "Epoch 1357/10000\n",
      "2700/2700 [==============================] - 0s 24us/step - loss: 4.3707e-04 - mean_absolute_error: 0.0134\n",
      "\n",
      "Epoch 01357: loss did not improve\n",
      "Epoch 1358/10000\n",
      "2700/2700 [==============================] - 0s 22us/step - loss: 4.3294e-04 - mean_absolute_error: 0.0132\n",
      "\n",
      "Epoch 01358: loss did not improve\n",
      "Epoch 1359/10000\n",
      "2700/2700 [==============================] - 0s 21us/step - loss: 4.3055e-04 - mean_absolute_error: 0.0132\n",
      "\n",
      "Epoch 01359: loss did not improve\n",
      "Epoch 1360/10000\n",
      "2700/2700 [==============================] - 0s 21us/step - loss: 4.2719e-04 - mean_absolute_error: 0.0129\n",
      "\n",
      "Epoch 01360: loss did not improve\n",
      "Epoch 1361/10000\n",
      "2700/2700 [==============================] - 0s 21us/step - loss: 4.3092e-04 - mean_absolute_error: 0.0131\n",
      "\n",
      "Epoch 01361: loss did not improve\n",
      "Epoch 1362/10000\n",
      "2700/2700 [==============================] - 0s 23us/step - loss: 4.2987e-04 - mean_absolute_error: 0.0129\n",
      "\n",
      "Epoch 01362: loss did not improve\n",
      "Epoch 1363/10000\n",
      "2700/2700 [==============================] - 0s 26us/step - loss: 4.3566e-04 - mean_absolute_error: 0.0134\n",
      "\n",
      "Epoch 01363: loss did not improve\n",
      "Epoch 1364/10000\n",
      "2700/2700 [==============================] - 0s 25us/step - loss: 4.3358e-04 - mean_absolute_error: 0.0133\n",
      "\n",
      "Epoch 01364: loss did not improve\n",
      "Epoch 1365/10000\n",
      "2700/2700 [==============================] - 0s 24us/step - loss: 4.3242e-04 - mean_absolute_error: 0.0132\n",
      "\n",
      "Epoch 01365: loss did not improve\n",
      "Epoch 1366/10000\n",
      "2700/2700 [==============================] - 0s 21us/step - loss: 4.2793e-04 - mean_absolute_error: 0.0130\n",
      "\n",
      "Epoch 01366: loss did not improve\n",
      "Epoch 1367/10000\n",
      "2700/2700 [==============================] - 0s 22us/step - loss: 4.2689e-04 - mean_absolute_error: 0.0130\n",
      "\n",
      "Epoch 01367: loss did not improve\n",
      "Epoch 1368/10000\n",
      "2700/2700 [==============================] - 0s 21us/step - loss: 4.3641e-04 - mean_absolute_error: 0.0133\n",
      "\n",
      "Epoch 01368: loss did not improve\n",
      "Epoch 1369/10000\n",
      "2700/2700 [==============================] - 0s 23us/step - loss: 4.2772e-04 - mean_absolute_error: 0.0129\n",
      "\n",
      "Epoch 01369: loss did not improve\n",
      "Epoch 1370/10000\n",
      "2700/2700 [==============================] - 0s 30us/step - loss: 4.2563e-04 - mean_absolute_error: 0.0130\n",
      "\n",
      "Epoch 01370: loss improved from 0.00043 to 0.00043, saving model to keras/best_baseline_weights.hdf5\n",
      "Epoch 1371/10000\n",
      "2700/2700 [==============================] - 0s 33us/step - loss: 4.3517e-04 - mean_absolute_error: 0.0132\n",
      "\n",
      "Epoch 01371: loss did not improve\n",
      "Epoch 1372/10000\n",
      "2700/2700 [==============================] - 0s 31us/step - loss: 4.2803e-04 - mean_absolute_error: 0.0131\n",
      "\n",
      "Epoch 01372: loss did not improve\n",
      "Epoch 1373/10000\n",
      "2700/2700 [==============================] - 0s 25us/step - loss: 4.3934e-04 - mean_absolute_error: 0.0134\n",
      "\n",
      "Epoch 01373: loss did not improve\n",
      "Epoch 1374/10000\n",
      "2700/2700 [==============================] - 0s 23us/step - loss: 4.2923e-04 - mean_absolute_error: 0.0130\n",
      "\n",
      "Epoch 01374: loss did not improve\n",
      "Epoch 1375/10000\n",
      "2700/2700 [==============================] - 0s 22us/step - loss: 4.2863e-04 - mean_absolute_error: 0.0130\n",
      "\n",
      "Epoch 01375: loss did not improve\n",
      "Epoch 1376/10000\n",
      "2700/2700 [==============================] - 0s 22us/step - loss: 4.3327e-04 - mean_absolute_error: 0.0132\n",
      "\n",
      "Epoch 01376: loss did not improve\n",
      "Epoch 1377/10000\n",
      "2700/2700 [==============================] - 0s 24us/step - loss: 4.2913e-04 - mean_absolute_error: 0.0131\n",
      "\n",
      "Epoch 01377: loss did not improve\n",
      "Epoch 1378/10000\n",
      "2700/2700 [==============================] - 0s 27us/step - loss: 4.3210e-04 - mean_absolute_error: 0.0133\n",
      "\n",
      "Epoch 01378: loss did not improve\n",
      "Epoch 1379/10000\n",
      "2700/2700 [==============================] - 0s 23us/step - loss: 4.3371e-04 - mean_absolute_error: 0.0131\n",
      "\n",
      "Epoch 01379: loss did not improve\n",
      "Epoch 1380/10000\n",
      "2700/2700 [==============================] - 0s 23us/step - loss: 4.3746e-04 - mean_absolute_error: 0.0134\n",
      "\n",
      "Epoch 01380: loss did not improve\n",
      "Epoch 1381/10000\n",
      "2700/2700 [==============================] - 0s 23us/step - loss: 4.2985e-04 - mean_absolute_error: 0.0132\n",
      "\n",
      "Epoch 01381: loss did not improve\n",
      "Epoch 1382/10000\n",
      "2700/2700 [==============================] - 0s 22us/step - loss: 4.2887e-04 - mean_absolute_error: 0.0130\n",
      "\n",
      "Epoch 01382: loss did not improve\n",
      "Epoch 1383/10000\n",
      "2700/2700 [==============================] - 0s 23us/step - loss: 4.3606e-04 - mean_absolute_error: 0.0134\n",
      "\n",
      "Epoch 01383: loss did not improve\n",
      "Epoch 1384/10000\n",
      "2700/2700 [==============================] - 0s 24us/step - loss: 4.2949e-04 - mean_absolute_error: 0.0130\n",
      "\n",
      "Epoch 01384: loss did not improve\n",
      "Epoch 1385/10000\n",
      "2700/2700 [==============================] - 0s 25us/step - loss: 4.3094e-04 - mean_absolute_error: 0.0132\n",
      "\n",
      "Epoch 01385: loss did not improve\n",
      "Epoch 1386/10000\n",
      "2700/2700 [==============================] - 0s 26us/step - loss: 4.3103e-04 - mean_absolute_error: 0.0131\n",
      "\n",
      "Epoch 01386: loss did not improve\n",
      "Epoch 1387/10000\n",
      "2700/2700 [==============================] - 0s 22us/step - loss: 4.2906e-04 - mean_absolute_error: 0.0131\n",
      "\n",
      "Epoch 01387: loss did not improve\n",
      "Epoch 1388/10000\n",
      "2700/2700 [==============================] - 0s 21us/step - loss: 4.3484e-04 - mean_absolute_error: 0.0132\n",
      "\n",
      "Epoch 01388: loss did not improve\n",
      "Epoch 1389/10000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2700/2700 [==============================] - 0s 22us/step - loss: 4.3648e-04 - mean_absolute_error: 0.0133\n",
      "\n",
      "Epoch 01389: loss did not improve\n",
      "Epoch 1390/10000\n",
      "2700/2700 [==============================] - 0s 24us/step - loss: 4.2891e-04 - mean_absolute_error: 0.0130\n",
      "\n",
      "Epoch 01390: loss did not improve\n",
      "Epoch 1391/10000\n",
      "2700/2700 [==============================] - 0s 23us/step - loss: 4.3212e-04 - mean_absolute_error: 0.0131\n",
      "\n",
      "Epoch 01391: loss did not improve\n",
      "Epoch 1392/10000\n",
      "2700/2700 [==============================] - 0s 23us/step - loss: 4.3147e-04 - mean_absolute_error: 0.0131\n",
      "\n",
      "Epoch 01392: loss did not improve\n",
      "Epoch 1393/10000\n",
      "2700/2700 [==============================] - 0s 23us/step - loss: 4.3370e-04 - mean_absolute_error: 0.0132\n",
      "\n",
      "Epoch 01393: loss did not improve\n",
      "Epoch 1394/10000\n",
      "2700/2700 [==============================] - 0s 25us/step - loss: 4.2947e-04 - mean_absolute_error: 0.0130\n",
      "\n",
      "Epoch 01394: loss did not improve\n",
      "Epoch 1395/10000\n",
      "2700/2700 [==============================] - 0s 24us/step - loss: 4.2919e-04 - mean_absolute_error: 0.0131\n",
      "\n",
      "Epoch 01395: loss did not improve\n",
      "Epoch 1396/10000\n",
      "2700/2700 [==============================] - 0s 23us/step - loss: 4.2968e-04 - mean_absolute_error: 0.0132\n",
      "\n",
      "Epoch 01396: loss did not improve\n",
      "Epoch 1397/10000\n",
      "2700/2700 [==============================] - 0s 22us/step - loss: 4.2856e-04 - mean_absolute_error: 0.0131\n",
      "\n",
      "Epoch 01397: loss did not improve\n",
      "Epoch 1398/10000\n",
      "2700/2700 [==============================] - 0s 22us/step - loss: 4.3069e-04 - mean_absolute_error: 0.0131\n",
      "\n",
      "Epoch 01398: loss did not improve\n",
      "Epoch 1399/10000\n",
      "2700/2700 [==============================] - 0s 23us/step - loss: 4.2750e-04 - mean_absolute_error: 0.0129\n",
      "\n",
      "Epoch 01399: loss did not improve\n",
      "Epoch 1400/10000\n",
      "2700/2700 [==============================] - 0s 22us/step - loss: 4.3176e-04 - mean_absolute_error: 0.0132\n",
      "\n",
      "Epoch 01400: loss did not improve\n",
      "Epoch 1401/10000\n",
      "2700/2700 [==============================] - 0s 22us/step - loss: 4.3469e-04 - mean_absolute_error: 0.0132\n",
      "\n",
      "Epoch 01401: loss did not improve\n",
      "Epoch 1402/10000\n",
      "2700/2700 [==============================] - 0s 21us/step - loss: 4.2882e-04 - mean_absolute_error: 0.0131\n",
      "\n",
      "Epoch 01402: loss did not improve\n",
      "Epoch 1403/10000\n",
      "2700/2700 [==============================] - 0s 33us/step - loss: 4.2906e-04 - mean_absolute_error: 0.0131\n",
      "\n",
      "Epoch 01403: loss did not improve\n",
      "Epoch 1404/10000\n",
      "2700/2700 [==============================] - 0s 29us/step - loss: 4.2759e-04 - mean_absolute_error: 0.0131\n",
      "\n",
      "Epoch 01404: loss did not improve\n",
      "Epoch 1405/10000\n",
      "2700/2700 [==============================] - 0s 22us/step - loss: 4.3066e-04 - mean_absolute_error: 0.0130\n",
      "\n",
      "Epoch 01405: loss did not improve\n",
      "Epoch 1406/10000\n",
      "2700/2700 [==============================] - 0s 21us/step - loss: 4.3563e-04 - mean_absolute_error: 0.0134\n",
      "\n",
      "Epoch 01406: loss did not improve\n",
      "Epoch 1407/10000\n",
      "2700/2700 [==============================] - 0s 22us/step - loss: 4.2751e-04 - mean_absolute_error: 0.0130\n",
      "\n",
      "Epoch 01407: loss did not improve\n",
      "Epoch 1408/10000\n",
      "2700/2700 [==============================] - 0s 22us/step - loss: 4.2679e-04 - mean_absolute_error: 0.0129\n",
      "\n",
      "Epoch 01408: loss did not improve\n",
      "Epoch 1409/10000\n",
      "2700/2700 [==============================] - 0s 22us/step - loss: 4.3234e-04 - mean_absolute_error: 0.0132\n",
      "\n",
      "Epoch 01409: loss did not improve\n",
      "Epoch 1410/10000\n",
      "2700/2700 [==============================] - 0s 22us/step - loss: 4.3447e-04 - mean_absolute_error: 0.0133\n",
      "\n",
      "Epoch 01410: loss did not improve\n",
      "Epoch 1411/10000\n",
      "2700/2700 [==============================] - 0s 22us/step - loss: 4.3358e-04 - mean_absolute_error: 0.0133\n",
      "\n",
      "Epoch 01411: loss did not improve\n",
      "Epoch 1412/10000\n",
      "2700/2700 [==============================] - 0s 23us/step - loss: 4.3108e-04 - mean_absolute_error: 0.0131\n",
      "\n",
      "Epoch 01412: loss did not improve\n",
      "Epoch 1413/10000\n",
      "2700/2700 [==============================] - 0s 23us/step - loss: 4.3690e-04 - mean_absolute_error: 0.0133\n",
      "\n",
      "Epoch 01413: loss did not improve\n",
      "Epoch 1414/10000\n",
      "2700/2700 [==============================] - 0s 21us/step - loss: 4.2817e-04 - mean_absolute_error: 0.0130\n",
      "\n",
      "Epoch 01414: loss did not improve\n",
      "Epoch 1415/10000\n",
      "2700/2700 [==============================] - 0s 21us/step - loss: 4.2836e-04 - mean_absolute_error: 0.0130\n",
      "\n",
      "Epoch 01415: loss did not improve\n",
      "Epoch 1416/10000\n",
      "2700/2700 [==============================] - 0s 21us/step - loss: 4.3659e-04 - mean_absolute_error: 0.0133\n",
      "\n",
      "Epoch 01416: loss did not improve\n",
      "Epoch 1417/10000\n",
      "2700/2700 [==============================] - 0s 21us/step - loss: 4.3094e-04 - mean_absolute_error: 0.0131\n",
      "\n",
      "Epoch 01417: loss did not improve\n",
      "Epoch 1418/10000\n",
      "2700/2700 [==============================] - 0s 21us/step - loss: 4.2628e-04 - mean_absolute_error: 0.0130\n",
      "\n",
      "Epoch 01418: loss did not improve\n",
      "Epoch 1419/10000\n",
      "2700/2700 [==============================] - 0s 21us/step - loss: 4.3072e-04 - mean_absolute_error: 0.0130\n",
      "\n",
      "Epoch 01419: loss did not improve\n",
      "Epoch 1420/10000\n",
      "2700/2700 [==============================] - 0s 22us/step - loss: 4.2821e-04 - mean_absolute_error: 0.0131\n",
      "\n",
      "Epoch 01420: loss did not improve\n",
      "Epoch 1421/10000\n",
      "2700/2700 [==============================] - 0s 23us/step - loss: 4.2753e-04 - mean_absolute_error: 0.0130\n",
      "\n",
      "Epoch 01421: loss did not improve\n",
      "Epoch 1422/10000\n",
      "2700/2700 [==============================] - 0s 23us/step - loss: 4.3217e-04 - mean_absolute_error: 0.0132\n",
      "\n",
      "Epoch 01422: loss did not improve\n",
      "Epoch 1423/10000\n",
      "2700/2700 [==============================] - 0s 22us/step - loss: 4.2547e-04 - mean_absolute_error: 0.0130\n",
      "\n",
      "Epoch 01423: loss improved from 0.00043 to 0.00043, saving model to keras/best_baseline_weights.hdf5\n",
      "Epoch 1424/10000\n",
      "2700/2700 [==============================] - 0s 23us/step - loss: 4.3293e-04 - mean_absolute_error: 0.0133\n",
      "\n",
      "Epoch 01424: loss did not improve\n",
      "Epoch 1425/10000\n",
      "2700/2700 [==============================] - 0s 22us/step - loss: 4.2758e-04 - mean_absolute_error: 0.0129\n",
      "\n",
      "Epoch 01425: loss did not improve\n",
      "Epoch 1426/10000\n",
      "2700/2700 [==============================] - 0s 25us/step - loss: 4.3049e-04 - mean_absolute_error: 0.0131\n",
      "\n",
      "Epoch 01426: loss did not improve\n",
      "Epoch 1427/10000\n",
      "2700/2700 [==============================] - 0s 22us/step - loss: 4.2697e-04 - mean_absolute_error: 0.0129\n",
      "\n",
      "Epoch 01427: loss did not improve\n",
      "Epoch 1428/10000\n",
      "2700/2700 [==============================] - 0s 21us/step - loss: 4.3077e-04 - mean_absolute_error: 0.0131\n",
      "\n",
      "Epoch 01428: loss did not improve\n",
      "Epoch 1429/10000\n",
      "2700/2700 [==============================] - 0s 22us/step - loss: 4.2978e-04 - mean_absolute_error: 0.0131\n",
      "\n",
      "Epoch 01429: loss did not improve\n",
      "Epoch 1430/10000\n",
      "2700/2700 [==============================] - 0s 23us/step - loss: 4.2713e-04 - mean_absolute_error: 0.0131\n",
      "\n",
      "Epoch 01430: loss did not improve\n",
      "Epoch 1431/10000\n",
      "2700/2700 [==============================] - 0s 21us/step - loss: 4.2777e-04 - mean_absolute_error: 0.0131\n",
      "\n",
      "Epoch 01431: loss did not improve\n",
      "Epoch 1432/10000\n",
      "2700/2700 [==============================] - 0s 24us/step - loss: 4.2946e-04 - mean_absolute_error: 0.0131\n",
      "\n",
      "Epoch 01432: loss did not improve\n",
      "Epoch 1433/10000\n",
      "2700/2700 [==============================] - 0s 21us/step - loss: 4.3193e-04 - mean_absolute_error: 0.0131\n",
      "\n",
      "Epoch 01433: loss did not improve\n",
      "Epoch 1434/10000\n",
      "2700/2700 [==============================] - 0s 21us/step - loss: 4.2571e-04 - mean_absolute_error: 0.0130\n",
      "\n",
      "Epoch 01434: loss did not improve\n",
      "Epoch 1435/10000\n",
      "2700/2700 [==============================] - 0s 21us/step - loss: 4.2713e-04 - mean_absolute_error: 0.0130\n",
      "\n",
      "Epoch 01435: loss did not improve\n",
      "Epoch 1436/10000\n",
      "2700/2700 [==============================] - 0s 22us/step - loss: 4.3282e-04 - mean_absolute_error: 0.0133\n",
      "\n",
      "Epoch 01436: loss did not improve\n",
      "Epoch 1437/10000\n",
      "2700/2700 [==============================] - 0s 21us/step - loss: 4.2789e-04 - mean_absolute_error: 0.0129\n",
      "\n",
      "Epoch 01437: loss did not improve\n",
      "Epoch 1438/10000\n",
      "2700/2700 [==============================] - 0s 23us/step - loss: 4.3035e-04 - mean_absolute_error: 0.0131\n",
      "\n",
      "Epoch 01438: loss did not improve\n",
      "Epoch 1439/10000\n",
      "2700/2700 [==============================] - 0s 22us/step - loss: 4.3109e-04 - mean_absolute_error: 0.0130\n",
      "\n",
      "Epoch 01439: loss did not improve\n",
      "Epoch 1440/10000\n",
      "2700/2700 [==============================] - 0s 21us/step - loss: 4.2493e-04 - mean_absolute_error: 0.0130\n",
      "\n",
      "Epoch 01440: loss improved from 0.00043 to 0.00042, saving model to keras/best_baseline_weights.hdf5\n",
      "Epoch 1441/10000\n",
      "2700/2700 [==============================] - 0s 21us/step - loss: 4.3989e-04 - mean_absolute_error: 0.0135\n",
      "\n",
      "Epoch 01441: loss did not improve\n",
      "Epoch 1442/10000\n",
      "2700/2700 [==============================] - 0s 21us/step - loss: 4.3039e-04 - mean_absolute_error: 0.0131\n",
      "\n",
      "Epoch 01442: loss did not improve\n",
      "Epoch 1443/10000\n",
      "2700/2700 [==============================] - 0s 22us/step - loss: 4.2730e-04 - mean_absolute_error: 0.0131\n",
      "\n",
      "Epoch 01443: loss did not improve\n",
      "Epoch 1444/10000\n",
      "2700/2700 [==============================] - 0s 23us/step - loss: 4.3318e-04 - mean_absolute_error: 0.0131\n",
      "\n",
      "Epoch 01444: loss did not improve\n",
      "Epoch 1445/10000\n",
      "2700/2700 [==============================] - 0s 21us/step - loss: 4.2694e-04 - mean_absolute_error: 0.0130\n",
      "\n",
      "Epoch 01445: loss did not improve\n",
      "Epoch 1446/10000\n",
      "2700/2700 [==============================] - 0s 21us/step - loss: 4.2773e-04 - mean_absolute_error: 0.0130\n",
      "\n",
      "Epoch 01446: loss did not improve\n",
      "Epoch 1447/10000\n",
      "2700/2700 [==============================] - 0s 23us/step - loss: 4.3442e-04 - mean_absolute_error: 0.0134\n",
      "\n",
      "Epoch 01447: loss did not improve\n",
      "Epoch 1448/10000\n",
      "2700/2700 [==============================] - 0s 22us/step - loss: 4.3049e-04 - mean_absolute_error: 0.0131\n",
      "\n",
      "Epoch 01448: loss did not improve\n",
      "Epoch 1449/10000\n",
      "2700/2700 [==============================] - 0s 21us/step - loss: 4.2634e-04 - mean_absolute_error: 0.0130\n",
      "\n",
      "Epoch 01449: loss did not improve\n",
      "Epoch 1450/10000\n",
      "2700/2700 [==============================] - 0s 22us/step - loss: 4.3245e-04 - mean_absolute_error: 0.0132\n",
      "\n",
      "Epoch 01450: loss did not improve\n",
      "Epoch 1451/10000\n",
      "2700/2700 [==============================] - 0s 21us/step - loss: 4.2688e-04 - mean_absolute_error: 0.0130\n",
      "\n",
      "Epoch 01451: loss did not improve\n",
      "Epoch 1452/10000\n",
      "2700/2700 [==============================] - 0s 22us/step - loss: 4.3198e-04 - mean_absolute_error: 0.0131\n",
      "\n",
      "Epoch 01452: loss did not improve\n",
      "Epoch 1453/10000\n",
      "2700/2700 [==============================] - 0s 22us/step - loss: 4.2837e-04 - mean_absolute_error: 0.0130\n",
      "\n",
      "Epoch 01453: loss did not improve\n",
      "Epoch 1454/10000\n",
      "2700/2700 [==============================] - 0s 22us/step - loss: 4.2737e-04 - mean_absolute_error: 0.0130\n",
      "\n",
      "Epoch 01454: loss did not improve\n",
      "Epoch 1455/10000\n",
      "2700/2700 [==============================] - 0s 22us/step - loss: 4.3048e-04 - mean_absolute_error: 0.0131\n",
      "\n",
      "Epoch 01455: loss did not improve\n",
      "Epoch 1456/10000\n",
      "2700/2700 [==============================] - 0s 23us/step - loss: 4.3023e-04 - mean_absolute_error: 0.0131\n",
      "\n",
      "Epoch 01456: loss did not improve\n",
      "Epoch 1457/10000\n",
      "2700/2700 [==============================] - 0s 22us/step - loss: 4.3595e-04 - mean_absolute_error: 0.0133\n",
      "\n",
      "Epoch 01457: loss did not improve\n",
      "Epoch 1458/10000\n",
      "2700/2700 [==============================] - 0s 21us/step - loss: 4.2986e-04 - mean_absolute_error: 0.0131\n",
      "\n",
      "Epoch 01458: loss did not improve\n",
      "Epoch 1459/10000\n",
      "2700/2700 [==============================] - 0s 21us/step - loss: 4.2633e-04 - mean_absolute_error: 0.0131\n",
      "\n",
      "Epoch 01459: loss did not improve\n",
      "Epoch 1460/10000\n",
      "2700/2700 [==============================] - 0s 21us/step - loss: 4.2452e-04 - mean_absolute_error: 0.0130\n",
      "\n",
      "Epoch 01460: loss improved from 0.00042 to 0.00042, saving model to keras/best_baseline_weights.hdf5\n",
      "Epoch 1461/10000\n",
      "2700/2700 [==============================] - 0s 21us/step - loss: 4.3293e-04 - mean_absolute_error: 0.0133\n",
      "\n",
      "Epoch 01461: loss did not improve\n",
      "Epoch 1462/10000\n",
      "2700/2700 [==============================] - 0s 21us/step - loss: 4.2894e-04 - mean_absolute_error: 0.0130\n",
      "\n",
      "Epoch 01462: loss did not improve\n",
      "Epoch 1463/10000\n",
      "2700/2700 [==============================] - 0s 22us/step - loss: 4.2790e-04 - mean_absolute_error: 0.0131\n",
      "\n",
      "Epoch 01463: loss did not improve\n",
      "Epoch 1464/10000\n",
      "2700/2700 [==============================] - 0s 21us/step - loss: 4.2996e-04 - mean_absolute_error: 0.0131\n",
      "\n",
      "Epoch 01464: loss did not improve\n",
      "Epoch 1465/10000\n",
      "2700/2700 [==============================] - 0s 24us/step - loss: 4.2664e-04 - mean_absolute_error: 0.0131\n",
      "\n",
      "Epoch 01465: loss did not improve\n",
      "Epoch 1466/10000\n",
      "2700/2700 [==============================] - 0s 21us/step - loss: 4.2541e-04 - mean_absolute_error: 0.0128\n",
      "\n",
      "Epoch 01466: loss did not improve\n",
      "Epoch 1467/10000\n",
      "2700/2700 [==============================] - 0s 21us/step - loss: 4.3422e-04 - mean_absolute_error: 0.0133\n",
      "\n",
      "Epoch 01467: loss did not improve\n",
      "Epoch 1468/10000\n",
      "2700/2700 [==============================] - 0s 21us/step - loss: 4.2707e-04 - mean_absolute_error: 0.0130\n",
      "\n",
      "Epoch 01468: loss did not improve\n",
      "Epoch 1469/10000\n",
      "2700/2700 [==============================] - 0s 21us/step - loss: 4.2591e-04 - mean_absolute_error: 0.0130\n",
      "\n",
      "Epoch 01469: loss did not improve\n",
      "Epoch 1470/10000\n",
      "2700/2700 [==============================] - 0s 21us/step - loss: 4.2841e-04 - mean_absolute_error: 0.0131\n",
      "\n",
      "Epoch 01470: loss did not improve\n",
      "Epoch 1471/10000\n",
      "2700/2700 [==============================] - 0s 22us/step - loss: 4.3006e-04 - mean_absolute_error: 0.0131\n",
      "\n",
      "Epoch 01471: loss did not improve\n",
      "Epoch 1472/10000\n",
      "2700/2700 [==============================] - 0s 27us/step - loss: 4.2897e-04 - mean_absolute_error: 0.0131\n",
      "\n",
      "Epoch 01472: loss did not improve\n",
      "Epoch 1473/10000\n",
      "2700/2700 [==============================] - 0s 23us/step - loss: 4.2997e-04 - mean_absolute_error: 0.0131\n",
      "\n",
      "Epoch 01473: loss did not improve\n",
      "Epoch 1474/10000\n",
      "2700/2700 [==============================] - 0s 27us/step - loss: 4.3206e-04 - mean_absolute_error: 0.0132\n",
      "\n",
      "Epoch 01474: loss did not improve\n",
      "Epoch 1475/10000\n",
      "2700/2700 [==============================] - 0s 27us/step - loss: 4.2600e-04 - mean_absolute_error: 0.0129\n",
      "\n",
      "Epoch 01475: loss did not improve\n",
      "Epoch 1476/10000\n",
      "2700/2700 [==============================] - 0s 21us/step - loss: 4.2591e-04 - mean_absolute_error: 0.0130\n",
      "\n",
      "Epoch 01476: loss did not improve\n",
      "Epoch 1477/10000\n",
      "2700/2700 [==============================] - 0s 21us/step - loss: 4.2665e-04 - mean_absolute_error: 0.0130\n",
      "\n",
      "Epoch 01477: loss did not improve\n",
      "Epoch 1478/10000\n",
      "2700/2700 [==============================] - 0s 21us/step - loss: 4.3487e-04 - mean_absolute_error: 0.0134\n",
      "\n",
      "Epoch 01478: loss did not improve\n",
      "Epoch 1479/10000\n",
      "2700/2700 [==============================] - 0s 28us/step - loss: 4.2761e-04 - mean_absolute_error: 0.0130\n",
      "\n",
      "Epoch 01479: loss did not improve\n",
      "Epoch 1480/10000\n",
      "2700/2700 [==============================] - 0s 23us/step - loss: 4.3646e-04 - mean_absolute_error: 0.0134\n",
      "\n",
      "Epoch 01480: loss did not improve\n",
      "Epoch 1481/10000\n",
      "2700/2700 [==============================] - 0s 21us/step - loss: 4.3064e-04 - mean_absolute_error: 0.0131\n",
      "\n",
      "Epoch 01481: loss did not improve\n",
      "Epoch 1482/10000\n",
      "2700/2700 [==============================] - 0s 29us/step - loss: 4.2824e-04 - mean_absolute_error: 0.0131\n",
      "\n",
      "Epoch 01482: loss did not improve\n",
      "Epoch 1483/10000\n",
      "2700/2700 [==============================] - 0s 25us/step - loss: 4.3027e-04 - mean_absolute_error: 0.0130\n",
      "\n",
      "Epoch 01483: loss did not improve\n",
      "Epoch 1484/10000\n",
      "2700/2700 [==============================] - 0s 22us/step - loss: 4.2849e-04 - mean_absolute_error: 0.0130\n",
      "\n",
      "Epoch 01484: loss did not improve\n",
      "Epoch 1485/10000\n",
      "2700/2700 [==============================] - 0s 30us/step - loss: 4.2964e-04 - mean_absolute_error: 0.0131\n",
      "\n",
      "Epoch 01485: loss did not improve\n",
      "Epoch 1486/10000\n",
      "2700/2700 [==============================] - 0s 22us/step - loss: 4.2905e-04 - mean_absolute_error: 0.0130\n",
      "\n",
      "Epoch 01486: loss did not improve\n",
      "Epoch 1487/10000\n",
      "2700/2700 [==============================] - 0s 22us/step - loss: 4.3145e-04 - mean_absolute_error: 0.0132\n",
      "\n",
      "Epoch 01487: loss did not improve\n",
      "Epoch 1488/10000\n",
      "2700/2700 [==============================] - 0s 27us/step - loss: 4.2642e-04 - mean_absolute_error: 0.0129\n",
      "\n",
      "Epoch 01488: loss did not improve\n",
      "Epoch 1489/10000\n",
      "2700/2700 [==============================] - 0s 26us/step - loss: 4.2614e-04 - mean_absolute_error: 0.0130\n",
      "\n",
      "Epoch 01489: loss did not improve\n",
      "Epoch 1490/10000\n",
      "2700/2700 [==============================] - 0s 24us/step - loss: 4.2857e-04 - mean_absolute_error: 0.0130\n",
      "\n",
      "Epoch 01490: loss did not improve\n",
      "Epoch 1491/10000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2700/2700 [==============================] - 0s 22us/step - loss: 4.2722e-04 - mean_absolute_error: 0.0130\n",
      "\n",
      "Epoch 01491: loss did not improve\n",
      "Epoch 1492/10000\n",
      "2700/2700 [==============================] - 0s 21us/step - loss: 4.2783e-04 - mean_absolute_error: 0.0130\n",
      "\n",
      "Epoch 01492: loss did not improve\n",
      "Epoch 1493/10000\n",
      "2700/2700 [==============================] - 0s 21us/step - loss: 4.2691e-04 - mean_absolute_error: 0.0130\n",
      "\n",
      "Epoch 01493: loss did not improve\n",
      "Epoch 1494/10000\n",
      "2700/2700 [==============================] - 0s 21us/step - loss: 4.2799e-04 - mean_absolute_error: 0.0130\n",
      "\n",
      "Epoch 01494: loss did not improve\n",
      "Epoch 1495/10000\n",
      "2700/2700 [==============================] - 0s 27us/step - loss: 4.3028e-04 - mean_absolute_error: 0.0132\n",
      "\n",
      "Epoch 01495: loss did not improve\n",
      "Epoch 1496/10000\n",
      "2700/2700 [==============================] - 0s 22us/step - loss: 4.2623e-04 - mean_absolute_error: 0.0130\n",
      "\n",
      "Epoch 01496: loss did not improve\n",
      "Epoch 1497/10000\n",
      "2700/2700 [==============================] - 0s 21us/step - loss: 4.2987e-04 - mean_absolute_error: 0.0132\n",
      "\n",
      "Epoch 01497: loss did not improve\n",
      "Epoch 1498/10000\n",
      "2700/2700 [==============================] - 0s 24us/step - loss: 4.3372e-04 - mean_absolute_error: 0.0133\n",
      "\n",
      "Epoch 01498: loss did not improve\n",
      "Epoch 1499/10000\n",
      "2700/2700 [==============================] - 0s 23us/step - loss: 4.3230e-04 - mean_absolute_error: 0.0133\n",
      "\n",
      "Epoch 01499: loss did not improve\n",
      "Epoch 1500/10000\n",
      "2700/2700 [==============================] - 0s 21us/step - loss: 4.3225e-04 - mean_absolute_error: 0.0132\n",
      "\n",
      "Epoch 01500: loss did not improve\n",
      "Epoch 1501/10000\n",
      "2700/2700 [==============================] - 0s 27us/step - loss: 4.2545e-04 - mean_absolute_error: 0.0130\n",
      "\n",
      "Epoch 01501: loss did not improve\n",
      "Epoch 1502/10000\n",
      "2700/2700 [==============================] - 0s 22us/step - loss: 4.2491e-04 - mean_absolute_error: 0.0130\n",
      "\n",
      "Epoch 01502: loss did not improve\n",
      "Epoch 1503/10000\n",
      "2700/2700 [==============================] - 0s 23us/step - loss: 4.3956e-04 - mean_absolute_error: 0.0134\n",
      "\n",
      "Epoch 01503: loss did not improve\n",
      "Epoch 1504/10000\n",
      "2700/2700 [==============================] - 0s 30us/step - loss: 4.2659e-04 - mean_absolute_error: 0.0130\n",
      "\n",
      "Epoch 01504: loss did not improve\n",
      "Epoch 1505/10000\n",
      "2700/2700 [==============================] - 0s 26us/step - loss: 4.2979e-04 - mean_absolute_error: 0.0131\n",
      "\n",
      "Epoch 01505: loss did not improve\n",
      "Epoch 1506/10000\n",
      "2700/2700 [==============================] - 0s 25us/step - loss: 4.2918e-04 - mean_absolute_error: 0.0131\n",
      "\n",
      "Epoch 01506: loss did not improve\n",
      "Epoch 1507/10000\n",
      "2700/2700 [==============================] - 0s 23us/step - loss: 4.2880e-04 - mean_absolute_error: 0.0131\n",
      "\n",
      "Epoch 01507: loss did not improve\n",
      "Epoch 1508/10000\n",
      "2700/2700 [==============================] - 0s 21us/step - loss: 4.3478e-04 - mean_absolute_error: 0.0133\n",
      "\n",
      "Epoch 01508: loss did not improve\n",
      "Epoch 1509/10000\n",
      "2700/2700 [==============================] - 0s 27us/step - loss: 4.2969e-04 - mean_absolute_error: 0.0131\n",
      "\n",
      "Epoch 01509: loss did not improve\n",
      "Epoch 1510/10000\n",
      "2700/2700 [==============================] - 0s 22us/step - loss: 4.3575e-04 - mean_absolute_error: 0.0134\n",
      "\n",
      "Epoch 01510: loss did not improve\n",
      "Epoch 1511/10000\n",
      "2700/2700 [==============================] - 0s 27us/step - loss: 4.3010e-04 - mean_absolute_error: 0.0132\n",
      "\n",
      "Epoch 01511: loss did not improve\n",
      "Epoch 1512/10000\n",
      "2700/2700 [==============================] - 0s 22us/step - loss: 4.2687e-04 - mean_absolute_error: 0.0130\n",
      "\n",
      "Epoch 01512: loss did not improve\n",
      "Epoch 1513/10000\n",
      "2700/2700 [==============================] - 0s 22us/step - loss: 4.2590e-04 - mean_absolute_error: 0.0129\n",
      "\n",
      "Epoch 01513: loss did not improve\n",
      "Epoch 1514/10000\n",
      "2700/2700 [==============================] - 0s 22us/step - loss: 4.3352e-04 - mean_absolute_error: 0.0133\n",
      "\n",
      "Epoch 01514: loss did not improve\n",
      "Epoch 1515/10000\n",
      "2700/2700 [==============================] - 0s 24us/step - loss: 4.2494e-04 - mean_absolute_error: 0.0130\n",
      "\n",
      "Epoch 01515: loss did not improve\n",
      "Epoch 1516/10000\n",
      "2700/2700 [==============================] - 0s 41us/step - loss: 4.2703e-04 - mean_absolute_error: 0.0130\n",
      "\n",
      "Epoch 01516: loss did not improve\n",
      "Epoch 1517/10000\n",
      "2700/2700 [==============================] - 0s 23us/step - loss: 4.2544e-04 - mean_absolute_error: 0.0131\n",
      "\n",
      "Epoch 01517: loss did not improve\n",
      "Epoch 1518/10000\n",
      "2700/2700 [==============================] - 0s 23us/step - loss: 4.2576e-04 - mean_absolute_error: 0.0130\n",
      "\n",
      "Epoch 01518: loss did not improve\n",
      "Epoch 1519/10000\n",
      "2700/2700 [==============================] - 0s 25us/step - loss: 4.2735e-04 - mean_absolute_error: 0.0130\n",
      "\n",
      "Epoch 01519: loss did not improve\n",
      "Epoch 1520/10000\n",
      "2700/2700 [==============================] - 0s 22us/step - loss: 4.2973e-04 - mean_absolute_error: 0.0131\n",
      "\n",
      "Epoch 01520: loss did not improve\n",
      "Epoch 1521/10000\n",
      "2700/2700 [==============================] - 0s 22us/step - loss: 4.2753e-04 - mean_absolute_error: 0.0131\n",
      "\n",
      "Epoch 01521: loss did not improve\n",
      "Epoch 1522/10000\n",
      "2700/2700 [==============================] - 0s 28us/step - loss: 4.2918e-04 - mean_absolute_error: 0.0132\n",
      "\n",
      "Epoch 01522: loss did not improve\n",
      "Epoch 1523/10000\n",
      "2700/2700 [==============================] - 0s 22us/step - loss: 4.2617e-04 - mean_absolute_error: 0.0130\n",
      "\n",
      "Epoch 01523: loss did not improve\n",
      "Epoch 1524/10000\n",
      "2700/2700 [==============================] - 0s 22us/step - loss: 4.2947e-04 - mean_absolute_error: 0.0131\n",
      "\n",
      "Epoch 01524: loss did not improve\n",
      "Epoch 1525/10000\n",
      "2700/2700 [==============================] - 0s 22us/step - loss: 4.2701e-04 - mean_absolute_error: 0.0131\n",
      "\n",
      "Epoch 01525: loss did not improve\n",
      "Epoch 1526/10000\n",
      "2700/2700 [==============================] - 0s 21us/step - loss: 4.2520e-04 - mean_absolute_error: 0.0129\n",
      "\n",
      "Epoch 01526: loss did not improve\n",
      "Epoch 1527/10000\n",
      "2700/2700 [==============================] - 0s 22us/step - loss: 4.2827e-04 - mean_absolute_error: 0.0131\n",
      "\n",
      "Epoch 01527: loss did not improve\n",
      "Epoch 1528/10000\n",
      "2700/2700 [==============================] - 0s 23us/step - loss: 4.2575e-04 - mean_absolute_error: 0.0130\n",
      "\n",
      "Epoch 01528: loss did not improve\n",
      "Epoch 1529/10000\n",
      "2700/2700 [==============================] - 0s 22us/step - loss: 4.3147e-04 - mean_absolute_error: 0.0133\n",
      "\n",
      "Epoch 01529: loss did not improve\n",
      "Epoch 1530/10000\n",
      "2700/2700 [==============================] - 0s 22us/step - loss: 4.2734e-04 - mean_absolute_error: 0.0130\n",
      "\n",
      "Epoch 01530: loss did not improve\n",
      "Epoch 1531/10000\n",
      "2700/2700 [==============================] - 0s 22us/step - loss: 4.3287e-04 - mean_absolute_error: 0.0133\n",
      "\n",
      "Epoch 01531: loss did not improve\n",
      "Epoch 1532/10000\n",
      "2700/2700 [==============================] - 0s 22us/step - loss: 4.3426e-04 - mean_absolute_error: 0.0133\n",
      "\n",
      "Epoch 01532: loss did not improve\n",
      "Epoch 1533/10000\n",
      "2700/2700 [==============================] - 0s 21us/step - loss: 4.2309e-04 - mean_absolute_error: 0.0129\n",
      "\n",
      "Epoch 01533: loss improved from 0.00042 to 0.00042, saving model to keras/best_baseline_weights.hdf5\n",
      "Epoch 1534/10000\n",
      "2700/2700 [==============================] - 0s 21us/step - loss: 4.2375e-04 - mean_absolute_error: 0.0129\n",
      "\n",
      "Epoch 01534: loss did not improve\n",
      "Epoch 1535/10000\n",
      "2700/2700 [==============================] - 0s 21us/step - loss: 4.2507e-04 - mean_absolute_error: 0.0130\n",
      "\n",
      "Epoch 01535: loss did not improve\n",
      "Epoch 1536/10000\n",
      "2700/2700 [==============================] - 0s 21us/step - loss: 4.2633e-04 - mean_absolute_error: 0.0130\n",
      "\n",
      "Epoch 01536: loss did not improve\n",
      "Epoch 1537/10000\n",
      "2700/2700 [==============================] - 0s 21us/step - loss: 4.2752e-04 - mean_absolute_error: 0.0131\n",
      "\n",
      "Epoch 01537: loss did not improve\n",
      "Epoch 1538/10000\n",
      "2700/2700 [==============================] - 0s 24us/step - loss: 4.2640e-04 - mean_absolute_error: 0.0130\n",
      "\n",
      "Epoch 01538: loss did not improve\n",
      "Epoch 1539/10000\n",
      "2700/2700 [==============================] - 0s 24us/step - loss: 4.3577e-04 - mean_absolute_error: 0.0132\n",
      "\n",
      "Epoch 01539: loss did not improve\n",
      "Epoch 1540/10000\n",
      "2700/2700 [==============================] - 0s 21us/step - loss: 4.2839e-04 - mean_absolute_error: 0.0132\n",
      "\n",
      "Epoch 01540: loss did not improve\n",
      "Epoch 1541/10000\n",
      "2700/2700 [==============================] - 0s 21us/step - loss: 4.2897e-04 - mean_absolute_error: 0.0131\n",
      "\n",
      "Epoch 01541: loss did not improve\n",
      "Epoch 1542/10000\n",
      "2700/2700 [==============================] - 0s 24us/step - loss: 4.3696e-04 - mean_absolute_error: 0.0134\n",
      "\n",
      "Epoch 01542: loss did not improve\n",
      "Epoch 1543/10000\n",
      "2700/2700 [==============================] - 0s 21us/step - loss: 4.3225e-04 - mean_absolute_error: 0.0132\n",
      "\n",
      "Epoch 01543: loss did not improve\n",
      "Epoch 1544/10000\n",
      "2700/2700 [==============================] - 0s 22us/step - loss: 4.2633e-04 - mean_absolute_error: 0.0131\n",
      "\n",
      "Epoch 01544: loss did not improve\n",
      "Epoch 1545/10000\n",
      "2700/2700 [==============================] - 0s 21us/step - loss: 4.2844e-04 - mean_absolute_error: 0.0131\n",
      "\n",
      "Epoch 01545: loss did not improve\n",
      "Epoch 1546/10000\n",
      "2700/2700 [==============================] - 0s 21us/step - loss: 4.2434e-04 - mean_absolute_error: 0.0130\n",
      "\n",
      "Epoch 01546: loss did not improve\n",
      "Epoch 1547/10000\n",
      "2700/2700 [==============================] - 0s 21us/step - loss: 4.2732e-04 - mean_absolute_error: 0.0131\n",
      "\n",
      "Epoch 01547: loss did not improve\n",
      "Epoch 1548/10000\n",
      "2700/2700 [==============================] - 0s 23us/step - loss: 4.2404e-04 - mean_absolute_error: 0.0129\n",
      "\n",
      "Epoch 01548: loss did not improve\n",
      "Epoch 1549/10000\n",
      "2700/2700 [==============================] - 0s 22us/step - loss: 4.2905e-04 - mean_absolute_error: 0.0133\n",
      "\n",
      "Epoch 01549: loss did not improve\n",
      "Epoch 1550/10000\n",
      "2700/2700 [==============================] - 0s 21us/step - loss: 4.3189e-04 - mean_absolute_error: 0.0133\n",
      "\n",
      "Epoch 01550: loss did not improve\n",
      "Epoch 1551/10000\n",
      "2700/2700 [==============================] - 0s 21us/step - loss: 4.4156e-04 - mean_absolute_error: 0.0136\n",
      "\n",
      "Epoch 01551: loss did not improve\n",
      "Epoch 1552/10000\n",
      "2700/2700 [==============================] - 0s 21us/step - loss: 4.2802e-04 - mean_absolute_error: 0.0131\n",
      "\n",
      "Epoch 01552: loss did not improve\n",
      "Epoch 1553/10000\n",
      "2700/2700 [==============================] - 0s 21us/step - loss: 4.2462e-04 - mean_absolute_error: 0.0130\n",
      "\n",
      "Epoch 01553: loss did not improve\n",
      "Epoch 1554/10000\n",
      "2700/2700 [==============================] - 0s 21us/step - loss: 4.2513e-04 - mean_absolute_error: 0.0130\n",
      "\n",
      "Epoch 01554: loss did not improve\n",
      "Epoch 1555/10000\n",
      "2700/2700 [==============================] - 0s 22us/step - loss: 4.2598e-04 - mean_absolute_error: 0.0130\n",
      "\n",
      "Epoch 01555: loss did not improve\n",
      "Epoch 1556/10000\n",
      "2700/2700 [==============================] - 0s 22us/step - loss: 4.3604e-04 - mean_absolute_error: 0.0134\n",
      "\n",
      "Epoch 01556: loss did not improve\n",
      "Epoch 1557/10000\n",
      "2700/2700 [==============================] - 0s 23us/step - loss: 4.3067e-04 - mean_absolute_error: 0.0131\n",
      "\n",
      "Epoch 01557: loss did not improve\n",
      "Epoch 1558/10000\n",
      "2700/2700 [==============================] - 0s 23us/step - loss: 4.2440e-04 - mean_absolute_error: 0.0129\n",
      "\n",
      "Epoch 01558: loss did not improve\n",
      "Epoch 1559/10000\n",
      "2700/2700 [==============================] - 0s 21us/step - loss: 4.2621e-04 - mean_absolute_error: 0.0130\n",
      "\n",
      "Epoch 01559: loss did not improve\n",
      "Epoch 1560/10000\n",
      "2700/2700 [==============================] - 0s 21us/step - loss: 4.2908e-04 - mean_absolute_error: 0.0131\n",
      "\n",
      "Epoch 01560: loss did not improve\n",
      "Epoch 1561/10000\n",
      "2700/2700 [==============================] - 0s 21us/step - loss: 4.2308e-04 - mean_absolute_error: 0.0130\n",
      "\n",
      "Epoch 01561: loss improved from 0.00042 to 0.00042, saving model to keras/best_baseline_weights.hdf5\n",
      "Epoch 1562/10000\n",
      "2700/2700 [==============================] - 0s 21us/step - loss: 4.2649e-04 - mean_absolute_error: 0.0130\n",
      "\n",
      "Epoch 01562: loss did not improve\n",
      "Epoch 1563/10000\n",
      "2700/2700 [==============================] - 0s 21us/step - loss: 4.3413e-04 - mean_absolute_error: 0.0135\n",
      "\n",
      "Epoch 01563: loss did not improve\n",
      "Epoch 1564/10000\n",
      "2700/2700 [==============================] - 0s 21us/step - loss: 4.2643e-04 - mean_absolute_error: 0.0130\n",
      "\n",
      "Epoch 01564: loss did not improve\n",
      "Epoch 1565/10000\n",
      "2700/2700 [==============================] - 0s 21us/step - loss: 4.3072e-04 - mean_absolute_error: 0.0131\n",
      "\n",
      "Epoch 01565: loss did not improve\n",
      "Epoch 1566/10000\n",
      "2700/2700 [==============================] - 0s 23us/step - loss: 4.2611e-04 - mean_absolute_error: 0.0130\n",
      "\n",
      "Epoch 01566: loss did not improve\n",
      "Epoch 1567/10000\n",
      "2700/2700 [==============================] - 0s 21us/step - loss: 4.2762e-04 - mean_absolute_error: 0.0131\n",
      "\n",
      "Epoch 01567: loss did not improve\n",
      "Epoch 1568/10000\n",
      "2700/2700 [==============================] - 0s 22us/step - loss: 4.3147e-04 - mean_absolute_error: 0.0133\n",
      "\n",
      "Epoch 01568: loss did not improve\n",
      "Epoch 1569/10000\n",
      "2700/2700 [==============================] - 0s 21us/step - loss: 4.2296e-04 - mean_absolute_error: 0.0129\n",
      "\n",
      "Epoch 01569: loss improved from 0.00042 to 0.00042, saving model to keras/best_baseline_weights.hdf5\n",
      "Epoch 1570/10000\n",
      "2700/2700 [==============================] - 0s 22us/step - loss: 4.3303e-04 - mean_absolute_error: 0.0134\n",
      "\n",
      "Epoch 01570: loss did not improve\n",
      "Epoch 1571/10000\n",
      "2700/2700 [==============================] - 0s 22us/step - loss: 4.3126e-04 - mean_absolute_error: 0.0132\n",
      "\n",
      "Epoch 01571: loss did not improve\n",
      "Epoch 1572/10000\n",
      "2700/2700 [==============================] - 0s 21us/step - loss: 4.2505e-04 - mean_absolute_error: 0.0130\n",
      "\n",
      "Epoch 01572: loss did not improve\n",
      "Epoch 1573/10000\n",
      "2700/2700 [==============================] - 0s 22us/step - loss: 4.2999e-04 - mean_absolute_error: 0.0132\n",
      "\n",
      "Epoch 01573: loss did not improve\n",
      "Epoch 1574/10000\n",
      "2700/2700 [==============================] - 0s 22us/step - loss: 4.2676e-04 - mean_absolute_error: 0.0130\n",
      "\n",
      "Epoch 01574: loss did not improve\n",
      "Epoch 1575/10000\n",
      "2700/2700 [==============================] - 0s 23us/step - loss: 4.2677e-04 - mean_absolute_error: 0.0130\n",
      "\n",
      "Epoch 01575: loss did not improve\n",
      "Epoch 1576/10000\n",
      "2700/2700 [==============================] - 0s 26us/step - loss: 4.2580e-04 - mean_absolute_error: 0.0130\n",
      "\n",
      "Epoch 01576: loss did not improve\n",
      "Epoch 1577/10000\n",
      "2700/2700 [==============================] - 0s 25us/step - loss: 4.2576e-04 - mean_absolute_error: 0.0130\n",
      "\n",
      "Epoch 01577: loss did not improve\n",
      "Epoch 1578/10000\n",
      "2700/2700 [==============================] - 0s 34us/step - loss: 4.3014e-04 - mean_absolute_error: 0.0131\n",
      "\n",
      "Epoch 01578: loss did not improve\n",
      "Epoch 1579/10000\n",
      "2700/2700 [==============================] - 0s 23us/step - loss: 4.2454e-04 - mean_absolute_error: 0.0129\n",
      "\n",
      "Epoch 01579: loss did not improve\n",
      "Epoch 1580/10000\n",
      "2700/2700 [==============================] - 0s 28us/step - loss: 4.2628e-04 - mean_absolute_error: 0.0131\n",
      "\n",
      "Epoch 01580: loss did not improve\n",
      "Epoch 1581/10000\n",
      "2700/2700 [==============================] - 0s 23us/step - loss: 4.2778e-04 - mean_absolute_error: 0.0130\n",
      "\n",
      "Epoch 01581: loss did not improve\n",
      "Epoch 1582/10000\n",
      "2700/2700 [==============================] - 0s 22us/step - loss: 4.2296e-04 - mean_absolute_error: 0.0129\n",
      "\n",
      "Epoch 01582: loss did not improve\n",
      "Epoch 1583/10000\n",
      "2700/2700 [==============================] - 0s 22us/step - loss: 4.2375e-04 - mean_absolute_error: 0.0130\n",
      "\n",
      "Epoch 01583: loss did not improve\n",
      "Epoch 1584/10000\n",
      "2700/2700 [==============================] - 0s 21us/step - loss: 4.2589e-04 - mean_absolute_error: 0.0132\n",
      "\n",
      "Epoch 01584: loss did not improve\n",
      "Epoch 1585/10000\n",
      "2700/2700 [==============================] - 0s 25us/step - loss: 4.2490e-04 - mean_absolute_error: 0.0129\n",
      "\n",
      "Epoch 01585: loss did not improve\n",
      "Epoch 1586/10000\n",
      "2700/2700 [==============================] - 0s 26us/step - loss: 4.2747e-04 - mean_absolute_error: 0.0131\n",
      "\n",
      "Epoch 01586: loss did not improve\n",
      "Epoch 1587/10000\n",
      "2700/2700 [==============================] - 0s 22us/step - loss: 4.3740e-04 - mean_absolute_error: 0.0134\n",
      "\n",
      "Epoch 01587: loss did not improve\n",
      "Epoch 1588/10000\n",
      "2700/2700 [==============================] - 0s 24us/step - loss: 4.2680e-04 - mean_absolute_error: 0.0129\n",
      "\n",
      "Epoch 01588: loss did not improve\n",
      "Epoch 1589/10000\n",
      "2700/2700 [==============================] - 0s 23us/step - loss: 4.2798e-04 - mean_absolute_error: 0.0131\n",
      "\n",
      "Epoch 01589: loss did not improve\n",
      "Epoch 1590/10000\n",
      "2700/2700 [==============================] - 0s 22us/step - loss: 4.2550e-04 - mean_absolute_error: 0.0130\n",
      "\n",
      "Epoch 01590: loss did not improve\n",
      "Epoch 1591/10000\n",
      "2700/2700 [==============================] - 0s 21us/step - loss: 4.2441e-04 - mean_absolute_error: 0.0130\n",
      "\n",
      "Epoch 01591: loss did not improve\n",
      "Epoch 1592/10000\n",
      "2700/2700 [==============================] - 0s 21us/step - loss: 4.3145e-04 - mean_absolute_error: 0.0131\n",
      "\n",
      "Epoch 01592: loss did not improve\n",
      "Epoch 1593/10000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2700/2700 [==============================] - 0s 21us/step - loss: 4.2443e-04 - mean_absolute_error: 0.0129\n",
      "\n",
      "Epoch 01593: loss did not improve\n",
      "Epoch 1594/10000\n",
      "2700/2700 [==============================] - 0s 21us/step - loss: 4.2442e-04 - mean_absolute_error: 0.0132\n",
      "\n",
      "Epoch 01594: loss did not improve\n",
      "Epoch 1595/10000\n",
      "2700/2700 [==============================] - 0s 22us/step - loss: 4.3022e-04 - mean_absolute_error: 0.0131\n",
      "\n",
      "Epoch 01595: loss did not improve\n",
      "Epoch 1596/10000\n",
      "2700/2700 [==============================] - 0s 24us/step - loss: 4.2335e-04 - mean_absolute_error: 0.0129\n",
      "\n",
      "Epoch 01596: loss did not improve\n",
      "Epoch 1597/10000\n",
      "2700/2700 [==============================] - 0s 23us/step - loss: 4.2509e-04 - mean_absolute_error: 0.0130\n",
      "\n",
      "Epoch 01597: loss did not improve\n",
      "Epoch 1598/10000\n",
      "2700/2700 [==============================] - 0s 24us/step - loss: 4.2485e-04 - mean_absolute_error: 0.0129\n",
      "\n",
      "Epoch 01598: loss did not improve\n",
      "Epoch 1599/10000\n",
      "2700/2700 [==============================] - 0s 24us/step - loss: 4.3456e-04 - mean_absolute_error: 0.0133\n",
      "\n",
      "Epoch 01599: loss did not improve\n",
      "Epoch 1600/10000\n",
      "2700/2700 [==============================] - 0s 21us/step - loss: 4.3460e-04 - mean_absolute_error: 0.0133\n",
      "\n",
      "Epoch 01600: loss did not improve\n",
      "Epoch 1601/10000\n",
      "2700/2700 [==============================] - 0s 27us/step - loss: 4.2997e-04 - mean_absolute_error: 0.0132\n",
      "\n",
      "Epoch 01601: loss did not improve\n",
      "Epoch 1602/10000\n",
      "2700/2700 [==============================] - 0s 23us/step - loss: 4.3216e-04 - mean_absolute_error: 0.0131\n",
      "\n",
      "Epoch 01602: loss did not improve\n",
      "Epoch 1603/10000\n",
      "2700/2700 [==============================] - 0s 26us/step - loss: 4.2380e-04 - mean_absolute_error: 0.0130\n",
      "\n",
      "Epoch 01603: loss did not improve\n",
      "Epoch 1604/10000\n",
      "2700/2700 [==============================] - 0s 23us/step - loss: 4.2923e-04 - mean_absolute_error: 0.0131\n",
      "\n",
      "Epoch 01604: loss did not improve\n",
      "Epoch 1605/10000\n",
      "2700/2700 [==============================] - 0s 21us/step - loss: 4.2364e-04 - mean_absolute_error: 0.0129\n",
      "\n",
      "Epoch 01605: loss did not improve\n",
      "Epoch 1606/10000\n",
      "2700/2700 [==============================] - 0s 21us/step - loss: 4.2475e-04 - mean_absolute_error: 0.0130\n",
      "\n",
      "Epoch 01606: loss did not improve\n",
      "Epoch 1607/10000\n",
      "2700/2700 [==============================] - 0s 21us/step - loss: 4.2628e-04 - mean_absolute_error: 0.0131\n",
      "\n",
      "Epoch 01607: loss did not improve\n",
      "Epoch 1608/10000\n",
      "2700/2700 [==============================] - 0s 21us/step - loss: 4.3071e-04 - mean_absolute_error: 0.0133\n",
      "\n",
      "Epoch 01608: loss did not improve\n",
      "Epoch 1609/10000\n",
      "2700/2700 [==============================] - 0s 21us/step - loss: 4.2335e-04 - mean_absolute_error: 0.0130\n",
      "\n",
      "Epoch 01609: loss did not improve\n",
      "Epoch 1610/10000\n",
      "2700/2700 [==============================] - 0s 28us/step - loss: 4.2993e-04 - mean_absolute_error: 0.0131\n",
      "\n",
      "Epoch 01610: loss did not improve\n",
      "Epoch 1611/10000\n",
      "2700/2700 [==============================] - 0s 21us/step - loss: 4.3040e-04 - mean_absolute_error: 0.0131\n",
      "\n",
      "Epoch 01611: loss did not improve\n",
      "Epoch 1612/10000\n",
      "2700/2700 [==============================] - 0s 25us/step - loss: 4.2467e-04 - mean_absolute_error: 0.0131\n",
      "\n",
      "Epoch 01612: loss did not improve\n",
      "Epoch 1613/10000\n",
      "2700/2700 [==============================] - 0s 22us/step - loss: 4.3163e-04 - mean_absolute_error: 0.0133\n",
      "\n",
      "Epoch 01613: loss did not improve\n",
      "Epoch 1614/10000\n",
      "2700/2700 [==============================] - 0s 29us/step - loss: 4.2480e-04 - mean_absolute_error: 0.0129\n",
      "\n",
      "Epoch 01614: loss did not improve\n",
      "Epoch 1615/10000\n",
      "2700/2700 [==============================] - 0s 33us/step - loss: 4.2509e-04 - mean_absolute_error: 0.0130\n",
      "\n",
      "Epoch 01615: loss did not improve\n",
      "Epoch 1616/10000\n",
      "2700/2700 [==============================] - 0s 23us/step - loss: 4.2617e-04 - mean_absolute_error: 0.0131\n",
      "\n",
      "Epoch 01616: loss did not improve\n",
      "Epoch 1617/10000\n",
      "2700/2700 [==============================] - 0s 21us/step - loss: 4.2861e-04 - mean_absolute_error: 0.0134\n",
      "\n",
      "Epoch 01617: loss did not improve\n",
      "Epoch 1618/10000\n",
      "2700/2700 [==============================] - 0s 21us/step - loss: 4.3967e-04 - mean_absolute_error: 0.0135\n",
      "\n",
      "Epoch 01618: loss did not improve\n",
      "Epoch 1619/10000\n",
      "2700/2700 [==============================] - 0s 21us/step - loss: 4.2805e-04 - mean_absolute_error: 0.0131\n",
      "\n",
      "Epoch 01619: loss did not improve\n",
      "Epoch 1620/10000\n",
      "2700/2700 [==============================] - 0s 22us/step - loss: 4.3254e-04 - mean_absolute_error: 0.0132\n",
      "\n",
      "Epoch 01620: loss did not improve\n",
      "Epoch 1621/10000\n",
      "2700/2700 [==============================] - 0s 21us/step - loss: 4.2375e-04 - mean_absolute_error: 0.0129\n",
      "\n",
      "Epoch 01621: loss did not improve\n",
      "Epoch 1622/10000\n",
      "2700/2700 [==============================] - 0s 22us/step - loss: 4.2594e-04 - mean_absolute_error: 0.0131\n",
      "\n",
      "Epoch 01622: loss did not improve\n",
      "Epoch 1623/10000\n",
      "2700/2700 [==============================] - 0s 24us/step - loss: 4.2751e-04 - mean_absolute_error: 0.0132\n",
      "\n",
      "Epoch 01623: loss did not improve\n",
      "Epoch 1624/10000\n",
      "2700/2700 [==============================] - 0s 22us/step - loss: 4.2511e-04 - mean_absolute_error: 0.0130\n",
      "\n",
      "Epoch 01624: loss did not improve\n",
      "Epoch 1625/10000\n",
      "2700/2700 [==============================] - 0s 21us/step - loss: 4.2488e-04 - mean_absolute_error: 0.0129\n",
      "\n",
      "Epoch 01625: loss did not improve\n",
      "Epoch 1626/10000\n",
      "2700/2700 [==============================] - 0s 21us/step - loss: 4.2429e-04 - mean_absolute_error: 0.0129\n",
      "\n",
      "Epoch 01626: loss did not improve\n",
      "Epoch 1627/10000\n",
      "2700/2700 [==============================] - 0s 22us/step - loss: 4.2511e-04 - mean_absolute_error: 0.0130\n",
      "\n",
      "Epoch 01627: loss did not improve\n",
      "Epoch 1628/10000\n",
      "2700/2700 [==============================] - 0s 21us/step - loss: 4.2514e-04 - mean_absolute_error: 0.0130\n",
      "\n",
      "Epoch 01628: loss did not improve\n",
      "Epoch 1629/10000\n",
      "2700/2700 [==============================] - 0s 21us/step - loss: 4.2270e-04 - mean_absolute_error: 0.0130\n",
      "\n",
      "Epoch 01629: loss improved from 0.00042 to 0.00042, saving model to keras/best_baseline_weights.hdf5\n",
      "Epoch 1630/10000\n",
      "2700/2700 [==============================] - 0s 23us/step - loss: 4.2722e-04 - mean_absolute_error: 0.0130\n",
      "\n",
      "Epoch 01630: loss did not improve\n",
      "Epoch 1631/10000\n",
      "2700/2700 [==============================] - 0s 21us/step - loss: 4.2414e-04 - mean_absolute_error: 0.0130\n",
      "\n",
      "Epoch 01631: loss did not improve\n",
      "Epoch 1632/10000\n",
      "2700/2700 [==============================] - 0s 23us/step - loss: 4.2891e-04 - mean_absolute_error: 0.0133\n",
      "\n",
      "Epoch 01632: loss did not improve\n",
      "Epoch 1633/10000\n",
      "2700/2700 [==============================] - 0s 22us/step - loss: 4.2096e-04 - mean_absolute_error: 0.0129\n",
      "\n",
      "Epoch 01633: loss improved from 0.00042 to 0.00042, saving model to keras/best_baseline_weights.hdf5\n",
      "Epoch 1634/10000\n",
      "2700/2700 [==============================] - 0s 22us/step - loss: 4.2305e-04 - mean_absolute_error: 0.0129\n",
      "\n",
      "Epoch 01634: loss did not improve\n",
      "Epoch 1635/10000\n",
      "2700/2700 [==============================] - 0s 23us/step - loss: 4.2180e-04 - mean_absolute_error: 0.0130\n",
      "\n",
      "Epoch 01635: loss did not improve\n",
      "Epoch 1636/10000\n",
      "2700/2700 [==============================] - 0s 21us/step - loss: 4.2766e-04 - mean_absolute_error: 0.0131\n",
      "\n",
      "Epoch 01636: loss did not improve\n",
      "Epoch 1637/10000\n",
      "2700/2700 [==============================] - 0s 21us/step - loss: 4.2670e-04 - mean_absolute_error: 0.0131\n",
      "\n",
      "Epoch 01637: loss did not improve\n",
      "Epoch 1638/10000\n",
      "2700/2700 [==============================] - 0s 21us/step - loss: 4.2205e-04 - mean_absolute_error: 0.0129\n",
      "\n",
      "Epoch 01638: loss did not improve\n",
      "Epoch 1639/10000\n",
      "2700/2700 [==============================] - 0s 22us/step - loss: 4.2333e-04 - mean_absolute_error: 0.0129\n",
      "\n",
      "Epoch 01639: loss did not improve\n",
      "Epoch 1640/10000\n",
      "2700/2700 [==============================] - 0s 23us/step - loss: 4.2508e-04 - mean_absolute_error: 0.0130\n",
      "\n",
      "Epoch 01640: loss did not improve\n",
      "Epoch 1641/10000\n",
      "2700/2700 [==============================] - 0s 22us/step - loss: 4.2844e-04 - mean_absolute_error: 0.0131\n",
      "\n",
      "Epoch 01641: loss did not improve\n",
      "Epoch 1642/10000\n",
      "2700/2700 [==============================] - 0s 21us/step - loss: 4.3786e-04 - mean_absolute_error: 0.0136\n",
      "\n",
      "Epoch 01642: loss did not improve\n",
      "Epoch 1643/10000\n",
      "2700/2700 [==============================] - 0s 22us/step - loss: 4.2255e-04 - mean_absolute_error: 0.0130\n",
      "\n",
      "Epoch 01643: loss did not improve\n",
      "Epoch 1644/10000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2700/2700 [==============================] - 0s 22us/step - loss: 4.2966e-04 - mean_absolute_error: 0.0131\n",
      "\n",
      "Epoch 01644: loss did not improve\n",
      "Epoch 1645/10000\n",
      "2700/2700 [==============================] - 0s 21us/step - loss: 4.2471e-04 - mean_absolute_error: 0.0130\n",
      "\n",
      "Epoch 01645: loss did not improve\n",
      "Epoch 1646/10000\n",
      "2700/2700 [==============================] - 0s 22us/step - loss: 4.2273e-04 - mean_absolute_error: 0.0130\n",
      "\n",
      "Epoch 01646: loss did not improve\n",
      "Epoch 1647/10000\n",
      "2700/2700 [==============================] - 0s 21us/step - loss: 4.2579e-04 - mean_absolute_error: 0.0130\n",
      "\n",
      "Epoch 01647: loss did not improve\n",
      "Epoch 1648/10000\n",
      "2700/2700 [==============================] - 0s 22us/step - loss: 4.2438e-04 - mean_absolute_error: 0.0129\n",
      "\n",
      "Epoch 01648: loss did not improve\n",
      "Epoch 1649/10000\n",
      "2700/2700 [==============================] - 0s 22us/step - loss: 4.2491e-04 - mean_absolute_error: 0.0130\n",
      "\n",
      "Epoch 01649: loss did not improve\n",
      "Epoch 1650/10000\n",
      "2700/2700 [==============================] - 0s 22us/step - loss: 4.2190e-04 - mean_absolute_error: 0.0129\n",
      "\n",
      "Epoch 01650: loss did not improve\n",
      "Epoch 1651/10000\n",
      "2700/2700 [==============================] - 0s 21us/step - loss: 4.2779e-04 - mean_absolute_error: 0.0132\n",
      "\n",
      "Epoch 01651: loss did not improve\n",
      "Epoch 1652/10000\n",
      "2700/2700 [==============================] - 0s 21us/step - loss: 4.2348e-04 - mean_absolute_error: 0.0129\n",
      "\n",
      "Epoch 01652: loss did not improve\n",
      "Epoch 1653/10000\n",
      "2700/2700 [==============================] - 0s 22us/step - loss: 4.2745e-04 - mean_absolute_error: 0.0130\n",
      "\n",
      "Epoch 01653: loss did not improve\n",
      "Epoch 1654/10000\n",
      "2700/2700 [==============================] - 0s 22us/step - loss: 4.2910e-04 - mean_absolute_error: 0.0133\n",
      "\n",
      "Epoch 01654: loss did not improve\n",
      "Epoch 1655/10000\n",
      "2700/2700 [==============================] - 0s 22us/step - loss: 4.2513e-04 - mean_absolute_error: 0.0130\n",
      "\n",
      "Epoch 01655: loss did not improve\n",
      "Epoch 1656/10000\n",
      "2700/2700 [==============================] - 0s 21us/step - loss: 4.2766e-04 - mean_absolute_error: 0.0131\n",
      "\n",
      "Epoch 01656: loss did not improve\n",
      "Epoch 1657/10000\n",
      "2700/2700 [==============================] - 0s 21us/step - loss: 4.2723e-04 - mean_absolute_error: 0.0130\n",
      "\n",
      "Epoch 01657: loss did not improve\n",
      "Epoch 1658/10000\n",
      "2700/2700 [==============================] - 0s 23us/step - loss: 4.2508e-04 - mean_absolute_error: 0.0130\n",
      "\n",
      "Epoch 01658: loss did not improve\n",
      "Epoch 1659/10000\n",
      "2700/2700 [==============================] - 0s 22us/step - loss: 4.2508e-04 - mean_absolute_error: 0.0130\n",
      "\n",
      "Epoch 01659: loss did not improve\n",
      "Epoch 1660/10000\n",
      "2700/2700 [==============================] - 0s 21us/step - loss: 4.2220e-04 - mean_absolute_error: 0.0129\n",
      "\n",
      "Epoch 01660: loss did not improve\n",
      "Epoch 1661/10000\n",
      "2700/2700 [==============================] - 0s 21us/step - loss: 4.2242e-04 - mean_absolute_error: 0.0129\n",
      "\n",
      "Epoch 01661: loss did not improve\n",
      "Epoch 1662/10000\n",
      "2700/2700 [==============================] - 0s 21us/step - loss: 4.2350e-04 - mean_absolute_error: 0.0130\n",
      "\n",
      "Epoch 01662: loss did not improve\n",
      "Epoch 1663/10000\n",
      "2700/2700 [==============================] - 0s 21us/step - loss: 4.3405e-04 - mean_absolute_error: 0.0134\n",
      "\n",
      "Epoch 01663: loss did not improve\n",
      "Epoch 1664/10000\n",
      "2700/2700 [==============================] - 0s 21us/step - loss: 4.2083e-04 - mean_absolute_error: 0.0129\n",
      "\n",
      "Epoch 01664: loss improved from 0.00042 to 0.00042, saving model to keras/best_baseline_weights.hdf5\n",
      "Epoch 1665/10000\n",
      "2700/2700 [==============================] - 0s 21us/step - loss: 4.2476e-04 - mean_absolute_error: 0.0130\n",
      "\n",
      "Epoch 01665: loss did not improve\n",
      "Epoch 1666/10000\n",
      "2700/2700 [==============================] - 0s 21us/step - loss: 4.2223e-04 - mean_absolute_error: 0.0129\n",
      "\n",
      "Epoch 01666: loss did not improve\n",
      "Epoch 1667/10000\n",
      "2700/2700 [==============================] - 0s 24us/step - loss: 4.2380e-04 - mean_absolute_error: 0.0130\n",
      "\n",
      "Epoch 01667: loss did not improve\n",
      "Epoch 1668/10000\n",
      "2700/2700 [==============================] - 0s 22us/step - loss: 4.2296e-04 - mean_absolute_error: 0.0129\n",
      "\n",
      "Epoch 01668: loss did not improve\n",
      "Epoch 1669/10000\n",
      "2700/2700 [==============================] - 0s 21us/step - loss: 4.2205e-04 - mean_absolute_error: 0.0129\n",
      "\n",
      "Epoch 01669: loss did not improve\n",
      "Epoch 1670/10000\n",
      "2700/2700 [==============================] - 0s 21us/step - loss: 4.2442e-04 - mean_absolute_error: 0.0130\n",
      "\n",
      "Epoch 01670: loss did not improve\n",
      "Epoch 1671/10000\n",
      "2700/2700 [==============================] - 0s 21us/step - loss: 4.2499e-04 - mean_absolute_error: 0.0130\n",
      "\n",
      "Epoch 01671: loss did not improve\n",
      "Epoch 1672/10000\n",
      "2700/2700 [==============================] - 0s 22us/step - loss: 4.2126e-04 - mean_absolute_error: 0.0129\n",
      "\n",
      "Epoch 01672: loss did not improve\n",
      "Epoch 1673/10000\n",
      "2700/2700 [==============================] - 0s 24us/step - loss: 4.2519e-04 - mean_absolute_error: 0.0130\n",
      "\n",
      "Epoch 01673: loss did not improve\n",
      "Epoch 1674/10000\n",
      "2700/2700 [==============================] - 0s 22us/step - loss: 4.2488e-04 - mean_absolute_error: 0.0130\n",
      "\n",
      "Epoch 01674: loss did not improve\n",
      "Epoch 1675/10000\n",
      "2700/2700 [==============================] - 0s 22us/step - loss: 4.2696e-04 - mean_absolute_error: 0.0131\n",
      "\n",
      "Epoch 01675: loss did not improve\n",
      "Epoch 1676/10000\n",
      "2700/2700 [==============================] - 0s 23us/step - loss: 4.2454e-04 - mean_absolute_error: 0.0130\n",
      "\n",
      "Epoch 01676: loss did not improve\n",
      "Epoch 1677/10000\n",
      "2700/2700 [==============================] - 0s 21us/step - loss: 4.2325e-04 - mean_absolute_error: 0.0130\n",
      "\n",
      "Epoch 01677: loss did not improve\n",
      "Epoch 1678/10000\n",
      "2700/2700 [==============================] - 0s 21us/step - loss: 4.1964e-04 - mean_absolute_error: 0.0128\n",
      "\n",
      "Epoch 01678: loss improved from 0.00042 to 0.00042, saving model to keras/best_baseline_weights.hdf5\n",
      "Epoch 1679/10000\n",
      "2700/2700 [==============================] - 0s 22us/step - loss: 4.2217e-04 - mean_absolute_error: 0.0129\n",
      "\n",
      "Epoch 01679: loss did not improve\n",
      "Epoch 1680/10000\n",
      "2700/2700 [==============================] - 0s 24us/step - loss: 4.2338e-04 - mean_absolute_error: 0.0130\n",
      "\n",
      "Epoch 01680: loss did not improve\n",
      "Epoch 1681/10000\n",
      "2700/2700 [==============================] - 0s 22us/step - loss: 4.2552e-04 - mean_absolute_error: 0.0130\n",
      "\n",
      "Epoch 01681: loss did not improve\n",
      "Epoch 1682/10000\n",
      "2700/2700 [==============================] - 0s 21us/step - loss: 4.2892e-04 - mean_absolute_error: 0.0133\n",
      "\n",
      "Epoch 01682: loss did not improve\n",
      "Epoch 1683/10000\n",
      "2700/2700 [==============================] - 0s 22us/step - loss: 4.2316e-04 - mean_absolute_error: 0.0129\n",
      "\n",
      "Epoch 01683: loss did not improve\n",
      "Epoch 1684/10000\n",
      "2700/2700 [==============================] - 0s 23us/step - loss: 4.2348e-04 - mean_absolute_error: 0.0130\n",
      "\n",
      "Epoch 01684: loss did not improve\n",
      "Epoch 1685/10000\n",
      "2700/2700 [==============================] - 0s 22us/step - loss: 4.2401e-04 - mean_absolute_error: 0.0130\n",
      "\n",
      "Epoch 01685: loss did not improve\n",
      "Epoch 1686/10000\n",
      "2700/2700 [==============================] - 0s 21us/step - loss: 4.2561e-04 - mean_absolute_error: 0.0129\n",
      "\n",
      "Epoch 01686: loss did not improve\n",
      "Epoch 1687/10000\n",
      "2700/2700 [==============================] - 0s 22us/step - loss: 4.2872e-04 - mean_absolute_error: 0.0131\n",
      "\n",
      "Epoch 01687: loss did not improve\n",
      "Epoch 1688/10000\n",
      "2700/2700 [==============================] - 0s 21us/step - loss: 4.2841e-04 - mean_absolute_error: 0.0131\n",
      "\n",
      "Epoch 01688: loss did not improve\n",
      "Epoch 1689/10000\n",
      "2700/2700 [==============================] - 0s 21us/step - loss: 4.2429e-04 - mean_absolute_error: 0.0131\n",
      "\n",
      "Epoch 01689: loss did not improve\n",
      "Epoch 1690/10000\n",
      "2700/2700 [==============================] - 0s 21us/step - loss: 4.2320e-04 - mean_absolute_error: 0.0130\n",
      "\n",
      "Epoch 01690: loss did not improve\n",
      "Epoch 1691/10000\n",
      "2700/2700 [==============================] - 0s 21us/step - loss: 4.3041e-04 - mean_absolute_error: 0.0132\n",
      "\n",
      "Epoch 01691: loss did not improve\n",
      "Epoch 1692/10000\n",
      "2700/2700 [==============================] - 0s 22us/step - loss: 4.2445e-04 - mean_absolute_error: 0.0130\n",
      "\n",
      "Epoch 01692: loss did not improve\n",
      "Epoch 1693/10000\n",
      "2700/2700 [==============================] - 0s 22us/step - loss: 4.2802e-04 - mean_absolute_error: 0.0132\n",
      "\n",
      "Epoch 01693: loss did not improve\n",
      "Epoch 1694/10000\n",
      "2700/2700 [==============================] - 0s 24us/step - loss: 4.2026e-04 - mean_absolute_error: 0.0128\n",
      "\n",
      "Epoch 01694: loss did not improve\n",
      "Epoch 1695/10000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2700/2700 [==============================] - 0s 23us/step - loss: 4.2137e-04 - mean_absolute_error: 0.0129\n",
      "\n",
      "Epoch 01695: loss did not improve\n",
      "Epoch 1696/10000\n",
      "2700/2700 [==============================] - 0s 22us/step - loss: 4.2445e-04 - mean_absolute_error: 0.0130\n",
      "\n",
      "Epoch 01696: loss did not improve\n",
      "Epoch 1697/10000\n",
      "2700/2700 [==============================] - 0s 21us/step - loss: 4.2428e-04 - mean_absolute_error: 0.0130\n",
      "\n",
      "Epoch 01697: loss did not improve\n",
      "Epoch 1698/10000\n",
      "2700/2700 [==============================] - 0s 21us/step - loss: 4.2185e-04 - mean_absolute_error: 0.0129\n",
      "\n",
      "Epoch 01698: loss did not improve\n",
      "Epoch 1699/10000\n",
      "2700/2700 [==============================] - 0s 21us/step - loss: 4.2541e-04 - mean_absolute_error: 0.0131\n",
      "\n",
      "Epoch 01699: loss did not improve\n",
      "Epoch 1700/10000\n",
      "2700/2700 [==============================] - 0s 22us/step - loss: 4.2468e-04 - mean_absolute_error: 0.0130\n",
      "\n",
      "Epoch 01700: loss did not improve\n",
      "Epoch 1701/10000\n",
      "2700/2700 [==============================] - 0s 22us/step - loss: 4.2605e-04 - mean_absolute_error: 0.0131\n",
      "\n",
      "Epoch 01701: loss did not improve\n",
      "Epoch 1702/10000\n",
      "2700/2700 [==============================] - 0s 23us/step - loss: 4.2573e-04 - mean_absolute_error: 0.0130\n",
      "\n",
      "Epoch 01702: loss did not improve\n",
      "Epoch 1703/10000\n",
      "2700/2700 [==============================] - 0s 22us/step - loss: 4.2093e-04 - mean_absolute_error: 0.0129\n",
      "\n",
      "Epoch 01703: loss did not improve\n",
      "Epoch 1704/10000\n",
      "2700/2700 [==============================] - 0s 21us/step - loss: 4.2187e-04 - mean_absolute_error: 0.0129\n",
      "\n",
      "Epoch 01704: loss did not improve\n",
      "Epoch 1705/10000\n",
      "2700/2700 [==============================] - 0s 21us/step - loss: 4.2568e-04 - mean_absolute_error: 0.0131\n",
      "\n",
      "Epoch 01705: loss did not improve\n",
      "Epoch 1706/10000\n",
      "2700/2700 [==============================] - 0s 21us/step - loss: 4.2673e-04 - mean_absolute_error: 0.0130\n",
      "\n",
      "Epoch 01706: loss did not improve\n",
      "Epoch 1707/10000\n",
      "2700/2700 [==============================] - 0s 22us/step - loss: 4.2131e-04 - mean_absolute_error: 0.0130\n",
      "\n",
      "Epoch 01707: loss did not improve\n",
      "Epoch 1708/10000\n",
      "2700/2700 [==============================] - 0s 21us/step - loss: 4.2209e-04 - mean_absolute_error: 0.0129\n",
      "\n",
      "Epoch 01708: loss did not improve\n",
      "Epoch 1709/10000\n",
      "2700/2700 [==============================] - 0s 21us/step - loss: 4.2465e-04 - mean_absolute_error: 0.0130\n",
      "\n",
      "Epoch 01709: loss did not improve\n",
      "Epoch 1710/10000\n",
      "2700/2700 [==============================] - 0s 21us/step - loss: 4.2427e-04 - mean_absolute_error: 0.0130\n",
      "\n",
      "Epoch 01710: loss did not improve\n",
      "Epoch 1711/10000\n",
      "2700/2700 [==============================] - 0s 23us/step - loss: 4.2158e-04 - mean_absolute_error: 0.0129\n",
      "\n",
      "Epoch 01711: loss did not improve\n",
      "Epoch 1712/10000\n",
      "2700/2700 [==============================] - 0s 22us/step - loss: 4.2509e-04 - mean_absolute_error: 0.0130\n",
      "\n",
      "Epoch 01712: loss did not improve\n",
      "Epoch 1713/10000\n",
      "2700/2700 [==============================] - 0s 21us/step - loss: 4.2342e-04 - mean_absolute_error: 0.0130\n",
      "\n",
      "Epoch 01713: loss did not improve\n",
      "Epoch 1714/10000\n",
      "2700/2700 [==============================] - 0s 21us/step - loss: 4.2778e-04 - mean_absolute_error: 0.0131\n",
      "\n",
      "Epoch 01714: loss did not improve\n",
      "Epoch 1715/10000\n",
      "2700/2700 [==============================] - 0s 23us/step - loss: 4.2441e-04 - mean_absolute_error: 0.0131\n",
      "\n",
      "Epoch 01715: loss did not improve\n",
      "Epoch 1716/10000\n",
      "2700/2700 [==============================] - 0s 21us/step - loss: 4.2206e-04 - mean_absolute_error: 0.0130\n",
      "\n",
      "Epoch 01716: loss did not improve\n",
      "Epoch 1717/10000\n",
      "2700/2700 [==============================] - 0s 21us/step - loss: 4.2367e-04 - mean_absolute_error: 0.0129\n",
      "\n",
      "Epoch 01717: loss did not improve\n",
      "Epoch 1718/10000\n",
      "2700/2700 [==============================] - 0s 21us/step - loss: 4.1831e-04 - mean_absolute_error: 0.0127\n",
      "\n",
      "Epoch 01718: loss improved from 0.00042 to 0.00042, saving model to keras/best_baseline_weights.hdf5\n",
      "Epoch 1719/10000\n",
      "2700/2700 [==============================] - 0s 21us/step - loss: 4.3142e-04 - mean_absolute_error: 0.0135\n",
      "\n",
      "Epoch 01719: loss did not improve\n",
      "Epoch 1720/10000\n",
      "2700/2700 [==============================] - 0s 24us/step - loss: 4.2425e-04 - mean_absolute_error: 0.0131\n",
      "\n",
      "Epoch 01720: loss did not improve\n",
      "Epoch 1721/10000\n",
      "2700/2700 [==============================] - 0s 22us/step - loss: 4.2725e-04 - mean_absolute_error: 0.0130\n",
      "\n",
      "Epoch 01721: loss did not improve\n",
      "Epoch 1722/10000\n",
      "2700/2700 [==============================] - 0s 23us/step - loss: 4.2118e-04 - mean_absolute_error: 0.0128\n",
      "\n",
      "Epoch 01722: loss did not improve\n",
      "Epoch 1723/10000\n",
      "2700/2700 [==============================] - 0s 23us/step - loss: 4.2181e-04 - mean_absolute_error: 0.0130\n",
      "\n",
      "Epoch 01723: loss did not improve\n",
      "Epoch 1724/10000\n",
      "2700/2700 [==============================] - 0s 24us/step - loss: 4.2273e-04 - mean_absolute_error: 0.0129\n",
      "\n",
      "Epoch 01724: loss did not improve\n",
      "Epoch 1725/10000\n",
      "2700/2700 [==============================] - 0s 24us/step - loss: 4.2001e-04 - mean_absolute_error: 0.0128\n",
      "\n",
      "Epoch 01725: loss did not improve\n",
      "Epoch 1726/10000\n",
      "2700/2700 [==============================] - 0s 29us/step - loss: 4.2170e-04 - mean_absolute_error: 0.0130\n",
      "\n",
      "Epoch 01726: loss did not improve\n",
      "Epoch 1727/10000\n",
      "2700/2700 [==============================] - 0s 22us/step - loss: 4.2475e-04 - mean_absolute_error: 0.0131\n",
      "\n",
      "Epoch 01727: loss did not improve\n",
      "Epoch 1728/10000\n",
      "2700/2700 [==============================] - 0s 23us/step - loss: 4.2449e-04 - mean_absolute_error: 0.0131\n",
      "\n",
      "Epoch 01728: loss did not improve\n",
      "Epoch 1729/10000\n",
      "2700/2700 [==============================] - 0s 23us/step - loss: 4.2638e-04 - mean_absolute_error: 0.0131\n",
      "\n",
      "Epoch 01729: loss did not improve\n",
      "Epoch 1730/10000\n",
      "2700/2700 [==============================] - 0s 26us/step - loss: 4.3033e-04 - mean_absolute_error: 0.0132\n",
      "\n",
      "Epoch 01730: loss did not improve\n",
      "Epoch 1731/10000\n",
      "2700/2700 [==============================] - 0s 27us/step - loss: 4.2322e-04 - mean_absolute_error: 0.0130\n",
      "\n",
      "Epoch 01731: loss did not improve\n",
      "Epoch 1732/10000\n",
      "2700/2700 [==============================] - 0s 28us/step - loss: 4.2426e-04 - mean_absolute_error: 0.0131\n",
      "\n",
      "Epoch 01732: loss did not improve\n",
      "Epoch 1733/10000\n",
      "2700/2700 [==============================] - 0s 27us/step - loss: 4.2481e-04 - mean_absolute_error: 0.0131\n",
      "\n",
      "Epoch 01733: loss did not improve\n",
      "Epoch 1734/10000\n",
      "2700/2700 [==============================] - 0s 26us/step - loss: 4.2184e-04 - mean_absolute_error: 0.0128\n",
      "\n",
      "Epoch 01734: loss did not improve\n",
      "Epoch 1735/10000\n",
      "2700/2700 [==============================] - 0s 28us/step - loss: 4.3119e-04 - mean_absolute_error: 0.0133\n",
      "\n",
      "Epoch 01735: loss did not improve\n",
      "Epoch 1736/10000\n",
      "2700/2700 [==============================] - 0s 25us/step - loss: 4.3003e-04 - mean_absolute_error: 0.0134\n",
      "\n",
      "Epoch 01736: loss did not improve\n",
      "Epoch 1737/10000\n",
      "2700/2700 [==============================] - 0s 22us/step - loss: 4.2805e-04 - mean_absolute_error: 0.0131\n",
      "\n",
      "Epoch 01737: loss did not improve\n",
      "Epoch 1738/10000\n",
      "2700/2700 [==============================] - 0s 22us/step - loss: 4.2183e-04 - mean_absolute_error: 0.0130\n",
      "\n",
      "Epoch 01738: loss did not improve\n",
      "Epoch 1739/10000\n",
      "2700/2700 [==============================] - 0s 22us/step - loss: 4.2785e-04 - mean_absolute_error: 0.0131\n",
      "\n",
      "Epoch 01739: loss did not improve\n",
      "Epoch 1740/10000\n",
      "2700/2700 [==============================] - 0s 22us/step - loss: 4.1987e-04 - mean_absolute_error: 0.0129\n",
      "\n",
      "Epoch 01740: loss did not improve\n",
      "Epoch 1741/10000\n",
      "2700/2700 [==============================] - 0s 22us/step - loss: 4.3102e-04 - mean_absolute_error: 0.0132\n",
      "\n",
      "Epoch 01741: loss did not improve\n",
      "Epoch 1742/10000\n",
      "2700/2700 [==============================] - 0s 21us/step - loss: 4.2153e-04 - mean_absolute_error: 0.0130\n",
      "\n",
      "Epoch 01742: loss did not improve\n",
      "Epoch 1743/10000\n",
      "2700/2700 [==============================] - 0s 35us/step - loss: 4.2333e-04 - mean_absolute_error: 0.0129\n",
      "\n",
      "Epoch 01743: loss did not improve\n",
      "Epoch 1744/10000\n",
      "2700/2700 [==============================] - 0s 32us/step - loss: 4.2028e-04 - mean_absolute_error: 0.0129\n",
      "\n",
      "Epoch 01744: loss did not improve\n",
      "Epoch 1745/10000\n",
      "2700/2700 [==============================] - 0s 26us/step - loss: 4.1766e-04 - mean_absolute_error: 0.0128\n",
      "\n",
      "Epoch 01745: loss improved from 0.00042 to 0.00042, saving model to keras/best_baseline_weights.hdf5\n",
      "Epoch 1746/10000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2700/2700 [==============================] - 0s 29us/step - loss: 4.2251e-04 - mean_absolute_error: 0.0130\n",
      "\n",
      "Epoch 01746: loss did not improve\n",
      "Epoch 1747/10000\n",
      "2700/2700 [==============================] - 0s 34us/step - loss: 4.2298e-04 - mean_absolute_error: 0.0129\n",
      "\n",
      "Epoch 01747: loss did not improve\n",
      "Epoch 1748/10000\n",
      "2700/2700 [==============================] - 0s 27us/step - loss: 4.2516e-04 - mean_absolute_error: 0.0130\n",
      "\n",
      "Epoch 01748: loss did not improve\n",
      "Epoch 1749/10000\n",
      "2700/2700 [==============================] - 0s 30us/step - loss: 4.3008e-04 - mean_absolute_error: 0.0132\n",
      "\n",
      "Epoch 01749: loss did not improve\n",
      "Epoch 1750/10000\n",
      "2700/2700 [==============================] - 0s 29us/step - loss: 4.1986e-04 - mean_absolute_error: 0.0129\n",
      "\n",
      "Epoch 01750: loss did not improve\n",
      "Epoch 1751/10000\n",
      "2700/2700 [==============================] - 0s 26us/step - loss: 4.2124e-04 - mean_absolute_error: 0.0130\n",
      "\n",
      "Epoch 01751: loss did not improve\n",
      "Epoch 1752/10000\n",
      "2700/2700 [==============================] - 0s 29us/step - loss: 4.1827e-04 - mean_absolute_error: 0.0127\n",
      "\n",
      "Epoch 01752: loss did not improve\n",
      "Epoch 1753/10000\n",
      "2700/2700 [==============================] - 0s 26us/step - loss: 4.1940e-04 - mean_absolute_error: 0.0128\n",
      "\n",
      "Epoch 01753: loss did not improve\n",
      "Epoch 1754/10000\n",
      "2700/2700 [==============================] - 0s 34us/step - loss: 4.2632e-04 - mean_absolute_error: 0.0131\n",
      "\n",
      "Epoch 01754: loss did not improve\n",
      "Epoch 1755/10000\n",
      "2700/2700 [==============================] - 0s 32us/step - loss: 4.2049e-04 - mean_absolute_error: 0.0128\n",
      "\n",
      "Epoch 01755: loss did not improve\n",
      "Epoch 1756/10000\n",
      "2700/2700 [==============================] - 0s 35us/step - loss: 4.2076e-04 - mean_absolute_error: 0.0129\n",
      "\n",
      "Epoch 01756: loss did not improve\n",
      "Epoch 1757/10000\n",
      "2700/2700 [==============================] - 0s 34us/step - loss: 4.2130e-04 - mean_absolute_error: 0.0131\n",
      "\n",
      "Epoch 01757: loss did not improve\n",
      "Epoch 1758/10000\n",
      "2700/2700 [==============================] - 0s 34us/step - loss: 4.1896e-04 - mean_absolute_error: 0.0128\n",
      "\n",
      "Epoch 01758: loss did not improve\n",
      "Epoch 1759/10000\n",
      "2700/2700 [==============================] - 0s 33us/step - loss: 4.2287e-04 - mean_absolute_error: 0.0130\n",
      "\n",
      "Epoch 01759: loss did not improve\n",
      "Epoch 1760/10000\n",
      "2700/2700 [==============================] - 0s 33us/step - loss: 4.1728e-04 - mean_absolute_error: 0.0127\n",
      "\n",
      "Epoch 01760: loss improved from 0.00042 to 0.00042, saving model to keras/best_baseline_weights.hdf5\n",
      "Epoch 1761/10000\n",
      "2700/2700 [==============================] - 0s 37us/step - loss: 4.2522e-04 - mean_absolute_error: 0.0130\n",
      "\n",
      "Epoch 01761: loss did not improve\n",
      "Epoch 1762/10000\n",
      "2700/2700 [==============================] - 0s 37us/step - loss: 4.1593e-04 - mean_absolute_error: 0.0127\n",
      "\n",
      "Epoch 01762: loss improved from 0.00042 to 0.00042, saving model to keras/best_baseline_weights.hdf5\n",
      "Epoch 1763/10000\n",
      "2700/2700 [==============================] - 0s 36us/step - loss: 4.2300e-04 - mean_absolute_error: 0.0129\n",
      "\n",
      "Epoch 01763: loss did not improve\n",
      "Epoch 1764/10000\n",
      "2700/2700 [==============================] - 0s 40us/step - loss: 4.2782e-04 - mean_absolute_error: 0.0131\n",
      "\n",
      "Epoch 01764: loss did not improve\n",
      "Epoch 1765/10000\n",
      "2700/2700 [==============================] - 0s 34us/step - loss: 4.2261e-04 - mean_absolute_error: 0.0129\n",
      "\n",
      "Epoch 01765: loss did not improve\n",
      "Epoch 1766/10000\n",
      "2700/2700 [==============================] - 0s 34us/step - loss: 4.3025e-04 - mean_absolute_error: 0.0132\n",
      "\n",
      "Epoch 01766: loss did not improve\n",
      "Epoch 1767/10000\n",
      "2700/2700 [==============================] - 0s 32us/step - loss: 4.1853e-04 - mean_absolute_error: 0.0127\n",
      "\n",
      "Epoch 01767: loss did not improve\n",
      "Epoch 1768/10000\n",
      "2700/2700 [==============================] - 0s 30us/step - loss: 4.2128e-04 - mean_absolute_error: 0.0129\n",
      "\n",
      "Epoch 01768: loss did not improve\n",
      "Epoch 1769/10000\n",
      "2700/2700 [==============================] - 0s 28us/step - loss: 4.1797e-04 - mean_absolute_error: 0.0128\n",
      "\n",
      "Epoch 01769: loss did not improve\n",
      "Epoch 1770/10000\n",
      "2700/2700 [==============================] - 0s 28us/step - loss: 4.1926e-04 - mean_absolute_error: 0.0129\n",
      "\n",
      "Epoch 01770: loss did not improve\n",
      "Epoch 1771/10000\n",
      "2700/2700 [==============================] - 0s 28us/step - loss: 4.2467e-04 - mean_absolute_error: 0.0131\n",
      "\n",
      "Epoch 01771: loss did not improve\n",
      "Epoch 1772/10000\n",
      "2700/2700 [==============================] - 0s 28us/step - loss: 4.2219e-04 - mean_absolute_error: 0.0129\n",
      "\n",
      "Epoch 01772: loss did not improve\n",
      "Epoch 1773/10000\n",
      "2700/2700 [==============================] - 0s 26us/step - loss: 4.2378e-04 - mean_absolute_error: 0.0130\n",
      "\n",
      "Epoch 01773: loss did not improve\n",
      "Epoch 1774/10000\n",
      "2700/2700 [==============================] - 0s 27us/step - loss: 4.2278e-04 - mean_absolute_error: 0.0130\n",
      "\n",
      "Epoch 01774: loss did not improve\n",
      "Epoch 1775/10000\n",
      "2700/2700 [==============================] - 0s 29us/step - loss: 4.1634e-04 - mean_absolute_error: 0.0128\n",
      "\n",
      "Epoch 01775: loss did not improve\n",
      "Epoch 1776/10000\n",
      "2700/2700 [==============================] - 0s 28us/step - loss: 4.2096e-04 - mean_absolute_error: 0.0129\n",
      "\n",
      "Epoch 01776: loss did not improve\n",
      "Epoch 1777/10000\n",
      "2700/2700 [==============================] - 0s 28us/step - loss: 4.2068e-04 - mean_absolute_error: 0.0128\n",
      "\n",
      "Epoch 01777: loss did not improve\n",
      "Epoch 1778/10000\n",
      "2700/2700 [==============================] - 0s 29us/step - loss: 4.2572e-04 - mean_absolute_error: 0.0130\n",
      "\n",
      "Epoch 01778: loss did not improve\n",
      "Epoch 1779/10000\n",
      "2700/2700 [==============================] - 0s 29us/step - loss: 4.2375e-04 - mean_absolute_error: 0.0129\n",
      "\n",
      "Epoch 01779: loss did not improve\n",
      "Epoch 1780/10000\n",
      "2700/2700 [==============================] - 0s 28us/step - loss: 4.2364e-04 - mean_absolute_error: 0.0131\n",
      "\n",
      "Epoch 01780: loss did not improve\n",
      "Epoch 1781/10000\n",
      "2700/2700 [==============================] - 0s 29us/step - loss: 4.2698e-04 - mean_absolute_error: 0.0131\n",
      "\n",
      "Epoch 01781: loss did not improve\n",
      "Epoch 1782/10000\n",
      "2700/2700 [==============================] - 0s 28us/step - loss: 4.2246e-04 - mean_absolute_error: 0.0129\n",
      "\n",
      "Epoch 01782: loss did not improve\n",
      "Epoch 1783/10000\n",
      "2700/2700 [==============================] - 0s 27us/step - loss: 4.1857e-04 - mean_absolute_error: 0.0129\n",
      "\n",
      "Epoch 01783: loss did not improve\n",
      "Epoch 1784/10000\n",
      "2700/2700 [==============================] - 0s 26us/step - loss: 4.2149e-04 - mean_absolute_error: 0.0129\n",
      "\n",
      "Epoch 01784: loss did not improve\n",
      "Epoch 1785/10000\n",
      "2700/2700 [==============================] - 0s 27us/step - loss: 4.2604e-04 - mean_absolute_error: 0.0131\n",
      "\n",
      "Epoch 01785: loss did not improve\n",
      "Epoch 1786/10000\n",
      "2700/2700 [==============================] - 0s 27us/step - loss: 4.2654e-04 - mean_absolute_error: 0.0131\n",
      "\n",
      "Epoch 01786: loss did not improve\n",
      "Epoch 1787/10000\n",
      "2700/2700 [==============================] - 0s 28us/step - loss: 4.2521e-04 - mean_absolute_error: 0.0132\n",
      "\n",
      "Epoch 01787: loss did not improve\n",
      "Epoch 1788/10000\n",
      "2700/2700 [==============================] - 0s 27us/step - loss: 4.1824e-04 - mean_absolute_error: 0.0129\n",
      "\n",
      "Epoch 01788: loss did not improve\n",
      "Epoch 1789/10000\n",
      "2700/2700 [==============================] - 0s 28us/step - loss: 4.2066e-04 - mean_absolute_error: 0.0129\n",
      "\n",
      "Epoch 01789: loss did not improve\n",
      "Epoch 1790/10000\n",
      "2700/2700 [==============================] - 0s 26us/step - loss: 4.1856e-04 - mean_absolute_error: 0.0128\n",
      "\n",
      "Epoch 01790: loss did not improve\n",
      "Epoch 1791/10000\n",
      "2700/2700 [==============================] - 0s 31us/step - loss: 4.1885e-04 - mean_absolute_error: 0.0128\n",
      "\n",
      "Epoch 01791: loss did not improve\n",
      "Epoch 1792/10000\n",
      "2700/2700 [==============================] - 0s 31us/step - loss: 4.2228e-04 - mean_absolute_error: 0.0129\n",
      "\n",
      "Epoch 01792: loss did not improve\n",
      "Epoch 1793/10000\n",
      "2700/2700 [==============================] - 0s 43us/step - loss: 4.2022e-04 - mean_absolute_error: 0.0129\n",
      "\n",
      "Epoch 01793: loss did not improve\n",
      "Epoch 1794/10000\n",
      "2700/2700 [==============================] - 0s 36us/step - loss: 4.2570e-04 - mean_absolute_error: 0.0130\n",
      "\n",
      "Epoch 01794: loss did not improve\n",
      "Epoch 1795/10000\n",
      "2700/2700 [==============================] - 0s 31us/step - loss: 4.2090e-04 - mean_absolute_error: 0.0129\n",
      "\n",
      "Epoch 01795: loss did not improve\n",
      "Epoch 1796/10000\n",
      "2700/2700 [==============================] - 0s 30us/step - loss: 4.2142e-04 - mean_absolute_error: 0.0129\n",
      "\n",
      "Epoch 01796: loss did not improve\n",
      "Epoch 1797/10000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2700/2700 [==============================] - 0s 28us/step - loss: 4.2063e-04 - mean_absolute_error: 0.0129\n",
      "\n",
      "Epoch 01797: loss did not improve\n",
      "Epoch 1798/10000\n",
      "2700/2700 [==============================] - 0s 29us/step - loss: 4.1935e-04 - mean_absolute_error: 0.0128\n",
      "\n",
      "Epoch 01798: loss did not improve\n",
      "Epoch 1799/10000\n",
      "2700/2700 [==============================] - 0s 28us/step - loss: 4.2357e-04 - mean_absolute_error: 0.0131\n",
      "\n",
      "Epoch 01799: loss did not improve\n",
      "Epoch 1800/10000\n",
      "2700/2700 [==============================] - 0s 27us/step - loss: 4.2402e-04 - mean_absolute_error: 0.0131\n",
      "\n",
      "Epoch 01800: loss did not improve\n",
      "Epoch 1801/10000\n",
      "2700/2700 [==============================] - 0s 27us/step - loss: 4.1764e-04 - mean_absolute_error: 0.0128\n",
      "\n",
      "Epoch 01801: loss did not improve\n",
      "Epoch 1802/10000\n",
      "2700/2700 [==============================] - 0s 26us/step - loss: 4.1832e-04 - mean_absolute_error: 0.0129\n",
      "\n",
      "Epoch 01802: loss did not improve\n",
      "Epoch 1803/10000\n",
      "2700/2700 [==============================] - 0s 28us/step - loss: 4.2086e-04 - mean_absolute_error: 0.0130\n",
      "\n",
      "Epoch 01803: loss did not improve\n",
      "Epoch 1804/10000\n",
      "2700/2700 [==============================] - 0s 26us/step - loss: 4.1713e-04 - mean_absolute_error: 0.0128\n",
      "\n",
      "Epoch 01804: loss did not improve\n",
      "Epoch 1805/10000\n",
      "2700/2700 [==============================] - 0s 27us/step - loss: 4.1699e-04 - mean_absolute_error: 0.0127\n",
      "\n",
      "Epoch 01805: loss did not improve\n",
      "Epoch 1806/10000\n",
      "2700/2700 [==============================] - 0s 27us/step - loss: 4.1895e-04 - mean_absolute_error: 0.0129: 0s - loss: 4.1142e-04 - mean_absolute_error: 0.012\n",
      "\n",
      "Epoch 01806: loss did not improve\n",
      "Epoch 1807/10000\n",
      "2700/2700 [==============================] - 0s 28us/step - loss: 4.1810e-04 - mean_absolute_error: 0.0129\n",
      "\n",
      "Epoch 01807: loss did not improve\n",
      "Epoch 1808/10000\n",
      "2700/2700 [==============================] - 0s 29us/step - loss: 4.1614e-04 - mean_absolute_error: 0.0127\n",
      "\n",
      "Epoch 01808: loss did not improve\n",
      "Epoch 1809/10000\n",
      "2700/2700 [==============================] - 0s 27us/step - loss: 4.1898e-04 - mean_absolute_error: 0.0128\n",
      "\n",
      "Epoch 01809: loss did not improve\n",
      "Epoch 1810/10000\n",
      "2700/2700 [==============================] - 0s 26us/step - loss: 4.1645e-04 - mean_absolute_error: 0.0127\n",
      "\n",
      "Epoch 01810: loss did not improve\n",
      "Epoch 1811/10000\n",
      "2700/2700 [==============================] - 0s 27us/step - loss: 4.1738e-04 - mean_absolute_error: 0.0128\n",
      "\n",
      "Epoch 01811: loss did not improve\n",
      "Epoch 1812/10000\n",
      "2700/2700 [==============================] - 0s 28us/step - loss: 4.2102e-04 - mean_absolute_error: 0.0129\n",
      "\n",
      "Epoch 01812: loss did not improve\n",
      "Epoch 1813/10000\n",
      "2700/2700 [==============================] - 0s 27us/step - loss: 4.2225e-04 - mean_absolute_error: 0.0128\n",
      "\n",
      "Epoch 01813: loss did not improve\n",
      "Epoch 1814/10000\n",
      "2700/2700 [==============================] - 0s 27us/step - loss: 4.2029e-04 - mean_absolute_error: 0.0129\n",
      "\n",
      "Epoch 01814: loss did not improve\n",
      "Epoch 1815/10000\n",
      "2700/2700 [==============================] - 0s 29us/step - loss: 4.1627e-04 - mean_absolute_error: 0.0128: 0s - loss: 4.0892e-04 - mean_absolute_error: 0.012\n",
      "\n",
      "Epoch 01815: loss did not improve\n",
      "Epoch 1816/10000\n",
      "2700/2700 [==============================] - 0s 29us/step - loss: 4.1985e-04 - mean_absolute_error: 0.0128\n",
      "\n",
      "Epoch 01816: loss did not improve\n",
      "Epoch 1817/10000\n",
      "2700/2700 [==============================] - 0s 29us/step - loss: 4.1904e-04 - mean_absolute_error: 0.0128\n",
      "\n",
      "Epoch 01817: loss did not improve\n",
      "Epoch 1818/10000\n",
      "2700/2700 [==============================] - 0s 28us/step - loss: 4.2522e-04 - mean_absolute_error: 0.0130\n",
      "\n",
      "Epoch 01818: loss did not improve\n",
      "Epoch 1819/10000\n",
      "2700/2700 [==============================] - 0s 28us/step - loss: 4.2793e-04 - mean_absolute_error: 0.0132\n",
      "\n",
      "Epoch 01819: loss did not improve\n",
      "Epoch 1820/10000\n",
      "2700/2700 [==============================] - 0s 28us/step - loss: 4.1649e-04 - mean_absolute_error: 0.0127\n",
      "\n",
      "Epoch 01820: loss did not improve\n",
      "Epoch 1821/10000\n",
      "2700/2700 [==============================] - 0s 29us/step - loss: 4.1801e-04 - mean_absolute_error: 0.0128\n",
      "\n",
      "Epoch 01821: loss did not improve\n",
      "Epoch 1822/10000\n",
      "2700/2700 [==============================] - 0s 28us/step - loss: 4.1700e-04 - mean_absolute_error: 0.0128\n",
      "\n",
      "Epoch 01822: loss did not improve\n",
      "Epoch 1823/10000\n",
      "2700/2700 [==============================] - 0s 26us/step - loss: 4.2142e-04 - mean_absolute_error: 0.0130\n",
      "\n",
      "Epoch 01823: loss did not improve\n",
      "Epoch 1824/10000\n",
      "2700/2700 [==============================] - 0s 28us/step - loss: 4.1806e-04 - mean_absolute_error: 0.0128\n",
      "\n",
      "Epoch 01824: loss did not improve\n",
      "Epoch 1825/10000\n",
      "2700/2700 [==============================] - 0s 26us/step - loss: 4.2087e-04 - mean_absolute_error: 0.0128\n",
      "\n",
      "Epoch 01825: loss did not improve\n",
      "Epoch 1826/10000\n",
      "2700/2700 [==============================] - 0s 29us/step - loss: 4.2471e-04 - mean_absolute_error: 0.0130\n",
      "\n",
      "Epoch 01826: loss did not improve\n",
      "Epoch 1827/10000\n",
      "2700/2700 [==============================] - 0s 32us/step - loss: 4.2350e-04 - mean_absolute_error: 0.0130\n",
      "\n",
      "Epoch 01827: loss did not improve\n",
      "Epoch 1828/10000\n",
      "2700/2700 [==============================] - 0s 43us/step - loss: 4.1989e-04 - mean_absolute_error: 0.0129\n",
      "\n",
      "Epoch 01828: loss did not improve\n",
      "Epoch 1829/10000\n",
      "2700/2700 [==============================] - 0s 41us/step - loss: 4.2181e-04 - mean_absolute_error: 0.0129\n",
      "\n",
      "Epoch 01829: loss did not improve\n",
      "Epoch 1830/10000\n",
      "2700/2700 [==============================] - 0s 35us/step - loss: 4.2056e-04 - mean_absolute_error: 0.0129\n",
      "\n",
      "Epoch 01830: loss did not improve\n",
      "Epoch 1831/10000\n",
      "2700/2700 [==============================] - 0s 34us/step - loss: 4.1753e-04 - mean_absolute_error: 0.0128\n",
      "\n",
      "Epoch 01831: loss did not improve\n",
      "Epoch 1832/10000\n",
      "2700/2700 [==============================] - 0s 37us/step - loss: 4.1824e-04 - mean_absolute_error: 0.0127\n",
      "\n",
      "Epoch 01832: loss did not improve\n",
      "Epoch 1833/10000\n",
      "2700/2700 [==============================] - 0s 41us/step - loss: 4.1728e-04 - mean_absolute_error: 0.0128\n",
      "\n",
      "Epoch 01833: loss did not improve\n",
      "Epoch 1834/10000\n",
      "2700/2700 [==============================] - 0s 32us/step - loss: 4.1521e-04 - mean_absolute_error: 0.0127\n",
      "\n",
      "Epoch 01834: loss improved from 0.00042 to 0.00042, saving model to keras/best_baseline_weights.hdf5\n",
      "Epoch 1835/10000\n",
      "2700/2700 [==============================] - 0s 31us/step - loss: 4.1717e-04 - mean_absolute_error: 0.0128\n",
      "\n",
      "Epoch 01835: loss did not improve\n",
      "Epoch 1836/10000\n",
      "2700/2700 [==============================] - 0s 32us/step - loss: 4.1866e-04 - mean_absolute_error: 0.0127\n",
      "\n",
      "Epoch 01836: loss did not improve\n",
      "Epoch 1837/10000\n",
      "2700/2700 [==============================] - 0s 29us/step - loss: 4.1830e-04 - mean_absolute_error: 0.0129\n",
      "\n",
      "Epoch 01837: loss did not improve\n",
      "Epoch 1838/10000\n",
      "2700/2700 [==============================] - 0s 31us/step - loss: 4.1926e-04 - mean_absolute_error: 0.0128\n",
      "\n",
      "Epoch 01838: loss did not improve\n",
      "Epoch 1839/10000\n",
      "2700/2700 [==============================] - 0s 49us/step - loss: 4.1889e-04 - mean_absolute_error: 0.0129\n",
      "\n",
      "Epoch 01839: loss did not improve\n",
      "Epoch 1840/10000\n",
      "2700/2700 [==============================] - 0s 39us/step - loss: 4.1776e-04 - mean_absolute_error: 0.0127\n",
      "\n",
      "Epoch 01840: loss did not improve\n",
      "Epoch 1841/10000\n",
      "2700/2700 [==============================] - 0s 42us/step - loss: 4.1662e-04 - mean_absolute_error: 0.0128\n",
      "\n",
      "Epoch 01841: loss did not improve\n",
      "Epoch 1842/10000\n",
      "2700/2700 [==============================] - 0s 39us/step - loss: 4.1902e-04 - mean_absolute_error: 0.0128\n",
      "\n",
      "Epoch 01842: loss did not improve\n",
      "Epoch 1843/10000\n",
      "2700/2700 [==============================] - 0s 32us/step - loss: 4.1799e-04 - mean_absolute_error: 0.0129\n",
      "\n",
      "Epoch 01843: loss did not improve\n",
      "Epoch 1844/10000\n",
      "2700/2700 [==============================] - 0s 33us/step - loss: 4.1549e-04 - mean_absolute_error: 0.0126\n",
      "\n",
      "Epoch 01844: loss did not improve\n",
      "Epoch 1845/10000\n",
      "2700/2700 [==============================] - 0s 34us/step - loss: 4.2063e-04 - mean_absolute_error: 0.0129\n",
      "\n",
      "Epoch 01845: loss did not improve\n",
      "Epoch 1846/10000\n",
      "2700/2700 [==============================] - 0s 34us/step - loss: 4.1774e-04 - mean_absolute_error: 0.0128\n",
      "\n",
      "Epoch 01846: loss did not improve\n",
      "Epoch 1847/10000\n",
      "2700/2700 [==============================] - 0s 33us/step - loss: 4.2583e-04 - mean_absolute_error: 0.0131\n",
      "\n",
      "Epoch 01847: loss did not improve\n",
      "Epoch 1848/10000\n",
      "2700/2700 [==============================] - 0s 27us/step - loss: 4.1827e-04 - mean_absolute_error: 0.0127\n",
      "\n",
      "Epoch 01848: loss did not improve\n",
      "Epoch 1849/10000\n",
      "2700/2700 [==============================] - 0s 27us/step - loss: 4.2179e-04 - mean_absolute_error: 0.0128\n",
      "\n",
      "Epoch 01849: loss did not improve\n",
      "Epoch 1850/10000\n",
      "2700/2700 [==============================] - 0s 26us/step - loss: 4.1630e-04 - mean_absolute_error: 0.0127\n",
      "\n",
      "Epoch 01850: loss did not improve\n",
      "Epoch 1851/10000\n",
      "2700/2700 [==============================] - 0s 28us/step - loss: 4.2495e-04 - mean_absolute_error: 0.0131\n",
      "\n",
      "Epoch 01851: loss did not improve\n",
      "Epoch 1852/10000\n",
      "2700/2700 [==============================] - 0s 26us/step - loss: 4.2899e-04 - mean_absolute_error: 0.0131\n",
      "\n",
      "Epoch 01852: loss did not improve\n",
      "Epoch 1853/10000\n",
      "2700/2700 [==============================] - 0s 26us/step - loss: 4.1972e-04 - mean_absolute_error: 0.0129\n",
      "\n",
      "Epoch 01853: loss did not improve\n",
      "Epoch 1854/10000\n",
      "2700/2700 [==============================] - 0s 28us/step - loss: 4.2230e-04 - mean_absolute_error: 0.0129\n",
      "\n",
      "Epoch 01854: loss did not improve\n",
      "Epoch 1855/10000\n",
      "2700/2700 [==============================] - 0s 28us/step - loss: 4.1636e-04 - mean_absolute_error: 0.0127\n",
      "\n",
      "Epoch 01855: loss did not improve\n",
      "Epoch 1856/10000\n",
      "2700/2700 [==============================] - 0s 35us/step - loss: 4.2337e-04 - mean_absolute_error: 0.0130\n",
      "\n",
      "Epoch 01856: loss did not improve\n",
      "Epoch 1857/10000\n",
      "2700/2700 [==============================] - 0s 34us/step - loss: 4.1566e-04 - mean_absolute_error: 0.0127\n",
      "\n",
      "Epoch 01857: loss did not improve\n",
      "Epoch 1858/10000\n",
      "2700/2700 [==============================] - 0s 33us/step - loss: 4.1928e-04 - mean_absolute_error: 0.0128\n",
      "\n",
      "Epoch 01858: loss did not improve\n",
      "Epoch 1859/10000\n",
      "2700/2700 [==============================] - 0s 33us/step - loss: 4.1617e-04 - mean_absolute_error: 0.0127\n",
      "\n",
      "Epoch 01859: loss did not improve\n",
      "Epoch 1860/10000\n",
      "2700/2700 [==============================] - 0s 34us/step - loss: 4.1689e-04 - mean_absolute_error: 0.0126\n",
      "\n",
      "Epoch 01860: loss did not improve\n",
      "Epoch 1861/10000\n",
      "2700/2700 [==============================] - 0s 36us/step - loss: 4.1996e-04 - mean_absolute_error: 0.0128\n",
      "\n",
      "Epoch 01861: loss did not improve\n",
      "Epoch 1862/10000\n",
      "2700/2700 [==============================] - 0s 32us/step - loss: 4.2383e-04 - mean_absolute_error: 0.0131\n",
      "\n",
      "Epoch 01862: loss did not improve\n",
      "Epoch 1863/10000\n",
      "2700/2700 [==============================] - 0s 33us/step - loss: 4.1593e-04 - mean_absolute_error: 0.0127\n",
      "\n",
      "Epoch 01863: loss did not improve\n",
      "Epoch 1864/10000\n",
      "2700/2700 [==============================] - 0s 35us/step - loss: 4.1343e-04 - mean_absolute_error: 0.0126\n",
      "\n",
      "Epoch 01864: loss improved from 0.00042 to 0.00041, saving model to keras/best_baseline_weights.hdf5\n",
      "Epoch 1865/10000\n",
      "2700/2700 [==============================] - 0s 34us/step - loss: 4.1574e-04 - mean_absolute_error: 0.0128\n",
      "\n",
      "Epoch 01865: loss did not improve\n",
      "Epoch 1866/10000\n",
      "2700/2700 [==============================] - 0s 31us/step - loss: 4.2095e-04 - mean_absolute_error: 0.0128\n",
      "\n",
      "Epoch 01866: loss did not improve\n",
      "Epoch 1867/10000\n",
      "2700/2700 [==============================] - 0s 39us/step - loss: 4.1787e-04 - mean_absolute_error: 0.0128\n",
      "\n",
      "Epoch 01867: loss did not improve\n",
      "Epoch 1868/10000\n",
      "2700/2700 [==============================] - 0s 35us/step - loss: 4.1361e-04 - mean_absolute_error: 0.0128\n",
      "\n",
      "Epoch 01868: loss did not improve\n",
      "Epoch 1869/10000\n",
      "2700/2700 [==============================] - 0s 35us/step - loss: 4.2016e-04 - mean_absolute_error: 0.0128\n",
      "\n",
      "Epoch 01869: loss did not improve\n",
      "Epoch 1870/10000\n",
      "2700/2700 [==============================] - 0s 33us/step - loss: 4.1479e-04 - mean_absolute_error: 0.0127\n",
      "\n",
      "Epoch 01870: loss did not improve\n",
      "Epoch 1871/10000\n",
      "2700/2700 [==============================] - 0s 37us/step - loss: 4.2268e-04 - mean_absolute_error: 0.0129\n",
      "\n",
      "Epoch 01871: loss did not improve\n",
      "Epoch 1872/10000\n",
      "2700/2700 [==============================] - 0s 33us/step - loss: 4.1928e-04 - mean_absolute_error: 0.0130\n",
      "\n",
      "Epoch 01872: loss did not improve\n",
      "Epoch 1873/10000\n",
      "2700/2700 [==============================] - 0s 36us/step - loss: 4.1821e-04 - mean_absolute_error: 0.0128\n",
      "\n",
      "Epoch 01873: loss did not improve\n",
      "Epoch 1874/10000\n",
      "2700/2700 [==============================] - 0s 38us/step - loss: 4.1579e-04 - mean_absolute_error: 0.0127\n",
      "\n",
      "Epoch 01874: loss did not improve\n",
      "Epoch 1875/10000\n",
      "2700/2700 [==============================] - 0s 36us/step - loss: 4.1692e-04 - mean_absolute_error: 0.0127\n",
      "\n",
      "Epoch 01875: loss did not improve\n",
      "Epoch 1876/10000\n",
      "2700/2700 [==============================] - 0s 32us/step - loss: 4.1753e-04 - mean_absolute_error: 0.0128\n",
      "\n",
      "Epoch 01876: loss did not improve\n",
      "Epoch 1877/10000\n",
      "2700/2700 [==============================] - 0s 31us/step - loss: 4.1937e-04 - mean_absolute_error: 0.0129\n",
      "\n",
      "Epoch 01877: loss did not improve\n",
      "Epoch 1878/10000\n",
      "2700/2700 [==============================] - 0s 30us/step - loss: 4.2741e-04 - mean_absolute_error: 0.0132\n",
      "\n",
      "Epoch 01878: loss did not improve\n",
      "Epoch 1879/10000\n",
      "2700/2700 [==============================] - 0s 31us/step - loss: 4.1836e-04 - mean_absolute_error: 0.0127\n",
      "\n",
      "Epoch 01879: loss did not improve\n",
      "Epoch 1880/10000\n",
      "2700/2700 [==============================] - 0s 32us/step - loss: 4.1391e-04 - mean_absolute_error: 0.0126\n",
      "\n",
      "Epoch 01880: loss did not improve\n",
      "Epoch 1881/10000\n",
      "2700/2700 [==============================] - 0s 30us/step - loss: 4.1396e-04 - mean_absolute_error: 0.0125\n",
      "\n",
      "Epoch 01881: loss did not improve\n",
      "Epoch 1882/10000\n",
      "2700/2700 [==============================] - 0s 30us/step - loss: 4.1918e-04 - mean_absolute_error: 0.0129\n",
      "\n",
      "Epoch 01882: loss did not improve\n",
      "Epoch 1883/10000\n",
      "2700/2700 [==============================] - 0s 35us/step - loss: 4.1841e-04 - mean_absolute_error: 0.0128\n",
      "\n",
      "Epoch 01883: loss did not improve\n",
      "Epoch 1884/10000\n",
      "2700/2700 [==============================] - 0s 38us/step - loss: 4.1820e-04 - mean_absolute_error: 0.0128\n",
      "\n",
      "Epoch 01884: loss did not improve\n",
      "Epoch 1885/10000\n",
      "2700/2700 [==============================] - 0s 35us/step - loss: 4.1703e-04 - mean_absolute_error: 0.0127\n",
      "\n",
      "Epoch 01885: loss did not improve\n",
      "Epoch 1886/10000\n",
      "2700/2700 [==============================] - 0s 34us/step - loss: 4.1495e-04 - mean_absolute_error: 0.0127\n",
      "\n",
      "Epoch 01886: loss did not improve\n",
      "Epoch 1887/10000\n",
      "2700/2700 [==============================] - 0s 34us/step - loss: 4.1363e-04 - mean_absolute_error: 0.0126\n",
      "\n",
      "Epoch 01887: loss did not improve\n",
      "Epoch 1888/10000\n",
      "2700/2700 [==============================] - 0s 34us/step - loss: 4.1268e-04 - mean_absolute_error: 0.0127\n",
      "\n",
      "Epoch 01888: loss improved from 0.00041 to 0.00041, saving model to keras/best_baseline_weights.hdf5\n",
      "Epoch 1889/10000\n",
      "2700/2700 [==============================] - 0s 34us/step - loss: 4.1277e-04 - mean_absolute_error: 0.0126\n",
      "\n",
      "Epoch 01889: loss did not improve\n",
      "Epoch 1890/10000\n",
      "2700/2700 [==============================] - 0s 32us/step - loss: 4.1423e-04 - mean_absolute_error: 0.0127\n",
      "\n",
      "Epoch 01890: loss did not improve\n",
      "Epoch 1891/10000\n",
      "2700/2700 [==============================] - 0s 32us/step - loss: 4.1770e-04 - mean_absolute_error: 0.0128\n",
      "\n",
      "Epoch 01891: loss did not improve\n",
      "Epoch 1892/10000\n",
      "2700/2700 [==============================] - 0s 36us/step - loss: 4.1922e-04 - mean_absolute_error: 0.0129\n",
      "\n",
      "Epoch 01892: loss did not improve\n",
      "Epoch 1893/10000\n",
      "2700/2700 [==============================] - 0s 29us/step - loss: 4.1618e-04 - mean_absolute_error: 0.0128\n",
      "\n",
      "Epoch 01893: loss did not improve\n",
      "Epoch 1894/10000\n",
      "2700/2700 [==============================] - 0s 30us/step - loss: 4.1475e-04 - mean_absolute_error: 0.0126\n",
      "\n",
      "Epoch 01894: loss did not improve\n",
      "Epoch 1895/10000\n",
      "2700/2700 [==============================] - 0s 33us/step - loss: 4.1270e-04 - mean_absolute_error: 0.0125\n",
      "\n",
      "Epoch 01895: loss did not improve\n",
      "Epoch 1896/10000\n",
      "2700/2700 [==============================] - 0s 31us/step - loss: 4.1345e-04 - mean_absolute_error: 0.0126\n",
      "\n",
      "Epoch 01896: loss did not improve\n",
      "Epoch 1897/10000\n",
      "2700/2700 [==============================] - 0s 28us/step - loss: 4.1180e-04 - mean_absolute_error: 0.0126\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 01897: loss improved from 0.00041 to 0.00041, saving model to keras/best_baseline_weights.hdf5\n",
      "Epoch 1898/10000\n",
      "2700/2700 [==============================] - 0s 32us/step - loss: 4.1544e-04 - mean_absolute_error: 0.0126\n",
      "\n",
      "Epoch 01898: loss did not improve\n",
      "Epoch 1899/10000\n",
      "2700/2700 [==============================] - 0s 27us/step - loss: 4.1257e-04 - mean_absolute_error: 0.0126\n",
      "\n",
      "Epoch 01899: loss did not improve\n",
      "Epoch 1900/10000\n",
      "2700/2700 [==============================] - 0s 29us/step - loss: 4.1650e-04 - mean_absolute_error: 0.0128\n",
      "\n",
      "Epoch 01900: loss did not improve\n",
      "Epoch 1901/10000\n",
      "2700/2700 [==============================] - 0s 26us/step - loss: 4.2249e-04 - mean_absolute_error: 0.0130\n",
      "\n",
      "Epoch 01901: loss did not improve\n",
      "Epoch 1902/10000\n",
      "2700/2700 [==============================] - 0s 25us/step - loss: 4.1464e-04 - mean_absolute_error: 0.0126\n",
      "\n",
      "Epoch 01902: loss did not improve\n",
      "Epoch 1903/10000\n",
      "2700/2700 [==============================] - 0s 28us/step - loss: 4.1616e-04 - mean_absolute_error: 0.0127\n",
      "\n",
      "Epoch 01903: loss did not improve\n",
      "Epoch 1904/10000\n",
      "2700/2700 [==============================] - 0s 28us/step - loss: 4.1542e-04 - mean_absolute_error: 0.0126\n",
      "\n",
      "Epoch 01904: loss did not improve\n",
      "Epoch 1905/10000\n",
      "2700/2700 [==============================] - 0s 29us/step - loss: 4.1509e-04 - mean_absolute_error: 0.0126\n",
      "\n",
      "Epoch 01905: loss did not improve\n",
      "Epoch 1906/10000\n",
      "2700/2700 [==============================] - 0s 28us/step - loss: 4.1407e-04 - mean_absolute_error: 0.0127\n",
      "\n",
      "Epoch 01906: loss did not improve\n",
      "Epoch 1907/10000\n",
      "2700/2700 [==============================] - 0s 28us/step - loss: 4.1501e-04 - mean_absolute_error: 0.0126\n",
      "\n",
      "Epoch 01907: loss did not improve\n",
      "Epoch 1908/10000\n",
      "2700/2700 [==============================] - 0s 26us/step - loss: 4.1268e-04 - mean_absolute_error: 0.0126\n",
      "\n",
      "Epoch 01908: loss did not improve\n",
      "Epoch 1909/10000\n",
      "2700/2700 [==============================] - 0s 29us/step - loss: 4.1541e-04 - mean_absolute_error: 0.0127\n",
      "\n",
      "Epoch 01909: loss did not improve\n",
      "Epoch 1910/10000\n",
      "2700/2700 [==============================] - 0s 30us/step - loss: 4.1326e-04 - mean_absolute_error: 0.0126\n",
      "\n",
      "Epoch 01910: loss did not improve\n",
      "Epoch 1911/10000\n",
      "2700/2700 [==============================] - 0s 29us/step - loss: 4.1651e-04 - mean_absolute_error: 0.0127\n",
      "\n",
      "Epoch 01911: loss did not improve\n",
      "Epoch 1912/10000\n",
      "2700/2700 [==============================] - 0s 28us/step - loss: 4.1449e-04 - mean_absolute_error: 0.0126\n",
      "\n",
      "Epoch 01912: loss did not improve\n",
      "Epoch 1913/10000\n",
      "2700/2700 [==============================] - 0s 27us/step - loss: 4.1284e-04 - mean_absolute_error: 0.0126\n",
      "\n",
      "Epoch 01913: loss did not improve\n",
      "Epoch 1914/10000\n",
      "2700/2700 [==============================] - 0s 27us/step - loss: 4.1584e-04 - mean_absolute_error: 0.0128\n",
      "\n",
      "Epoch 01914: loss did not improve\n",
      "Epoch 1915/10000\n",
      "2700/2700 [==============================] - 0s 26us/step - loss: 4.1175e-04 - mean_absolute_error: 0.0126\n",
      "\n",
      "Epoch 01915: loss improved from 0.00041 to 0.00041, saving model to keras/best_baseline_weights.hdf5\n",
      "Epoch 1916/10000\n",
      "2700/2700 [==============================] - 0s 29us/step - loss: 4.1206e-04 - mean_absolute_error: 0.0125\n",
      "\n",
      "Epoch 01916: loss did not improve\n",
      "Epoch 1917/10000\n",
      "2700/2700 [==============================] - 0s 29us/step - loss: 4.1243e-04 - mean_absolute_error: 0.0126\n",
      "\n",
      "Epoch 01917: loss did not improve\n",
      "Epoch 1918/10000\n",
      "2700/2700 [==============================] - 0s 30us/step - loss: 4.1841e-04 - mean_absolute_error: 0.0129\n",
      "\n",
      "Epoch 01918: loss did not improve\n",
      "Epoch 1919/10000\n",
      "2700/2700 [==============================] - 0s 32us/step - loss: 4.1582e-04 - mean_absolute_error: 0.0127\n",
      "\n",
      "Epoch 01919: loss did not improve\n",
      "Epoch 1920/10000\n",
      "2700/2700 [==============================] - 0s 31us/step - loss: 4.1947e-04 - mean_absolute_error: 0.0129\n",
      "\n",
      "Epoch 01920: loss did not improve\n",
      "Epoch 1921/10000\n",
      "2700/2700 [==============================] - 0s 36us/step - loss: 4.1740e-04 - mean_absolute_error: 0.0128\n",
      "\n",
      "Epoch 01921: loss did not improve\n",
      "Epoch 1922/10000\n",
      "2700/2700 [==============================] - 0s 30us/step - loss: 4.1431e-04 - mean_absolute_error: 0.0127\n",
      "\n",
      "Epoch 01922: loss did not improve\n",
      "Epoch 1923/10000\n",
      "2700/2700 [==============================] - 0s 27us/step - loss: 4.1437e-04 - mean_absolute_error: 0.0126\n",
      "\n",
      "Epoch 01923: loss did not improve\n",
      "Epoch 1924/10000\n",
      "2700/2700 [==============================] - 0s 30us/step - loss: 4.1080e-04 - mean_absolute_error: 0.0126\n",
      "\n",
      "Epoch 01924: loss improved from 0.00041 to 0.00041, saving model to keras/best_baseline_weights.hdf5\n",
      "Epoch 1925/10000\n",
      "2700/2700 [==============================] - 0s 35us/step - loss: 4.1233e-04 - mean_absolute_error: 0.0125\n",
      "\n",
      "Epoch 01925: loss did not improve\n",
      "Epoch 1926/10000\n",
      "2700/2700 [==============================] - 0s 33us/step - loss: 4.1141e-04 - mean_absolute_error: 0.0125\n",
      "\n",
      "Epoch 01926: loss did not improve\n",
      "Epoch 1927/10000\n",
      "2700/2700 [==============================] - 0s 30us/step - loss: 4.1195e-04 - mean_absolute_error: 0.0125\n",
      "\n",
      "Epoch 01927: loss did not improve\n",
      "Epoch 1928/10000\n",
      "2700/2700 [==============================] - 0s 37us/step - loss: 4.1084e-04 - mean_absolute_error: 0.0125\n",
      "\n",
      "Epoch 01928: loss did not improve\n",
      "Epoch 1929/10000\n",
      "2700/2700 [==============================] - 0s 33us/step - loss: 4.1522e-04 - mean_absolute_error: 0.0127\n",
      "\n",
      "Epoch 01929: loss did not improve\n",
      "Epoch 1930/10000\n",
      "2700/2700 [==============================] - 0s 32us/step - loss: 4.0958e-04 - mean_absolute_error: 0.0125\n",
      "\n",
      "Epoch 01930: loss improved from 0.00041 to 0.00041, saving model to keras/best_baseline_weights.hdf5\n",
      "Epoch 1931/10000\n",
      "2700/2700 [==============================] - 0s 33us/step - loss: 4.1531e-04 - mean_absolute_error: 0.0127\n",
      "\n",
      "Epoch 01931: loss did not improve\n",
      "Epoch 1932/10000\n",
      "2700/2700 [==============================] - 0s 27us/step - loss: 4.1558e-04 - mean_absolute_error: 0.0128\n",
      "\n",
      "Epoch 01932: loss did not improve\n",
      "Epoch 1933/10000\n",
      "2700/2700 [==============================] - 0s 32us/step - loss: 4.1157e-04 - mean_absolute_error: 0.0125\n",
      "\n",
      "Epoch 01933: loss did not improve\n",
      "Epoch 1934/10000\n",
      "2700/2700 [==============================] - 0s 28us/step - loss: 4.1354e-04 - mean_absolute_error: 0.0127\n",
      "\n",
      "Epoch 01934: loss did not improve\n",
      "Epoch 1935/10000\n",
      "2700/2700 [==============================] - 0s 28us/step - loss: 4.1407e-04 - mean_absolute_error: 0.0127\n",
      "\n",
      "Epoch 01935: loss did not improve\n",
      "Epoch 1936/10000\n",
      "2700/2700 [==============================] - 0s 35us/step - loss: 4.1206e-04 - mean_absolute_error: 0.0125\n",
      "\n",
      "Epoch 01936: loss did not improve\n",
      "Epoch 1937/10000\n",
      "2700/2700 [==============================] - 0s 34us/step - loss: 4.1070e-04 - mean_absolute_error: 0.0125\n",
      "\n",
      "Epoch 01937: loss did not improve\n",
      "Epoch 1938/10000\n",
      "2700/2700 [==============================] - 0s 34us/step - loss: 4.1432e-04 - mean_absolute_error: 0.0127\n",
      "\n",
      "Epoch 01938: loss did not improve\n",
      "Epoch 1939/10000\n",
      "2700/2700 [==============================] - 0s 35us/step - loss: 4.1011e-04 - mean_absolute_error: 0.0125\n",
      "\n",
      "Epoch 01939: loss did not improve\n",
      "Epoch 1940/10000\n",
      "2700/2700 [==============================] - 0s 31us/step - loss: 4.1028e-04 - mean_absolute_error: 0.0125\n",
      "\n",
      "Epoch 01940: loss did not improve\n",
      "Epoch 1941/10000\n",
      "2700/2700 [==============================] - 0s 30us/step - loss: 4.0980e-04 - mean_absolute_error: 0.0125\n",
      "\n",
      "Epoch 01941: loss did not improve\n",
      "Epoch 1942/10000\n",
      "2700/2700 [==============================] - 0s 30us/step - loss: 4.1331e-04 - mean_absolute_error: 0.0127\n",
      "\n",
      "Epoch 01942: loss did not improve\n",
      "Epoch 1943/10000\n",
      "2700/2700 [==============================] - 0s 28us/step - loss: 4.2079e-04 - mean_absolute_error: 0.0131\n",
      "\n",
      "Epoch 01943: loss did not improve\n",
      "Epoch 1944/10000\n",
      "2700/2700 [==============================] - 0s 32us/step - loss: 4.1261e-04 - mean_absolute_error: 0.0126\n",
      "\n",
      "Epoch 01944: loss did not improve\n",
      "Epoch 1945/10000\n",
      "2700/2700 [==============================] - 0s 29us/step - loss: 4.1473e-04 - mean_absolute_error: 0.0127\n",
      "\n",
      "Epoch 01945: loss did not improve\n",
      "Epoch 1946/10000\n",
      "2700/2700 [==============================] - 0s 37us/step - loss: 4.1252e-04 - mean_absolute_error: 0.0126\n",
      "\n",
      "Epoch 01946: loss did not improve\n",
      "Epoch 1947/10000\n",
      "2700/2700 [==============================] - 0s 32us/step - loss: 4.1133e-04 - mean_absolute_error: 0.0125\n",
      "\n",
      "Epoch 01947: loss did not improve\n",
      "Epoch 1948/10000\n",
      "2700/2700 [==============================] - 0s 33us/step - loss: 4.1599e-04 - mean_absolute_error: 0.0130\n",
      "\n",
      "Epoch 01948: loss did not improve\n",
      "Epoch 1949/10000\n",
      "2700/2700 [==============================] - 0s 30us/step - loss: 4.1317e-04 - mean_absolute_error: 0.0126\n",
      "\n",
      "Epoch 01949: loss did not improve\n",
      "Epoch 1950/10000\n",
      "2700/2700 [==============================] - 0s 30us/step - loss: 4.1986e-04 - mean_absolute_error: 0.0128\n",
      "\n",
      "Epoch 01950: loss did not improve\n",
      "Epoch 1951/10000\n",
      "2700/2700 [==============================] - 0s 27us/step - loss: 4.0947e-04 - mean_absolute_error: 0.0124\n",
      "\n",
      "Epoch 01951: loss improved from 0.00041 to 0.00041, saving model to keras/best_baseline_weights.hdf5\n",
      "Epoch 1952/10000\n",
      "2700/2700 [==============================] - 0s 28us/step - loss: 4.1751e-04 - mean_absolute_error: 0.0129\n",
      "\n",
      "Epoch 01952: loss did not improve\n",
      "Epoch 1953/10000\n",
      "2700/2700 [==============================] - 0s 29us/step - loss: 4.0933e-04 - mean_absolute_error: 0.0124\n",
      "\n",
      "Epoch 01953: loss improved from 0.00041 to 0.00041, saving model to keras/best_baseline_weights.hdf5\n",
      "Epoch 1954/10000\n",
      "2700/2700 [==============================] - 0s 29us/step - loss: 4.1029e-04 - mean_absolute_error: 0.0124\n",
      "\n",
      "Epoch 01954: loss did not improve\n",
      "Epoch 1955/10000\n",
      "2700/2700 [==============================] - 0s 28us/step - loss: 4.1071e-04 - mean_absolute_error: 0.0126\n",
      "\n",
      "Epoch 01955: loss did not improve\n",
      "Epoch 1956/10000\n",
      "2700/2700 [==============================] - 0s 28us/step - loss: 4.1017e-04 - mean_absolute_error: 0.0124\n",
      "\n",
      "Epoch 01956: loss did not improve\n",
      "Epoch 1957/10000\n",
      "2700/2700 [==============================] - 0s 28us/step - loss: 4.1561e-04 - mean_absolute_error: 0.0128\n",
      "\n",
      "Epoch 01957: loss did not improve\n",
      "Epoch 1958/10000\n",
      "2700/2700 [==============================] - 0s 34us/step - loss: 4.1548e-04 - mean_absolute_error: 0.0126\n",
      "\n",
      "Epoch 01958: loss did not improve\n",
      "Epoch 1959/10000\n",
      "2700/2700 [==============================] - 0s 36us/step - loss: 4.1006e-04 - mean_absolute_error: 0.0124\n",
      "\n",
      "Epoch 01959: loss did not improve\n",
      "Epoch 1960/10000\n",
      "2700/2700 [==============================] - 0s 34us/step - loss: 4.0928e-04 - mean_absolute_error: 0.0125\n",
      "\n",
      "Epoch 01960: loss improved from 0.00041 to 0.00041, saving model to keras/best_baseline_weights.hdf5\n",
      "Epoch 1961/10000\n",
      "2700/2700 [==============================] - 0s 33us/step - loss: 4.1056e-04 - mean_absolute_error: 0.0124\n",
      "\n",
      "Epoch 01961: loss did not improve\n",
      "Epoch 1962/10000\n",
      "2700/2700 [==============================] - 0s 29us/step - loss: 4.0772e-04 - mean_absolute_error: 0.0124\n",
      "\n",
      "Epoch 01962: loss improved from 0.00041 to 0.00041, saving model to keras/best_baseline_weights.hdf5\n",
      "Epoch 1963/10000\n",
      "2700/2700 [==============================] - 0s 30us/step - loss: 4.0801e-04 - mean_absolute_error: 0.0124\n",
      "\n",
      "Epoch 01963: loss did not improve\n",
      "Epoch 1964/10000\n",
      "2700/2700 [==============================] - 0s 34us/step - loss: 4.1077e-04 - mean_absolute_error: 0.0125\n",
      "\n",
      "Epoch 01964: loss did not improve\n",
      "Epoch 1965/10000\n",
      "2700/2700 [==============================] - 0s 30us/step - loss: 4.2434e-04 - mean_absolute_error: 0.0132\n",
      "\n",
      "Epoch 01965: loss did not improve\n",
      "Epoch 1966/10000\n",
      "2700/2700 [==============================] - 0s 29us/step - loss: 4.0998e-04 - mean_absolute_error: 0.0124\n",
      "\n",
      "Epoch 01966: loss did not improve\n",
      "Epoch 1967/10000\n",
      "2700/2700 [==============================] - 0s 37us/step - loss: 4.0775e-04 - mean_absolute_error: 0.0125\n",
      "\n",
      "Epoch 01967: loss did not improve\n",
      "Epoch 1968/10000\n",
      "2700/2700 [==============================] - 0s 31us/step - loss: 4.1613e-04 - mean_absolute_error: 0.0126\n",
      "\n",
      "Epoch 01968: loss did not improve\n",
      "Epoch 1969/10000\n",
      "2700/2700 [==============================] - 0s 32us/step - loss: 4.1292e-04 - mean_absolute_error: 0.0125\n",
      "\n",
      "Epoch 01969: loss did not improve\n",
      "Epoch 1970/10000\n",
      "2700/2700 [==============================] - 0s 31us/step - loss: 4.0966e-04 - mean_absolute_error: 0.0124\n",
      "\n",
      "Epoch 01970: loss did not improve\n",
      "Epoch 1971/10000\n",
      "2700/2700 [==============================] - 0s 30us/step - loss: 4.1199e-04 - mean_absolute_error: 0.0125\n",
      "\n",
      "Epoch 01971: loss did not improve\n",
      "Epoch 1972/10000\n",
      "2700/2700 [==============================] - 0s 29us/step - loss: 4.0745e-04 - mean_absolute_error: 0.0123\n",
      "\n",
      "Epoch 01972: loss improved from 0.00041 to 0.00041, saving model to keras/best_baseline_weights.hdf5\n",
      "Epoch 1973/10000\n",
      "2700/2700 [==============================] - 0s 31us/step - loss: 4.0750e-04 - mean_absolute_error: 0.0124\n",
      "\n",
      "Epoch 01973: loss did not improve\n",
      "Epoch 1974/10000\n",
      "2700/2700 [==============================] - 0s 30us/step - loss: 4.0651e-04 - mean_absolute_error: 0.0123\n",
      "\n",
      "Epoch 01974: loss improved from 0.00041 to 0.00041, saving model to keras/best_baseline_weights.hdf5\n",
      "Epoch 1975/10000\n",
      "2700/2700 [==============================] - 0s 40us/step - loss: 4.0893e-04 - mean_absolute_error: 0.0124\n",
      "\n",
      "Epoch 01975: loss did not improve\n",
      "Epoch 1976/10000\n",
      "2700/2700 [==============================] - 0s 43us/step - loss: 4.1012e-04 - mean_absolute_error: 0.0124\n",
      "\n",
      "Epoch 01976: loss did not improve\n",
      "Epoch 1977/10000\n",
      "2700/2700 [==============================] - 0s 34us/step - loss: 4.1045e-04 - mean_absolute_error: 0.0125\n",
      "\n",
      "Epoch 01977: loss did not improve\n",
      "Epoch 1978/10000\n",
      "2700/2700 [==============================] - 0s 28us/step - loss: 4.1117e-04 - mean_absolute_error: 0.0125\n",
      "\n",
      "Epoch 01978: loss did not improve\n",
      "Epoch 1979/10000\n",
      "2700/2700 [==============================] - 0s 28us/step - loss: 4.1112e-04 - mean_absolute_error: 0.0125\n",
      "\n",
      "Epoch 01979: loss did not improve\n",
      "Epoch 1980/10000\n",
      "2700/2700 [==============================] - 0s 32us/step - loss: 4.1245e-04 - mean_absolute_error: 0.0126\n",
      "\n",
      "Epoch 01980: loss did not improve\n",
      "Epoch 1981/10000\n",
      "2700/2700 [==============================] - 0s 29us/step - loss: 4.1157e-04 - mean_absolute_error: 0.0125\n",
      "\n",
      "Epoch 01981: loss did not improve\n",
      "Epoch 1982/10000\n",
      "2700/2700 [==============================] - 0s 32us/step - loss: 4.1169e-04 - mean_absolute_error: 0.0125\n",
      "\n",
      "Epoch 01982: loss did not improve\n",
      "Epoch 1983/10000\n",
      "2700/2700 [==============================] - 0s 33us/step - loss: 4.1705e-04 - mean_absolute_error: 0.0129\n",
      "\n",
      "Epoch 01983: loss did not improve\n",
      "Epoch 1984/10000\n",
      "2700/2700 [==============================] - 0s 29us/step - loss: 4.0881e-04 - mean_absolute_error: 0.0124\n",
      "\n",
      "Epoch 01984: loss did not improve\n",
      "Epoch 1985/10000\n",
      "2700/2700 [==============================] - 0s 31us/step - loss: 4.0796e-04 - mean_absolute_error: 0.0123\n",
      "\n",
      "Epoch 01985: loss did not improve\n",
      "Epoch 1986/10000\n",
      "2700/2700 [==============================] - 0s 30us/step - loss: 4.1469e-04 - mean_absolute_error: 0.0129\n",
      "\n",
      "Epoch 01986: loss did not improve\n",
      "Epoch 1987/10000\n",
      "2700/2700 [==============================] - 0s 31us/step - loss: 4.1537e-04 - mean_absolute_error: 0.0128\n",
      "\n",
      "Epoch 01987: loss did not improve\n",
      "Epoch 1988/10000\n",
      "2700/2700 [==============================] - 0s 31us/step - loss: 4.0506e-04 - mean_absolute_error: 0.0123\n",
      "\n",
      "Epoch 01988: loss improved from 0.00041 to 0.00041, saving model to keras/best_baseline_weights.hdf5\n",
      "Epoch 1989/10000\n",
      "2700/2700 [==============================] - 0s 36us/step - loss: 4.0579e-04 - mean_absolute_error: 0.0123\n",
      "\n",
      "Epoch 01989: loss did not improve\n",
      "Epoch 1990/10000\n",
      "2700/2700 [==============================] - 0s 36us/step - loss: 4.0815e-04 - mean_absolute_error: 0.0123\n",
      "\n",
      "Epoch 01990: loss did not improve\n",
      "Epoch 1991/10000\n",
      "2700/2700 [==============================] - 0s 29us/step - loss: 4.0773e-04 - mean_absolute_error: 0.0124\n",
      "\n",
      "Epoch 01991: loss did not improve\n",
      "Epoch 1992/10000\n",
      "2700/2700 [==============================] - 0s 27us/step - loss: 4.0779e-04 - mean_absolute_error: 0.0124\n",
      "\n",
      "Epoch 01992: loss did not improve\n",
      "Epoch 1993/10000\n",
      "2700/2700 [==============================] - 0s 33us/step - loss: 4.0408e-04 - mean_absolute_error: 0.0123\n",
      "\n",
      "Epoch 01993: loss improved from 0.00041 to 0.00040, saving model to keras/best_baseline_weights.hdf5\n",
      "Epoch 1994/10000\n",
      "2700/2700 [==============================] - 0s 32us/step - loss: 4.0744e-04 - mean_absolute_error: 0.0123\n",
      "\n",
      "Epoch 01994: loss did not improve\n",
      "Epoch 1995/10000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2700/2700 [==============================] - 0s 29us/step - loss: 4.1111e-04 - mean_absolute_error: 0.0126\n",
      "\n",
      "Epoch 01995: loss did not improve\n",
      "Epoch 1996/10000\n",
      "2700/2700 [==============================] - 0s 34us/step - loss: 4.1194e-04 - mean_absolute_error: 0.0125\n",
      "\n",
      "Epoch 01996: loss did not improve\n",
      "Epoch 1997/10000\n",
      "2700/2700 [==============================] - 0s 31us/step - loss: 4.0759e-04 - mean_absolute_error: 0.0124\n",
      "\n",
      "Epoch 01997: loss did not improve\n",
      "Epoch 1998/10000\n",
      "2700/2700 [==============================] - 0s 30us/step - loss: 4.0635e-04 - mean_absolute_error: 0.0123\n",
      "\n",
      "Epoch 01998: loss did not improve\n",
      "Epoch 1999/10000\n",
      "2700/2700 [==============================] - 0s 32us/step - loss: 4.0906e-04 - mean_absolute_error: 0.0125\n",
      "\n",
      "Epoch 01999: loss did not improve\n",
      "Epoch 2000/10000\n",
      "2700/2700 [==============================] - 0s 29us/step - loss: 4.0497e-04 - mean_absolute_error: 0.0122\n",
      "\n",
      "Epoch 02000: loss did not improve\n",
      "Epoch 2001/10000\n",
      "2700/2700 [==============================] - 0s 31us/step - loss: 4.1030e-04 - mean_absolute_error: 0.0125\n",
      "\n",
      "Epoch 02001: loss did not improve\n",
      "Epoch 2002/10000\n",
      "2700/2700 [==============================] - 0s 28us/step - loss: 4.0551e-04 - mean_absolute_error: 0.0123\n",
      "\n",
      "Epoch 02002: loss did not improve\n",
      "Epoch 2003/10000\n",
      "2700/2700 [==============================] - 0s 29us/step - loss: 4.0577e-04 - mean_absolute_error: 0.0123\n",
      "\n",
      "Epoch 02003: loss did not improve\n",
      "Epoch 2004/10000\n",
      "2700/2700 [==============================] - 0s 28us/step - loss: 4.0485e-04 - mean_absolute_error: 0.0124\n",
      "\n",
      "Epoch 02004: loss did not improve\n",
      "Epoch 2005/10000\n",
      "2700/2700 [==============================] - 0s 27us/step - loss: 4.0618e-04 - mean_absolute_error: 0.0123\n",
      "\n",
      "Epoch 02005: loss did not improve\n",
      "Epoch 2006/10000\n",
      "2700/2700 [==============================] - 0s 31us/step - loss: 4.1186e-04 - mean_absolute_error: 0.0128\n",
      "\n",
      "Epoch 02006: loss did not improve\n",
      "Epoch 2007/10000\n",
      "2700/2700 [==============================] - 0s 34us/step - loss: 4.1176e-04 - mean_absolute_error: 0.0125\n",
      "\n",
      "Epoch 02007: loss did not improve\n",
      "Epoch 2008/10000\n",
      "2700/2700 [==============================] - 0s 30us/step - loss: 4.0709e-04 - mean_absolute_error: 0.0124\n",
      "\n",
      "Epoch 02008: loss did not improve\n",
      "Epoch 2009/10000\n",
      "2700/2700 [==============================] - 0s 32us/step - loss: 4.0453e-04 - mean_absolute_error: 0.0122\n",
      "\n",
      "Epoch 02009: loss did not improve\n",
      "Epoch 2010/10000\n",
      "2700/2700 [==============================] - 0s 28us/step - loss: 4.2625e-04 - mean_absolute_error: 0.0132\n",
      "\n",
      "Epoch 02010: loss did not improve\n",
      "Epoch 2011/10000\n",
      "2700/2700 [==============================] - 0s 31us/step - loss: 4.0849e-04 - mean_absolute_error: 0.0125\n",
      "\n",
      "Epoch 02011: loss did not improve\n",
      "Epoch 2012/10000\n",
      "2700/2700 [==============================] - 0s 32us/step - loss: 4.2631e-04 - mean_absolute_error: 0.0133\n",
      "\n",
      "Epoch 02012: loss did not improve\n",
      "Epoch 2013/10000\n",
      "2700/2700 [==============================] - 0s 28us/step - loss: 4.0384e-04 - mean_absolute_error: 0.0123\n",
      "\n",
      "Epoch 02013: loss improved from 0.00040 to 0.00040, saving model to keras/best_baseline_weights.hdf5\n",
      "Epoch 2014/10000\n",
      "2700/2700 [==============================] - 0s 35us/step - loss: 4.0738e-04 - mean_absolute_error: 0.0124\n",
      "\n",
      "Epoch 02014: loss did not improve\n",
      "Epoch 2015/10000\n",
      "2700/2700 [==============================] - 0s 38us/step - loss: 4.0201e-04 - mean_absolute_error: 0.0122\n",
      "\n",
      "Epoch 02015: loss improved from 0.00040 to 0.00040, saving model to keras/best_baseline_weights.hdf5\n",
      "Epoch 2016/10000\n",
      "2700/2700 [==============================] - 0s 31us/step - loss: 4.0833e-04 - mean_absolute_error: 0.0123\n",
      "\n",
      "Epoch 02016: loss did not improve\n",
      "Epoch 2017/10000\n",
      "2700/2700 [==============================] - 0s 36us/step - loss: 4.0439e-04 - mean_absolute_error: 0.0124\n",
      "\n",
      "Epoch 02017: loss did not improve\n",
      "Epoch 2018/10000\n",
      "2700/2700 [==============================] - 0s 29us/step - loss: 4.0548e-04 - mean_absolute_error: 0.0122\n",
      "\n",
      "Epoch 02018: loss did not improve\n",
      "Epoch 2019/10000\n",
      "2700/2700 [==============================] - 0s 28us/step - loss: 4.0297e-04 - mean_absolute_error: 0.0122\n",
      "\n",
      "Epoch 02019: loss did not improve\n",
      "Epoch 2020/10000\n",
      "2700/2700 [==============================] - 0s 29us/step - loss: 4.0699e-04 - mean_absolute_error: 0.0123\n",
      "\n",
      "Epoch 02020: loss did not improve\n",
      "Epoch 2021/10000\n",
      "2700/2700 [==============================] - 0s 30us/step - loss: 4.0393e-04 - mean_absolute_error: 0.0122\n",
      "\n",
      "Epoch 02021: loss did not improve\n",
      "Epoch 2022/10000\n",
      "2700/2700 [==============================] - 0s 33us/step - loss: 4.0835e-04 - mean_absolute_error: 0.0125\n",
      "\n",
      "Epoch 02022: loss did not improve\n",
      "Epoch 2023/10000\n",
      "2700/2700 [==============================] - 0s 30us/step - loss: 4.0859e-04 - mean_absolute_error: 0.0125\n",
      "\n",
      "Epoch 02023: loss did not improve\n",
      "Epoch 2024/10000\n",
      "2700/2700 [==============================] - 0s 35us/step - loss: 4.1075e-04 - mean_absolute_error: 0.0125\n",
      "\n",
      "Epoch 02024: loss did not improve\n",
      "Epoch 2025/10000\n",
      "2700/2700 [==============================] - 0s 32us/step - loss: 4.0301e-04 - mean_absolute_error: 0.0122\n",
      "\n",
      "Epoch 02025: loss did not improve\n",
      "Epoch 2026/10000\n",
      "2700/2700 [==============================] - 0s 27us/step - loss: 4.0947e-04 - mean_absolute_error: 0.0124\n",
      "\n",
      "Epoch 02026: loss did not improve\n",
      "Epoch 2027/10000\n",
      "2700/2700 [==============================] - 0s 31us/step - loss: 4.0670e-04 - mean_absolute_error: 0.0124\n",
      "\n",
      "Epoch 02027: loss did not improve\n",
      "Epoch 2028/10000\n",
      "2700/2700 [==============================] - 0s 29us/step - loss: 4.0248e-04 - mean_absolute_error: 0.0121\n",
      "\n",
      "Epoch 02028: loss did not improve\n",
      "Epoch 2029/10000\n",
      "2700/2700 [==============================] - 0s 31us/step - loss: 4.0361e-04 - mean_absolute_error: 0.0121\n",
      "\n",
      "Epoch 02029: loss did not improve\n",
      "Epoch 2030/10000\n",
      "2700/2700 [==============================] - 0s 31us/step - loss: 4.0065e-04 - mean_absolute_error: 0.0122\n",
      "\n",
      "Epoch 02030: loss improved from 0.00040 to 0.00040, saving model to keras/best_baseline_weights.hdf5\n",
      "Epoch 2031/10000\n",
      "2700/2700 [==============================] - 0s 34us/step - loss: 4.0361e-04 - mean_absolute_error: 0.0122\n",
      "\n",
      "Epoch 02031: loss did not improve\n",
      "Epoch 2032/10000\n",
      "2700/2700 [==============================] - 0s 33us/step - loss: 4.0836e-04 - mean_absolute_error: 0.0124\n",
      "\n",
      "Epoch 02032: loss did not improve\n",
      "Epoch 2033/10000\n",
      "2700/2700 [==============================] - 0s 30us/step - loss: 4.0336e-04 - mean_absolute_error: 0.0122\n",
      "\n",
      "Epoch 02033: loss did not improve\n",
      "Epoch 2034/10000\n",
      "2700/2700 [==============================] - 0s 28us/step - loss: 4.0674e-04 - mean_absolute_error: 0.0123\n",
      "\n",
      "Epoch 02034: loss did not improve\n",
      "Epoch 2035/10000\n",
      "2700/2700 [==============================] - 0s 33us/step - loss: 4.0312e-04 - mean_absolute_error: 0.0122\n",
      "\n",
      "Epoch 02035: loss did not improve\n",
      "Epoch 2036/10000\n",
      "2700/2700 [==============================] - 0s 28us/step - loss: 4.0256e-04 - mean_absolute_error: 0.0122\n",
      "\n",
      "Epoch 02036: loss did not improve\n",
      "Epoch 2037/10000\n",
      "2700/2700 [==============================] - 0s 31us/step - loss: 4.0832e-04 - mean_absolute_error: 0.0124\n",
      "\n",
      "Epoch 02037: loss did not improve\n",
      "Epoch 2038/10000\n",
      "2700/2700 [==============================] - 0s 33us/step - loss: 4.0971e-04 - mean_absolute_error: 0.0125\n",
      "\n",
      "Epoch 02038: loss did not improve\n",
      "Epoch 2039/10000\n",
      "2700/2700 [==============================] - 0s 34us/step - loss: 4.0865e-04 - mean_absolute_error: 0.0124\n",
      "\n",
      "Epoch 02039: loss did not improve\n",
      "Epoch 2040/10000\n",
      "2700/2700 [==============================] - 0s 33us/step - loss: 4.0500e-04 - mean_absolute_error: 0.0122\n",
      "\n",
      "Epoch 02040: loss did not improve\n",
      "Epoch 2041/10000\n",
      "2700/2700 [==============================] - 0s 30us/step - loss: 4.0325e-04 - mean_absolute_error: 0.0122\n",
      "\n",
      "Epoch 02041: loss did not improve\n",
      "Epoch 2042/10000\n",
      "2700/2700 [==============================] - 0s 30us/step - loss: 4.0368e-04 - mean_absolute_error: 0.0123\n",
      "\n",
      "Epoch 02042: loss did not improve\n",
      "Epoch 2043/10000\n",
      "2700/2700 [==============================] - 0s 31us/step - loss: 4.0658e-04 - mean_absolute_error: 0.0123\n",
      "\n",
      "Epoch 02043: loss did not improve\n",
      "Epoch 2044/10000\n",
      "2700/2700 [==============================] - 0s 30us/step - loss: 4.0296e-04 - mean_absolute_error: 0.0122\n",
      "\n",
      "Epoch 02044: loss did not improve\n",
      "Epoch 2045/10000\n",
      "2700/2700 [==============================] - 0s 31us/step - loss: 4.0866e-04 - mean_absolute_error: 0.0126\n",
      "\n",
      "Epoch 02045: loss did not improve\n",
      "Epoch 2046/10000\n",
      "2700/2700 [==============================] - 0s 30us/step - loss: 4.0680e-04 - mean_absolute_error: 0.0124\n",
      "\n",
      "Epoch 02046: loss did not improve\n",
      "Epoch 2047/10000\n",
      "2700/2700 [==============================] - 0s 30us/step - loss: 4.0580e-04 - mean_absolute_error: 0.0123\n",
      "\n",
      "Epoch 02047: loss did not improve\n",
      "Epoch 2048/10000\n",
      "2700/2700 [==============================] - 0s 31us/step - loss: 4.0625e-04 - mean_absolute_error: 0.0125\n",
      "\n",
      "Epoch 02048: loss did not improve\n",
      "Epoch 2049/10000\n",
      "2700/2700 [==============================] - 0s 31us/step - loss: 4.0211e-04 - mean_absolute_error: 0.0122\n",
      "\n",
      "Epoch 02049: loss did not improve\n",
      "Epoch 2050/10000\n",
      "2700/2700 [==============================] - 0s 26us/step - loss: 4.0087e-04 - mean_absolute_error: 0.0121\n",
      "\n",
      "Epoch 02050: loss did not improve\n",
      "Epoch 2051/10000\n",
      "2700/2700 [==============================] - 0s 30us/step - loss: 4.0504e-04 - mean_absolute_error: 0.0123\n",
      "\n",
      "Epoch 02051: loss did not improve\n",
      "Epoch 2052/10000\n",
      "2700/2700 [==============================] - 0s 28us/step - loss: 4.0281e-04 - mean_absolute_error: 0.0122\n",
      "\n",
      "Epoch 02052: loss did not improve\n",
      "Epoch 2053/10000\n",
      "2700/2700 [==============================] - 0s 29us/step - loss: 4.0268e-04 - mean_absolute_error: 0.0122\n",
      "\n",
      "Epoch 02053: loss did not improve\n",
      "Epoch 2054/10000\n",
      "2700/2700 [==============================] - 0s 31us/step - loss: 4.0501e-04 - mean_absolute_error: 0.0123\n",
      "\n",
      "Epoch 02054: loss did not improve\n",
      "Epoch 2055/10000\n",
      "2700/2700 [==============================] - 0s 32us/step - loss: 4.0819e-04 - mean_absolute_error: 0.0124\n",
      "\n",
      "Epoch 02055: loss did not improve\n",
      "Epoch 2056/10000\n",
      "2700/2700 [==============================] - 0s 30us/step - loss: 4.0264e-04 - mean_absolute_error: 0.0122\n",
      "\n",
      "Epoch 02056: loss did not improve\n",
      "Epoch 2057/10000\n",
      "2700/2700 [==============================] - 0s 29us/step - loss: 4.0034e-04 - mean_absolute_error: 0.0122\n",
      "\n",
      "Epoch 02057: loss improved from 0.00040 to 0.00040, saving model to keras/best_baseline_weights.hdf5\n",
      "Epoch 2058/10000\n",
      "2700/2700 [==============================] - 0s 31us/step - loss: 4.0776e-04 - mean_absolute_error: 0.0126\n",
      "\n",
      "Epoch 02058: loss did not improve\n",
      "Epoch 2059/10000\n",
      "2700/2700 [==============================] - 0s 29us/step - loss: 4.0262e-04 - mean_absolute_error: 0.0122\n",
      "\n",
      "Epoch 02059: loss did not improve\n",
      "Epoch 2060/10000\n",
      "2700/2700 [==============================] - 0s 37us/step - loss: 4.0191e-04 - mean_absolute_error: 0.0122\n",
      "\n",
      "Epoch 02060: loss did not improve\n",
      "Epoch 2061/10000\n",
      "2700/2700 [==============================] - 0s 34us/step - loss: 4.0142e-04 - mean_absolute_error: 0.0122\n",
      "\n",
      "Epoch 02061: loss did not improve\n",
      "Epoch 2062/10000\n",
      "2700/2700 [==============================] - 0s 33us/step - loss: 4.0645e-04 - mean_absolute_error: 0.0124\n",
      "\n",
      "Epoch 02062: loss did not improve\n",
      "Epoch 2063/10000\n",
      "2700/2700 [==============================] - 0s 32us/step - loss: 4.0248e-04 - mean_absolute_error: 0.0123\n",
      "\n",
      "Epoch 02063: loss did not improve\n",
      "Epoch 2064/10000\n",
      "2700/2700 [==============================] - 0s 30us/step - loss: 4.0102e-04 - mean_absolute_error: 0.0122\n",
      "\n",
      "Epoch 02064: loss did not improve\n",
      "Epoch 2065/10000\n",
      "2700/2700 [==============================] - 0s 33us/step - loss: 4.0476e-04 - mean_absolute_error: 0.0124\n",
      "\n",
      "Epoch 02065: loss did not improve\n",
      "Epoch 2066/10000\n",
      "2700/2700 [==============================] - 0s 29us/step - loss: 4.0137e-04 - mean_absolute_error: 0.0122\n",
      "\n",
      "Epoch 02066: loss did not improve\n",
      "Epoch 2067/10000\n",
      "2700/2700 [==============================] - 0s 33us/step - loss: 4.0881e-04 - mean_absolute_error: 0.0126\n",
      "\n",
      "Epoch 02067: loss did not improve\n",
      "Epoch 2068/10000\n",
      "2700/2700 [==============================] - 0s 32us/step - loss: 4.1296e-04 - mean_absolute_error: 0.0126\n",
      "\n",
      "Epoch 02068: loss did not improve\n",
      "Epoch 2069/10000\n",
      "2700/2700 [==============================] - 0s 32us/step - loss: 4.0210e-04 - mean_absolute_error: 0.0123\n",
      "\n",
      "Epoch 02069: loss did not improve\n",
      "Epoch 2070/10000\n",
      "2700/2700 [==============================] - 0s 29us/step - loss: 4.0175e-04 - mean_absolute_error: 0.0121\n",
      "\n",
      "Epoch 02070: loss did not improve\n",
      "Epoch 2071/10000\n",
      "2700/2700 [==============================] - 0s 30us/step - loss: 4.0533e-04 - mean_absolute_error: 0.0124\n",
      "\n",
      "Epoch 02071: loss did not improve\n",
      "Epoch 2072/10000\n",
      "2700/2700 [==============================] - 0s 32us/step - loss: 4.0234e-04 - mean_absolute_error: 0.0122\n",
      "\n",
      "Epoch 02072: loss did not improve\n",
      "Epoch 2073/10000\n",
      "2700/2700 [==============================] - 0s 34us/step - loss: 4.0062e-04 - mean_absolute_error: 0.0121\n",
      "\n",
      "Epoch 02073: loss did not improve\n",
      "Epoch 2074/10000\n",
      "2700/2700 [==============================] - 0s 32us/step - loss: 4.0104e-04 - mean_absolute_error: 0.0121\n",
      "\n",
      "Epoch 02074: loss did not improve\n",
      "Epoch 2075/10000\n",
      "2700/2700 [==============================] - 0s 32us/step - loss: 4.0319e-04 - mean_absolute_error: 0.0123\n",
      "\n",
      "Epoch 02075: loss did not improve\n",
      "Epoch 2076/10000\n",
      "2700/2700 [==============================] - 0s 30us/step - loss: 3.9863e-04 - mean_absolute_error: 0.0120\n",
      "\n",
      "Epoch 02076: loss improved from 0.00040 to 0.00040, saving model to keras/best_baseline_weights.hdf5\n",
      "Epoch 2077/10000\n",
      "2700/2700 [==============================] - 0s 30us/step - loss: 3.9836e-04 - mean_absolute_error: 0.0121\n",
      "\n",
      "Epoch 02077: loss improved from 0.00040 to 0.00040, saving model to keras/best_baseline_weights.hdf5\n",
      "Epoch 2078/10000\n",
      "2700/2700 [==============================] - 0s 31us/step - loss: 3.9749e-04 - mean_absolute_error: 0.0120\n",
      "\n",
      "Epoch 02078: loss improved from 0.00040 to 0.00040, saving model to keras/best_baseline_weights.hdf5\n",
      "Epoch 2079/10000\n",
      "2700/2700 [==============================] - 0s 36us/step - loss: 4.0770e-04 - mean_absolute_error: 0.0124\n",
      "\n",
      "Epoch 02079: loss did not improve\n",
      "Epoch 2080/10000\n",
      "2700/2700 [==============================] - 0s 28us/step - loss: 4.0034e-04 - mean_absolute_error: 0.0121\n",
      "\n",
      "Epoch 02080: loss did not improve\n",
      "Epoch 2081/10000\n",
      "2700/2700 [==============================] - 0s 31us/step - loss: 4.0601e-04 - mean_absolute_error: 0.0125\n",
      "\n",
      "Epoch 02081: loss did not improve\n",
      "Epoch 2082/10000\n",
      "2700/2700 [==============================] - 0s 29us/step - loss: 4.0132e-04 - mean_absolute_error: 0.0121\n",
      "\n",
      "Epoch 02082: loss did not improve\n",
      "Epoch 2083/10000\n",
      "2700/2700 [==============================] - 0s 30us/step - loss: 3.9963e-04 - mean_absolute_error: 0.0121\n",
      "\n",
      "Epoch 02083: loss did not improve\n",
      "Epoch 2084/10000\n",
      "2700/2700 [==============================] - 0s 28us/step - loss: 3.9512e-04 - mean_absolute_error: 0.0119\n",
      "\n",
      "Epoch 02084: loss improved from 0.00040 to 0.00040, saving model to keras/best_baseline_weights.hdf5\n",
      "Epoch 2085/10000\n",
      "2700/2700 [==============================] - 0s 29us/step - loss: 3.9968e-04 - mean_absolute_error: 0.0122\n",
      "\n",
      "Epoch 02085: loss did not improve\n",
      "Epoch 2086/10000\n",
      "2700/2700 [==============================] - 0s 30us/step - loss: 4.0094e-04 - mean_absolute_error: 0.0122\n",
      "\n",
      "Epoch 02086: loss did not improve\n",
      "Epoch 2087/10000\n",
      "2700/2700 [==============================] - 0s 30us/step - loss: 3.9674e-04 - mean_absolute_error: 0.0119\n",
      "\n",
      "Epoch 02087: loss did not improve\n",
      "Epoch 2088/10000\n",
      "2700/2700 [==============================] - 0s 29us/step - loss: 4.0065e-04 - mean_absolute_error: 0.0123\n",
      "\n",
      "Epoch 02088: loss did not improve\n",
      "Epoch 2089/10000\n",
      "2700/2700 [==============================] - 0s 29us/step - loss: 3.9899e-04 - mean_absolute_error: 0.0120\n",
      "\n",
      "Epoch 02089: loss did not improve\n",
      "Epoch 2090/10000\n",
      "2700/2700 [==============================] - 0s 29us/step - loss: 4.0217e-04 - mean_absolute_error: 0.0122\n",
      "\n",
      "Epoch 02090: loss did not improve\n",
      "Epoch 2091/10000\n",
      "2700/2700 [==============================] - 0s 30us/step - loss: 3.9591e-04 - mean_absolute_error: 0.0119\n",
      "\n",
      "Epoch 02091: loss did not improve\n",
      "Epoch 2092/10000\n",
      "2700/2700 [==============================] - 0s 38us/step - loss: 3.9941e-04 - mean_absolute_error: 0.0121\n",
      "\n",
      "Epoch 02092: loss did not improve\n",
      "Epoch 2093/10000\n",
      "2700/2700 [==============================] - 0s 36us/step - loss: 4.0222e-04 - mean_absolute_error: 0.0122\n",
      "\n",
      "Epoch 02093: loss did not improve\n",
      "Epoch 2094/10000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2700/2700 [==============================] - 0s 34us/step - loss: 3.9878e-04 - mean_absolute_error: 0.0122\n",
      "\n",
      "Epoch 02094: loss did not improve\n",
      "Epoch 2095/10000\n",
      "2700/2700 [==============================] - 0s 27us/step - loss: 3.9448e-04 - mean_absolute_error: 0.0118\n",
      "\n",
      "Epoch 02095: loss improved from 0.00040 to 0.00039, saving model to keras/best_baseline_weights.hdf5\n",
      "Epoch 2096/10000\n",
      "2700/2700 [==============================] - 0s 26us/step - loss: 3.9718e-04 - mean_absolute_error: 0.0122\n",
      "\n",
      "Epoch 02096: loss did not improve\n",
      "Epoch 2097/10000\n",
      "2700/2700 [==============================] - 0s 30us/step - loss: 3.9582e-04 - mean_absolute_error: 0.0119\n",
      "\n",
      "Epoch 02097: loss did not improve\n",
      "Epoch 2098/10000\n",
      "2700/2700 [==============================] - 0s 29us/step - loss: 3.9866e-04 - mean_absolute_error: 0.0122\n",
      "\n",
      "Epoch 02098: loss did not improve\n",
      "Epoch 2099/10000\n",
      "2700/2700 [==============================] - 0s 31us/step - loss: 3.9330e-04 - mean_absolute_error: 0.0120\n",
      "\n",
      "Epoch 02099: loss improved from 0.00039 to 0.00039, saving model to keras/best_baseline_weights.hdf5\n",
      "Epoch 2100/10000\n",
      "2700/2700 [==============================] - 0s 33us/step - loss: 3.9883e-04 - mean_absolute_error: 0.0121\n",
      "\n",
      "Epoch 02100: loss did not improve\n",
      "Epoch 2101/10000\n",
      "2700/2700 [==============================] - 0s 31us/step - loss: 3.9836e-04 - mean_absolute_error: 0.0120\n",
      "\n",
      "Epoch 02101: loss did not improve\n",
      "Epoch 2102/10000\n",
      "2700/2700 [==============================] - 0s 29us/step - loss: 3.9924e-04 - mean_absolute_error: 0.0121\n",
      "\n",
      "Epoch 02102: loss did not improve\n",
      "Epoch 2103/10000\n",
      "2700/2700 [==============================] - 0s 32us/step - loss: 4.0491e-04 - mean_absolute_error: 0.0124\n",
      "\n",
      "Epoch 02103: loss did not improve\n",
      "Epoch 2104/10000\n",
      "2700/2700 [==============================] - 0s 28us/step - loss: 3.9889e-04 - mean_absolute_error: 0.0121\n",
      "\n",
      "Epoch 02104: loss did not improve\n",
      "Epoch 2105/10000\n",
      "2700/2700 [==============================] - 0s 29us/step - loss: 3.9791e-04 - mean_absolute_error: 0.0121\n",
      "\n",
      "Epoch 02105: loss did not improve\n",
      "Epoch 2106/10000\n",
      "2700/2700 [==============================] - 0s 27us/step - loss: 3.9301e-04 - mean_absolute_error: 0.0119\n",
      "\n",
      "Epoch 02106: loss improved from 0.00039 to 0.00039, saving model to keras/best_baseline_weights.hdf5\n",
      "Epoch 2107/10000\n",
      "2700/2700 [==============================] - 0s 30us/step - loss: 3.9586e-04 - mean_absolute_error: 0.0120\n",
      "\n",
      "Epoch 02107: loss did not improve\n",
      "Epoch 2108/10000\n",
      "2700/2700 [==============================] - 0s 33us/step - loss: 3.9656e-04 - mean_absolute_error: 0.0122\n",
      "\n",
      "Epoch 02108: loss did not improve\n",
      "Epoch 2109/10000\n",
      "2700/2700 [==============================] - 0s 35us/step - loss: 3.9394e-04 - mean_absolute_error: 0.0119\n",
      "\n",
      "Epoch 02109: loss did not improve\n",
      "Epoch 2110/10000\n",
      "2700/2700 [==============================] - 0s 33us/step - loss: 4.0470e-04 - mean_absolute_error: 0.0125\n",
      "\n",
      "Epoch 02110: loss did not improve\n",
      "Epoch 2111/10000\n",
      "2700/2700 [==============================] - 0s 29us/step - loss: 3.9283e-04 - mean_absolute_error: 0.0119\n",
      "\n",
      "Epoch 02111: loss improved from 0.00039 to 0.00039, saving model to keras/best_baseline_weights.hdf5\n",
      "Epoch 2112/10000\n",
      "2700/2700 [==============================] - 0s 29us/step - loss: 3.9330e-04 - mean_absolute_error: 0.0119\n",
      "\n",
      "Epoch 02112: loss did not improve\n",
      "Epoch 2113/10000\n",
      "2700/2700 [==============================] - 0s 31us/step - loss: 4.0162e-04 - mean_absolute_error: 0.0123\n",
      "\n",
      "Epoch 02113: loss did not improve\n",
      "Epoch 2114/10000\n",
      "2700/2700 [==============================] - 0s 32us/step - loss: 3.9415e-04 - mean_absolute_error: 0.0119\n",
      "\n",
      "Epoch 02114: loss did not improve\n",
      "Epoch 2115/10000\n",
      "2700/2700 [==============================] - 0s 30us/step - loss: 3.9308e-04 - mean_absolute_error: 0.0119\n",
      "\n",
      "Epoch 02115: loss did not improve\n",
      "Epoch 2116/10000\n",
      "2700/2700 [==============================] - 0s 32us/step - loss: 3.9531e-04 - mean_absolute_error: 0.0120\n",
      "\n",
      "Epoch 02116: loss did not improve\n",
      "Epoch 2117/10000\n",
      "2700/2700 [==============================] - 0s 33us/step - loss: 3.9783e-04 - mean_absolute_error: 0.0121\n",
      "\n",
      "Epoch 02117: loss did not improve\n",
      "Epoch 2118/10000\n",
      "2700/2700 [==============================] - 0s 28us/step - loss: 3.9661e-04 - mean_absolute_error: 0.0121\n",
      "\n",
      "Epoch 02118: loss did not improve\n",
      "Epoch 2119/10000\n",
      "2700/2700 [==============================] - 0s 29us/step - loss: 3.9579e-04 - mean_absolute_error: 0.0119\n",
      "\n",
      "Epoch 02119: loss did not improve\n",
      "Epoch 2120/10000\n",
      "2700/2700 [==============================] - 0s 30us/step - loss: 3.9756e-04 - mean_absolute_error: 0.0122\n",
      "\n",
      "Epoch 02120: loss did not improve\n",
      "Epoch 2121/10000\n",
      "2700/2700 [==============================] - 0s 29us/step - loss: 3.9292e-04 - mean_absolute_error: 0.0119\n",
      "\n",
      "Epoch 02121: loss did not improve\n",
      "Epoch 2122/10000\n",
      "2700/2700 [==============================] - 0s 29us/step - loss: 4.0161e-04 - mean_absolute_error: 0.0124\n",
      "\n",
      "Epoch 02122: loss did not improve\n",
      "Epoch 2123/10000\n",
      "2700/2700 [==============================] - 0s 37us/step - loss: 3.9332e-04 - mean_absolute_error: 0.0120\n",
      "\n",
      "Epoch 02123: loss did not improve\n",
      "Epoch 2124/10000\n",
      "2700/2700 [==============================] - 0s 35us/step - loss: 3.9544e-04 - mean_absolute_error: 0.0120\n",
      "\n",
      "Epoch 02124: loss did not improve\n",
      "Epoch 2125/10000\n",
      "2700/2700 [==============================] - 0s 29us/step - loss: 3.9981e-04 - mean_absolute_error: 0.0122\n",
      "\n",
      "Epoch 02125: loss did not improve\n",
      "Epoch 2126/10000\n",
      "2700/2700 [==============================] - 0s 28us/step - loss: 3.9749e-04 - mean_absolute_error: 0.0120\n",
      "\n",
      "Epoch 02126: loss did not improve\n",
      "Epoch 2127/10000\n",
      "2700/2700 [==============================] - 0s 29us/step - loss: 3.9384e-04 - mean_absolute_error: 0.0119\n",
      "\n",
      "Epoch 02127: loss did not improve\n",
      "Epoch 2128/10000\n",
      "2700/2700 [==============================] - 0s 35us/step - loss: 3.9011e-04 - mean_absolute_error: 0.0119\n",
      "\n",
      "Epoch 02128: loss improved from 0.00039 to 0.00039, saving model to keras/best_baseline_weights.hdf5\n",
      "Epoch 2129/10000\n",
      "2700/2700 [==============================] - 0s 34us/step - loss: 3.9231e-04 - mean_absolute_error: 0.0119\n",
      "\n",
      "Epoch 02129: loss did not improve\n",
      "Epoch 2130/10000\n",
      "2700/2700 [==============================] - 0s 30us/step - loss: 3.9142e-04 - mean_absolute_error: 0.0119\n",
      "\n",
      "Epoch 02130: loss did not improve\n",
      "Epoch 2131/10000\n",
      "2700/2700 [==============================] - 0s 29us/step - loss: 3.9295e-04 - mean_absolute_error: 0.0120\n",
      "\n",
      "Epoch 02131: loss did not improve\n",
      "Epoch 2132/10000\n",
      "2700/2700 [==============================] - 0s 27us/step - loss: 3.9295e-04 - mean_absolute_error: 0.0120\n",
      "\n",
      "Epoch 02132: loss did not improve\n",
      "Epoch 2133/10000\n",
      "2700/2700 [==============================] - 0s 30us/step - loss: 3.9170e-04 - mean_absolute_error: 0.0119\n",
      "\n",
      "Epoch 02133: loss did not improve\n",
      "Epoch 2134/10000\n",
      "2700/2700 [==============================] - 0s 29us/step - loss: 3.9085e-04 - mean_absolute_error: 0.0118\n",
      "\n",
      "Epoch 02134: loss did not improve\n",
      "Epoch 2135/10000\n",
      "2700/2700 [==============================] - 0s 31us/step - loss: 3.9264e-04 - mean_absolute_error: 0.0119\n",
      "\n",
      "Epoch 02135: loss did not improve\n",
      "Epoch 2136/10000\n",
      "2700/2700 [==============================] - 0s 28us/step - loss: 3.9096e-04 - mean_absolute_error: 0.0119\n",
      "\n",
      "Epoch 02136: loss did not improve\n",
      "Epoch 2137/10000\n",
      "2700/2700 [==============================] - 0s 29us/step - loss: 3.9661e-04 - mean_absolute_error: 0.0121\n",
      "\n",
      "Epoch 02137: loss did not improve\n",
      "Epoch 2138/10000\n",
      "2700/2700 [==============================] - 0s 33us/step - loss: 3.9730e-04 - mean_absolute_error: 0.0121\n",
      "\n",
      "Epoch 02138: loss did not improve\n",
      "Epoch 2139/10000\n",
      "2700/2700 [==============================] - 0s 30us/step - loss: 3.9418e-04 - mean_absolute_error: 0.0120\n",
      "\n",
      "Epoch 02139: loss did not improve\n",
      "Epoch 2140/10000\n",
      "2700/2700 [==============================] - 0s 29us/step - loss: 3.9504e-04 - mean_absolute_error: 0.0121\n",
      "\n",
      "Epoch 02140: loss did not improve\n",
      "Epoch 2141/10000\n",
      "2700/2700 [==============================] - 0s 36us/step - loss: 3.9404e-04 - mean_absolute_error: 0.0119\n",
      "\n",
      "Epoch 02141: loss did not improve\n",
      "Epoch 2142/10000\n",
      "2700/2700 [==============================] - 0s 30us/step - loss: 3.9256e-04 - mean_absolute_error: 0.0118\n",
      "\n",
      "Epoch 02142: loss did not improve\n",
      "Epoch 2143/10000\n",
      "2700/2700 [==============================] - 0s 30us/step - loss: 3.9256e-04 - mean_absolute_error: 0.0119\n",
      "\n",
      "Epoch 02143: loss did not improve\n",
      "Epoch 2144/10000\n",
      "2700/2700 [==============================] - 0s 25us/step - loss: 3.9439e-04 - mean_absolute_error: 0.0122\n",
      "\n",
      "Epoch 02144: loss did not improve\n",
      "Epoch 2145/10000\n",
      "2700/2700 [==============================] - 0s 28us/step - loss: 3.9223e-04 - mean_absolute_error: 0.0120\n",
      "\n",
      "Epoch 02145: loss did not improve\n",
      "Epoch 2146/10000\n",
      "2700/2700 [==============================] - 0s 27us/step - loss: 3.9022e-04 - mean_absolute_error: 0.0119\n",
      "\n",
      "Epoch 02146: loss did not improve\n",
      "Epoch 2147/10000\n",
      "2700/2700 [==============================] - 0s 26us/step - loss: 3.9417e-04 - mean_absolute_error: 0.0121\n",
      "\n",
      "Epoch 02147: loss did not improve\n",
      "Epoch 2148/10000\n",
      "2700/2700 [==============================] - 0s 28us/step - loss: 3.9400e-04 - mean_absolute_error: 0.0122\n",
      "\n",
      "Epoch 02148: loss did not improve\n",
      "Epoch 2149/10000\n",
      "2700/2700 [==============================] - 0s 26us/step - loss: 3.9619e-04 - mean_absolute_error: 0.0121\n",
      "\n",
      "Epoch 02149: loss did not improve\n",
      "Epoch 2150/10000\n",
      "2700/2700 [==============================] - 0s 28us/step - loss: 3.9157e-04 - mean_absolute_error: 0.0118\n",
      "\n",
      "Epoch 02150: loss did not improve\n",
      "Epoch 2151/10000\n",
      "2700/2700 [==============================] - 0s 26us/step - loss: 3.9260e-04 - mean_absolute_error: 0.0120\n",
      "\n",
      "Epoch 02151: loss did not improve\n",
      "Epoch 2152/10000\n",
      "2700/2700 [==============================] - 0s 29us/step - loss: 3.9131e-04 - mean_absolute_error: 0.0120\n",
      "\n",
      "Epoch 02152: loss did not improve\n",
      "Epoch 2153/10000\n",
      "2700/2700 [==============================] - 0s 25us/step - loss: 3.9511e-04 - mean_absolute_error: 0.0121\n",
      "\n",
      "Epoch 02153: loss did not improve\n",
      "Epoch 2154/10000\n",
      "2700/2700 [==============================] - 0s 27us/step - loss: 3.9042e-04 - mean_absolute_error: 0.0119: 0s - loss: 4.2293e-04 - mean_absolute_error: 0.012\n",
      "\n",
      "Epoch 02154: loss did not improve\n",
      "Epoch 2155/10000\n",
      "2700/2700 [==============================] - 0s 26us/step - loss: 3.8943e-04 - mean_absolute_error: 0.0119\n",
      "\n",
      "Epoch 02155: loss improved from 0.00039 to 0.00039, saving model to keras/best_baseline_weights.hdf5\n",
      "Epoch 2156/10000\n",
      "2700/2700 [==============================] - 0s 28us/step - loss: 3.8927e-04 - mean_absolute_error: 0.0119\n",
      "\n",
      "Epoch 02156: loss improved from 0.00039 to 0.00039, saving model to keras/best_baseline_weights.hdf5\n",
      "Epoch 2157/10000\n",
      "2700/2700 [==============================] - 0s 29us/step - loss: 3.9334e-04 - mean_absolute_error: 0.0122\n",
      "\n",
      "Epoch 02157: loss did not improve\n",
      "Epoch 2158/10000\n",
      "2700/2700 [==============================] - 0s 27us/step - loss: 3.9166e-04 - mean_absolute_error: 0.0119\n",
      "\n",
      "Epoch 02158: loss did not improve\n",
      "Epoch 2159/10000\n",
      "2700/2700 [==============================] - 0s 27us/step - loss: 3.9036e-04 - mean_absolute_error: 0.0119\n",
      "\n",
      "Epoch 02159: loss did not improve\n",
      "Epoch 2160/10000\n",
      "2700/2700 [==============================] - 0s 26us/step - loss: 3.9230e-04 - mean_absolute_error: 0.0120\n",
      "\n",
      "Epoch 02160: loss did not improve\n",
      "Epoch 2161/10000\n",
      "2700/2700 [==============================] - 0s 28us/step - loss: 3.9276e-04 - mean_absolute_error: 0.0120\n",
      "\n",
      "Epoch 02161: loss did not improve\n",
      "Epoch 2162/10000\n",
      "2700/2700 [==============================] - 0s 26us/step - loss: 3.8852e-04 - mean_absolute_error: 0.0119\n",
      "\n",
      "Epoch 02162: loss improved from 0.00039 to 0.00039, saving model to keras/best_baseline_weights.hdf5\n",
      "Epoch 2163/10000\n",
      "2700/2700 [==============================] - 0s 32us/step - loss: 3.9311e-04 - mean_absolute_error: 0.0122\n",
      "\n",
      "Epoch 02163: loss did not improve\n",
      "Epoch 2164/10000\n",
      "2700/2700 [==============================] - 0s 26us/step - loss: 3.9378e-04 - mean_absolute_error: 0.0121\n",
      "\n",
      "Epoch 02164: loss did not improve\n",
      "Epoch 2165/10000\n",
      "2700/2700 [==============================] - 0s 26us/step - loss: 3.9136e-04 - mean_absolute_error: 0.0121\n",
      "\n",
      "Epoch 02165: loss did not improve\n",
      "Epoch 2166/10000\n",
      "2700/2700 [==============================] - 0s 27us/step - loss: 3.8627e-04 - mean_absolute_error: 0.0117\n",
      "\n",
      "Epoch 02166: loss improved from 0.00039 to 0.00039, saving model to keras/best_baseline_weights.hdf5\n",
      "Epoch 2167/10000\n",
      "2700/2700 [==============================] - 0s 26us/step - loss: 3.8782e-04 - mean_absolute_error: 0.0118\n",
      "\n",
      "Epoch 02167: loss did not improve\n",
      "Epoch 2168/10000\n",
      "2700/2700 [==============================] - 0s 26us/step - loss: 3.8890e-04 - mean_absolute_error: 0.0118\n",
      "\n",
      "Epoch 02168: loss did not improve\n",
      "Epoch 2169/10000\n",
      "2700/2700 [==============================] - 0s 29us/step - loss: 3.9273e-04 - mean_absolute_error: 0.0120\n",
      "\n",
      "Epoch 02169: loss did not improve\n",
      "Epoch 2170/10000\n",
      "2700/2700 [==============================] - 0s 29us/step - loss: 3.8921e-04 - mean_absolute_error: 0.0119\n",
      "\n",
      "Epoch 02170: loss did not improve\n",
      "Epoch 2171/10000\n",
      "2700/2700 [==============================] - 0s 27us/step - loss: 3.8948e-04 - mean_absolute_error: 0.0119\n",
      "\n",
      "Epoch 02171: loss did not improve\n",
      "Epoch 2172/10000\n",
      "2700/2700 [==============================] - 0s 27us/step - loss: 3.8596e-04 - mean_absolute_error: 0.0118\n",
      "\n",
      "Epoch 02172: loss improved from 0.00039 to 0.00039, saving model to keras/best_baseline_weights.hdf5\n",
      "Epoch 2173/10000\n",
      "2700/2700 [==============================] - 0s 28us/step - loss: 3.9072e-04 - mean_absolute_error: 0.0120\n",
      "\n",
      "Epoch 02173: loss did not improve\n",
      "Epoch 2174/10000\n",
      "2700/2700 [==============================] - 0s 29us/step - loss: 3.8594e-04 - mean_absolute_error: 0.0118\n",
      "\n",
      "Epoch 02174: loss improved from 0.00039 to 0.00039, saving model to keras/best_baseline_weights.hdf5\n",
      "Epoch 2175/10000\n",
      "2700/2700 [==============================] - 0s 30us/step - loss: 3.9037e-04 - mean_absolute_error: 0.0121\n",
      "\n",
      "Epoch 02175: loss did not improve\n",
      "Epoch 2176/10000\n",
      "2700/2700 [==============================] - 0s 26us/step - loss: 3.8967e-04 - mean_absolute_error: 0.0122\n",
      "\n",
      "Epoch 02176: loss did not improve\n",
      "Epoch 2177/10000\n",
      "2700/2700 [==============================] - 0s 28us/step - loss: 3.8651e-04 - mean_absolute_error: 0.0119\n",
      "\n",
      "Epoch 02177: loss did not improve\n",
      "Epoch 2178/10000\n",
      "2700/2700 [==============================] - 0s 27us/step - loss: 3.8886e-04 - mean_absolute_error: 0.0119\n",
      "\n",
      "Epoch 02178: loss did not improve\n",
      "Epoch 2179/10000\n",
      "2700/2700 [==============================] - 0s 30us/step - loss: 3.9172e-04 - mean_absolute_error: 0.0120\n",
      "\n",
      "Epoch 02179: loss did not improve\n",
      "Epoch 2180/10000\n",
      "2700/2700 [==============================] - 0s 27us/step - loss: 3.8441e-04 - mean_absolute_error: 0.0118\n",
      "\n",
      "Epoch 02180: loss improved from 0.00039 to 0.00038, saving model to keras/best_baseline_weights.hdf5\n",
      "Epoch 2181/10000\n",
      "2700/2700 [==============================] - 0s 27us/step - loss: 3.8728e-04 - mean_absolute_error: 0.0118\n",
      "\n",
      "Epoch 02181: loss did not improve\n",
      "Epoch 2182/10000\n",
      "2700/2700 [==============================] - 0s 28us/step - loss: 4.0198e-04 - mean_absolute_error: 0.0125\n",
      "\n",
      "Epoch 02182: loss did not improve\n",
      "Epoch 2183/10000\n",
      "2700/2700 [==============================] - 0s 29us/step - loss: 3.8771e-04 - mean_absolute_error: 0.0119\n",
      "\n",
      "Epoch 02183: loss did not improve\n",
      "Epoch 2184/10000\n",
      "2700/2700 [==============================] - 0s 27us/step - loss: 3.8427e-04 - mean_absolute_error: 0.0117\n",
      "\n",
      "Epoch 02184: loss improved from 0.00038 to 0.00038, saving model to keras/best_baseline_weights.hdf5\n",
      "Epoch 2185/10000\n",
      "2700/2700 [==============================] - 0s 27us/step - loss: 3.9085e-04 - mean_absolute_error: 0.0121\n",
      "\n",
      "Epoch 02185: loss did not improve\n",
      "Epoch 2186/10000\n",
      "2700/2700 [==============================] - 0s 28us/step - loss: 3.8429e-04 - mean_absolute_error: 0.0118\n",
      "\n",
      "Epoch 02186: loss did not improve\n",
      "Epoch 2187/10000\n",
      "2700/2700 [==============================] - 0s 27us/step - loss: 3.8533e-04 - mean_absolute_error: 0.0118\n",
      "\n",
      "Epoch 02187: loss did not improve\n",
      "Epoch 2188/10000\n",
      "2700/2700 [==============================] - 0s 27us/step - loss: 3.9084e-04 - mean_absolute_error: 0.0122\n",
      "\n",
      "Epoch 02188: loss did not improve\n",
      "Epoch 2189/10000\n",
      "2700/2700 [==============================] - 0s 28us/step - loss: 3.8636e-04 - mean_absolute_error: 0.0119\n",
      "\n",
      "Epoch 02189: loss did not improve\n",
      "Epoch 2190/10000\n",
      "2700/2700 [==============================] - 0s 28us/step - loss: 3.8626e-04 - mean_absolute_error: 0.0118\n",
      "\n",
      "Epoch 02190: loss did not improve\n",
      "Epoch 2191/10000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2700/2700 [==============================] - 0s 35us/step - loss: 3.8504e-04 - mean_absolute_error: 0.0120\n",
      "\n",
      "Epoch 02191: loss did not improve\n",
      "Epoch 2192/10000\n",
      "2700/2700 [==============================] - 0s 51us/step - loss: 3.8936e-04 - mean_absolute_error: 0.0120\n",
      "\n",
      "Epoch 02192: loss did not improve\n",
      "Epoch 2193/10000\n",
      "2700/2700 [==============================] - 0s 52us/step - loss: 3.8409e-04 - mean_absolute_error: 0.0118\n",
      "\n",
      "Epoch 02193: loss improved from 0.00038 to 0.00038, saving model to keras/best_baseline_weights.hdf5\n",
      "Epoch 2194/10000\n",
      "2700/2700 [==============================] - 0s 58us/step - loss: 3.9121e-04 - mean_absolute_error: 0.0122\n",
      "\n",
      "Epoch 02194: loss did not improve\n",
      "Epoch 2195/10000\n",
      "2700/2700 [==============================] - 0s 59us/step - loss: 3.8315e-04 - mean_absolute_error: 0.0118\n",
      "\n",
      "Epoch 02195: loss improved from 0.00038 to 0.00038, saving model to keras/best_baseline_weights.hdf5\n",
      "Epoch 2196/10000\n",
      "2700/2700 [==============================] - 0s 51us/step - loss: 3.8896e-04 - mean_absolute_error: 0.0120\n",
      "\n",
      "Epoch 02196: loss did not improve\n",
      "Epoch 2197/10000\n",
      "2700/2700 [==============================] - 0s 49us/step - loss: 3.8930e-04 - mean_absolute_error: 0.0120\n",
      "\n",
      "Epoch 02197: loss did not improve\n",
      "Epoch 2198/10000\n",
      "2700/2700 [==============================] - 0s 56us/step - loss: 3.9093e-04 - mean_absolute_error: 0.0123\n",
      "\n",
      "Epoch 02198: loss did not improve\n",
      "Epoch 2199/10000\n",
      "2700/2700 [==============================] - 0s 54us/step - loss: 3.8503e-04 - mean_absolute_error: 0.0118\n",
      "\n",
      "Epoch 02199: loss did not improve\n",
      "Epoch 2200/10000\n",
      "2700/2700 [==============================] - 0s 56us/step - loss: 3.8332e-04 - mean_absolute_error: 0.0120\n",
      "\n",
      "Epoch 02200: loss did not improve\n",
      "Epoch 2201/10000\n",
      "2700/2700 [==============================] - 0s 57us/step - loss: 3.8189e-04 - mean_absolute_error: 0.0117\n",
      "\n",
      "Epoch 02201: loss improved from 0.00038 to 0.00038, saving model to keras/best_baseline_weights.hdf5\n",
      "Epoch 2202/10000\n",
      "2700/2700 [==============================] - 0s 41us/step - loss: 3.8430e-04 - mean_absolute_error: 0.0118\n",
      "\n",
      "Epoch 02202: loss did not improve\n",
      "Epoch 2203/10000\n",
      "2700/2700 [==============================] - 0s 35us/step - loss: 3.8648e-04 - mean_absolute_error: 0.0118\n",
      "\n",
      "Epoch 02203: loss did not improve\n",
      "Epoch 2204/10000\n",
      "2700/2700 [==============================] - 0s 38us/step - loss: 3.8462e-04 - mean_absolute_error: 0.0118\n",
      "\n",
      "Epoch 02204: loss did not improve\n",
      "Epoch 2205/10000\n",
      "2700/2700 [==============================] - 0s 34us/step - loss: 3.8538e-04 - mean_absolute_error: 0.0119\n",
      "\n",
      "Epoch 02205: loss did not improve\n",
      "Epoch 2206/10000\n",
      "2700/2700 [==============================] - 0s 36us/step - loss: 3.8877e-04 - mean_absolute_error: 0.0119\n",
      "\n",
      "Epoch 02206: loss did not improve\n",
      "Epoch 2207/10000\n",
      "2700/2700 [==============================] - 0s 31us/step - loss: 3.8403e-04 - mean_absolute_error: 0.0119\n",
      "\n",
      "Epoch 02207: loss did not improve\n",
      "Epoch 2208/10000\n",
      "2700/2700 [==============================] - 0s 30us/step - loss: 3.8258e-04 - mean_absolute_error: 0.0118\n",
      "\n",
      "Epoch 02208: loss did not improve\n",
      "Epoch 2209/10000\n",
      "2700/2700 [==============================] - 0s 29us/step - loss: 3.8196e-04 - mean_absolute_error: 0.0119\n",
      "\n",
      "Epoch 02209: loss did not improve\n",
      "Epoch 2210/10000\n",
      "2700/2700 [==============================] - 0s 30us/step - loss: 3.9116e-04 - mean_absolute_error: 0.0122\n",
      "\n",
      "Epoch 02210: loss did not improve\n",
      "Epoch 2211/10000\n",
      "2700/2700 [==============================] - 0s 30us/step - loss: 3.8986e-04 - mean_absolute_error: 0.0121\n",
      "\n",
      "Epoch 02211: loss did not improve\n",
      "Epoch 2212/10000\n",
      "2700/2700 [==============================] - 0s 31us/step - loss: 3.8578e-04 - mean_absolute_error: 0.0120\n",
      "\n",
      "Epoch 02212: loss did not improve\n",
      "Epoch 2213/10000\n",
      "2700/2700 [==============================] - 0s 30us/step - loss: 3.8436e-04 - mean_absolute_error: 0.0120\n",
      "\n",
      "Epoch 02213: loss did not improve\n",
      "Epoch 2214/10000\n",
      "2700/2700 [==============================] - 0s 30us/step - loss: 3.7997e-04 - mean_absolute_error: 0.0118\n",
      "\n",
      "Epoch 02214: loss improved from 0.00038 to 0.00038, saving model to keras/best_baseline_weights.hdf5\n",
      "Epoch 2215/10000\n",
      "2700/2700 [==============================] - 0s 37us/step - loss: 3.8290e-04 - mean_absolute_error: 0.0119\n",
      "\n",
      "Epoch 02215: loss did not improve\n",
      "Epoch 2216/10000\n",
      "2700/2700 [==============================] - 0s 33us/step - loss: 3.8468e-04 - mean_absolute_error: 0.0119\n",
      "\n",
      "Epoch 02216: loss did not improve\n",
      "Epoch 2217/10000\n",
      "2700/2700 [==============================] - 0s 31us/step - loss: 3.8149e-04 - mean_absolute_error: 0.0119\n",
      "\n",
      "Epoch 02217: loss did not improve\n",
      "Epoch 2218/10000\n",
      "2700/2700 [==============================] - 0s 31us/step - loss: 3.7839e-04 - mean_absolute_error: 0.0117\n",
      "\n",
      "Epoch 02218: loss improved from 0.00038 to 0.00038, saving model to keras/best_baseline_weights.hdf5\n",
      "Epoch 2219/10000\n",
      "2700/2700 [==============================] - 0s 30us/step - loss: 3.8457e-04 - mean_absolute_error: 0.0119\n",
      "\n",
      "Epoch 02219: loss did not improve\n",
      "Epoch 2220/10000\n",
      "2700/2700 [==============================] - 0s 29us/step - loss: 3.8223e-04 - mean_absolute_error: 0.0119\n",
      "\n",
      "Epoch 02220: loss did not improve\n",
      "Epoch 2221/10000\n",
      "2700/2700 [==============================] - 0s 30us/step - loss: 3.8107e-04 - mean_absolute_error: 0.0120\n",
      "\n",
      "Epoch 02221: loss did not improve\n",
      "Epoch 2222/10000\n",
      "2700/2700 [==============================] - 0s 31us/step - loss: 3.8933e-04 - mean_absolute_error: 0.0121\n",
      "\n",
      "Epoch 02222: loss did not improve\n",
      "Epoch 2223/10000\n",
      "2700/2700 [==============================] - 0s 30us/step - loss: 3.8056e-04 - mean_absolute_error: 0.0117\n",
      "\n",
      "Epoch 02223: loss did not improve\n",
      "Epoch 2224/10000\n",
      "2700/2700 [==============================] - 0s 29us/step - loss: 3.7967e-04 - mean_absolute_error: 0.0117\n",
      "\n",
      "Epoch 02224: loss did not improve\n",
      "Epoch 2225/10000\n",
      "2700/2700 [==============================] - 0s 32us/step - loss: 3.8412e-04 - mean_absolute_error: 0.0120\n",
      "\n",
      "Epoch 02225: loss did not improve\n",
      "Epoch 2226/10000\n",
      "2700/2700 [==============================] - 0s 32us/step - loss: 3.7828e-04 - mean_absolute_error: 0.0118\n",
      "\n",
      "Epoch 02226: loss improved from 0.00038 to 0.00038, saving model to keras/best_baseline_weights.hdf5\n",
      "Epoch 2227/10000\n",
      "2700/2700 [==============================] - 0s 33us/step - loss: 3.7969e-04 - mean_absolute_error: 0.0118\n",
      "\n",
      "Epoch 02227: loss did not improve\n",
      "Epoch 2228/10000\n",
      "2700/2700 [==============================] - 0s 31us/step - loss: 3.8568e-04 - mean_absolute_error: 0.0120\n",
      "\n",
      "Epoch 02228: loss did not improve\n",
      "Epoch 2229/10000\n",
      "2700/2700 [==============================] - 0s 29us/step - loss: 3.7986e-04 - mean_absolute_error: 0.0117\n",
      "\n",
      "Epoch 02229: loss did not improve\n",
      "Epoch 2230/10000\n",
      "2700/2700 [==============================] - 0s 30us/step - loss: 3.8439e-04 - mean_absolute_error: 0.0120\n",
      "\n",
      "Epoch 02230: loss did not improve\n",
      "Epoch 2231/10000\n",
      "2700/2700 [==============================] - 0s 29us/step - loss: 3.8013e-04 - mean_absolute_error: 0.0118\n",
      "\n",
      "Epoch 02231: loss did not improve\n",
      "Epoch 2232/10000\n",
      "2700/2700 [==============================] - 0s 36us/step - loss: 3.8438e-04 - mean_absolute_error: 0.0120\n",
      "\n",
      "Epoch 02232: loss did not improve\n",
      "Epoch 2233/10000\n",
      "2700/2700 [==============================] - 0s 35us/step - loss: 3.7991e-04 - mean_absolute_error: 0.0118\n",
      "\n",
      "Epoch 02233: loss did not improve\n",
      "Epoch 2234/10000\n",
      "2700/2700 [==============================] - 0s 30us/step - loss: 3.8564e-04 - mean_absolute_error: 0.0121\n",
      "\n",
      "Epoch 02234: loss did not improve\n",
      "Epoch 2235/10000\n",
      "2700/2700 [==============================] - 0s 31us/step - loss: 3.7792e-04 - mean_absolute_error: 0.0117\n",
      "\n",
      "Epoch 02235: loss improved from 0.00038 to 0.00038, saving model to keras/best_baseline_weights.hdf5\n",
      "Epoch 2236/10000\n",
      "2700/2700 [==============================] - 0s 33us/step - loss: 3.7798e-04 - mean_absolute_error: 0.0118\n",
      "\n",
      "Epoch 02236: loss did not improve\n",
      "Epoch 2237/10000\n",
      "2700/2700 [==============================] - 0s 34us/step - loss: 3.7830e-04 - mean_absolute_error: 0.0118\n",
      "\n",
      "Epoch 02237: loss did not improve\n",
      "Epoch 2238/10000\n",
      "2700/2700 [==============================] - 0s 31us/step - loss: 3.9133e-04 - mean_absolute_error: 0.0123\n",
      "\n",
      "Epoch 02238: loss did not improve\n",
      "Epoch 2239/10000\n",
      "2700/2700 [==============================] - 0s 31us/step - loss: 3.7662e-04 - mean_absolute_error: 0.0117\n",
      "\n",
      "Epoch 02239: loss improved from 0.00038 to 0.00038, saving model to keras/best_baseline_weights.hdf5\n",
      "Epoch 2240/10000\n",
      "2700/2700 [==============================] - 0s 32us/step - loss: 3.8977e-04 - mean_absolute_error: 0.0123\n",
      "\n",
      "Epoch 02240: loss did not improve\n",
      "Epoch 2241/10000\n",
      "2700/2700 [==============================] - 0s 30us/step - loss: 3.8236e-04 - mean_absolute_error: 0.0119\n",
      "\n",
      "Epoch 02241: loss did not improve\n",
      "Epoch 2242/10000\n",
      "2700/2700 [==============================] - 0s 28us/step - loss: 3.8413e-04 - mean_absolute_error: 0.0122\n",
      "\n",
      "Epoch 02242: loss did not improve\n",
      "Epoch 2243/10000\n",
      "2700/2700 [==============================] - 0s 34us/step - loss: 3.7636e-04 - mean_absolute_error: 0.0117\n",
      "\n",
      "Epoch 02243: loss improved from 0.00038 to 0.00038, saving model to keras/best_baseline_weights.hdf5\n",
      "Epoch 2244/10000\n",
      "2700/2700 [==============================] - 0s 56us/step - loss: 3.8003e-04 - mean_absolute_error: 0.0121\n",
      "\n",
      "Epoch 02244: loss did not improve\n",
      "Epoch 2245/10000\n",
      "2700/2700 [==============================] - 0s 37us/step - loss: 3.7898e-04 - mean_absolute_error: 0.0118\n",
      "\n",
      "Epoch 02245: loss did not improve\n",
      "Epoch 2246/10000\n",
      "2700/2700 [==============================] - 0s 41us/step - loss: 3.7818e-04 - mean_absolute_error: 0.0119\n",
      "\n",
      "Epoch 02246: loss did not improve\n",
      "Epoch 2247/10000\n",
      "2700/2700 [==============================] - 0s 57us/step - loss: 3.8165e-04 - mean_absolute_error: 0.0120\n",
      "\n",
      "Epoch 02247: loss did not improve\n",
      "Epoch 2248/10000\n",
      "2700/2700 [==============================] - 0s 33us/step - loss: 3.8117e-04 - mean_absolute_error: 0.0120\n",
      "\n",
      "Epoch 02248: loss did not improve\n",
      "Epoch 2249/10000\n",
      "2700/2700 [==============================] - 0s 29us/step - loss: 3.8296e-04 - mean_absolute_error: 0.0120\n",
      "\n",
      "Epoch 02249: loss did not improve\n",
      "Epoch 2250/10000\n",
      "2700/2700 [==============================] - 0s 31us/step - loss: 3.7660e-04 - mean_absolute_error: 0.0118\n",
      "\n",
      "Epoch 02250: loss did not improve\n",
      "Epoch 2251/10000\n",
      "2700/2700 [==============================] - 0s 29us/step - loss: 3.8789e-04 - mean_absolute_error: 0.0123\n",
      "\n",
      "Epoch 02251: loss did not improve\n",
      "Epoch 2252/10000\n",
      "2700/2700 [==============================] - 0s 29us/step - loss: 3.7934e-04 - mean_absolute_error: 0.0119\n",
      "\n",
      "Epoch 02252: loss did not improve\n",
      "Epoch 2253/10000\n",
      "2700/2700 [==============================] - 0s 28us/step - loss: 3.8344e-04 - mean_absolute_error: 0.0120\n",
      "\n",
      "Epoch 02253: loss did not improve\n",
      "Epoch 2254/10000\n",
      "2700/2700 [==============================] - 0s 28us/step - loss: 3.7473e-04 - mean_absolute_error: 0.0117\n",
      "\n",
      "Epoch 02254: loss improved from 0.00038 to 0.00037, saving model to keras/best_baseline_weights.hdf5\n",
      "Epoch 2255/10000\n",
      "2700/2700 [==============================] - 0s 29us/step - loss: 3.7863e-04 - mean_absolute_error: 0.0119\n",
      "\n",
      "Epoch 02255: loss did not improve\n",
      "Epoch 2256/10000\n",
      "2700/2700 [==============================] - 0s 28us/step - loss: 3.7669e-04 - mean_absolute_error: 0.0117\n",
      "\n",
      "Epoch 02256: loss did not improve\n",
      "Epoch 2257/10000\n",
      "2700/2700 [==============================] - 0s 27us/step - loss: 3.7810e-04 - mean_absolute_error: 0.0118\n",
      "\n",
      "Epoch 02257: loss did not improve\n",
      "Epoch 2258/10000\n",
      "2700/2700 [==============================] - 0s 28us/step - loss: 3.7581e-04 - mean_absolute_error: 0.0118\n",
      "\n",
      "Epoch 02258: loss did not improve\n",
      "Epoch 2259/10000\n",
      "2700/2700 [==============================] - 0s 30us/step - loss: 3.7807e-04 - mean_absolute_error: 0.0119\n",
      "\n",
      "Epoch 02259: loss did not improve\n",
      "Epoch 2260/10000\n",
      "2700/2700 [==============================] - 0s 28us/step - loss: 3.7390e-04 - mean_absolute_error: 0.0117\n",
      "\n",
      "Epoch 02260: loss improved from 0.00037 to 0.00037, saving model to keras/best_baseline_weights.hdf5\n",
      "Epoch 2261/10000\n",
      "2700/2700 [==============================] - 0s 31us/step - loss: 3.7820e-04 - mean_absolute_error: 0.0119\n",
      "\n",
      "Epoch 02261: loss did not improve\n",
      "Epoch 2262/10000\n",
      "2700/2700 [==============================] - 0s 31us/step - loss: 3.7887e-04 - mean_absolute_error: 0.0119\n",
      "\n",
      "Epoch 02262: loss did not improve\n",
      "Epoch 2263/10000\n",
      "2700/2700 [==============================] - 0s 28us/step - loss: 3.8644e-04 - mean_absolute_error: 0.0123\n",
      "\n",
      "Epoch 02263: loss did not improve\n",
      "Epoch 2264/10000\n",
      "2700/2700 [==============================] - 0s 28us/step - loss: 3.7693e-04 - mean_absolute_error: 0.0119\n",
      "\n",
      "Epoch 02264: loss did not improve\n",
      "Epoch 2265/10000\n",
      "2700/2700 [==============================] - 0s 28us/step - loss: 3.7907e-04 - mean_absolute_error: 0.0120\n",
      "\n",
      "Epoch 02265: loss did not improve\n",
      "Epoch 2266/10000\n",
      "2700/2700 [==============================] - 0s 26us/step - loss: 3.7442e-04 - mean_absolute_error: 0.0118\n",
      "\n",
      "Epoch 02266: loss did not improve\n",
      "Epoch 2267/10000\n",
      "2700/2700 [==============================] - 0s 31us/step - loss: 3.7479e-04 - mean_absolute_error: 0.0118\n",
      "\n",
      "Epoch 02267: loss did not improve\n",
      "Epoch 2268/10000\n",
      "2700/2700 [==============================] - 0s 29us/step - loss: 3.7451e-04 - mean_absolute_error: 0.0117\n",
      "\n",
      "Epoch 02268: loss did not improve\n",
      "Epoch 2269/10000\n",
      "2700/2700 [==============================] - 0s 31us/step - loss: 3.7664e-04 - mean_absolute_error: 0.0120\n",
      "\n",
      "Epoch 02269: loss did not improve\n",
      "Epoch 2270/10000\n",
      "2700/2700 [==============================] - 0s 28us/step - loss: 3.8033e-04 - mean_absolute_error: 0.0121\n",
      "\n",
      "Epoch 02270: loss did not improve\n",
      "Epoch 2271/10000\n",
      "2700/2700 [==============================] - 0s 27us/step - loss: 3.7926e-04 - mean_absolute_error: 0.0119\n",
      "\n",
      "Epoch 02271: loss did not improve\n",
      "Epoch 2272/10000\n",
      "2700/2700 [==============================] - 0s 29us/step - loss: 3.7445e-04 - mean_absolute_error: 0.0120\n",
      "\n",
      "Epoch 02272: loss did not improve\n",
      "Epoch 2273/10000\n",
      "2700/2700 [==============================] - 0s 29us/step - loss: 3.7581e-04 - mean_absolute_error: 0.0119\n",
      "\n",
      "Epoch 02273: loss did not improve\n",
      "Epoch 2274/10000\n",
      "2700/2700 [==============================] - 0s 28us/step - loss: 3.7566e-04 - mean_absolute_error: 0.0118\n",
      "\n",
      "Epoch 02274: loss did not improve\n",
      "Epoch 2275/10000\n",
      "2700/2700 [==============================] - 0s 29us/step - loss: 3.7491e-04 - mean_absolute_error: 0.0118\n",
      "\n",
      "Epoch 02275: loss did not improve\n",
      "Epoch 2276/10000\n",
      "2700/2700 [==============================] - 0s 28us/step - loss: 3.7602e-04 - mean_absolute_error: 0.0119\n",
      "\n",
      "Epoch 02276: loss did not improve\n",
      "Epoch 2277/10000\n",
      "2700/2700 [==============================] - 0s 27us/step - loss: 3.7448e-04 - mean_absolute_error: 0.0119\n",
      "\n",
      "Epoch 02277: loss did not improve\n",
      "Epoch 2278/10000\n",
      "2700/2700 [==============================] - 0s 29us/step - loss: 3.7592e-04 - mean_absolute_error: 0.0119\n",
      "\n",
      "Epoch 02278: loss did not improve\n",
      "Epoch 2279/10000\n",
      "2700/2700 [==============================] - 0s 29us/step - loss: 3.7482e-04 - mean_absolute_error: 0.0120\n",
      "\n",
      "Epoch 02279: loss did not improve\n",
      "Epoch 2280/10000\n",
      "2700/2700 [==============================] - 0s 27us/step - loss: 3.7704e-04 - mean_absolute_error: 0.0120\n",
      "\n",
      "Epoch 02280: loss did not improve\n",
      "Epoch 2281/10000\n",
      "2700/2700 [==============================] - 0s 28us/step - loss: 3.7241e-04 - mean_absolute_error: 0.0117\n",
      "\n",
      "Epoch 02281: loss improved from 0.00037 to 0.00037, saving model to keras/best_baseline_weights.hdf5\n",
      "Epoch 2282/10000\n",
      "2700/2700 [==============================] - 0s 31us/step - loss: 3.7406e-04 - mean_absolute_error: 0.0119\n",
      "\n",
      "Epoch 02282: loss did not improve\n",
      "Epoch 2283/10000\n",
      "2700/2700 [==============================] - 0s 29us/step - loss: 3.7379e-04 - mean_absolute_error: 0.0118\n",
      "\n",
      "Epoch 02283: loss did not improve\n",
      "Epoch 2284/10000\n",
      "2700/2700 [==============================] - 0s 30us/step - loss: 3.7656e-04 - mean_absolute_error: 0.0120\n",
      "\n",
      "Epoch 02284: loss did not improve\n",
      "Epoch 2285/10000\n",
      "2700/2700 [==============================] - 0s 29us/step - loss: 3.7312e-04 - mean_absolute_error: 0.0118\n",
      "\n",
      "Epoch 02285: loss did not improve\n",
      "Epoch 2286/10000\n",
      "2700/2700 [==============================] - 0s 30us/step - loss: 3.7263e-04 - mean_absolute_error: 0.0118\n",
      "\n",
      "Epoch 02286: loss did not improve\n",
      "Epoch 2287/10000\n",
      "2700/2700 [==============================] - 0s 30us/step - loss: 3.7391e-04 - mean_absolute_error: 0.0119\n",
      "\n",
      "Epoch 02287: loss did not improve\n",
      "Epoch 2288/10000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2700/2700 [==============================] - 0s 29us/step - loss: 3.7232e-04 - mean_absolute_error: 0.0118\n",
      "\n",
      "Epoch 02288: loss improved from 0.00037 to 0.00037, saving model to keras/best_baseline_weights.hdf5\n",
      "Epoch 2289/10000\n",
      "2700/2700 [==============================] - 0s 30us/step - loss: 3.7440e-04 - mean_absolute_error: 0.0119\n",
      "\n",
      "Epoch 02289: loss did not improve\n",
      "Epoch 2290/10000\n",
      "2700/2700 [==============================] - ETA: 0s - loss: 3.6884e-04 - mean_absolute_error: 0.011 - 0s 29us/step - loss: 3.8266e-04 - mean_absolute_error: 0.0123\n",
      "\n",
      "Epoch 02290: loss did not improve\n",
      "Epoch 2291/10000\n",
      "2700/2700 [==============================] - 0s 28us/step - loss: 3.7329e-04 - mean_absolute_error: 0.0119\n",
      "\n",
      "Epoch 02291: loss did not improve\n",
      "Epoch 2292/10000\n",
      "2700/2700 [==============================] - 0s 28us/step - loss: 3.7544e-04 - mean_absolute_error: 0.0121\n",
      "\n",
      "Epoch 02292: loss did not improve\n",
      "Epoch 2293/10000\n",
      "2700/2700 [==============================] - 0s 28us/step - loss: 3.7340e-04 - mean_absolute_error: 0.0118\n",
      "\n",
      "Epoch 02293: loss did not improve\n",
      "Epoch 2294/10000\n",
      "2700/2700 [==============================] - 0s 28us/step - loss: 3.7231e-04 - mean_absolute_error: 0.0119\n",
      "\n",
      "Epoch 02294: loss improved from 0.00037 to 0.00037, saving model to keras/best_baseline_weights.hdf5\n",
      "Epoch 2295/10000\n",
      "2700/2700 [==============================] - 0s 26us/step - loss: 3.7314e-04 - mean_absolute_error: 0.0119\n",
      "\n",
      "Epoch 02295: loss did not improve\n",
      "Epoch 2296/10000\n",
      "2700/2700 [==============================] - 0s 29us/step - loss: 3.7646e-04 - mean_absolute_error: 0.0121\n",
      "\n",
      "Epoch 02296: loss did not improve\n",
      "Epoch 2297/10000\n",
      "2700/2700 [==============================] - 0s 30us/step - loss: 3.7168e-04 - mean_absolute_error: 0.0118\n",
      "\n",
      "Epoch 02297: loss improved from 0.00037 to 0.00037, saving model to keras/best_baseline_weights.hdf5\n",
      "Epoch 2298/10000\n",
      "2700/2700 [==============================] - 0s 29us/step - loss: 3.7395e-04 - mean_absolute_error: 0.0119\n",
      "\n",
      "Epoch 02298: loss did not improve\n",
      "Epoch 2299/10000\n",
      "2700/2700 [==============================] - 0s 31us/step - loss: 3.7315e-04 - mean_absolute_error: 0.0118\n",
      "\n",
      "Epoch 02299: loss did not improve\n",
      "Epoch 2300/10000\n",
      "2700/2700 [==============================] - 0s 29us/step - loss: 3.7143e-04 - mean_absolute_error: 0.0119\n",
      "\n",
      "Epoch 02300: loss improved from 0.00037 to 0.00037, saving model to keras/best_baseline_weights.hdf5\n",
      "Epoch 2301/10000\n",
      "2700/2700 [==============================] - 0s 27us/step - loss: 3.7742e-04 - mean_absolute_error: 0.0121\n",
      "\n",
      "Epoch 02301: loss did not improve\n",
      "Epoch 2302/10000\n",
      "2700/2700 [==============================] - 0s 26us/step - loss: 3.7685e-04 - mean_absolute_error: 0.0121\n",
      "\n",
      "Epoch 02302: loss did not improve\n",
      "Epoch 2303/10000\n",
      "2700/2700 [==============================] - 0s 28us/step - loss: 3.7030e-04 - mean_absolute_error: 0.0118\n",
      "\n",
      "Epoch 02303: loss improved from 0.00037 to 0.00037, saving model to keras/best_baseline_weights.hdf5\n",
      "Epoch 2304/10000\n",
      "2700/2700 [==============================] - 0s 28us/step - loss: 3.6883e-04 - mean_absolute_error: 0.0117\n",
      "\n",
      "Epoch 02304: loss improved from 0.00037 to 0.00037, saving model to keras/best_baseline_weights.hdf5\n",
      "Epoch 2305/10000\n",
      "2700/2700 [==============================] - 0s 29us/step - loss: 3.7796e-04 - mean_absolute_error: 0.0122\n",
      "\n",
      "Epoch 02305: loss did not improve\n",
      "Epoch 2306/10000\n",
      "2700/2700 [==============================] - 0s 26us/step - loss: 3.7752e-04 - mean_absolute_error: 0.0121\n",
      "\n",
      "Epoch 02306: loss did not improve\n",
      "Epoch 2307/10000\n",
      "2700/2700 [==============================] - 0s 28us/step - loss: 3.7323e-04 - mean_absolute_error: 0.0120\n",
      "\n",
      "Epoch 02307: loss did not improve\n",
      "Epoch 2308/10000\n",
      "2700/2700 [==============================] - 0s 28us/step - loss: 3.7308e-04 - mean_absolute_error: 0.0119\n",
      "\n",
      "Epoch 02308: loss did not improve\n",
      "Epoch 2309/10000\n",
      "2700/2700 [==============================] - 0s 28us/step - loss: 3.7182e-04 - mean_absolute_error: 0.0118\n",
      "\n",
      "Epoch 02309: loss did not improve\n",
      "Epoch 2310/10000\n",
      "2700/2700 [==============================] - 0s 27us/step - loss: 3.7336e-04 - mean_absolute_error: 0.0119\n",
      "\n",
      "Epoch 02310: loss did not improve\n",
      "Epoch 2311/10000\n",
      "2700/2700 [==============================] - 0s 29us/step - loss: 3.7630e-04 - mean_absolute_error: 0.0121\n",
      "\n",
      "Epoch 02311: loss did not improve\n",
      "Epoch 2312/10000\n",
      "2700/2700 [==============================] - 0s 29us/step - loss: 3.6939e-04 - mean_absolute_error: 0.0118\n",
      "\n",
      "Epoch 02312: loss did not improve\n",
      "Epoch 2313/10000\n",
      "2700/2700 [==============================] - 0s 28us/step - loss: 3.7362e-04 - mean_absolute_error: 0.0120\n",
      "\n",
      "Epoch 02313: loss did not improve\n",
      "Epoch 2314/10000\n",
      "2700/2700 [==============================] - 0s 28us/step - loss: 3.7461e-04 - mean_absolute_error: 0.0120\n",
      "\n",
      "Epoch 02314: loss did not improve\n",
      "Epoch 2315/10000\n",
      "2700/2700 [==============================] - 0s 30us/step - loss: 3.7220e-04 - mean_absolute_error: 0.0120\n",
      "\n",
      "Epoch 02315: loss did not improve\n",
      "Epoch 2316/10000\n",
      "2700/2700 [==============================] - 0s 28us/step - loss: 3.7274e-04 - mean_absolute_error: 0.0119\n",
      "\n",
      "Epoch 02316: loss did not improve\n",
      "Epoch 2317/10000\n",
      "2700/2700 [==============================] - 0s 28us/step - loss: 3.7005e-04 - mean_absolute_error: 0.0118\n",
      "\n",
      "Epoch 02317: loss did not improve\n",
      "Epoch 2318/10000\n",
      "2700/2700 [==============================] - 0s 30us/step - loss: 3.7380e-04 - mean_absolute_error: 0.0121\n",
      "\n",
      "Epoch 02318: loss did not improve\n",
      "Epoch 2319/10000\n",
      "2700/2700 [==============================] - 0s 30us/step - loss: 3.7450e-04 - mean_absolute_error: 0.0121\n",
      "\n",
      "Epoch 02319: loss did not improve\n",
      "Epoch 2320/10000\n",
      "2700/2700 [==============================] - 0s 29us/step - loss: 3.7777e-04 - mean_absolute_error: 0.0121\n",
      "\n",
      "Epoch 02320: loss did not improve\n",
      "Epoch 2321/10000\n",
      "2700/2700 [==============================] - 0s 27us/step - loss: 3.7058e-04 - mean_absolute_error: 0.0119\n",
      "\n",
      "Epoch 02321: loss did not improve\n",
      "Epoch 2322/10000\n",
      "2700/2700 [==============================] - 0s 28us/step - loss: 3.7426e-04 - mean_absolute_error: 0.0120\n",
      "\n",
      "Epoch 02322: loss did not improve\n",
      "Epoch 2323/10000\n",
      "2700/2700 [==============================] - 0s 26us/step - loss: 3.7388e-04 - mean_absolute_error: 0.0120\n",
      "\n",
      "Epoch 02323: loss did not improve\n",
      "Epoch 2324/10000\n",
      "2700/2700 [==============================] - 0s 28us/step - loss: 3.7057e-04 - mean_absolute_error: 0.0120\n",
      "\n",
      "Epoch 02324: loss did not improve\n",
      "Epoch 2325/10000\n",
      "2700/2700 [==============================] - 0s 29us/step - loss: 3.6958e-04 - mean_absolute_error: 0.0118\n",
      "\n",
      "Epoch 02325: loss did not improve\n",
      "Epoch 2326/10000\n",
      "2700/2700 [==============================] - 0s 29us/step - loss: 3.7446e-04 - mean_absolute_error: 0.0122\n",
      "\n",
      "Epoch 02326: loss did not improve\n",
      "Epoch 2327/10000\n",
      "2700/2700 [==============================] - 0s 28us/step - loss: 3.7619e-04 - mean_absolute_error: 0.0121\n",
      "\n",
      "Epoch 02327: loss did not improve\n",
      "Epoch 2328/10000\n",
      "2700/2700 [==============================] - 0s 29us/step - loss: 3.7234e-04 - mean_absolute_error: 0.0121\n",
      "\n",
      "Epoch 02328: loss did not improve\n",
      "Epoch 2329/10000\n",
      "2700/2700 [==============================] - 0s 31us/step - loss: 3.6698e-04 - mean_absolute_error: 0.0119\n",
      "\n",
      "Epoch 02329: loss improved from 0.00037 to 0.00037, saving model to keras/best_baseline_weights.hdf5\n",
      "Epoch 2330/10000\n",
      "2700/2700 [==============================] - 0s 32us/step - loss: 3.6875e-04 - mean_absolute_error: 0.0120\n",
      "\n",
      "Epoch 02330: loss did not improve\n",
      "Epoch 2331/10000\n",
      "2700/2700 [==============================] - 0s 28us/step - loss: 3.7124e-04 - mean_absolute_error: 0.0120\n",
      "\n",
      "Epoch 02331: loss did not improve\n",
      "Epoch 2332/10000\n",
      "2700/2700 [==============================] - 0s 27us/step - loss: 3.6857e-04 - mean_absolute_error: 0.0119\n",
      "\n",
      "Epoch 02332: loss did not improve\n",
      "Epoch 2333/10000\n",
      "2700/2700 [==============================] - 0s 28us/step - loss: 3.7593e-04 - mean_absolute_error: 0.0122\n",
      "\n",
      "Epoch 02333: loss did not improve\n",
      "Epoch 2334/10000\n",
      "2700/2700 [==============================] - 0s 29us/step - loss: 3.7347e-04 - mean_absolute_error: 0.0120\n",
      "\n",
      "Epoch 02334: loss did not improve\n",
      "Epoch 2335/10000\n",
      "2700/2700 [==============================] - 0s 29us/step - loss: 3.7096e-04 - mean_absolute_error: 0.0120\n",
      "\n",
      "Epoch 02335: loss did not improve\n",
      "Epoch 2336/10000\n",
      "2700/2700 [==============================] - 0s 32us/step - loss: 3.7584e-04 - mean_absolute_error: 0.0122\n",
      "\n",
      "Epoch 02336: loss did not improve\n",
      "Epoch 2337/10000\n",
      "2700/2700 [==============================] - 0s 28us/step - loss: 3.7361e-04 - mean_absolute_error: 0.0121\n",
      "\n",
      "Epoch 02337: loss did not improve\n",
      "Epoch 2338/10000\n",
      "2700/2700 [==============================] - 0s 28us/step - loss: 3.6990e-04 - mean_absolute_error: 0.0120\n",
      "\n",
      "Epoch 02338: loss did not improve\n",
      "Epoch 2339/10000\n",
      "2700/2700 [==============================] - 0s 26us/step - loss: 3.6919e-04 - mean_absolute_error: 0.0119\n",
      "\n",
      "Epoch 02339: loss did not improve\n",
      "Epoch 2340/10000\n",
      "2700/2700 [==============================] - 0s 26us/step - loss: 3.7241e-04 - mean_absolute_error: 0.0121\n",
      "\n",
      "Epoch 02340: loss did not improve\n",
      "Epoch 2341/10000\n",
      "2700/2700 [==============================] - 0s 29us/step - loss: 3.6619e-04 - mean_absolute_error: 0.0118\n",
      "\n",
      "Epoch 02341: loss improved from 0.00037 to 0.00037, saving model to keras/best_baseline_weights.hdf5\n",
      "Epoch 2342/10000\n",
      "2700/2700 [==============================] - 0s 30us/step - loss: 3.7027e-04 - mean_absolute_error: 0.0119\n",
      "\n",
      "Epoch 02342: loss did not improve\n",
      "Epoch 2343/10000\n",
      "2700/2700 [==============================] - 0s 28us/step - loss: 3.7405e-04 - mean_absolute_error: 0.0122\n",
      "\n",
      "Epoch 02343: loss did not improve\n",
      "Epoch 2344/10000\n",
      "2700/2700 [==============================] - 0s 28us/step - loss: 3.7045e-04 - mean_absolute_error: 0.0121\n",
      "\n",
      "Epoch 02344: loss did not improve\n",
      "Epoch 2345/10000\n",
      "2700/2700 [==============================] - 0s 28us/step - loss: 3.6734e-04 - mean_absolute_error: 0.0120\n",
      "\n",
      "Epoch 02345: loss did not improve\n",
      "Epoch 2346/10000\n",
      "2700/2700 [==============================] - 0s 27us/step - loss: 3.7062e-04 - mean_absolute_error: 0.0119\n",
      "\n",
      "Epoch 02346: loss did not improve\n",
      "Epoch 2347/10000\n",
      "2700/2700 [==============================] - 0s 26us/step - loss: 3.6840e-04 - mean_absolute_error: 0.0120\n",
      "\n",
      "Epoch 02347: loss did not improve\n",
      "Epoch 2348/10000\n",
      "2700/2700 [==============================] - 0s 28us/step - loss: 3.6997e-04 - mean_absolute_error: 0.0119\n",
      "\n",
      "Epoch 02348: loss did not improve\n",
      "Epoch 2349/10000\n",
      "2700/2700 [==============================] - 0s 29us/step - loss: 3.7005e-04 - mean_absolute_error: 0.0120\n",
      "\n",
      "Epoch 02349: loss did not improve\n",
      "Epoch 2350/10000\n",
      "2700/2700 [==============================] - 0s 36us/step - loss: 3.6803e-04 - mean_absolute_error: 0.0119\n",
      "\n",
      "Epoch 02350: loss did not improve\n",
      "Epoch 2351/10000\n",
      "2700/2700 [==============================] - 0s 49us/step - loss: 3.6981e-04 - mean_absolute_error: 0.0120\n",
      "\n",
      "Epoch 02351: loss did not improve\n",
      "Epoch 2352/10000\n",
      "2700/2700 [==============================] - 0s 48us/step - loss: 3.6763e-04 - mean_absolute_error: 0.0120\n",
      "\n",
      "Epoch 02352: loss did not improve\n",
      "Epoch 2353/10000\n",
      "2700/2700 [==============================] - 0s 37us/step - loss: 3.7135e-04 - mean_absolute_error: 0.0121\n",
      "\n",
      "Epoch 02353: loss did not improve\n",
      "Epoch 2354/10000\n",
      "2700/2700 [==============================] - 0s 36us/step - loss: 3.6567e-04 - mean_absolute_error: 0.0119\n",
      "\n",
      "Epoch 02354: loss improved from 0.00037 to 0.00037, saving model to keras/best_baseline_weights.hdf5\n",
      "Epoch 2355/10000\n",
      "2700/2700 [==============================] - 0s 28us/step - loss: 3.6924e-04 - mean_absolute_error: 0.0120\n",
      "\n",
      "Epoch 02355: loss did not improve\n",
      "Epoch 2356/10000\n",
      "2700/2700 [==============================] - 0s 28us/step - loss: 3.6648e-04 - mean_absolute_error: 0.0119\n",
      "\n",
      "Epoch 02356: loss did not improve\n",
      "Epoch 2357/10000\n",
      "2700/2700 [==============================] - 0s 29us/step - loss: 3.7207e-04 - mean_absolute_error: 0.0121\n",
      "\n",
      "Epoch 02357: loss did not improve\n",
      "Epoch 2358/10000\n",
      "2700/2700 [==============================] - 0s 28us/step - loss: 3.6976e-04 - mean_absolute_error: 0.0120\n",
      "\n",
      "Epoch 02358: loss did not improve\n",
      "Epoch 2359/10000\n",
      "2700/2700 [==============================] - 0s 29us/step - loss: 3.6821e-04 - mean_absolute_error: 0.0121\n",
      "\n",
      "Epoch 02359: loss did not improve\n",
      "Epoch 2360/10000\n",
      "2700/2700 [==============================] - 0s 25us/step - loss: 3.7227e-04 - mean_absolute_error: 0.0121\n",
      "\n",
      "Epoch 02360: loss did not improve\n",
      "Epoch 2361/10000\n",
      "2700/2700 [==============================] - 0s 29us/step - loss: 3.7164e-04 - mean_absolute_error: 0.0123\n",
      "\n",
      "Epoch 02361: loss did not improve\n",
      "Epoch 2362/10000\n",
      "2700/2700 [==============================] - 0s 29us/step - loss: 3.6910e-04 - mean_absolute_error: 0.0120\n",
      "\n",
      "Epoch 02362: loss did not improve\n",
      "Epoch 2363/10000\n",
      "2700/2700 [==============================] - 0s 27us/step - loss: 3.7000e-04 - mean_absolute_error: 0.0122\n",
      "\n",
      "Epoch 02363: loss did not improve\n",
      "Epoch 2364/10000\n",
      "2700/2700 [==============================] - 0s 27us/step - loss: 3.7152e-04 - mean_absolute_error: 0.0122\n",
      "\n",
      "Epoch 02364: loss did not improve\n",
      "Epoch 2365/10000\n",
      "2700/2700 [==============================] - 0s 28us/step - loss: 3.6598e-04 - mean_absolute_error: 0.0120\n",
      "\n",
      "Epoch 02365: loss did not improve\n",
      "Epoch 2366/10000\n",
      "2700/2700 [==============================] - 0s 29us/step - loss: 3.7256e-04 - mean_absolute_error: 0.0123\n",
      "\n",
      "Epoch 02366: loss did not improve\n",
      "Epoch 2367/10000\n",
      "2700/2700 [==============================] - 0s 27us/step - loss: 3.7375e-04 - mean_absolute_error: 0.0122\n",
      "\n",
      "Epoch 02367: loss did not improve\n",
      "Epoch 2368/10000\n",
      "2700/2700 [==============================] - 0s 26us/step - loss: 3.6684e-04 - mean_absolute_error: 0.0119\n",
      "\n",
      "Epoch 02368: loss did not improve\n",
      "Epoch 2369/10000\n",
      "2700/2700 [==============================] - 0s 29us/step - loss: 3.6944e-04 - mean_absolute_error: 0.0121\n",
      "\n",
      "Epoch 02369: loss did not improve\n",
      "Epoch 2370/10000\n",
      "2700/2700 [==============================] - 0s 29us/step - loss: 3.6798e-04 - mean_absolute_error: 0.0120\n",
      "\n",
      "Epoch 02370: loss did not improve\n",
      "Epoch 2371/10000\n",
      "2700/2700 [==============================] - 0s 28us/step - loss: 3.6540e-04 - mean_absolute_error: 0.0120\n",
      "\n",
      "Epoch 02371: loss improved from 0.00037 to 0.00037, saving model to keras/best_baseline_weights.hdf5\n",
      "Epoch 2372/10000\n",
      "2700/2700 [==============================] - 0s 27us/step - loss: 3.6573e-04 - mean_absolute_error: 0.0120\n",
      "\n",
      "Epoch 02372: loss did not improve\n",
      "Epoch 2373/10000\n",
      "2700/2700 [==============================] - 0s 27us/step - loss: 3.7259e-04 - mean_absolute_error: 0.0121\n",
      "\n",
      "Epoch 02373: loss did not improve\n",
      "Epoch 2374/10000\n",
      "2700/2700 [==============================] - 0s 28us/step - loss: 3.7229e-04 - mean_absolute_error: 0.0124\n",
      "\n",
      "Epoch 02374: loss did not improve\n",
      "Epoch 2375/10000\n",
      "2700/2700 [==============================] - 0s 27us/step - loss: 3.6683e-04 - mean_absolute_error: 0.0121\n",
      "\n",
      "Epoch 02375: loss did not improve\n",
      "Epoch 2376/10000\n",
      "2700/2700 [==============================] - 0s 27us/step - loss: 3.6478e-04 - mean_absolute_error: 0.0119\n",
      "\n",
      "Epoch 02376: loss improved from 0.00037 to 0.00036, saving model to keras/best_baseline_weights.hdf5\n",
      "Epoch 2377/10000\n",
      "2700/2700 [==============================] - 0s 28us/step - loss: 3.6767e-04 - mean_absolute_error: 0.0120\n",
      "\n",
      "Epoch 02377: loss did not improve\n",
      "Epoch 2378/10000\n",
      "2700/2700 [==============================] - 0s 35us/step - loss: 3.6497e-04 - mean_absolute_error: 0.0119\n",
      "\n",
      "Epoch 02378: loss did not improve\n",
      "Epoch 2379/10000\n",
      "2700/2700 [==============================] - 0s 29us/step - loss: 3.6479e-04 - mean_absolute_error: 0.0120\n",
      "\n",
      "Epoch 02379: loss did not improve\n",
      "Epoch 2380/10000\n",
      "2700/2700 [==============================] - 0s 28us/step - loss: 3.6800e-04 - mean_absolute_error: 0.0121\n",
      "\n",
      "Epoch 02380: loss did not improve\n",
      "Epoch 2381/10000\n",
      "2700/2700 [==============================] - 0s 28us/step - loss: 3.6406e-04 - mean_absolute_error: 0.0119\n",
      "\n",
      "Epoch 02381: loss improved from 0.00036 to 0.00036, saving model to keras/best_baseline_weights.hdf5\n",
      "Epoch 2382/10000\n",
      "2700/2700 [==============================] - 0s 28us/step - loss: 3.6302e-04 - mean_absolute_error: 0.0120\n",
      "\n",
      "Epoch 02382: loss improved from 0.00036 to 0.00036, saving model to keras/best_baseline_weights.hdf5\n",
      "Epoch 2383/10000\n",
      "2700/2700 [==============================] - 0s 26us/step - loss: 3.6809e-04 - mean_absolute_error: 0.0121\n",
      "\n",
      "Epoch 02383: loss did not improve\n",
      "Epoch 2384/10000\n",
      "2700/2700 [==============================] - 0s 28us/step - loss: 3.7237e-04 - mean_absolute_error: 0.0123\n",
      "\n",
      "Epoch 02384: loss did not improve\n",
      "Epoch 2385/10000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2700/2700 [==============================] - 0s 29us/step - loss: 3.6506e-04 - mean_absolute_error: 0.0119\n",
      "\n",
      "Epoch 02385: loss did not improve\n",
      "Epoch 2386/10000\n",
      "2700/2700 [==============================] - 0s 27us/step - loss: 3.6907e-04 - mean_absolute_error: 0.0122\n",
      "\n",
      "Epoch 02386: loss did not improve\n",
      "Epoch 2387/10000\n",
      "2700/2700 [==============================] - 0s 28us/step - loss: 3.6699e-04 - mean_absolute_error: 0.0121\n",
      "\n",
      "Epoch 02387: loss did not improve\n",
      "Epoch 2388/10000\n",
      "2700/2700 [==============================] - 0s 27us/step - loss: 3.6942e-04 - mean_absolute_error: 0.0121\n",
      "\n",
      "Epoch 02388: loss did not improve\n",
      "Epoch 2389/10000\n",
      "2700/2700 [==============================] - 0s 27us/step - loss: 3.6545e-04 - mean_absolute_error: 0.0120\n",
      "\n",
      "Epoch 02389: loss did not improve\n",
      "Epoch 2390/10000\n",
      "2700/2700 [==============================] - 0s 29us/step - loss: 3.7690e-04 - mean_absolute_error: 0.0124\n",
      "\n",
      "Epoch 02390: loss did not improve\n",
      "Epoch 2391/10000\n",
      "2700/2700 [==============================] - 0s 26us/step - loss: 3.7955e-04 - mean_absolute_error: 0.0126\n",
      "\n",
      "Epoch 02391: loss did not improve\n",
      "Epoch 2392/10000\n",
      "2700/2700 [==============================] - 0s 27us/step - loss: 3.7019e-04 - mean_absolute_error: 0.0123\n",
      "\n",
      "Epoch 02392: loss did not improve\n",
      "Epoch 2393/10000\n",
      "2700/2700 [==============================] - 0s 27us/step - loss: 3.6528e-04 - mean_absolute_error: 0.0120\n",
      "\n",
      "Epoch 02393: loss did not improve\n",
      "Epoch 2394/10000\n",
      "2700/2700 [==============================] - 0s 28us/step - loss: 3.6404e-04 - mean_absolute_error: 0.0121\n",
      "\n",
      "Epoch 02394: loss did not improve\n",
      "Epoch 2395/10000\n",
      "2700/2700 [==============================] - 0s 28us/step - loss: 3.6610e-04 - mean_absolute_error: 0.0121\n",
      "\n",
      "Epoch 02395: loss did not improve\n",
      "Epoch 2396/10000\n",
      "2700/2700 [==============================] - 0s 28us/step - loss: 3.6786e-04 - mean_absolute_error: 0.0121\n",
      "\n",
      "Epoch 02396: loss did not improve\n",
      "Epoch 2397/10000\n",
      "2700/2700 [==============================] - 0s 28us/step - loss: 3.6839e-04 - mean_absolute_error: 0.0123\n",
      "\n",
      "Epoch 02397: loss did not improve\n",
      "Epoch 2398/10000\n",
      "2700/2700 [==============================] - 0s 27us/step - loss: 3.6424e-04 - mean_absolute_error: 0.0121\n",
      "\n",
      "Epoch 02398: loss did not improve\n",
      "Epoch 2399/10000\n",
      "2700/2700 [==============================] - 0s 28us/step - loss: 3.6314e-04 - mean_absolute_error: 0.0120\n",
      "\n",
      "Epoch 02399: loss did not improve\n",
      "Epoch 2400/10000\n",
      "2700/2700 [==============================] - 0s 32us/step - loss: 3.6702e-04 - mean_absolute_error: 0.0121\n",
      "\n",
      "Epoch 02400: loss did not improve\n",
      "Epoch 2401/10000\n",
      "2700/2700 [==============================] - 0s 33us/step - loss: 3.6907e-04 - mean_absolute_error: 0.0123\n",
      "\n",
      "Epoch 02401: loss did not improve\n",
      "Epoch 2402/10000\n",
      "2700/2700 [==============================] - 0s 31us/step - loss: 3.6333e-04 - mean_absolute_error: 0.0120\n",
      "\n",
      "Epoch 02402: loss did not improve\n",
      "Epoch 2403/10000\n",
      "2700/2700 [==============================] - 0s 32us/step - loss: 3.6427e-04 - mean_absolute_error: 0.0120\n",
      "\n",
      "Epoch 02403: loss did not improve\n",
      "Epoch 2404/10000\n",
      "2700/2700 [==============================] - 0s 32us/step - loss: 3.6707e-04 - mean_absolute_error: 0.0122\n",
      "\n",
      "Epoch 02404: loss did not improve\n",
      "Epoch 2405/10000\n",
      "2700/2700 [==============================] - 0s 27us/step - loss: 3.6779e-04 - mean_absolute_error: 0.0122\n",
      "\n",
      "Epoch 02405: loss did not improve\n",
      "Epoch 2406/10000\n",
      "2700/2700 [==============================] - 0s 27us/step - loss: 3.6226e-04 - mean_absolute_error: 0.0121\n",
      "\n",
      "Epoch 02406: loss improved from 0.00036 to 0.00036, saving model to keras/best_baseline_weights.hdf5\n",
      "Epoch 2407/10000\n",
      "2700/2700 [==============================] - 0s 27us/step - loss: 3.6458e-04 - mean_absolute_error: 0.0120\n",
      "\n",
      "Epoch 02407: loss did not improve\n",
      "Epoch 2408/10000\n",
      "2700/2700 [==============================] - 0s 28us/step - loss: 3.6375e-04 - mean_absolute_error: 0.0121\n",
      "\n",
      "Epoch 02408: loss did not improve\n",
      "Epoch 2409/10000\n",
      "2700/2700 [==============================] - 0s 28us/step - loss: 3.6827e-04 - mean_absolute_error: 0.0122\n",
      "\n",
      "Epoch 02409: loss did not improve\n",
      "Epoch 2410/10000\n",
      "2700/2700 [==============================] - 0s 29us/step - loss: 3.6293e-04 - mean_absolute_error: 0.0120\n",
      "\n",
      "Epoch 02410: loss did not improve\n",
      "Epoch 2411/10000\n",
      "2700/2700 [==============================] - 0s 47us/step - loss: 3.6652e-04 - mean_absolute_error: 0.0121\n",
      "\n",
      "Epoch 02411: loss did not improve\n",
      "Epoch 2412/10000\n",
      "2700/2700 [==============================] - 0s 29us/step - loss: 3.6104e-04 - mean_absolute_error: 0.0120\n",
      "\n",
      "Epoch 02412: loss improved from 0.00036 to 0.00036, saving model to keras/best_baseline_weights.hdf5\n",
      "Epoch 2413/10000\n",
      "2700/2700 [==============================] - 0s 42us/step - loss: 3.6481e-04 - mean_absolute_error: 0.0121\n",
      "\n",
      "Epoch 02413: loss did not improve\n",
      "Epoch 2414/10000\n",
      "2700/2700 [==============================] - 0s 26us/step - loss: 3.6669e-04 - mean_absolute_error: 0.0121\n",
      "\n",
      "Epoch 02414: loss did not improve\n",
      "Epoch 2415/10000\n",
      "2700/2700 [==============================] - 0s 26us/step - loss: 3.6175e-04 - mean_absolute_error: 0.0120\n",
      "\n",
      "Epoch 02415: loss did not improve\n",
      "Epoch 2416/10000\n",
      "2700/2700 [==============================] - 0s 24us/step - loss: 3.6681e-04 - mean_absolute_error: 0.0121\n",
      "\n",
      "Epoch 02416: loss did not improve\n",
      "Epoch 2417/10000\n",
      "2700/2700 [==============================] - 0s 28us/step - loss: 3.6734e-04 - mean_absolute_error: 0.0122\n",
      "\n",
      "Epoch 02417: loss did not improve\n",
      "Epoch 2418/10000\n",
      "2700/2700 [==============================] - 0s 23us/step - loss: 3.6312e-04 - mean_absolute_error: 0.0121\n",
      "\n",
      "Epoch 02418: loss did not improve\n",
      "Epoch 2419/10000\n",
      "2700/2700 [==============================] - 0s 24us/step - loss: 3.6327e-04 - mean_absolute_error: 0.0120\n",
      "\n",
      "Epoch 02419: loss did not improve\n",
      "Epoch 2420/10000\n",
      "2700/2700 [==============================] - 0s 25us/step - loss: 3.6595e-04 - mean_absolute_error: 0.0122\n",
      "\n",
      "Epoch 02420: loss did not improve\n",
      "Epoch 2421/10000\n",
      "2700/2700 [==============================] - 0s 32us/step - loss: 3.6274e-04 - mean_absolute_error: 0.0121\n",
      "\n",
      "Epoch 02421: loss did not improve\n",
      "Epoch 2422/10000\n",
      "2700/2700 [==============================] - 0s 34us/step - loss: 3.6890e-04 - mean_absolute_error: 0.0124\n",
      "\n",
      "Epoch 02422: loss did not improve\n",
      "Epoch 2423/10000\n",
      "2700/2700 [==============================] - 0s 34us/step - loss: 3.6153e-04 - mean_absolute_error: 0.0120\n",
      "\n",
      "Epoch 02423: loss did not improve\n",
      "Epoch 2424/10000\n",
      "2700/2700 [==============================] - 0s 29us/step - loss: 3.6065e-04 - mean_absolute_error: 0.0119\n",
      "\n",
      "Epoch 02424: loss improved from 0.00036 to 0.00036, saving model to keras/best_baseline_weights.hdf5\n",
      "Epoch 2425/10000\n",
      "2700/2700 [==============================] - 0s 25us/step - loss: 3.6673e-04 - mean_absolute_error: 0.0122\n",
      "\n",
      "Epoch 02425: loss did not improve\n",
      "Epoch 2426/10000\n",
      "2700/2700 [==============================] - 0s 27us/step - loss: 3.6084e-04 - mean_absolute_error: 0.0120\n",
      "\n",
      "Epoch 02426: loss did not improve\n",
      "Epoch 2427/10000\n",
      "2700/2700 [==============================] - 0s 23us/step - loss: 3.5983e-04 - mean_absolute_error: 0.0120\n",
      "\n",
      "Epoch 02427: loss improved from 0.00036 to 0.00036, saving model to keras/best_baseline_weights.hdf5\n",
      "Epoch 2428/10000\n",
      "2700/2700 [==============================] - 0s 26us/step - loss: 3.6809e-04 - mean_absolute_error: 0.0123\n",
      "\n",
      "Epoch 02428: loss did not improve\n",
      "Epoch 2429/10000\n",
      "2700/2700 [==============================] - 0s 28us/step - loss: 3.6720e-04 - mean_absolute_error: 0.0123\n",
      "\n",
      "Epoch 02429: loss did not improve\n",
      "Epoch 2430/10000\n",
      "2700/2700 [==============================] - 0s 28us/step - loss: 3.6161e-04 - mean_absolute_error: 0.0120\n",
      "\n",
      "Epoch 02430: loss did not improve\n",
      "Epoch 2431/10000\n",
      "2700/2700 [==============================] - 0s 27us/step - loss: 3.6659e-04 - mean_absolute_error: 0.0122\n",
      "\n",
      "Epoch 02431: loss did not improve\n",
      "Epoch 2432/10000\n",
      "2700/2700 [==============================] - 0s 25us/step - loss: 3.6214e-04 - mean_absolute_error: 0.0121\n",
      "\n",
      "Epoch 02432: loss did not improve\n",
      "Epoch 2433/10000\n",
      "2700/2700 [==============================] - 0s 24us/step - loss: 3.6314e-04 - mean_absolute_error: 0.0121\n",
      "\n",
      "Epoch 02433: loss did not improve\n",
      "Epoch 2434/10000\n",
      "2700/2700 [==============================] - 0s 24us/step - loss: 3.6326e-04 - mean_absolute_error: 0.0121\n",
      "\n",
      "Epoch 02434: loss did not improve\n",
      "Epoch 2435/10000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2700/2700 [==============================] - 0s 25us/step - loss: 3.6337e-04 - mean_absolute_error: 0.0121\n",
      "\n",
      "Epoch 02435: loss did not improve\n",
      "Epoch 2436/10000\n",
      "2700/2700 [==============================] - 0s 29us/step - loss: 3.6286e-04 - mean_absolute_error: 0.0121\n",
      "\n",
      "Epoch 02436: loss did not improve\n",
      "Epoch 2437/10000\n",
      "2700/2700 [==============================] - 0s 30us/step - loss: 3.6509e-04 - mean_absolute_error: 0.0121\n",
      "\n",
      "Epoch 02437: loss did not improve\n",
      "Epoch 2438/10000\n",
      "2700/2700 [==============================] - 0s 33us/step - loss: 3.6169e-04 - mean_absolute_error: 0.0121\n",
      "\n",
      "Epoch 02438: loss did not improve\n",
      "Epoch 2439/10000\n",
      "2700/2700 [==============================] - 0s 31us/step - loss: 3.6381e-04 - mean_absolute_error: 0.0122\n",
      "\n",
      "Epoch 02439: loss did not improve\n",
      "Epoch 2440/10000\n",
      "2700/2700 [==============================] - 0s 30us/step - loss: 3.6114e-04 - mean_absolute_error: 0.0120\n",
      "\n",
      "Epoch 02440: loss did not improve\n",
      "Epoch 2441/10000\n",
      "2700/2700 [==============================] - 0s 32us/step - loss: 3.6265e-04 - mean_absolute_error: 0.0121\n",
      "\n",
      "Epoch 02441: loss did not improve\n",
      "Epoch 2442/10000\n",
      "2700/2700 [==============================] - 0s 26us/step - loss: 3.6280e-04 - mean_absolute_error: 0.0122\n",
      "\n",
      "Epoch 02442: loss did not improve\n",
      "Epoch 2443/10000\n",
      "2700/2700 [==============================] - 0s 25us/step - loss: 3.6146e-04 - mean_absolute_error: 0.0121\n",
      "\n",
      "Epoch 02443: loss did not improve\n",
      "Epoch 2444/10000\n",
      "2700/2700 [==============================] - 0s 25us/step - loss: 3.6409e-04 - mean_absolute_error: 0.0121\n",
      "\n",
      "Epoch 02444: loss did not improve\n",
      "Epoch 2445/10000\n",
      "2700/2700 [==============================] - 0s 27us/step - loss: 3.5971e-04 - mean_absolute_error: 0.0120\n",
      "\n",
      "Epoch 02445: loss improved from 0.00036 to 0.00036, saving model to keras/best_baseline_weights.hdf5\n",
      "Epoch 2446/10000\n",
      "2700/2700 [==============================] - 0s 23us/step - loss: 3.6169e-04 - mean_absolute_error: 0.0121\n",
      "\n",
      "Epoch 02446: loss did not improve\n",
      "Epoch 2447/10000\n",
      "2700/2700 [==============================] - 0s 25us/step - loss: 3.6215e-04 - mean_absolute_error: 0.0121\n",
      "\n",
      "Epoch 02447: loss did not improve\n",
      "Epoch 2448/10000\n",
      "2700/2700 [==============================] - 0s 25us/step - loss: 3.6243e-04 - mean_absolute_error: 0.0121\n",
      "\n",
      "Epoch 02448: loss did not improve\n",
      "Epoch 2449/10000\n",
      "2700/2700 [==============================] - 0s 23us/step - loss: 3.6332e-04 - mean_absolute_error: 0.0122\n",
      "\n",
      "Epoch 02449: loss did not improve\n",
      "Epoch 2450/10000\n",
      "2700/2700 [==============================] - 0s 25us/step - loss: 3.6881e-04 - mean_absolute_error: 0.0124\n",
      "\n",
      "Epoch 02450: loss did not improve\n",
      "Epoch 2451/10000\n",
      "2700/2700 [==============================] - 0s 27us/step - loss: 3.6282e-04 - mean_absolute_error: 0.0121\n",
      "\n",
      "Epoch 02451: loss did not improve\n",
      "Epoch 2452/10000\n",
      "2700/2700 [==============================] - 0s 26us/step - loss: 3.6431e-04 - mean_absolute_error: 0.0123\n",
      "\n",
      "Epoch 02452: loss did not improve\n",
      "Epoch 2453/10000\n",
      "2700/2700 [==============================] - 0s 25us/step - loss: 3.5962e-04 - mean_absolute_error: 0.0120\n",
      "\n",
      "Epoch 02453: loss improved from 0.00036 to 0.00036, saving model to keras/best_baseline_weights.hdf5\n",
      "Epoch 2454/10000\n",
      "2700/2700 [==============================] - 0s 27us/step - loss: 3.6039e-04 - mean_absolute_error: 0.0121\n",
      "\n",
      "Epoch 02454: loss did not improve\n",
      "Epoch 2455/10000\n",
      "2700/2700 [==============================] - 0s 27us/step - loss: 3.6078e-04 - mean_absolute_error: 0.0121\n",
      "\n",
      "Epoch 02455: loss did not improve\n",
      "Epoch 2456/10000\n",
      "2700/2700 [==============================] - 0s 23us/step - loss: 3.5979e-04 - mean_absolute_error: 0.0121\n",
      "\n",
      "Epoch 02456: loss did not improve\n",
      "Epoch 2457/10000\n",
      "2700/2700 [==============================] - 0s 25us/step - loss: 3.6337e-04 - mean_absolute_error: 0.0122\n",
      "\n",
      "Epoch 02457: loss did not improve\n",
      "Epoch 2458/10000\n",
      "2700/2700 [==============================] - 0s 26us/step - loss: 3.6132e-04 - mean_absolute_error: 0.0121\n",
      "\n",
      "Epoch 02458: loss did not improve\n",
      "Epoch 2459/10000\n",
      "2700/2700 [==============================] - 0s 23us/step - loss: 3.6513e-04 - mean_absolute_error: 0.0121\n",
      "\n",
      "Epoch 02459: loss did not improve\n",
      "Epoch 2460/10000\n",
      "2700/2700 [==============================] - 0s 27us/step - loss: 3.6821e-04 - mean_absolute_error: 0.0123\n",
      "\n",
      "Epoch 02460: loss did not improve\n",
      "Epoch 2461/10000\n",
      "2700/2700 [==============================] - 0s 25us/step - loss: 3.6189e-04 - mean_absolute_error: 0.0121\n",
      "\n",
      "Epoch 02461: loss did not improve\n",
      "Epoch 2462/10000\n",
      "2700/2700 [==============================] - 0s 24us/step - loss: 3.6014e-04 - mean_absolute_error: 0.0121\n",
      "\n",
      "Epoch 02462: loss did not improve\n",
      "Epoch 2463/10000\n",
      "2700/2700 [==============================] - 0s 25us/step - loss: 3.6737e-04 - mean_absolute_error: 0.0123\n",
      "\n",
      "Epoch 02463: loss did not improve\n",
      "Epoch 2464/10000\n",
      "2700/2700 [==============================] - 0s 27us/step - loss: 3.5821e-04 - mean_absolute_error: 0.0120\n",
      "\n",
      "Epoch 02464: loss improved from 0.00036 to 0.00036, saving model to keras/best_baseline_weights.hdf5\n",
      "Epoch 2465/10000\n",
      "2700/2700 [==============================] - 0s 24us/step - loss: 3.5856e-04 - mean_absolute_error: 0.0120\n",
      "\n",
      "Epoch 02465: loss did not improve\n",
      "Epoch 2466/10000\n",
      "2700/2700 [==============================] - 0s 27us/step - loss: 3.5851e-04 - mean_absolute_error: 0.0120\n",
      "\n",
      "Epoch 02466: loss did not improve\n",
      "Epoch 2467/10000\n",
      "2700/2700 [==============================] - 0s 33us/step - loss: 3.6967e-04 - mean_absolute_error: 0.0127\n",
      "\n",
      "Epoch 02467: loss did not improve\n",
      "Epoch 2468/10000\n",
      "2700/2700 [==============================] - 0s 25us/step - loss: 3.6098e-04 - mean_absolute_error: 0.0121\n",
      "\n",
      "Epoch 02468: loss did not improve\n",
      "Epoch 2469/10000\n",
      "2700/2700 [==============================] - 0s 25us/step - loss: 3.6368e-04 - mean_absolute_error: 0.0123\n",
      "\n",
      "Epoch 02469: loss did not improve\n",
      "Epoch 2470/10000\n",
      "2700/2700 [==============================] - 0s 24us/step - loss: 3.5694e-04 - mean_absolute_error: 0.0120\n",
      "\n",
      "Epoch 02470: loss improved from 0.00036 to 0.00036, saving model to keras/best_baseline_weights.hdf5\n",
      "Epoch 2471/10000\n",
      "2700/2700 [==============================] - 0s 25us/step - loss: 3.5804e-04 - mean_absolute_error: 0.0120\n",
      "\n",
      "Epoch 02471: loss did not improve\n",
      "Epoch 2472/10000\n",
      "2700/2700 [==============================] - 0s 27us/step - loss: 3.5958e-04 - mean_absolute_error: 0.0121\n",
      "\n",
      "Epoch 02472: loss did not improve\n",
      "Epoch 2473/10000\n",
      "2700/2700 [==============================] - 0s 24us/step - loss: 3.5993e-04 - mean_absolute_error: 0.0121\n",
      "\n",
      "Epoch 02473: loss did not improve\n",
      "Epoch 2474/10000\n",
      "2700/2700 [==============================] - 0s 24us/step - loss: 3.5992e-04 - mean_absolute_error: 0.0121\n",
      "\n",
      "Epoch 02474: loss did not improve\n",
      "Epoch 2475/10000\n",
      "2700/2700 [==============================] - 0s 25us/step - loss: 3.6570e-04 - mean_absolute_error: 0.0124\n",
      "\n",
      "Epoch 02475: loss did not improve\n",
      "Epoch 2476/10000\n",
      "2700/2700 [==============================] - 0s 24us/step - loss: 3.5776e-04 - mean_absolute_error: 0.0121\n",
      "\n",
      "Epoch 02476: loss did not improve\n",
      "Epoch 2477/10000\n",
      "2700/2700 [==============================] - 0s 25us/step - loss: 3.5716e-04 - mean_absolute_error: 0.0120\n",
      "\n",
      "Epoch 02477: loss did not improve\n",
      "Epoch 2478/10000\n",
      "2700/2700 [==============================] - 0s 25us/step - loss: 3.6017e-04 - mean_absolute_error: 0.0121\n",
      "\n",
      "Epoch 02478: loss did not improve\n",
      "Epoch 2479/10000\n",
      "2700/2700 [==============================] - 0s 24us/step - loss: 3.5814e-04 - mean_absolute_error: 0.0120\n",
      "\n",
      "Epoch 02479: loss did not improve\n",
      "Epoch 2480/10000\n",
      "2700/2700 [==============================] - 0s 23us/step - loss: 3.6969e-04 - mean_absolute_error: 0.0125\n",
      "\n",
      "Epoch 02480: loss did not improve\n",
      "Epoch 2481/10000\n",
      "2700/2700 [==============================] - 0s 26us/step - loss: 3.6321e-04 - mean_absolute_error: 0.0122\n",
      "\n",
      "Epoch 02481: loss did not improve\n",
      "Epoch 2482/10000\n",
      "2700/2700 [==============================] - 0s 27us/step - loss: 3.6737e-04 - mean_absolute_error: 0.0125\n",
      "\n",
      "Epoch 02482: loss did not improve\n",
      "Epoch 2483/10000\n",
      "2700/2700 [==============================] - 0s 23us/step - loss: 3.6215e-04 - mean_absolute_error: 0.0122\n",
      "\n",
      "Epoch 02483: loss did not improve\n",
      "Epoch 2484/10000\n",
      "2700/2700 [==============================] - 0s 26us/step - loss: 3.5823e-04 - mean_absolute_error: 0.0121\n",
      "\n",
      "Epoch 02484: loss did not improve\n",
      "Epoch 2485/10000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2700/2700 [==============================] - 0s 25us/step - loss: 3.6418e-04 - mean_absolute_error: 0.0122\n",
      "\n",
      "Epoch 02485: loss did not improve\n",
      "Epoch 2486/10000\n",
      "2700/2700 [==============================] - 0s 23us/step - loss: 3.5741e-04 - mean_absolute_error: 0.0121\n",
      "\n",
      "Epoch 02486: loss did not improve\n",
      "Epoch 2487/10000\n",
      "2700/2700 [==============================] - 0s 25us/step - loss: 3.6026e-04 - mean_absolute_error: 0.0121\n",
      "\n",
      "Epoch 02487: loss did not improve\n",
      "Epoch 2488/10000\n",
      "2700/2700 [==============================] - 0s 25us/step - loss: 3.5921e-04 - mean_absolute_error: 0.0121\n",
      "\n",
      "Epoch 02488: loss did not improve\n",
      "Epoch 2489/10000\n",
      "2700/2700 [==============================] - 0s 24us/step - loss: 3.6737e-04 - mean_absolute_error: 0.0124\n",
      "\n",
      "Epoch 02489: loss did not improve\n",
      "Epoch 2490/10000\n",
      "2700/2700 [==============================] - 0s 27us/step - loss: 3.5737e-04 - mean_absolute_error: 0.0120\n",
      "\n",
      "Epoch 02490: loss did not improve\n",
      "Epoch 2491/10000\n",
      "2700/2700 [==============================] - 0s 25us/step - loss: 3.6331e-04 - mean_absolute_error: 0.0122\n",
      "\n",
      "Epoch 02491: loss did not improve\n",
      "Epoch 2492/10000\n",
      "2700/2700 [==============================] - 0s 24us/step - loss: 3.6826e-04 - mean_absolute_error: 0.0126\n",
      "\n",
      "Epoch 02492: loss did not improve\n",
      "Epoch 2493/10000\n",
      "2700/2700 [==============================] - 0s 27us/step - loss: 3.6407e-04 - mean_absolute_error: 0.0122\n",
      "\n",
      "Epoch 02493: loss did not improve\n",
      "Epoch 2494/10000\n",
      "2700/2700 [==============================] - 0s 25us/step - loss: 3.5945e-04 - mean_absolute_error: 0.0122\n",
      "\n",
      "Epoch 02494: loss did not improve\n",
      "Epoch 2495/10000\n",
      "2700/2700 [==============================] - 0s 23us/step - loss: 3.6401e-04 - mean_absolute_error: 0.0123\n",
      "\n",
      "Epoch 02495: loss did not improve\n",
      "Epoch 2496/10000\n",
      "2700/2700 [==============================] - 0s 25us/step - loss: 3.6468e-04 - mean_absolute_error: 0.0123\n",
      "\n",
      "Epoch 02496: loss did not improve\n",
      "Epoch 2497/10000\n",
      "2700/2700 [==============================] - 0s 27us/step - loss: 3.5654e-04 - mean_absolute_error: 0.0121\n",
      "\n",
      "Epoch 02497: loss improved from 0.00036 to 0.00036, saving model to keras/best_baseline_weights.hdf5\n",
      "Epoch 2498/10000\n",
      "2700/2700 [==============================] - 0s 25us/step - loss: 3.6072e-04 - mean_absolute_error: 0.0123\n",
      "\n",
      "Epoch 02498: loss did not improve\n",
      "Epoch 2499/10000\n",
      "2700/2700 [==============================] - 0s 23us/step - loss: 3.6097e-04 - mean_absolute_error: 0.0122\n",
      "\n",
      "Epoch 02499: loss did not improve\n",
      "Epoch 2500/10000\n",
      "2700/2700 [==============================] - 0s 25us/step - loss: 3.6174e-04 - mean_absolute_error: 0.0122\n",
      "\n",
      "Epoch 02500: loss did not improve\n",
      "Epoch 2501/10000\n",
      "2700/2700 [==============================] - 0s 26us/step - loss: 3.5879e-04 - mean_absolute_error: 0.0121\n",
      "\n",
      "Epoch 02501: loss did not improve\n",
      "Epoch 2502/10000\n",
      "2700/2700 [==============================] - 0s 23us/step - loss: 3.5822e-04 - mean_absolute_error: 0.0122\n",
      "\n",
      "Epoch 02502: loss did not improve\n",
      "Epoch 2503/10000\n",
      "2700/2700 [==============================] - 0s 25us/step - loss: 3.6238e-04 - mean_absolute_error: 0.0122\n",
      "\n",
      "Epoch 02503: loss did not improve\n",
      "Epoch 2504/10000\n",
      "2700/2700 [==============================] - 0s 25us/step - loss: 3.5541e-04 - mean_absolute_error: 0.0121\n",
      "\n",
      "Epoch 02504: loss improved from 0.00036 to 0.00036, saving model to keras/best_baseline_weights.hdf5\n",
      "Epoch 2505/10000\n",
      "2700/2700 [==============================] - 0s 29us/step - loss: 3.6167e-04 - mean_absolute_error: 0.0123\n",
      "\n",
      "Epoch 02505: loss did not improve\n",
      "Epoch 2506/10000\n",
      "2700/2700 [==============================] - 0s 24us/step - loss: 3.5689e-04 - mean_absolute_error: 0.0121\n",
      "\n",
      "Epoch 02506: loss did not improve\n",
      "Epoch 2507/10000\n",
      "2700/2700 [==============================] - 0s 25us/step - loss: 3.5886e-04 - mean_absolute_error: 0.0121\n",
      "\n",
      "Epoch 02507: loss did not improve\n",
      "Epoch 2508/10000\n",
      "2700/2700 [==============================] - 0s 25us/step - loss: 3.7562e-04 - mean_absolute_error: 0.0128\n",
      "\n",
      "Epoch 02508: loss did not improve\n",
      "Epoch 2509/10000\n",
      "2700/2700 [==============================] - 0s 23us/step - loss: 3.6118e-04 - mean_absolute_error: 0.0122\n",
      "\n",
      "Epoch 02509: loss did not improve\n",
      "Epoch 2510/10000\n",
      "2700/2700 [==============================] - 0s 25us/step - loss: 3.5660e-04 - mean_absolute_error: 0.0121\n",
      "\n",
      "Epoch 02510: loss did not improve\n",
      "Epoch 2511/10000\n",
      "2700/2700 [==============================] - 0s 25us/step - loss: 3.6069e-04 - mean_absolute_error: 0.0122\n",
      "\n",
      "Epoch 02511: loss did not improve\n",
      "Epoch 2512/10000\n",
      "2700/2700 [==============================] - 0s 24us/step - loss: 3.5895e-04 - mean_absolute_error: 0.0121\n",
      "\n",
      "Epoch 02512: loss did not improve\n",
      "Epoch 2513/10000\n",
      "2700/2700 [==============================] - 0s 26us/step - loss: 3.5955e-04 - mean_absolute_error: 0.0122\n",
      "\n",
      "Epoch 02513: loss did not improve\n",
      "Epoch 2514/10000\n",
      "2700/2700 [==============================] - 0s 25us/step - loss: 3.5851e-04 - mean_absolute_error: 0.0122\n",
      "\n",
      "Epoch 02514: loss did not improve\n",
      "Epoch 2515/10000\n",
      "2700/2700 [==============================] - 0s 24us/step - loss: 3.6051e-04 - mean_absolute_error: 0.0121\n",
      "\n",
      "Epoch 02515: loss did not improve\n",
      "Epoch 2516/10000\n",
      "2700/2700 [==============================] - 0s 25us/step - loss: 3.5830e-04 - mean_absolute_error: 0.0122\n",
      "\n",
      "Epoch 02516: loss did not improve\n",
      "Epoch 2517/10000\n",
      "2700/2700 [==============================] - 0s 24us/step - loss: 3.5558e-04 - mean_absolute_error: 0.0121\n",
      "\n",
      "Epoch 02517: loss did not improve\n",
      "Epoch 2518/10000\n",
      "2700/2700 [==============================] - 0s 24us/step - loss: 3.5743e-04 - mean_absolute_error: 0.0122\n",
      "\n",
      "Epoch 02518: loss did not improve\n",
      "Epoch 2519/10000\n",
      "2700/2700 [==============================] - 0s 27us/step - loss: 3.5835e-04 - mean_absolute_error: 0.0121\n",
      "\n",
      "Epoch 02519: loss did not improve\n",
      "Epoch 2520/10000\n",
      "2700/2700 [==============================] - 0s 30us/step - loss: 3.5975e-04 - mean_absolute_error: 0.0122\n",
      "\n",
      "Epoch 02520: loss did not improve\n",
      "Epoch 2521/10000\n",
      "2700/2700 [==============================] - 0s 24us/step - loss: 3.6929e-04 - mean_absolute_error: 0.0125\n",
      "\n",
      "Epoch 02521: loss did not improve\n",
      "Epoch 2522/10000\n",
      "2700/2700 [==============================] - 0s 25us/step - loss: 3.6061e-04 - mean_absolute_error: 0.0122\n",
      "\n",
      "Epoch 02522: loss did not improve\n",
      "Epoch 2523/10000\n",
      "2700/2700 [==============================] - 0s 25us/step - loss: 3.6090e-04 - mean_absolute_error: 0.0124\n",
      "\n",
      "Epoch 02523: loss did not improve\n",
      "Epoch 2524/10000\n",
      "2700/2700 [==============================] - 0s 24us/step - loss: 3.6091e-04 - mean_absolute_error: 0.0122\n",
      "\n",
      "Epoch 02524: loss did not improve\n",
      "Epoch 2525/10000\n",
      "2700/2700 [==============================] - 0s 25us/step - loss: 3.6485e-04 - mean_absolute_error: 0.0124\n",
      "\n",
      "Epoch 02525: loss did not improve\n",
      "Epoch 2526/10000\n",
      "2700/2700 [==============================] - 0s 25us/step - loss: 3.5729e-04 - mean_absolute_error: 0.0121\n",
      "\n",
      "Epoch 02526: loss did not improve\n",
      "Epoch 2527/10000\n",
      "2700/2700 [==============================] - 0s 24us/step - loss: 3.6139e-04 - mean_absolute_error: 0.0123\n",
      "\n",
      "Epoch 02527: loss did not improve\n",
      "Epoch 2528/10000\n",
      "2700/2700 [==============================] - 0s 26us/step - loss: 3.5392e-04 - mean_absolute_error: 0.0120\n",
      "\n",
      "Epoch 02528: loss improved from 0.00036 to 0.00035, saving model to keras/best_baseline_weights.hdf5\n",
      "Epoch 2529/10000\n",
      "2700/2700 [==============================] - 0s 31us/step - loss: 3.5608e-04 - mean_absolute_error: 0.0121\n",
      "\n",
      "Epoch 02529: loss did not improve\n",
      "Epoch 2530/10000\n",
      "2700/2700 [==============================] - 0s 25us/step - loss: 3.5619e-04 - mean_absolute_error: 0.0121\n",
      "\n",
      "Epoch 02530: loss did not improve\n",
      "Epoch 2531/10000\n",
      "2700/2700 [==============================] - 0s 24us/step - loss: 3.5867e-04 - mean_absolute_error: 0.0122\n",
      "\n",
      "Epoch 02531: loss did not improve\n",
      "Epoch 2532/10000\n",
      "2700/2700 [==============================] - 0s 25us/step - loss: 3.6270e-04 - mean_absolute_error: 0.0124\n",
      "\n",
      "Epoch 02532: loss did not improve\n",
      "Epoch 2533/10000\n",
      "2700/2700 [==============================] - 0s 25us/step - loss: 3.6279e-04 - mean_absolute_error: 0.0124\n",
      "\n",
      "Epoch 02533: loss did not improve\n",
      "Epoch 2534/10000\n",
      "2700/2700 [==============================] - 0s 24us/step - loss: 3.5916e-04 - mean_absolute_error: 0.0122\n",
      "\n",
      "Epoch 02534: loss did not improve\n",
      "Epoch 2535/10000\n",
      "2700/2700 [==============================] - 0s 27us/step - loss: 3.5405e-04 - mean_absolute_error: 0.0121\n",
      "\n",
      "Epoch 02535: loss did not improve\n",
      "Epoch 2536/10000\n",
      "2700/2700 [==============================] - 0s 27us/step - loss: 3.5341e-04 - mean_absolute_error: 0.0121\n",
      "\n",
      "Epoch 02536: loss improved from 0.00035 to 0.00035, saving model to keras/best_baseline_weights.hdf5\n",
      "Epoch 2537/10000\n",
      "2700/2700 [==============================] - 0s 25us/step - loss: 3.5559e-04 - mean_absolute_error: 0.0121\n",
      "\n",
      "Epoch 02537: loss did not improve\n",
      "Epoch 2538/10000\n",
      "2700/2700 [==============================] - 0s 24us/step - loss: 3.5636e-04 - mean_absolute_error: 0.0122\n",
      "\n",
      "Epoch 02538: loss did not improve\n",
      "Epoch 2539/10000\n",
      "2700/2700 [==============================] - 0s 25us/step - loss: 3.5566e-04 - mean_absolute_error: 0.0121\n",
      "\n",
      "Epoch 02539: loss did not improve\n",
      "Epoch 2540/10000\n",
      "2700/2700 [==============================] - 0s 25us/step - loss: 3.5451e-04 - mean_absolute_error: 0.0121\n",
      "\n",
      "Epoch 02540: loss did not improve\n",
      "Epoch 2541/10000\n",
      "2700/2700 [==============================] - 0s 23us/step - loss: 3.6014e-04 - mean_absolute_error: 0.0122\n",
      "\n",
      "Epoch 02541: loss did not improve\n",
      "Epoch 2542/10000\n",
      "2700/2700 [==============================] - 0s 26us/step - loss: 3.5337e-04 - mean_absolute_error: 0.0121\n",
      "\n",
      "Epoch 02542: loss improved from 0.00035 to 0.00035, saving model to keras/best_baseline_weights.hdf5\n",
      "Epoch 2543/10000\n",
      "2700/2700 [==============================] - 0s 26us/step - loss: 3.5691e-04 - mean_absolute_error: 0.0122\n",
      "\n",
      "Epoch 02543: loss did not improve\n",
      "Epoch 2544/10000\n",
      "2700/2700 [==============================] - 0s 24us/step - loss: 3.5713e-04 - mean_absolute_error: 0.0121\n",
      "\n",
      "Epoch 02544: loss did not improve\n",
      "Epoch 2545/10000\n",
      "2700/2700 [==============================] - 0s 25us/step - loss: 3.6332e-04 - mean_absolute_error: 0.0123\n",
      "\n",
      "Epoch 02545: loss did not improve\n",
      "Epoch 2546/10000\n",
      "2700/2700 [==============================] - 0s 25us/step - loss: 3.5855e-04 - mean_absolute_error: 0.0123\n",
      "\n",
      "Epoch 02546: loss did not improve\n",
      "Epoch 2547/10000\n",
      "2700/2700 [==============================] - 0s 23us/step - loss: 3.5586e-04 - mean_absolute_error: 0.0122\n",
      "\n",
      "Epoch 02547: loss did not improve\n",
      "Epoch 2548/10000\n",
      "2700/2700 [==============================] - 0s 26us/step - loss: 3.6260e-04 - mean_absolute_error: 0.0124\n",
      "\n",
      "Epoch 02548: loss did not improve\n",
      "Epoch 2549/10000\n",
      "2700/2700 [==============================] - 0s 25us/step - loss: 3.5491e-04 - mean_absolute_error: 0.0121\n",
      "\n",
      "Epoch 02549: loss did not improve\n",
      "Epoch 2550/10000\n",
      "2700/2700 [==============================] - 0s 26us/step - loss: 3.6401e-04 - mean_absolute_error: 0.0125\n",
      "\n",
      "Epoch 02550: loss did not improve\n",
      "Epoch 2551/10000\n",
      "2700/2700 [==============================] - 0s 24us/step - loss: 3.5702e-04 - mean_absolute_error: 0.0121\n",
      "\n",
      "Epoch 02551: loss did not improve\n",
      "Epoch 2552/10000\n",
      "2700/2700 [==============================] - 0s 26us/step - loss: 3.6638e-04 - mean_absolute_error: 0.0124\n",
      "\n",
      "Epoch 02552: loss did not improve\n",
      "Epoch 2553/10000\n",
      "2700/2700 [==============================] - 0s 27us/step - loss: 3.5635e-04 - mean_absolute_error: 0.0121\n",
      "\n",
      "Epoch 02553: loss did not improve\n",
      "Epoch 2554/10000\n",
      "2700/2700 [==============================] - 0s 25us/step - loss: 3.6267e-04 - mean_absolute_error: 0.0124\n",
      "\n",
      "Epoch 02554: loss did not improve\n",
      "Epoch 2555/10000\n",
      "2700/2700 [==============================] - 0s 27us/step - loss: 3.5640e-04 - mean_absolute_error: 0.0122\n",
      "\n",
      "Epoch 02555: loss did not improve\n",
      "Epoch 2556/10000\n",
      "2700/2700 [==============================] - 0s 26us/step - loss: 3.5574e-04 - mean_absolute_error: 0.0122\n",
      "\n",
      "Epoch 02556: loss did not improve\n",
      "Epoch 2557/10000\n",
      "2700/2700 [==============================] - 0s 25us/step - loss: 3.5944e-04 - mean_absolute_error: 0.0123\n",
      "\n",
      "Epoch 02557: loss did not improve\n",
      "Epoch 2558/10000\n",
      "2700/2700 [==============================] - 0s 28us/step - loss: 3.5964e-04 - mean_absolute_error: 0.0123\n",
      "\n",
      "Epoch 02558: loss did not improve\n",
      "Epoch 2559/10000\n",
      "2700/2700 [==============================] - 0s 25us/step - loss: 3.5966e-04 - mean_absolute_error: 0.0123\n",
      "\n",
      "Epoch 02559: loss did not improve\n",
      "Epoch 2560/10000\n",
      "2700/2700 [==============================] - 0s 25us/step - loss: 3.5573e-04 - mean_absolute_error: 0.0122\n",
      "\n",
      "Epoch 02560: loss did not improve\n",
      "Epoch 2561/10000\n",
      "2700/2700 [==============================] - 0s 27us/step - loss: 3.5720e-04 - mean_absolute_error: 0.0122\n",
      "\n",
      "Epoch 02561: loss did not improve\n",
      "Epoch 2562/10000\n",
      "2700/2700 [==============================] - 0s 26us/step - loss: 3.5449e-04 - mean_absolute_error: 0.0121\n",
      "\n",
      "Epoch 02562: loss did not improve\n",
      "Epoch 2563/10000\n",
      "2700/2700 [==============================] - 0s 26us/step - loss: 3.6269e-04 - mean_absolute_error: 0.0123\n",
      "\n",
      "Epoch 02563: loss did not improve\n",
      "Epoch 2564/10000\n",
      "2700/2700 [==============================] - 0s 24us/step - loss: 3.5835e-04 - mean_absolute_error: 0.0123\n",
      "\n",
      "Epoch 02564: loss did not improve\n",
      "Epoch 2565/10000\n",
      "2700/2700 [==============================] - 0s 26us/step - loss: 3.6001e-04 - mean_absolute_error: 0.0124\n",
      "\n",
      "Epoch 02565: loss did not improve\n",
      "Epoch 2566/10000\n",
      "2700/2700 [==============================] - 0s 26us/step - loss: 3.5590e-04 - mean_absolute_error: 0.0121\n",
      "\n",
      "Epoch 02566: loss did not improve\n",
      "Epoch 2567/10000\n",
      "2700/2700 [==============================] - 0s 24us/step - loss: 3.5390e-04 - mean_absolute_error: 0.0121\n",
      "\n",
      "Epoch 02567: loss did not improve\n",
      "Epoch 2568/10000\n",
      "2700/2700 [==============================] - 0s 25us/step - loss: 3.5843e-04 - mean_absolute_error: 0.0122\n",
      "\n",
      "Epoch 02568: loss did not improve\n",
      "Epoch 2569/10000\n",
      "2700/2700 [==============================] - 0s 26us/step - loss: 3.5486e-04 - mean_absolute_error: 0.0122\n",
      "\n",
      "Epoch 02569: loss did not improve\n",
      "Epoch 2570/10000\n",
      "2700/2700 [==============================] - 0s 26us/step - loss: 3.5974e-04 - mean_absolute_error: 0.0124\n",
      "\n",
      "Epoch 02570: loss did not improve\n",
      "Epoch 2571/10000\n",
      "2700/2700 [==============================] - 0s 25us/step - loss: 3.5320e-04 - mean_absolute_error: 0.0121\n",
      "\n",
      "Epoch 02571: loss improved from 0.00035 to 0.00035, saving model to keras/best_baseline_weights.hdf5\n",
      "Epoch 2572/10000\n",
      "2700/2700 [==============================] - 0s 25us/step - loss: 3.5628e-04 - mean_absolute_error: 0.0121\n",
      "\n",
      "Epoch 02572: loss did not improve\n",
      "Epoch 2573/10000\n",
      "2700/2700 [==============================] - 0s 27us/step - loss: 3.5732e-04 - mean_absolute_error: 0.0123\n",
      "\n",
      "Epoch 02573: loss did not improve\n",
      "Epoch 2574/10000\n",
      "2700/2700 [==============================] - 0s 24us/step - loss: 3.5888e-04 - mean_absolute_error: 0.0123\n",
      "\n",
      "Epoch 02574: loss did not improve\n",
      "Epoch 2575/10000\n",
      "2700/2700 [==============================] - 0s 25us/step - loss: 3.5801e-04 - mean_absolute_error: 0.0123\n",
      "\n",
      "Epoch 02575: loss did not improve\n",
      "Epoch 2576/10000\n",
      "2700/2700 [==============================] - 0s 25us/step - loss: 3.5609e-04 - mean_absolute_error: 0.0122\n",
      "\n",
      "Epoch 02576: loss did not improve\n",
      "Epoch 2577/10000\n",
      "2700/2700 [==============================] - 0s 24us/step - loss: 3.5611e-04 - mean_absolute_error: 0.0123\n",
      "\n",
      "Epoch 02577: loss did not improve\n",
      "Epoch 2578/10000\n",
      "2700/2700 [==============================] - 0s 26us/step - loss: 3.5486e-04 - mean_absolute_error: 0.0121\n",
      "\n",
      "Epoch 02578: loss did not improve\n",
      "Epoch 2579/10000\n",
      "2700/2700 [==============================] - 0s 27us/step - loss: 3.5523e-04 - mean_absolute_error: 0.0122\n",
      "\n",
      "Epoch 02579: loss did not improve\n",
      "Epoch 2580/10000\n",
      "2700/2700 [==============================] - 0s 29us/step - loss: 3.5670e-04 - mean_absolute_error: 0.0122\n",
      "\n",
      "Epoch 02580: loss did not improve\n",
      "Epoch 2581/10000\n",
      "2700/2700 [==============================] - 0s 31us/step - loss: 3.5377e-04 - mean_absolute_error: 0.0122\n",
      "\n",
      "Epoch 02581: loss did not improve\n",
      "Epoch 2582/10000\n",
      "2700/2700 [==============================] - ETA: 0s - loss: 3.2188e-04 - mean_absolute_error: 0.011 - 0s 32us/step - loss: 3.5264e-04 - mean_absolute_error: 0.0122\n",
      "\n",
      "Epoch 02582: loss improved from 0.00035 to 0.00035, saving model to keras/best_baseline_weights.hdf5\n",
      "Epoch 2583/10000\n",
      "2700/2700 [==============================] - 0s 25us/step - loss: 3.5664e-04 - mean_absolute_error: 0.0122\n",
      "\n",
      "Epoch 02583: loss did not improve\n",
      "Epoch 2584/10000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2700/2700 [==============================] - 0s 25us/step - loss: 3.5321e-04 - mean_absolute_error: 0.0121\n",
      "\n",
      "Epoch 02584: loss did not improve\n",
      "Epoch 2585/10000\n",
      "2700/2700 [==============================] - 0s 24us/step - loss: 3.5470e-04 - mean_absolute_error: 0.0122\n",
      "\n",
      "Epoch 02585: loss did not improve\n",
      "Epoch 2586/10000\n",
      "2700/2700 [==============================] - 0s 28us/step - loss: 3.5341e-04 - mean_absolute_error: 0.0121\n",
      "\n",
      "Epoch 02586: loss did not improve\n",
      "Epoch 2587/10000\n",
      "2700/2700 [==============================] - 0s 39us/step - loss: 3.5810e-04 - mean_absolute_error: 0.0123\n",
      "\n",
      "Epoch 02587: loss did not improve\n",
      "Epoch 2588/10000\n",
      "2700/2700 [==============================] - 0s 31us/step - loss: 3.5491e-04 - mean_absolute_error: 0.0122\n",
      "\n",
      "Epoch 02588: loss did not improve\n",
      "Epoch 2589/10000\n",
      "2700/2700 [==============================] - 0s 27us/step - loss: 3.5481e-04 - mean_absolute_error: 0.0122\n",
      "\n",
      "Epoch 02589: loss did not improve\n",
      "Epoch 2590/10000\n",
      "2700/2700 [==============================] - 0s 26us/step - loss: 3.5598e-04 - mean_absolute_error: 0.0122\n",
      "\n",
      "Epoch 02590: loss did not improve\n",
      "Epoch 2591/10000\n",
      "2700/2700 [==============================] - 0s 25us/step - loss: 3.5565e-04 - mean_absolute_error: 0.0122\n",
      "\n",
      "Epoch 02591: loss did not improve\n",
      "Epoch 2592/10000\n",
      "2700/2700 [==============================] - 0s 26us/step - loss: 3.5944e-04 - mean_absolute_error: 0.0123\n",
      "\n",
      "Epoch 02592: loss did not improve\n",
      "Epoch 2593/10000\n",
      "2700/2700 [==============================] - 0s 26us/step - loss: 3.5474e-04 - mean_absolute_error: 0.0122\n",
      "\n",
      "Epoch 02593: loss did not improve\n",
      "Epoch 2594/10000\n",
      "2700/2700 [==============================] - 0s 28us/step - loss: 3.5717e-04 - mean_absolute_error: 0.0122\n",
      "\n",
      "Epoch 02594: loss did not improve\n",
      "Epoch 2595/10000\n",
      "2700/2700 [==============================] - 0s 27us/step - loss: 3.6135e-04 - mean_absolute_error: 0.0122\n",
      "\n",
      "Epoch 02595: loss did not improve\n",
      "Epoch 2596/10000\n",
      "2700/2700 [==============================] - 0s 25us/step - loss: 3.5952e-04 - mean_absolute_error: 0.0124\n",
      "\n",
      "Epoch 02596: loss did not improve\n",
      "Epoch 2597/10000\n",
      "2700/2700 [==============================] - 0s 25us/step - loss: 3.5958e-04 - mean_absolute_error: 0.0123\n",
      "\n",
      "Epoch 02597: loss did not improve\n",
      "Epoch 2598/10000\n",
      "2700/2700 [==============================] - 0s 23us/step - loss: 3.6116e-04 - mean_absolute_error: 0.0124\n",
      "\n",
      "Epoch 02598: loss did not improve\n",
      "Epoch 2599/10000\n",
      "2700/2700 [==============================] - 0s 26us/step - loss: 3.5840e-04 - mean_absolute_error: 0.0123\n",
      "\n",
      "Epoch 02599: loss did not improve\n",
      "Epoch 2600/10000\n",
      "2700/2700 [==============================] - 0s 25us/step - loss: 3.5449e-04 - mean_absolute_error: 0.0122\n",
      "\n",
      "Epoch 02600: loss did not improve\n",
      "Epoch 2601/10000\n",
      "2700/2700 [==============================] - 0s 24us/step - loss: 3.5270e-04 - mean_absolute_error: 0.0121\n",
      "\n",
      "Epoch 02601: loss did not improve\n",
      "Epoch 2602/10000\n",
      "2700/2700 [==============================] - 0s 26us/step - loss: 3.5275e-04 - mean_absolute_error: 0.0121\n",
      "\n",
      "Epoch 02602: loss did not improve\n",
      "Epoch 2603/10000\n",
      "2700/2700 [==============================] - 0s 27us/step - loss: 3.5421e-04 - mean_absolute_error: 0.0122\n",
      "\n",
      "Epoch 02603: loss did not improve\n",
      "Epoch 2604/10000\n",
      "2700/2700 [==============================] - 0s 25us/step - loss: 3.5434e-04 - mean_absolute_error: 0.0122\n",
      "\n",
      "Epoch 02604: loss did not improve\n",
      "Epoch 2605/10000\n",
      "2700/2700 [==============================] - 0s 27us/step - loss: 3.5514e-04 - mean_absolute_error: 0.0122\n",
      "\n",
      "Epoch 02605: loss did not improve\n",
      "Epoch 2606/10000\n",
      "2700/2700 [==============================] - 0s 25us/step - loss: 3.6266e-04 - mean_absolute_error: 0.0124\n",
      "\n",
      "Epoch 02606: loss did not improve\n",
      "Epoch 2607/10000\n",
      "2700/2700 [==============================] - 0s 28us/step - loss: 3.5900e-04 - mean_absolute_error: 0.0124\n",
      "\n",
      "Epoch 02607: loss did not improve\n",
      "Epoch 2608/10000\n",
      "2700/2700 [==============================] - 0s 42us/step - loss: 3.5299e-04 - mean_absolute_error: 0.0122\n",
      "\n",
      "Epoch 02608: loss did not improve\n",
      "Epoch 2609/10000\n",
      "2700/2700 [==============================] - 0s 25us/step - loss: 3.5416e-04 - mean_absolute_error: 0.0122\n",
      "\n",
      "Epoch 02609: loss did not improve\n",
      "Epoch 2610/10000\n",
      "2700/2700 [==============================] - 0s 25us/step - loss: 3.5397e-04 - mean_absolute_error: 0.0121\n",
      "\n",
      "Epoch 02610: loss did not improve\n",
      "Epoch 2611/10000\n",
      "2700/2700 [==============================] - 0s 26us/step - loss: 3.5857e-04 - mean_absolute_error: 0.0124\n",
      "\n",
      "Epoch 02611: loss did not improve\n",
      "Epoch 2612/10000\n",
      "2700/2700 [==============================] - 0s 24us/step - loss: 3.5496e-04 - mean_absolute_error: 0.0122\n",
      "\n",
      "Epoch 02612: loss did not improve\n",
      "Epoch 2613/10000\n",
      "2700/2700 [==============================] - 0s 25us/step - loss: 3.5447e-04 - mean_absolute_error: 0.0122\n",
      "\n",
      "Epoch 02613: loss did not improve\n",
      "Epoch 2614/10000\n",
      "2700/2700 [==============================] - 0s 25us/step - loss: 3.4963e-04 - mean_absolute_error: 0.0120\n",
      "\n",
      "Epoch 02614: loss improved from 0.00035 to 0.00035, saving model to keras/best_baseline_weights.hdf5\n",
      "Epoch 2615/10000\n",
      "2700/2700 [==============================] - 0s 23us/step - loss: 3.5673e-04 - mean_absolute_error: 0.0123\n",
      "\n",
      "Epoch 02615: loss did not improve\n",
      "Epoch 2616/10000\n",
      "2700/2700 [==============================] - 0s 26us/step - loss: 3.5794e-04 - mean_absolute_error: 0.0123\n",
      "\n",
      "Epoch 02616: loss did not improve\n",
      "Epoch 2617/10000\n",
      "2700/2700 [==============================] - 0s 25us/step - loss: 3.5697e-04 - mean_absolute_error: 0.0123\n",
      "\n",
      "Epoch 02617: loss did not improve\n",
      "Epoch 2618/10000\n",
      "2700/2700 [==============================] - 0s 25us/step - loss: 3.5486e-04 - mean_absolute_error: 0.0122\n",
      "\n",
      "Epoch 02618: loss did not improve\n",
      "Epoch 2619/10000\n",
      "2700/2700 [==============================] - 0s 24us/step - loss: 3.5467e-04 - mean_absolute_error: 0.0123\n",
      "\n",
      "Epoch 02619: loss did not improve\n",
      "Epoch 2620/10000\n",
      "2700/2700 [==============================] - 0s 25us/step - loss: 3.5071e-04 - mean_absolute_error: 0.0121\n",
      "\n",
      "Epoch 02620: loss did not improve\n",
      "Epoch 2621/10000\n",
      "2700/2700 [==============================] - 0s 24us/step - loss: 3.5708e-04 - mean_absolute_error: 0.0124\n",
      "\n",
      "Epoch 02621: loss did not improve\n",
      "Epoch 2622/10000\n",
      "2700/2700 [==============================] - 0s 25us/step - loss: 3.6077e-04 - mean_absolute_error: 0.0125\n",
      "\n",
      "Epoch 02622: loss did not improve\n",
      "Epoch 2623/10000\n",
      "2700/2700 [==============================] - 0s 25us/step - loss: 3.5441e-04 - mean_absolute_error: 0.0122\n",
      "\n",
      "Epoch 02623: loss did not improve\n",
      "Epoch 2624/10000\n",
      "2700/2700 [==============================] - 0s 25us/step - loss: 3.5157e-04 - mean_absolute_error: 0.0121\n",
      "\n",
      "Epoch 02624: loss did not improve\n",
      "Epoch 2625/10000\n",
      "2700/2700 [==============================] - 0s 24us/step - loss: 3.6421e-04 - mean_absolute_error: 0.0127\n",
      "\n",
      "Epoch 02625: loss did not improve\n",
      "Epoch 2626/10000\n",
      "2700/2700 [==============================] - 0s 25us/step - loss: 3.5290e-04 - mean_absolute_error: 0.0122\n",
      "\n",
      "Epoch 02626: loss did not improve\n",
      "Epoch 2627/10000\n",
      "2700/2700 [==============================] - 0s 25us/step - loss: 3.5324e-04 - mean_absolute_error: 0.0122\n",
      "\n",
      "Epoch 02627: loss did not improve\n",
      "Epoch 2628/10000\n",
      "2700/2700 [==============================] - 0s 24us/step - loss: 3.6139e-04 - mean_absolute_error: 0.0124\n",
      "\n",
      "Epoch 02628: loss did not improve\n",
      "Epoch 2629/10000\n",
      "2700/2700 [==============================] - 0s 25us/step - loss: 3.5479e-04 - mean_absolute_error: 0.0122\n",
      "\n",
      "Epoch 02629: loss did not improve\n",
      "Epoch 2630/10000\n",
      "2700/2700 [==============================] - 0s 25us/step - loss: 3.5688e-04 - mean_absolute_error: 0.0123\n",
      "\n",
      "Epoch 02630: loss did not improve\n",
      "Epoch 2631/10000\n",
      "2700/2700 [==============================] - 0s 25us/step - loss: 3.5372e-04 - mean_absolute_error: 0.0122\n",
      "\n",
      "Epoch 02631: loss did not improve\n",
      "Epoch 2632/10000\n",
      "2700/2700 [==============================] - 0s 25us/step - loss: 3.6076e-04 - mean_absolute_error: 0.0124\n",
      "\n",
      "Epoch 02632: loss did not improve\n",
      "Epoch 2633/10000\n",
      "2700/2700 [==============================] - 0s 25us/step - loss: 3.5123e-04 - mean_absolute_error: 0.0121\n",
      "\n",
      "Epoch 02633: loss did not improve\n",
      "Epoch 2634/10000\n",
      "2700/2700 [==============================] - 0s 23us/step - loss: 3.5255e-04 - mean_absolute_error: 0.0122\n",
      "\n",
      "Epoch 02634: loss did not improve\n",
      "Epoch 2635/10000\n",
      "2700/2700 [==============================] - 0s 26us/step - loss: 3.6129e-04 - mean_absolute_error: 0.0124\n",
      "\n",
      "Epoch 02635: loss did not improve\n",
      "Epoch 2636/10000\n",
      "2700/2700 [==============================] - 0s 24us/step - loss: 3.5638e-04 - mean_absolute_error: 0.0123\n",
      "\n",
      "Epoch 02636: loss did not improve\n",
      "Epoch 2637/10000\n",
      "2700/2700 [==============================] - 0s 26us/step - loss: 3.5203e-04 - mean_absolute_error: 0.0122\n",
      "\n",
      "Epoch 02637: loss did not improve\n",
      "Epoch 2638/10000\n",
      "2700/2700 [==============================] - 0s 25us/step - loss: 3.4996e-04 - mean_absolute_error: 0.0121\n",
      "\n",
      "Epoch 02638: loss did not improve\n",
      "Epoch 2639/10000\n",
      "2700/2700 [==============================] - 0s 25us/step - loss: 3.5297e-04 - mean_absolute_error: 0.0122\n",
      "\n",
      "Epoch 02639: loss did not improve\n",
      "Epoch 2640/10000\n",
      "2700/2700 [==============================] - 0s 24us/step - loss: 3.5353e-04 - mean_absolute_error: 0.0122\n",
      "\n",
      "Epoch 02640: loss did not improve\n",
      "Epoch 2641/10000\n",
      "2700/2700 [==============================] - 0s 26us/step - loss: 3.5487e-04 - mean_absolute_error: 0.0122\n",
      "\n",
      "Epoch 02641: loss did not improve\n",
      "Epoch 2642/10000\n",
      "2700/2700 [==============================] - 0s 25us/step - loss: 3.4865e-04 - mean_absolute_error: 0.0121\n",
      "\n",
      "Epoch 02642: loss improved from 0.00035 to 0.00035, saving model to keras/best_baseline_weights.hdf5\n",
      "Epoch 2643/10000\n",
      "2700/2700 [==============================] - 0s 23us/step - loss: 3.5303e-04 - mean_absolute_error: 0.0123\n",
      "\n",
      "Epoch 02643: loss did not improve\n",
      "Epoch 2644/10000\n",
      "2700/2700 [==============================] - 0s 25us/step - loss: 3.5254e-04 - mean_absolute_error: 0.0122\n",
      "\n",
      "Epoch 02644: loss did not improve\n",
      "Epoch 2645/10000\n",
      "2700/2700 [==============================] - 0s 25us/step - loss: 3.5589e-04 - mean_absolute_error: 0.0123\n",
      "\n",
      "Epoch 02645: loss did not improve\n",
      "Epoch 2646/10000\n",
      "2700/2700 [==============================] - ETA: 0s - loss: 3.6624e-04 - mean_absolute_error: 0.012 - 0s 23us/step - loss: 3.5168e-04 - mean_absolute_error: 0.0121\n",
      "\n",
      "Epoch 02646: loss did not improve\n",
      "Epoch 2647/10000\n",
      "2700/2700 [==============================] - 0s 26us/step - loss: 3.4959e-04 - mean_absolute_error: 0.0122\n",
      "\n",
      "Epoch 02647: loss did not improve\n",
      "Epoch 2648/10000\n",
      "2700/2700 [==============================] - 0s 25us/step - loss: 3.5344e-04 - mean_absolute_error: 0.0123\n",
      "\n",
      "Epoch 02648: loss did not improve\n",
      "Epoch 2649/10000\n",
      "2700/2700 [==============================] - 0s 24us/step - loss: 3.5136e-04 - mean_absolute_error: 0.0122\n",
      "\n",
      "Epoch 02649: loss did not improve\n",
      "Epoch 2650/10000\n",
      "2700/2700 [==============================] - 0s 26us/step - loss: 3.5495e-04 - mean_absolute_error: 0.0122\n",
      "\n",
      "Epoch 02650: loss did not improve\n",
      "Epoch 2651/10000\n",
      "2700/2700 [==============================] - 0s 25us/step - loss: 3.5094e-04 - mean_absolute_error: 0.0122\n",
      "\n",
      "Epoch 02651: loss did not improve\n",
      "Epoch 2652/10000\n",
      "2700/2700 [==============================] - 0s 23us/step - loss: 3.5451e-04 - mean_absolute_error: 0.0122\n",
      "\n",
      "Epoch 02652: loss did not improve\n",
      "Epoch 2653/10000\n",
      "2700/2700 [==============================] - 0s 25us/step - loss: 3.5439e-04 - mean_absolute_error: 0.0122\n",
      "\n",
      "Epoch 02653: loss did not improve\n",
      "Epoch 2654/10000\n",
      "2700/2700 [==============================] - 0s 27us/step - loss: 3.4938e-04 - mean_absolute_error: 0.0121\n",
      "\n",
      "Epoch 02654: loss did not improve\n",
      "Epoch 2655/10000\n",
      "2700/2700 [==============================] - 0s 25us/step - loss: 3.4851e-04 - mean_absolute_error: 0.0122\n",
      "\n",
      "Epoch 02655: loss improved from 0.00035 to 0.00035, saving model to keras/best_baseline_weights.hdf5\n",
      "Epoch 2656/10000\n",
      "2700/2700 [==============================] - 0s 23us/step - loss: 3.5020e-04 - mean_absolute_error: 0.0121\n",
      "\n",
      "Epoch 02656: loss did not improve\n",
      "Epoch 2657/10000\n",
      "2700/2700 [==============================] - 0s 25us/step - loss: 3.5531e-04 - mean_absolute_error: 0.0123\n",
      "\n",
      "Epoch 02657: loss did not improve\n",
      "Epoch 2658/10000\n",
      "2700/2700 [==============================] - 0s 24us/step - loss: 3.4992e-04 - mean_absolute_error: 0.0121\n",
      "\n",
      "Epoch 02658: loss did not improve\n",
      "Epoch 2659/10000\n",
      "2700/2700 [==============================] - 0s 23us/step - loss: 3.5825e-04 - mean_absolute_error: 0.0124\n",
      "\n",
      "Epoch 02659: loss did not improve\n",
      "Epoch 2660/10000\n",
      "2700/2700 [==============================] - 0s 25us/step - loss: 3.5232e-04 - mean_absolute_error: 0.0123\n",
      "\n",
      "Epoch 02660: loss did not improve\n",
      "Epoch 2661/10000\n",
      "2700/2700 [==============================] - 0s 27us/step - loss: 3.4697e-04 - mean_absolute_error: 0.0120\n",
      "\n",
      "Epoch 02661: loss improved from 0.00035 to 0.00035, saving model to keras/best_baseline_weights.hdf5\n",
      "Epoch 2662/10000\n",
      "2700/2700 [==============================] - 0s 25us/step - loss: 3.5715e-04 - mean_absolute_error: 0.0124\n",
      "\n",
      "Epoch 02662: loss did not improve\n",
      "Epoch 2663/10000\n",
      "2700/2700 [==============================] - 0s 26us/step - loss: 3.4839e-04 - mean_absolute_error: 0.0121\n",
      "\n",
      "Epoch 02663: loss did not improve\n",
      "Epoch 2664/10000\n",
      "2700/2700 [==============================] - 0s 26us/step - loss: 3.4848e-04 - mean_absolute_error: 0.0121\n",
      "\n",
      "Epoch 02664: loss did not improve\n",
      "Epoch 2665/10000\n",
      "2700/2700 [==============================] - 0s 25us/step - loss: 3.4868e-04 - mean_absolute_error: 0.0121\n",
      "\n",
      "Epoch 02665: loss did not improve\n",
      "Epoch 2666/10000\n",
      "2700/2700 [==============================] - 0s 25us/step - loss: 3.5320e-04 - mean_absolute_error: 0.0122\n",
      "\n",
      "Epoch 02666: loss did not improve\n",
      "Epoch 2667/10000\n",
      "2700/2700 [==============================] - 0s 30us/step - loss: 3.4804e-04 - mean_absolute_error: 0.0121\n",
      "\n",
      "Epoch 02667: loss did not improve\n",
      "Epoch 2668/10000\n",
      "2700/2700 [==============================] - 0s 26us/step - loss: 3.4969e-04 - mean_absolute_error: 0.0122\n",
      "\n",
      "Epoch 02668: loss did not improve\n",
      "Epoch 2669/10000\n",
      "2700/2700 [==============================] - 0s 24us/step - loss: 3.5346e-04 - mean_absolute_error: 0.0122\n",
      "\n",
      "Epoch 02669: loss did not improve\n",
      "Epoch 2670/10000\n",
      "2700/2700 [==============================] - 0s 28us/step - loss: 3.5804e-04 - mean_absolute_error: 0.0125\n",
      "\n",
      "Epoch 02670: loss did not improve\n",
      "Epoch 2671/10000\n",
      "2700/2700 [==============================] - 0s 28us/step - loss: 3.5410e-04 - mean_absolute_error: 0.0123\n",
      "\n",
      "Epoch 02671: loss did not improve\n",
      "Epoch 2672/10000\n",
      "2700/2700 [==============================] - 0s 26us/step - loss: 3.5518e-04 - mean_absolute_error: 0.0123\n",
      "\n",
      "Epoch 02672: loss did not improve\n",
      "Epoch 2673/10000\n",
      "2700/2700 [==============================] - 0s 25us/step - loss: 3.5628e-04 - mean_absolute_error: 0.0124\n",
      "\n",
      "Epoch 02673: loss did not improve\n",
      "Epoch 2674/10000\n",
      "2700/2700 [==============================] - 0s 27us/step - loss: 3.5269e-04 - mean_absolute_error: 0.0122\n",
      "\n",
      "Epoch 02674: loss did not improve\n",
      "Epoch 2675/10000\n",
      "2700/2700 [==============================] - 0s 27us/step - loss: 3.4837e-04 - mean_absolute_error: 0.0121\n",
      "\n",
      "Epoch 02675: loss did not improve\n",
      "Epoch 2676/10000\n",
      "2700/2700 [==============================] - 0s 24us/step - loss: 3.5191e-04 - mean_absolute_error: 0.0122\n",
      "\n",
      "Epoch 02676: loss did not improve\n",
      "Epoch 2677/10000\n",
      "2700/2700 [==============================] - 0s 26us/step - loss: 3.4824e-04 - mean_absolute_error: 0.0121\n",
      "\n",
      "Epoch 02677: loss did not improve\n",
      "Epoch 2678/10000\n",
      "2700/2700 [==============================] - 0s 26us/step - loss: 3.5385e-04 - mean_absolute_error: 0.0123\n",
      "\n",
      "Epoch 02678: loss did not improve\n",
      "Epoch 2679/10000\n",
      "2700/2700 [==============================] - 0s 23us/step - loss: 3.5106e-04 - mean_absolute_error: 0.0122\n",
      "\n",
      "Epoch 02679: loss did not improve\n",
      "Epoch 2680/10000\n",
      "2700/2700 [==============================] - 0s 25us/step - loss: 3.5246e-04 - mean_absolute_error: 0.0122\n",
      "\n",
      "Epoch 02680: loss did not improve\n",
      "Epoch 2681/10000\n",
      "2700/2700 [==============================] - 0s 29us/step - loss: 3.5018e-04 - mean_absolute_error: 0.0122\n",
      "\n",
      "Epoch 02681: loss did not improve\n",
      "Epoch 2682/10000\n",
      "2700/2700 [==============================] - 0s 34us/step - loss: 3.5679e-04 - mean_absolute_error: 0.0123\n",
      "\n",
      "Epoch 02682: loss did not improve\n",
      "Epoch 2683/10000\n",
      "2700/2700 [==============================] - 0s 34us/step - loss: 3.4602e-04 - mean_absolute_error: 0.0121\n",
      "\n",
      "Epoch 02683: loss improved from 0.00035 to 0.00035, saving model to keras/best_baseline_weights.hdf5\n",
      "Epoch 2684/10000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2700/2700 [==============================] - 0s 25us/step - loss: 3.5824e-04 - mean_absolute_error: 0.0125\n",
      "\n",
      "Epoch 02684: loss did not improve\n",
      "Epoch 2685/10000\n",
      "2700/2700 [==============================] - 0s 24us/step - loss: 3.5317e-04 - mean_absolute_error: 0.0122\n",
      "\n",
      "Epoch 02685: loss did not improve\n",
      "Epoch 2686/10000\n",
      "2700/2700 [==============================] - 0s 27us/step - loss: 3.4733e-04 - mean_absolute_error: 0.0122\n",
      "\n",
      "Epoch 02686: loss did not improve\n",
      "Epoch 2687/10000\n",
      "2700/2700 [==============================] - 0s 27us/step - loss: 3.5530e-04 - mean_absolute_error: 0.0123\n",
      "\n",
      "Epoch 02687: loss did not improve\n",
      "Epoch 2688/10000\n",
      "2700/2700 [==============================] - 0s 29us/step - loss: 3.5305e-04 - mean_absolute_error: 0.0123\n",
      "\n",
      "Epoch 02688: loss did not improve\n",
      "Epoch 2689/10000\n",
      "2700/2700 [==============================] - 0s 28us/step - loss: 3.4869e-04 - mean_absolute_error: 0.0121\n",
      "\n",
      "Epoch 02689: loss did not improve\n",
      "Epoch 2690/10000\n",
      "2700/2700 [==============================] - 0s 26us/step - loss: 3.5173e-04 - mean_absolute_error: 0.0122\n",
      "\n",
      "Epoch 02690: loss did not improve\n",
      "Epoch 2691/10000\n",
      "2700/2700 [==============================] - 0s 26us/step - loss: 3.4776e-04 - mean_absolute_error: 0.0121\n",
      "\n",
      "Epoch 02691: loss did not improve\n",
      "Epoch 2692/10000\n",
      "2700/2700 [==============================] - 0s 24us/step - loss: 3.5186e-04 - mean_absolute_error: 0.0122\n",
      "\n",
      "Epoch 02692: loss did not improve\n",
      "Epoch 2693/10000\n",
      "2700/2700 [==============================] - 0s 39us/step - loss: 3.4979e-04 - mean_absolute_error: 0.0121\n",
      "\n",
      "Epoch 02693: loss did not improve\n",
      "Epoch 2694/10000\n",
      "2700/2700 [==============================] - 0s 43us/step - loss: 3.4681e-04 - mean_absolute_error: 0.0121\n",
      "\n",
      "Epoch 02694: loss did not improve\n",
      "Epoch 2695/10000\n",
      "2700/2700 [==============================] - 0s 28us/step - loss: 3.4558e-04 - mean_absolute_error: 0.0121\n",
      "\n",
      "Epoch 02695: loss improved from 0.00035 to 0.00035, saving model to keras/best_baseline_weights.hdf5\n",
      "Epoch 2696/10000\n",
      "2700/2700 [==============================] - 0s 26us/step - loss: 3.5025e-04 - mean_absolute_error: 0.0121\n",
      "\n",
      "Epoch 02696: loss did not improve\n",
      "Epoch 2697/10000\n",
      "2700/2700 [==============================] - 0s 25us/step - loss: 3.4819e-04 - mean_absolute_error: 0.0122\n",
      "\n",
      "Epoch 02697: loss did not improve\n",
      "Epoch 2698/10000\n",
      "2700/2700 [==============================] - 0s 24us/step - loss: 3.5700e-04 - mean_absolute_error: 0.0124\n",
      "\n",
      "Epoch 02698: loss did not improve\n",
      "Epoch 2699/10000\n",
      "2700/2700 [==============================] - 0s 26us/step - loss: 3.4646e-04 - mean_absolute_error: 0.0121\n",
      "\n",
      "Epoch 02699: loss did not improve\n",
      "Epoch 2700/10000\n",
      "2700/2700 [==============================] - 0s 26us/step - loss: 3.4822e-04 - mean_absolute_error: 0.0121\n",
      "\n",
      "Epoch 02700: loss did not improve\n",
      "Epoch 2701/10000\n",
      "2700/2700 [==============================] - 0s 30us/step - loss: 3.4701e-04 - mean_absolute_error: 0.0121\n",
      "\n",
      "Epoch 02701: loss did not improve\n",
      "Epoch 2702/10000\n",
      "2700/2700 [==============================] - 0s 24us/step - loss: 3.4984e-04 - mean_absolute_error: 0.0122\n",
      "\n",
      "Epoch 02702: loss did not improve\n",
      "Epoch 2703/10000\n",
      "2700/2700 [==============================] - 0s 27us/step - loss: 3.4770e-04 - mean_absolute_error: 0.0121\n",
      "\n",
      "Epoch 02703: loss did not improve\n",
      "Epoch 2704/10000\n",
      "2700/2700 [==============================] - 0s 27us/step - loss: 3.4950e-04 - mean_absolute_error: 0.0123\n",
      "\n",
      "Epoch 02704: loss did not improve\n",
      "Epoch 2705/10000\n",
      "2700/2700 [==============================] - 0s 23us/step - loss: 3.5567e-04 - mean_absolute_error: 0.0124\n",
      "\n",
      "Epoch 02705: loss did not improve\n",
      "Epoch 2706/10000\n",
      "2700/2700 [==============================] - 0s 27us/step - loss: 3.4996e-04 - mean_absolute_error: 0.0122\n",
      "\n",
      "Epoch 02706: loss did not improve\n",
      "Epoch 2707/10000\n",
      "2700/2700 [==============================] - 0s 26us/step - loss: 3.5145e-04 - mean_absolute_error: 0.0123\n",
      "\n",
      "Epoch 02707: loss did not improve\n",
      "Epoch 2708/10000\n",
      "2700/2700 [==============================] - 0s 24us/step - loss: 3.5018e-04 - mean_absolute_error: 0.0122\n",
      "\n",
      "Epoch 02708: loss did not improve\n",
      "Epoch 2709/10000\n",
      "2700/2700 [==============================] - 0s 25us/step - loss: 3.4889e-04 - mean_absolute_error: 0.0122\n",
      "\n",
      "Epoch 02709: loss did not improve\n",
      "Epoch 2710/10000\n",
      "2700/2700 [==============================] - 0s 25us/step - loss: 3.5172e-04 - mean_absolute_error: 0.0123\n",
      "\n",
      "Epoch 02710: loss did not improve\n",
      "Epoch 2711/10000\n",
      "2700/2700 [==============================] - 0s 23us/step - loss: 3.5471e-04 - mean_absolute_error: 0.0124\n",
      "\n",
      "Epoch 02711: loss did not improve\n",
      "Epoch 2712/10000\n",
      "2700/2700 [==============================] - 0s 25us/step - loss: 3.5424e-04 - mean_absolute_error: 0.0124\n",
      "\n",
      "Epoch 02712: loss did not improve\n",
      "Epoch 2713/10000\n",
      "2700/2700 [==============================] - 0s 26us/step - loss: 3.5006e-04 - mean_absolute_error: 0.0122\n",
      "\n",
      "Epoch 02713: loss did not improve\n",
      "Epoch 2714/10000\n",
      "2700/2700 [==============================] - 0s 27us/step - loss: 3.4652e-04 - mean_absolute_error: 0.0121\n",
      "\n",
      "Epoch 02714: loss did not improve\n",
      "Epoch 2715/10000\n",
      "2700/2700 [==============================] - 0s 24us/step - loss: 3.4766e-04 - mean_absolute_error: 0.0121\n",
      "\n",
      "Epoch 02715: loss did not improve\n",
      "Epoch 2716/10000\n",
      "2700/2700 [==============================] - 0s 26us/step - loss: 3.4794e-04 - mean_absolute_error: 0.0122\n",
      "\n",
      "Epoch 02716: loss did not improve\n",
      "Epoch 2717/10000\n",
      "2700/2700 [==============================] - 0s 26us/step - loss: 3.5004e-04 - mean_absolute_error: 0.0123\n",
      "\n",
      "Epoch 02717: loss did not improve\n",
      "Epoch 2718/10000\n",
      "2700/2700 [==============================] - 0s 26us/step - loss: 3.4761e-04 - mean_absolute_error: 0.0121\n",
      "\n",
      "Epoch 02718: loss did not improve\n",
      "Epoch 2719/10000\n",
      "2700/2700 [==============================] - 0s 26us/step - loss: 3.5026e-04 - mean_absolute_error: 0.0122\n",
      "\n",
      "Epoch 02719: loss did not improve\n",
      "Epoch 2720/10000\n",
      "2700/2700 [==============================] - 0s 26us/step - loss: 3.5066e-04 - mean_absolute_error: 0.0123\n",
      "\n",
      "Epoch 02720: loss did not improve\n",
      "Epoch 2721/10000\n",
      "2700/2700 [==============================] - 0s 25us/step - loss: 3.5355e-04 - mean_absolute_error: 0.0123\n",
      "\n",
      "Epoch 02721: loss did not improve\n",
      "Epoch 2722/10000\n",
      "2700/2700 [==============================] - 0s 25us/step - loss: 3.4822e-04 - mean_absolute_error: 0.0121\n",
      "\n",
      "Epoch 02722: loss did not improve\n",
      "Epoch 2723/10000\n",
      "2700/2700 [==============================] - 0s 27us/step - loss: 3.4797e-04 - mean_absolute_error: 0.0122\n",
      "\n",
      "Epoch 02723: loss did not improve\n",
      "Epoch 2724/10000\n",
      "2700/2700 [==============================] - 0s 27us/step - loss: 3.5199e-04 - mean_absolute_error: 0.0123\n",
      "\n",
      "Epoch 02724: loss did not improve\n",
      "Epoch 2725/10000\n",
      "2700/2700 [==============================] - 0s 25us/step - loss: 3.5160e-04 - mean_absolute_error: 0.0122\n",
      "\n",
      "Epoch 02725: loss did not improve\n",
      "Epoch 2726/10000\n",
      "2700/2700 [==============================] - 0s 26us/step - loss: 3.4823e-04 - mean_absolute_error: 0.0121\n",
      "\n",
      "Epoch 02726: loss did not improve\n",
      "Epoch 2727/10000\n",
      "2700/2700 [==============================] - 0s 24us/step - loss: 3.5656e-04 - mean_absolute_error: 0.0124\n",
      "\n",
      "Epoch 02727: loss did not improve\n",
      "Epoch 2728/10000\n",
      "2700/2700 [==============================] - 0s 25us/step - loss: 3.5245e-04 - mean_absolute_error: 0.0125\n",
      "\n",
      "Epoch 02728: loss did not improve\n",
      "Epoch 2729/10000\n",
      "2700/2700 [==============================] - 0s 27us/step - loss: 3.4932e-04 - mean_absolute_error: 0.0122\n",
      "\n",
      "Epoch 02729: loss did not improve\n",
      "Epoch 2730/10000\n",
      "2700/2700 [==============================] - 0s 28us/step - loss: 3.4970e-04 - mean_absolute_error: 0.0122\n",
      "\n",
      "Epoch 02730: loss did not improve\n",
      "Epoch 2731/10000\n",
      "2700/2700 [==============================] - 0s 25us/step - loss: 3.5030e-04 - mean_absolute_error: 0.0123\n",
      "\n",
      "Epoch 02731: loss did not improve\n",
      "Epoch 2732/10000\n",
      "2700/2700 [==============================] - 0s 27us/step - loss: 3.4522e-04 - mean_absolute_error: 0.0120\n",
      "\n",
      "Epoch 02732: loss improved from 0.00035 to 0.00035, saving model to keras/best_baseline_weights.hdf5\n",
      "Epoch 2733/10000\n",
      "2700/2700 [==============================] - 0s 31us/step - loss: 3.5224e-04 - mean_absolute_error: 0.0124\n",
      "\n",
      "Epoch 02733: loss did not improve\n",
      "Epoch 2734/10000\n",
      "2700/2700 [==============================] - 0s 26us/step - loss: 3.4913e-04 - mean_absolute_error: 0.0122\n",
      "\n",
      "Epoch 02734: loss did not improve\n",
      "Epoch 2735/10000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2700/2700 [==============================] - 0s 27us/step - loss: 3.4895e-04 - mean_absolute_error: 0.0122\n",
      "\n",
      "Epoch 02735: loss did not improve\n",
      "Epoch 2736/10000\n",
      "2700/2700 [==============================] - 0s 24us/step - loss: 3.4502e-04 - mean_absolute_error: 0.0120\n",
      "\n",
      "Epoch 02736: loss improved from 0.00035 to 0.00035, saving model to keras/best_baseline_weights.hdf5\n",
      "Epoch 2737/10000\n",
      "2700/2700 [==============================] - 0s 28us/step - loss: 3.4685e-04 - mean_absolute_error: 0.0122\n",
      "\n",
      "Epoch 02737: loss did not improve\n",
      "Epoch 2738/10000\n",
      "2700/2700 [==============================] - 0s 30us/step - loss: 3.4864e-04 - mean_absolute_error: 0.0121\n",
      "\n",
      "Epoch 02738: loss did not improve\n",
      "Epoch 2739/10000\n",
      "2700/2700 [==============================] - 0s 31us/step - loss: 3.4514e-04 - mean_absolute_error: 0.0121\n",
      "\n",
      "Epoch 02739: loss did not improve\n",
      "Epoch 2740/10000\n",
      "2700/2700 [==============================] - 0s 28us/step - loss: 3.4625e-04 - mean_absolute_error: 0.0121\n",
      "\n",
      "Epoch 02740: loss did not improve\n",
      "Epoch 2741/10000\n",
      "2700/2700 [==============================] - 0s 27us/step - loss: 3.4483e-04 - mean_absolute_error: 0.0121\n",
      "\n",
      "Epoch 02741: loss improved from 0.00035 to 0.00034, saving model to keras/best_baseline_weights.hdf5\n",
      "Epoch 2742/10000\n",
      "2700/2700 [==============================] - 0s 27us/step - loss: 3.4503e-04 - mean_absolute_error: 0.0120\n",
      "\n",
      "Epoch 02742: loss did not improve\n",
      "Epoch 2743/10000\n",
      "2700/2700 [==============================] - 0s 26us/step - loss: 3.4611e-04 - mean_absolute_error: 0.0122\n",
      "\n",
      "Epoch 02743: loss did not improve\n",
      "Epoch 2744/10000\n",
      "2700/2700 [==============================] - 0s 25us/step - loss: 3.4866e-04 - mean_absolute_error: 0.0122\n",
      "\n",
      "Epoch 02744: loss did not improve\n",
      "Epoch 2745/10000\n",
      "2700/2700 [==============================] - 0s 25us/step - loss: 3.4843e-04 - mean_absolute_error: 0.0121\n",
      "\n",
      "Epoch 02745: loss did not improve\n",
      "Epoch 2746/10000\n",
      "2700/2700 [==============================] - 0s 25us/step - loss: 3.4438e-04 - mean_absolute_error: 0.0121\n",
      "\n",
      "Epoch 02746: loss improved from 0.00034 to 0.00034, saving model to keras/best_baseline_weights.hdf5\n",
      "Epoch 2747/10000\n",
      "2700/2700 [==============================] - 0s 47us/step - loss: 3.4518e-04 - mean_absolute_error: 0.0120\n",
      "\n",
      "Epoch 02747: loss did not improve\n",
      "Epoch 2748/10000\n",
      "2700/2700 [==============================] - 0s 30us/step - loss: 3.4818e-04 - mean_absolute_error: 0.0122\n",
      "\n",
      "Epoch 02748: loss did not improve\n",
      "Epoch 2749/10000\n",
      "2700/2700 [==============================] - 0s 36us/step - loss: 3.4485e-04 - mean_absolute_error: 0.0121\n",
      "\n",
      "Epoch 02749: loss did not improve\n",
      "Epoch 2750/10000\n",
      "2700/2700 [==============================] - 0s 41us/step - loss: 3.4815e-04 - mean_absolute_error: 0.0121\n",
      "\n",
      "Epoch 02750: loss did not improve\n",
      "Epoch 2751/10000\n",
      "2700/2700 [==============================] - 0s 33us/step - loss: 3.5144e-04 - mean_absolute_error: 0.0123\n",
      "\n",
      "Epoch 02751: loss did not improve\n",
      "Epoch 2752/10000\n",
      "2700/2700 [==============================] - 0s 29us/step - loss: 3.5198e-04 - mean_absolute_error: 0.0124\n",
      "\n",
      "Epoch 02752: loss did not improve\n",
      "Epoch 2753/10000\n",
      "2700/2700 [==============================] - 0s 31us/step - loss: 3.5403e-04 - mean_absolute_error: 0.0123\n",
      "\n",
      "Epoch 02753: loss did not improve\n",
      "Epoch 2754/10000\n",
      "2700/2700 [==============================] - 0s 32us/step - loss: 3.4985e-04 - mean_absolute_error: 0.0122\n",
      "\n",
      "Epoch 02754: loss did not improve\n",
      "Epoch 2755/10000\n",
      "2700/2700 [==============================] - 0s 29us/step - loss: 3.4451e-04 - mean_absolute_error: 0.0120\n",
      "\n",
      "Epoch 02755: loss did not improve\n",
      "Epoch 2756/10000\n",
      "2700/2700 [==============================] - 0s 31us/step - loss: 3.4744e-04 - mean_absolute_error: 0.0122\n",
      "\n",
      "Epoch 02756: loss did not improve\n",
      "Epoch 2757/10000\n",
      "2700/2700 [==============================] - 0s 29us/step - loss: 3.4536e-04 - mean_absolute_error: 0.0121\n",
      "\n",
      "Epoch 02757: loss did not improve\n",
      "Epoch 2758/10000\n",
      "2700/2700 [==============================] - 0s 28us/step - loss: 3.5552e-04 - mean_absolute_error: 0.0124\n",
      "\n",
      "Epoch 02758: loss did not improve\n",
      "Epoch 2759/10000\n",
      "2700/2700 [==============================] - 0s 30us/step - loss: 3.4672e-04 - mean_absolute_error: 0.0121\n",
      "\n",
      "Epoch 02759: loss did not improve\n",
      "Epoch 2760/10000\n",
      "2700/2700 [==============================] - 0s 37us/step - loss: 3.4314e-04 - mean_absolute_error: 0.0120\n",
      "\n",
      "Epoch 02760: loss improved from 0.00034 to 0.00034, saving model to keras/best_baseline_weights.hdf5\n",
      "Epoch 2761/10000\n",
      "2700/2700 [==============================] - 0s 35us/step - loss: 3.4661e-04 - mean_absolute_error: 0.0121\n",
      "\n",
      "Epoch 02761: loss did not improve\n",
      "Epoch 2762/10000\n",
      "2700/2700 [==============================] - 0s 35us/step - loss: 3.5174e-04 - mean_absolute_error: 0.0123\n",
      "\n",
      "Epoch 02762: loss did not improve\n",
      "Epoch 2763/10000\n",
      "2700/2700 [==============================] - 0s 29us/step - loss: 3.4502e-04 - mean_absolute_error: 0.0121\n",
      "\n",
      "Epoch 02763: loss did not improve\n",
      "Epoch 2764/10000\n",
      "2700/2700 [==============================] - 0s 30us/step - loss: 3.5211e-04 - mean_absolute_error: 0.0124\n",
      "\n",
      "Epoch 02764: loss did not improve\n",
      "Epoch 2765/10000\n",
      "2700/2700 [==============================] - 0s 29us/step - loss: 3.4187e-04 - mean_absolute_error: 0.0119\n",
      "\n",
      "Epoch 02765: loss improved from 0.00034 to 0.00034, saving model to keras/best_baseline_weights.hdf5\n",
      "Epoch 2766/10000\n",
      "2700/2700 [==============================] - 0s 30us/step - loss: 3.4595e-04 - mean_absolute_error: 0.0122\n",
      "\n",
      "Epoch 02766: loss did not improve\n",
      "Epoch 2767/10000\n",
      "2700/2700 [==============================] - 0s 28us/step - loss: 3.4727e-04 - mean_absolute_error: 0.0121\n",
      "\n",
      "Epoch 02767: loss did not improve\n",
      "Epoch 2768/10000\n",
      "2700/2700 [==============================] - 0s 30us/step - loss: 3.4174e-04 - mean_absolute_error: 0.0120\n",
      "\n",
      "Epoch 02768: loss improved from 0.00034 to 0.00034, saving model to keras/best_baseline_weights.hdf5\n",
      "Epoch 2769/10000\n",
      "2700/2700 [==============================] - 0s 32us/step - loss: 3.4587e-04 - mean_absolute_error: 0.0122\n",
      "\n",
      "Epoch 02769: loss did not improve\n",
      "Epoch 2770/10000\n",
      "2700/2700 [==============================] - 0s 29us/step - loss: 3.5245e-04 - mean_absolute_error: 0.0123\n",
      "\n",
      "Epoch 02770: loss did not improve\n",
      "Epoch 2771/10000\n",
      "2700/2700 [==============================] - 0s 29us/step - loss: 3.4297e-04 - mean_absolute_error: 0.0121\n",
      "\n",
      "Epoch 02771: loss did not improve\n",
      "Epoch 2772/10000\n",
      "2700/2700 [==============================] - 0s 28us/step - loss: 3.4278e-04 - mean_absolute_error: 0.0120\n",
      "\n",
      "Epoch 02772: loss did not improve\n",
      "Epoch 2773/10000\n",
      "2700/2700 [==============================] - 0s 29us/step - loss: 3.4481e-04 - mean_absolute_error: 0.0121\n",
      "\n",
      "Epoch 02773: loss did not improve\n",
      "Epoch 2774/10000\n",
      "2700/2700 [==============================] - 0s 29us/step - loss: 3.4614e-04 - mean_absolute_error: 0.0121\n",
      "\n",
      "Epoch 02774: loss did not improve\n",
      "Epoch 2775/10000\n",
      "2700/2700 [==============================] - 0s 29us/step - loss: 3.4675e-04 - mean_absolute_error: 0.0122\n",
      "\n",
      "Epoch 02775: loss did not improve\n",
      "Epoch 2776/10000\n",
      "2700/2700 [==============================] - 0s 29us/step - loss: 3.4272e-04 - mean_absolute_error: 0.0120\n",
      "\n",
      "Epoch 02776: loss did not improve\n",
      "Epoch 2777/10000\n",
      "2700/2700 [==============================] - 0s 28us/step - loss: 3.4671e-04 - mean_absolute_error: 0.0122\n",
      "\n",
      "Epoch 02777: loss did not improve\n",
      "Epoch 2778/10000\n",
      "2700/2700 [==============================] - 0s 27us/step - loss: 3.4444e-04 - mean_absolute_error: 0.0121\n",
      "\n",
      "Epoch 02778: loss did not improve\n",
      "Epoch 2779/10000\n",
      "2700/2700 [==============================] - 0s 29us/step - loss: 3.4670e-04 - mean_absolute_error: 0.0122\n",
      "\n",
      "Epoch 02779: loss did not improve\n",
      "Epoch 2780/10000\n",
      "2700/2700 [==============================] - 0s 37us/step - loss: 3.4989e-04 - mean_absolute_error: 0.0123\n",
      "\n",
      "Epoch 02780: loss did not improve\n",
      "Epoch 2781/10000\n",
      "2700/2700 [==============================] - 0s 30us/step - loss: 3.4577e-04 - mean_absolute_error: 0.0121\n",
      "\n",
      "Epoch 02781: loss did not improve\n",
      "Epoch 2782/10000\n",
      "2700/2700 [==============================] - 0s 32us/step - loss: 3.4594e-04 - mean_absolute_error: 0.0121\n",
      "\n",
      "Epoch 02782: loss did not improve\n",
      "Epoch 2783/10000\n",
      "2700/2700 [==============================] - 0s 29us/step - loss: 3.4795e-04 - mean_absolute_error: 0.0122\n",
      "\n",
      "Epoch 02783: loss did not improve\n",
      "Epoch 2784/10000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2700/2700 [==============================] - 0s 28us/step - loss: 3.4308e-04 - mean_absolute_error: 0.0120\n",
      "\n",
      "Epoch 02784: loss did not improve\n",
      "Epoch 2785/10000\n",
      "2700/2700 [==============================] - 0s 29us/step - loss: 3.4998e-04 - mean_absolute_error: 0.0122\n",
      "\n",
      "Epoch 02785: loss did not improve\n",
      "Epoch 2786/10000\n",
      "2700/2700 [==============================] - 0s 27us/step - loss: 3.4799e-04 - mean_absolute_error: 0.0122\n",
      "\n",
      "Epoch 02786: loss did not improve\n",
      "Epoch 2787/10000\n",
      "2700/2700 [==============================] - 0s 30us/step - loss: 3.4431e-04 - mean_absolute_error: 0.0121\n",
      "\n",
      "Epoch 02787: loss did not improve\n",
      "Epoch 2788/10000\n",
      "2700/2700 [==============================] - 0s 40us/step - loss: 3.4163e-04 - mean_absolute_error: 0.0120\n",
      "\n",
      "Epoch 02788: loss improved from 0.00034 to 0.00034, saving model to keras/best_baseline_weights.hdf5\n",
      "Epoch 2789/10000\n",
      "2700/2700 [==============================] - 0s 31us/step - loss: 3.4207e-04 - mean_absolute_error: 0.0120\n",
      "\n",
      "Epoch 02789: loss did not improve\n",
      "Epoch 2790/10000\n",
      "2700/2700 [==============================] - 0s 28us/step - loss: 3.4256e-04 - mean_absolute_error: 0.0120\n",
      "\n",
      "Epoch 02790: loss did not improve\n",
      "Epoch 2791/10000\n",
      "2700/2700 [==============================] - 0s 28us/step - loss: 3.4563e-04 - mean_absolute_error: 0.0122\n",
      "\n",
      "Epoch 02791: loss did not improve\n",
      "Epoch 2792/10000\n",
      "2700/2700 [==============================] - 0s 30us/step - loss: 3.4298e-04 - mean_absolute_error: 0.0121\n",
      "\n",
      "Epoch 02792: loss did not improve\n",
      "Epoch 2793/10000\n",
      "2700/2700 [==============================] - 0s 28us/step - loss: 3.4298e-04 - mean_absolute_error: 0.0120\n",
      "\n",
      "Epoch 02793: loss did not improve\n",
      "Epoch 2794/10000\n",
      "2700/2700 [==============================] - 0s 32us/step - loss: 3.4476e-04 - mean_absolute_error: 0.0122\n",
      "\n",
      "Epoch 02794: loss did not improve\n",
      "Epoch 2795/10000\n",
      "2700/2700 [==============================] - 0s 29us/step - loss: 3.4006e-04 - mean_absolute_error: 0.0120\n",
      "\n",
      "Epoch 02795: loss improved from 0.00034 to 0.00034, saving model to keras/best_baseline_weights.hdf5\n",
      "Epoch 2796/10000\n",
      "2700/2700 [==============================] - 0s 31us/step - loss: 3.4176e-04 - mean_absolute_error: 0.0121\n",
      "\n",
      "Epoch 02796: loss did not improve\n",
      "Epoch 2797/10000\n",
      "2700/2700 [==============================] - 0s 28us/step - loss: 3.5147e-04 - mean_absolute_error: 0.0124\n",
      "\n",
      "Epoch 02797: loss did not improve\n",
      "Epoch 2798/10000\n",
      "2700/2700 [==============================] - 0s 30us/step - loss: 3.4512e-04 - mean_absolute_error: 0.0122\n",
      "\n",
      "Epoch 02798: loss did not improve\n",
      "Epoch 2799/10000\n",
      "2700/2700 [==============================] - 0s 29us/step - loss: 3.4332e-04 - mean_absolute_error: 0.0121\n",
      "\n",
      "Epoch 02799: loss did not improve\n",
      "Epoch 2800/10000\n",
      "2700/2700 [==============================] - 0s 31us/step - loss: 3.4380e-04 - mean_absolute_error: 0.0121\n",
      "\n",
      "Epoch 02800: loss did not improve\n",
      "Epoch 2801/10000\n",
      "2700/2700 [==============================] - 0s 31us/step - loss: 3.4342e-04 - mean_absolute_error: 0.0121\n",
      "\n",
      "Epoch 02801: loss did not improve\n",
      "Epoch 2802/10000\n",
      "2700/2700 [==============================] - 0s 28us/step - loss: 3.4315e-04 - mean_absolute_error: 0.0121\n",
      "\n",
      "Epoch 02802: loss did not improve\n",
      "Epoch 2803/10000\n",
      "2700/2700 [==============================] - 0s 30us/step - loss: 3.4400e-04 - mean_absolute_error: 0.0121\n",
      "\n",
      "Epoch 02803: loss did not improve\n",
      "Epoch 2804/10000\n",
      "2700/2700 [==============================] - 0s 28us/step - loss: 3.4764e-04 - mean_absolute_error: 0.0123\n",
      "\n",
      "Epoch 02804: loss did not improve\n",
      "Epoch 2805/10000\n",
      "2700/2700 [==============================] - 0s 29us/step - loss: 3.4146e-04 - mean_absolute_error: 0.0120\n",
      "\n",
      "Epoch 02805: loss did not improve\n",
      "Epoch 2806/10000\n",
      "2700/2700 [==============================] - 0s 31us/step - loss: 3.4105e-04 - mean_absolute_error: 0.0121\n",
      "\n",
      "Epoch 02806: loss did not improve\n",
      "Epoch 2807/10000\n",
      "2700/2700 [==============================] - 0s 32us/step - loss: 3.4009e-04 - mean_absolute_error: 0.0120\n",
      "\n",
      "Epoch 02807: loss did not improve\n",
      "Epoch 2808/10000\n",
      "2700/2700 [==============================] - 0s 28us/step - loss: 3.4417e-04 - mean_absolute_error: 0.0120\n",
      "\n",
      "Epoch 02808: loss did not improve\n",
      "Epoch 2809/10000\n",
      "2700/2700 [==============================] - 0s 30us/step - loss: 3.4362e-04 - mean_absolute_error: 0.0121\n",
      "\n",
      "Epoch 02809: loss did not improve\n",
      "Epoch 2810/10000\n",
      "2700/2700 [==============================] - 0s 31us/step - loss: 3.3993e-04 - mean_absolute_error: 0.0120\n",
      "\n",
      "Epoch 02810: loss improved from 0.00034 to 0.00034, saving model to keras/best_baseline_weights.hdf5\n",
      "Epoch 2811/10000\n",
      "2700/2700 [==============================] - 0s 30us/step - loss: 3.4096e-04 - mean_absolute_error: 0.0120\n",
      "\n",
      "Epoch 02811: loss did not improve\n",
      "Epoch 2812/10000\n",
      "2700/2700 [==============================] - 0s 30us/step - loss: 3.4999e-04 - mean_absolute_error: 0.0124\n",
      "\n",
      "Epoch 02812: loss did not improve\n",
      "Epoch 2813/10000\n",
      "2700/2700 [==============================] - 0s 26us/step - loss: 3.3952e-04 - mean_absolute_error: 0.0119\n",
      "\n",
      "Epoch 02813: loss improved from 0.00034 to 0.00034, saving model to keras/best_baseline_weights.hdf5\n",
      "Epoch 2814/10000\n",
      "2700/2700 [==============================] - 0s 33us/step - loss: 3.4676e-04 - mean_absolute_error: 0.0122\n",
      "\n",
      "Epoch 02814: loss did not improve\n",
      "Epoch 2815/10000\n",
      "2700/2700 [==============================] - 0s 33us/step - loss: 3.4369e-04 - mean_absolute_error: 0.0121\n",
      "\n",
      "Epoch 02815: loss did not improve\n",
      "Epoch 2816/10000\n",
      "2700/2700 [==============================] - 0s 35us/step - loss: 3.3899e-04 - mean_absolute_error: 0.0119\n",
      "\n",
      "Epoch 02816: loss improved from 0.00034 to 0.00034, saving model to keras/best_baseline_weights.hdf5\n",
      "Epoch 2817/10000\n",
      "2700/2700 [==============================] - 0s 33us/step - loss: 3.3954e-04 - mean_absolute_error: 0.0120\n",
      "\n",
      "Epoch 02817: loss did not improve\n",
      "Epoch 2818/10000\n",
      "2700/2700 [==============================] - 0s 31us/step - loss: 3.4439e-04 - mean_absolute_error: 0.0122\n",
      "\n",
      "Epoch 02818: loss did not improve\n",
      "Epoch 2819/10000\n",
      "2700/2700 [==============================] - 0s 30us/step - loss: 3.4013e-04 - mean_absolute_error: 0.0120\n",
      "\n",
      "Epoch 02819: loss did not improve\n",
      "Epoch 2820/10000\n",
      "2700/2700 [==============================] - 0s 28us/step - loss: 3.4158e-04 - mean_absolute_error: 0.0121\n",
      "\n",
      "Epoch 02820: loss did not improve\n",
      "Epoch 2821/10000\n",
      "2700/2700 [==============================] - 0s 26us/step - loss: 3.4276e-04 - mean_absolute_error: 0.0122\n",
      "\n",
      "Epoch 02821: loss did not improve\n",
      "Epoch 2822/10000\n",
      "2700/2700 [==============================] - 0s 27us/step - loss: 3.5002e-04 - mean_absolute_error: 0.0122\n",
      "\n",
      "Epoch 02822: loss did not improve\n",
      "Epoch 2823/10000\n",
      "2700/2700 [==============================] - 0s 29us/step - loss: 3.3911e-04 - mean_absolute_error: 0.0120\n",
      "\n",
      "Epoch 02823: loss did not improve\n",
      "Epoch 2824/10000\n",
      "2700/2700 [==============================] - 0s 30us/step - loss: 3.4410e-04 - mean_absolute_error: 0.0122\n",
      "\n",
      "Epoch 02824: loss did not improve\n",
      "Epoch 2825/10000\n",
      "2700/2700 [==============================] - 0s 29us/step - loss: 3.4412e-04 - mean_absolute_error: 0.0122\n",
      "\n",
      "Epoch 02825: loss did not improve\n",
      "Epoch 2826/10000\n",
      "2700/2700 [==============================] - 0s 29us/step - loss: 3.4027e-04 - mean_absolute_error: 0.0120\n",
      "\n",
      "Epoch 02826: loss did not improve\n",
      "Epoch 2827/10000\n",
      "2700/2700 [==============================] - 0s 29us/step - loss: 3.4186e-04 - mean_absolute_error: 0.0121\n",
      "\n",
      "Epoch 02827: loss did not improve\n",
      "Epoch 2828/10000\n",
      "2700/2700 [==============================] - 0s 33us/step - loss: 3.4223e-04 - mean_absolute_error: 0.0120\n",
      "\n",
      "Epoch 02828: loss did not improve\n",
      "Epoch 2829/10000\n",
      "2700/2700 [==============================] - 0s 37us/step - loss: 3.3973e-04 - mean_absolute_error: 0.0120\n",
      "\n",
      "Epoch 02829: loss did not improve\n",
      "Epoch 2830/10000\n",
      "2700/2700 [==============================] - 0s 37us/step - loss: 3.4057e-04 - mean_absolute_error: 0.0120\n",
      "\n",
      "Epoch 02830: loss did not improve\n",
      "Epoch 2831/10000\n",
      "2700/2700 [==============================] - 0s 32us/step - loss: 3.4477e-04 - mean_absolute_error: 0.0122\n",
      "\n",
      "Epoch 02831: loss did not improve\n",
      "Epoch 2832/10000\n",
      "2700/2700 [==============================] - 0s 31us/step - loss: 3.4030e-04 - mean_absolute_error: 0.0120\n",
      "\n",
      "Epoch 02832: loss did not improve\n",
      "Epoch 2833/10000\n",
      "2700/2700 [==============================] - 0s 27us/step - loss: 3.4100e-04 - mean_absolute_error: 0.0120\n",
      "\n",
      "Epoch 02833: loss did not improve\n",
      "Epoch 2834/10000\n",
      "2700/2700 [==============================] - 0s 28us/step - loss: 3.4276e-04 - mean_absolute_error: 0.0121\n",
      "\n",
      "Epoch 02834: loss did not improve\n",
      "Epoch 2835/10000\n",
      "2700/2700 [==============================] - 0s 28us/step - loss: 3.4103e-04 - mean_absolute_error: 0.0121\n",
      "\n",
      "Epoch 02835: loss did not improve\n",
      "Epoch 2836/10000\n",
      "2700/2700 [==============================] - 0s 29us/step - loss: 3.4037e-04 - mean_absolute_error: 0.0121\n",
      "\n",
      "Epoch 02836: loss did not improve\n",
      "Epoch 2837/10000\n",
      "2700/2700 [==============================] - 0s 25us/step - loss: 3.4375e-04 - mean_absolute_error: 0.0121\n",
      "\n",
      "Epoch 02837: loss did not improve\n",
      "Epoch 2838/10000\n",
      "2700/2700 [==============================] - 0s 29us/step - loss: 3.4881e-04 - mean_absolute_error: 0.0122\n",
      "\n",
      "Epoch 02838: loss did not improve\n",
      "Epoch 2839/10000\n",
      "2700/2700 [==============================] - 0s 30us/step - loss: 3.4680e-04 - mean_absolute_error: 0.0123\n",
      "\n",
      "Epoch 02839: loss did not improve\n",
      "Epoch 2840/10000\n",
      "2700/2700 [==============================] - 0s 27us/step - loss: 3.4088e-04 - mean_absolute_error: 0.0121\n",
      "\n",
      "Epoch 02840: loss did not improve\n",
      "Epoch 2841/10000\n",
      "2700/2700 [==============================] - 0s 27us/step - loss: 3.4268e-04 - mean_absolute_error: 0.0121\n",
      "\n",
      "Epoch 02841: loss did not improve\n",
      "Epoch 2842/10000\n",
      "2700/2700 [==============================] - 0s 29us/step - loss: 3.3758e-04 - mean_absolute_error: 0.0120\n",
      "\n",
      "Epoch 02842: loss improved from 0.00034 to 0.00034, saving model to keras/best_baseline_weights.hdf5\n",
      "Epoch 2843/10000\n",
      "2700/2700 [==============================] - 0s 35us/step - loss: 3.4331e-04 - mean_absolute_error: 0.0122\n",
      "\n",
      "Epoch 02843: loss did not improve\n",
      "Epoch 2844/10000\n",
      "2700/2700 [==============================] - 0s 42us/step - loss: 3.4052e-04 - mean_absolute_error: 0.0120\n",
      "\n",
      "Epoch 02844: loss did not improve\n",
      "Epoch 2845/10000\n",
      "2700/2700 [==============================] - 0s 43us/step - loss: 3.4253e-04 - mean_absolute_error: 0.0121\n",
      "\n",
      "Epoch 02845: loss did not improve\n",
      "Epoch 2846/10000\n",
      "2700/2700 [==============================] - 0s 29us/step - loss: 3.3707e-04 - mean_absolute_error: 0.0120\n",
      "\n",
      "Epoch 02846: loss improved from 0.00034 to 0.00034, saving model to keras/best_baseline_weights.hdf5\n",
      "Epoch 2847/10000\n",
      "2700/2700 [==============================] - 0s 31us/step - loss: 3.3967e-04 - mean_absolute_error: 0.0120\n",
      "\n",
      "Epoch 02847: loss did not improve\n",
      "Epoch 2848/10000\n",
      "2700/2700 [==============================] - 0s 29us/step - loss: 3.3887e-04 - mean_absolute_error: 0.0120\n",
      "\n",
      "Epoch 02848: loss did not improve\n",
      "Epoch 2849/10000\n",
      "2700/2700 [==============================] - 0s 31us/step - loss: 3.4883e-04 - mean_absolute_error: 0.0123\n",
      "\n",
      "Epoch 02849: loss did not improve\n",
      "Epoch 2850/10000\n",
      "2700/2700 [==============================] - 0s 34us/step - loss: 3.4185e-04 - mean_absolute_error: 0.0121\n",
      "\n",
      "Epoch 02850: loss did not improve\n",
      "Epoch 2851/10000\n",
      "2700/2700 [==============================] - 0s 30us/step - loss: 3.3717e-04 - mean_absolute_error: 0.0119\n",
      "\n",
      "Epoch 02851: loss did not improve\n",
      "Epoch 2852/10000\n",
      "2700/2700 [==============================] - 0s 29us/step - loss: 3.4393e-04 - mean_absolute_error: 0.0121\n",
      "\n",
      "Epoch 02852: loss did not improve\n",
      "Epoch 2853/10000\n",
      "2700/2700 [==============================] - 0s 28us/step - loss: 3.4392e-04 - mean_absolute_error: 0.0121\n",
      "\n",
      "Epoch 02853: loss did not improve\n",
      "Epoch 2854/10000\n",
      "2700/2700 [==============================] - 0s 29us/step - loss: 3.4174e-04 - mean_absolute_error: 0.0121\n",
      "\n",
      "Epoch 02854: loss did not improve\n",
      "Epoch 2855/10000\n",
      "2700/2700 [==============================] - 0s 34us/step - loss: 3.3871e-04 - mean_absolute_error: 0.0120\n",
      "\n",
      "Epoch 02855: loss did not improve\n",
      "Epoch 2856/10000\n",
      "2700/2700 [==============================] - 0s 29us/step - loss: 3.4093e-04 - mean_absolute_error: 0.0120\n",
      "\n",
      "Epoch 02856: loss did not improve\n",
      "Epoch 2857/10000\n",
      "2700/2700 [==============================] - 0s 33us/step - loss: 3.4443e-04 - mean_absolute_error: 0.0122\n",
      "\n",
      "Epoch 02857: loss did not improve\n",
      "Epoch 2858/10000\n",
      "2700/2700 [==============================] - 0s 27us/step - loss: 3.3796e-04 - mean_absolute_error: 0.0120\n",
      "\n",
      "Epoch 02858: loss did not improve\n",
      "Epoch 2859/10000\n",
      "2700/2700 [==============================] - 0s 29us/step - loss: 3.4052e-04 - mean_absolute_error: 0.0119\n",
      "\n",
      "Epoch 02859: loss did not improve\n",
      "Epoch 2860/10000\n",
      "2700/2700 [==============================] - 0s 28us/step - loss: 3.4339e-04 - mean_absolute_error: 0.0122\n",
      "\n",
      "Epoch 02860: loss did not improve\n",
      "Epoch 2861/10000\n",
      "2700/2700 [==============================] - 0s 28us/step - loss: 3.4417e-04 - mean_absolute_error: 0.0122\n",
      "\n",
      "Epoch 02861: loss did not improve\n",
      "Epoch 2862/10000\n",
      "2700/2700 [==============================] - 0s 28us/step - loss: 3.3988e-04 - mean_absolute_error: 0.0120\n",
      "\n",
      "Epoch 02862: loss did not improve\n",
      "Epoch 2863/10000\n",
      "2700/2700 [==============================] - 0s 29us/step - loss: 3.4101e-04 - mean_absolute_error: 0.0120\n",
      "\n",
      "Epoch 02863: loss did not improve\n",
      "Epoch 2864/10000\n",
      "2700/2700 [==============================] - 0s 34us/step - loss: 3.4145e-04 - mean_absolute_error: 0.0122\n",
      "\n",
      "Epoch 02864: loss did not improve\n",
      "Epoch 2865/10000\n",
      "2700/2700 [==============================] - 0s 36us/step - loss: 3.3809e-04 - mean_absolute_error: 0.0120\n",
      "\n",
      "Epoch 02865: loss did not improve\n",
      "Epoch 2866/10000\n",
      "2700/2700 [==============================] - 0s 31us/step - loss: 3.3821e-04 - mean_absolute_error: 0.0120\n",
      "\n",
      "Epoch 02866: loss did not improve\n",
      "Epoch 2867/10000\n",
      "2700/2700 [==============================] - 0s 33us/step - loss: 3.3881e-04 - mean_absolute_error: 0.0120\n",
      "\n",
      "Epoch 02867: loss did not improve\n",
      "Epoch 2868/10000\n",
      "2700/2700 [==============================] - 0s 29us/step - loss: 3.3719e-04 - mean_absolute_error: 0.0120\n",
      "\n",
      "Epoch 02868: loss did not improve\n",
      "Epoch 2869/10000\n",
      "2700/2700 [==============================] - 0s 28us/step - loss: 3.3972e-04 - mean_absolute_error: 0.0120\n",
      "\n",
      "Epoch 02869: loss did not improve\n",
      "Epoch 2870/10000\n",
      "2700/2700 [==============================] - 0s 30us/step - loss: 3.3922e-04 - mean_absolute_error: 0.0120\n",
      "\n",
      "Epoch 02870: loss did not improve\n",
      "Epoch 2871/10000\n",
      "2700/2700 [==============================] - 0s 28us/step - loss: 3.3679e-04 - mean_absolute_error: 0.0119\n",
      "\n",
      "Epoch 02871: loss improved from 0.00034 to 0.00034, saving model to keras/best_baseline_weights.hdf5\n",
      "Epoch 2872/10000\n",
      "2700/2700 [==============================] - 0s 28us/step - loss: 3.3835e-04 - mean_absolute_error: 0.0120\n",
      "\n",
      "Epoch 02872: loss did not improve\n",
      "Epoch 2873/10000\n",
      "2700/2700 [==============================] - 0s 28us/step - loss: 3.3887e-04 - mean_absolute_error: 0.0120\n",
      "\n",
      "Epoch 02873: loss did not improve\n",
      "Epoch 2874/10000\n",
      "2700/2700 [==============================] - 0s 27us/step - loss: 3.3573e-04 - mean_absolute_error: 0.0119\n",
      "\n",
      "Epoch 02874: loss improved from 0.00034 to 0.00034, saving model to keras/best_baseline_weights.hdf5\n",
      "Epoch 2875/10000\n",
      "2700/2700 [==============================] - 0s 27us/step - loss: 3.4112e-04 - mean_absolute_error: 0.0121\n",
      "\n",
      "Epoch 02875: loss did not improve\n",
      "Epoch 2876/10000\n",
      "2700/2700 [==============================] - 0s 32us/step - loss: 3.4932e-04 - mean_absolute_error: 0.0124\n",
      "\n",
      "Epoch 02876: loss did not improve\n",
      "Epoch 2877/10000\n",
      "2700/2700 [==============================] - 0s 30us/step - loss: 3.3706e-04 - mean_absolute_error: 0.0120\n",
      "\n",
      "Epoch 02877: loss did not improve\n",
      "Epoch 2878/10000\n",
      "2700/2700 [==============================] - 0s 43us/step - loss: 3.4075e-04 - mean_absolute_error: 0.0121\n",
      "\n",
      "Epoch 02878: loss did not improve\n",
      "Epoch 2879/10000\n",
      "2700/2700 [==============================] - 0s 33us/step - loss: 3.4119e-04 - mean_absolute_error: 0.0122\n",
      "\n",
      "Epoch 02879: loss did not improve\n",
      "Epoch 2880/10000\n",
      "2700/2700 [==============================] - 0s 36us/step - loss: 3.4072e-04 - mean_absolute_error: 0.0122\n",
      "\n",
      "Epoch 02880: loss did not improve\n",
      "Epoch 2881/10000\n",
      "2700/2700 [==============================] - 0s 31us/step - loss: 3.3906e-04 - mean_absolute_error: 0.0121\n",
      "\n",
      "Epoch 02881: loss did not improve\n",
      "Epoch 2882/10000\n",
      "2700/2700 [==============================] - 0s 32us/step - loss: 3.3791e-04 - mean_absolute_error: 0.0120\n",
      "\n",
      "Epoch 02882: loss did not improve\n",
      "Epoch 2883/10000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2700/2700 [==============================] - 0s 27us/step - loss: 3.4299e-04 - mean_absolute_error: 0.0121\n",
      "\n",
      "Epoch 02883: loss did not improve\n",
      "Epoch 2884/10000\n",
      "2700/2700 [==============================] - 0s 28us/step - loss: 3.3624e-04 - mean_absolute_error: 0.0120\n",
      "\n",
      "Epoch 02884: loss did not improve\n",
      "Epoch 2885/10000\n",
      "2700/2700 [==============================] - 0s 30us/step - loss: 3.3906e-04 - mean_absolute_error: 0.0120\n",
      "\n",
      "Epoch 02885: loss did not improve\n",
      "Epoch 2886/10000\n",
      "2700/2700 [==============================] - 0s 27us/step - loss: 3.3566e-04 - mean_absolute_error: 0.0119\n",
      "\n",
      "Epoch 02886: loss improved from 0.00034 to 0.00034, saving model to keras/best_baseline_weights.hdf5\n",
      "Epoch 2887/10000\n",
      "2700/2700 [==============================] - 0s 29us/step - loss: 3.3841e-04 - mean_absolute_error: 0.0120\n",
      "\n",
      "Epoch 02887: loss did not improve\n",
      "Epoch 2888/10000\n",
      "2700/2700 [==============================] - 0s 26us/step - loss: 3.3750e-04 - mean_absolute_error: 0.0120\n",
      "\n",
      "Epoch 02888: loss did not improve\n",
      "Epoch 2889/10000\n",
      "2700/2700 [==============================] - 0s 29us/step - loss: 3.3700e-04 - mean_absolute_error: 0.0119\n",
      "\n",
      "Epoch 02889: loss did not improve\n",
      "Epoch 2890/10000\n",
      "2700/2700 [==============================] - 0s 29us/step - loss: 3.4701e-04 - mean_absolute_error: 0.0123\n",
      "\n",
      "Epoch 02890: loss did not improve\n",
      "Epoch 2891/10000\n",
      "2700/2700 [==============================] - 0s 28us/step - loss: 3.4081e-04 - mean_absolute_error: 0.0121\n",
      "\n",
      "Epoch 02891: loss did not improve\n",
      "Epoch 2892/10000\n",
      "2700/2700 [==============================] - 0s 29us/step - loss: 3.3645e-04 - mean_absolute_error: 0.0120\n",
      "\n",
      "Epoch 02892: loss did not improve\n",
      "Epoch 2893/10000\n",
      "2700/2700 [==============================] - 0s 29us/step - loss: 3.3849e-04 - mean_absolute_error: 0.0120\n",
      "\n",
      "Epoch 02893: loss did not improve\n",
      "Epoch 2894/10000\n",
      "2700/2700 [==============================] - 0s 28us/step - loss: 3.3529e-04 - mean_absolute_error: 0.0120\n",
      "\n",
      "Epoch 02894: loss improved from 0.00034 to 0.00034, saving model to keras/best_baseline_weights.hdf5\n",
      "Epoch 2895/10000\n",
      "2700/2700 [==============================] - 0s 29us/step - loss: 3.4065e-04 - mean_absolute_error: 0.0121\n",
      "\n",
      "Epoch 02895: loss did not improve\n",
      "Epoch 2896/10000\n",
      "2700/2700 [==============================] - 0s 28us/step - loss: 3.3451e-04 - mean_absolute_error: 0.0119\n",
      "\n",
      "Epoch 02896: loss improved from 0.00034 to 0.00033, saving model to keras/best_baseline_weights.hdf5\n",
      "Epoch 2897/10000\n",
      "2700/2700 [==============================] - 0s 29us/step - loss: 3.4688e-04 - mean_absolute_error: 0.0123\n",
      "\n",
      "Epoch 02897: loss did not improve\n",
      "Epoch 2898/10000\n",
      "2700/2700 [==============================] - 0s 26us/step - loss: 3.3669e-04 - mean_absolute_error: 0.0120\n",
      "\n",
      "Epoch 02898: loss did not improve\n",
      "Epoch 2899/10000\n",
      "2700/2700 [==============================] - 0s 31us/step - loss: 3.4287e-04 - mean_absolute_error: 0.0123\n",
      "\n",
      "Epoch 02899: loss did not improve\n",
      "Epoch 2900/10000\n",
      "2700/2700 [==============================] - 0s 30us/step - loss: 3.3578e-04 - mean_absolute_error: 0.0119\n",
      "\n",
      "Epoch 02900: loss did not improve\n",
      "Epoch 2901/10000\n",
      "2700/2700 [==============================] - 0s 29us/step - loss: 3.3877e-04 - mean_absolute_error: 0.0121\n",
      "\n",
      "Epoch 02901: loss did not improve\n",
      "Epoch 2902/10000\n",
      "2700/2700 [==============================] - 0s 29us/step - loss: 3.3906e-04 - mean_absolute_error: 0.0120\n",
      "\n",
      "Epoch 02902: loss did not improve\n",
      "Epoch 2903/10000\n",
      "2700/2700 [==============================] - 0s 28us/step - loss: 3.4471e-04 - mean_absolute_error: 0.0122\n",
      "\n",
      "Epoch 02903: loss did not improve\n",
      "Epoch 2904/10000\n",
      "2700/2700 [==============================] - 0s 31us/step - loss: 3.3779e-04 - mean_absolute_error: 0.0120\n",
      "\n",
      "Epoch 02904: loss did not improve\n",
      "Epoch 2905/10000\n",
      "2700/2700 [==============================] - 0s 30us/step - loss: 3.3272e-04 - mean_absolute_error: 0.0119\n",
      "\n",
      "Epoch 02905: loss improved from 0.00033 to 0.00033, saving model to keras/best_baseline_weights.hdf5\n",
      "Epoch 2906/10000\n",
      "2700/2700 [==============================] - 0s 30us/step - loss: 3.3445e-04 - mean_absolute_error: 0.0119\n",
      "\n",
      "Epoch 02906: loss did not improve\n",
      "Epoch 2907/10000\n",
      "2700/2700 [==============================] - 0s 29us/step - loss: 3.3353e-04 - mean_absolute_error: 0.0118\n",
      "\n",
      "Epoch 02907: loss did not improve\n",
      "Epoch 2908/10000\n",
      "2700/2700 [==============================] - 0s 28us/step - loss: 3.3527e-04 - mean_absolute_error: 0.0119\n",
      "\n",
      "Epoch 02908: loss did not improve\n",
      "Epoch 2909/10000\n",
      "2700/2700 [==============================] - 0s 29us/step - loss: 3.3873e-04 - mean_absolute_error: 0.0121\n",
      "\n",
      "Epoch 02909: loss did not improve\n",
      "Epoch 2910/10000\n",
      "2700/2700 [==============================] - 0s 29us/step - loss: 3.3650e-04 - mean_absolute_error: 0.0120\n",
      "\n",
      "Epoch 02910: loss did not improve\n",
      "Epoch 2911/10000\n",
      "2700/2700 [==============================] - 0s 28us/step - loss: 3.3283e-04 - mean_absolute_error: 0.0120\n",
      "\n",
      "Epoch 02911: loss did not improve\n",
      "Epoch 2912/10000\n",
      "2700/2700 [==============================] - 0s 34us/step - loss: 3.3318e-04 - mean_absolute_error: 0.0118\n",
      "\n",
      "Epoch 02912: loss did not improve\n",
      "Epoch 2913/10000\n",
      "2700/2700 [==============================] - 0s 40us/step - loss: 3.3410e-04 - mean_absolute_error: 0.0119\n",
      "\n",
      "Epoch 02913: loss did not improve\n",
      "Epoch 2914/10000\n",
      "2700/2700 [==============================] - 0s 33us/step - loss: 3.3728e-04 - mean_absolute_error: 0.0121\n",
      "\n",
      "Epoch 02914: loss did not improve\n",
      "Epoch 2915/10000\n",
      "2700/2700 [==============================] - 0s 33us/step - loss: 3.3347e-04 - mean_absolute_error: 0.0119\n",
      "\n",
      "Epoch 02915: loss did not improve\n",
      "Epoch 2916/10000\n",
      "2700/2700 [==============================] - 0s 30us/step - loss: 3.3466e-04 - mean_absolute_error: 0.0119\n",
      "\n",
      "Epoch 02916: loss did not improve\n",
      "Epoch 2917/10000\n",
      "2700/2700 [==============================] - 0s 36us/step - loss: 3.3342e-04 - mean_absolute_error: 0.0118\n",
      "\n",
      "Epoch 02917: loss did not improve\n",
      "Epoch 2918/10000\n",
      "2700/2700 [==============================] - 0s 34us/step - loss: 3.3575e-04 - mean_absolute_error: 0.0119\n",
      "\n",
      "Epoch 02918: loss did not improve\n",
      "Epoch 2919/10000\n",
      "2700/2700 [==============================] - 0s 51us/step - loss: 3.3785e-04 - mean_absolute_error: 0.0120\n",
      "\n",
      "Epoch 02919: loss did not improve\n",
      "Epoch 2920/10000\n",
      "2700/2700 [==============================] - 0s 31us/step - loss: 3.3454e-04 - mean_absolute_error: 0.0119\n",
      "\n",
      "Epoch 02920: loss did not improve\n",
      "Epoch 2921/10000\n",
      "2700/2700 [==============================] - 0s 28us/step - loss: 3.3172e-04 - mean_absolute_error: 0.0118\n",
      "\n",
      "Epoch 02921: loss improved from 0.00033 to 0.00033, saving model to keras/best_baseline_weights.hdf5\n",
      "Epoch 2922/10000\n",
      "2700/2700 [==============================] - 0s 32us/step - loss: 3.3813e-04 - mean_absolute_error: 0.0121\n",
      "\n",
      "Epoch 02922: loss did not improve\n",
      "Epoch 2923/10000\n",
      "2700/2700 [==============================] - 0s 35us/step - loss: 3.3460e-04 - mean_absolute_error: 0.0120\n",
      "\n",
      "Epoch 02923: loss did not improve\n",
      "Epoch 2924/10000\n",
      "2700/2700 [==============================] - 0s 36us/step - loss: 3.3528e-04 - mean_absolute_error: 0.0119\n",
      "\n",
      "Epoch 02924: loss did not improve\n",
      "Epoch 2925/10000\n",
      "2700/2700 [==============================] - 0s 32us/step - loss: 3.4119e-04 - mean_absolute_error: 0.0121\n",
      "\n",
      "Epoch 02925: loss did not improve\n",
      "Epoch 2926/10000\n",
      "2700/2700 [==============================] - 0s 34us/step - loss: 3.3709e-04 - mean_absolute_error: 0.0120\n",
      "\n",
      "Epoch 02926: loss did not improve\n",
      "Epoch 2927/10000\n",
      "2700/2700 [==============================] - 0s 29us/step - loss: 3.3503e-04 - mean_absolute_error: 0.0119\n",
      "\n",
      "Epoch 02927: loss did not improve\n",
      "Epoch 2928/10000\n",
      "2700/2700 [==============================] - 0s 28us/step - loss: 3.3288e-04 - mean_absolute_error: 0.0118\n",
      "\n",
      "Epoch 02928: loss did not improve\n",
      "Epoch 2929/10000\n",
      "2700/2700 [==============================] - 0s 27us/step - loss: 3.3104e-04 - mean_absolute_error: 0.0118\n",
      "\n",
      "Epoch 02929: loss improved from 0.00033 to 0.00033, saving model to keras/best_baseline_weights.hdf5\n",
      "Epoch 2930/10000\n",
      "2700/2700 [==============================] - 0s 28us/step - loss: 3.3773e-04 - mean_absolute_error: 0.0120\n",
      "\n",
      "Epoch 02930: loss did not improve\n",
      "Epoch 2931/10000\n",
      "2700/2700 [==============================] - 0s 29us/step - loss: 3.3243e-04 - mean_absolute_error: 0.0119\n",
      "\n",
      "Epoch 02931: loss did not improve\n",
      "Epoch 2932/10000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2700/2700 [==============================] - 0s 30us/step - loss: 3.3375e-04 - mean_absolute_error: 0.0119\n",
      "\n",
      "Epoch 02932: loss did not improve\n",
      "Epoch 2933/10000\n",
      "2700/2700 [==============================] - 0s 30us/step - loss: 3.3723e-04 - mean_absolute_error: 0.0120\n",
      "\n",
      "Epoch 02933: loss did not improve\n",
      "Epoch 2934/10000\n",
      "2700/2700 [==============================] - 0s 26us/step - loss: 3.3399e-04 - mean_absolute_error: 0.0119\n",
      "\n",
      "Epoch 02934: loss did not improve\n",
      "Epoch 2935/10000\n",
      "2700/2700 [==============================] - 0s 29us/step - loss: 3.3324e-04 - mean_absolute_error: 0.0119\n",
      "\n",
      "Epoch 02935: loss did not improve\n",
      "Epoch 2936/10000\n",
      "2700/2700 [==============================] - 0s 30us/step - loss: 3.3797e-04 - mean_absolute_error: 0.0121\n",
      "\n",
      "Epoch 02936: loss did not improve\n",
      "Epoch 2937/10000\n",
      "2700/2700 [==============================] - 0s 27us/step - loss: 3.3123e-04 - mean_absolute_error: 0.0119\n",
      "\n",
      "Epoch 02937: loss did not improve\n",
      "Epoch 2938/10000\n",
      "2700/2700 [==============================] - 0s 25us/step - loss: 3.4952e-04 - mean_absolute_error: 0.0124\n",
      "\n",
      "Epoch 02938: loss did not improve\n",
      "Epoch 2939/10000\n",
      "2700/2700 [==============================] - 0s 27us/step - loss: 3.3358e-04 - mean_absolute_error: 0.0120\n",
      "\n",
      "Epoch 02939: loss did not improve\n",
      "Epoch 2940/10000\n",
      "2700/2700 [==============================] - 0s 28us/step - loss: 3.3409e-04 - mean_absolute_error: 0.0120\n",
      "\n",
      "Epoch 02940: loss did not improve\n",
      "Epoch 2941/10000\n",
      "2700/2700 [==============================] - 0s 28us/step - loss: 3.3913e-04 - mean_absolute_error: 0.0120\n",
      "\n",
      "Epoch 02941: loss did not improve\n",
      "Epoch 2942/10000\n",
      "2700/2700 [==============================] - 0s 29us/step - loss: 3.3398e-04 - mean_absolute_error: 0.0120\n",
      "\n",
      "Epoch 02942: loss did not improve\n",
      "Epoch 2943/10000\n",
      "2700/2700 [==============================] - 0s 33us/step - loss: 3.3291e-04 - mean_absolute_error: 0.0119\n",
      "\n",
      "Epoch 02943: loss did not improve\n",
      "Epoch 2944/10000\n",
      "2700/2700 [==============================] - 0s 35us/step - loss: 3.3098e-04 - mean_absolute_error: 0.0119\n",
      "\n",
      "Epoch 02944: loss improved from 0.00033 to 0.00033, saving model to keras/best_baseline_weights.hdf5\n",
      "Epoch 2945/10000\n",
      "2700/2700 [==============================] - 0s 37us/step - loss: 3.3210e-04 - mean_absolute_error: 0.0118\n",
      "\n",
      "Epoch 02945: loss did not improve\n",
      "Epoch 2946/10000\n",
      "2700/2700 [==============================] - 0s 32us/step - loss: 3.3135e-04 - mean_absolute_error: 0.0119\n",
      "\n",
      "Epoch 02946: loss did not improve\n",
      "Epoch 2947/10000\n",
      "2700/2700 [==============================] - 0s 28us/step - loss: 3.3337e-04 - mean_absolute_error: 0.0119\n",
      "\n",
      "Epoch 02947: loss did not improve\n",
      "Epoch 2948/10000\n",
      "2700/2700 [==============================] - 0s 28us/step - loss: 3.3520e-04 - mean_absolute_error: 0.0120\n",
      "\n",
      "Epoch 02948: loss did not improve\n",
      "Epoch 2949/10000\n",
      "2700/2700 [==============================] - 0s 27us/step - loss: 3.3365e-04 - mean_absolute_error: 0.0119\n",
      "\n",
      "Epoch 02949: loss did not improve\n",
      "Epoch 2950/10000\n",
      "2700/2700 [==============================] - 0s 28us/step - loss: 3.2977e-04 - mean_absolute_error: 0.0118\n",
      "\n",
      "Epoch 02950: loss improved from 0.00033 to 0.00033, saving model to keras/best_baseline_weights.hdf5\n",
      "Epoch 2951/10000\n",
      "2700/2700 [==============================] - 0s 29us/step - loss: 3.2917e-04 - mean_absolute_error: 0.0118\n",
      "\n",
      "Epoch 02951: loss improved from 0.00033 to 0.00033, saving model to keras/best_baseline_weights.hdf5\n",
      "Epoch 2952/10000\n",
      "2700/2700 [==============================] - 0s 30us/step - loss: 3.3165e-04 - mean_absolute_error: 0.0119\n",
      "\n",
      "Epoch 02952: loss did not improve\n",
      "Epoch 2953/10000\n",
      "2700/2700 [==============================] - 0s 29us/step - loss: 3.3047e-04 - mean_absolute_error: 0.0118\n",
      "\n",
      "Epoch 02953: loss did not improve\n",
      "Epoch 2954/10000\n",
      "2700/2700 [==============================] - 0s 28us/step - loss: 3.3456e-04 - mean_absolute_error: 0.0120\n",
      "\n",
      "Epoch 02954: loss did not improve\n",
      "Epoch 2955/10000\n",
      "2700/2700 [==============================] - 0s 31us/step - loss: 3.3217e-04 - mean_absolute_error: 0.0118\n",
      "\n",
      "Epoch 02955: loss did not improve\n",
      "Epoch 2956/10000\n",
      "2700/2700 [==============================] - 0s 30us/step - loss: 3.3506e-04 - mean_absolute_error: 0.0120\n",
      "\n",
      "Epoch 02956: loss did not improve\n",
      "Epoch 2957/10000\n",
      "2700/2700 [==============================] - 0s 29us/step - loss: 3.2862e-04 - mean_absolute_error: 0.0118\n",
      "\n",
      "Epoch 02957: loss improved from 0.00033 to 0.00033, saving model to keras/best_baseline_weights.hdf5\n",
      "Epoch 2958/10000\n",
      "2700/2700 [==============================] - 0s 28us/step - loss: 3.4114e-04 - mean_absolute_error: 0.0121\n",
      "\n",
      "Epoch 02958: loss did not improve\n",
      "Epoch 2959/10000\n",
      "2700/2700 [==============================] - 0s 31us/step - loss: 3.3381e-04 - mean_absolute_error: 0.0120\n",
      "\n",
      "Epoch 02959: loss did not improve\n",
      "Epoch 2960/10000\n",
      "2700/2700 [==============================] - 0s 31us/step - loss: 3.2981e-04 - mean_absolute_error: 0.0118\n",
      "\n",
      "Epoch 02960: loss did not improve\n",
      "Epoch 2961/10000\n",
      "2700/2700 [==============================] - 0s 28us/step - loss: 3.3411e-04 - mean_absolute_error: 0.0120\n",
      "\n",
      "Epoch 02961: loss did not improve\n",
      "Epoch 2962/10000\n",
      "2700/2700 [==============================] - 0s 29us/step - loss: 3.3652e-04 - mean_absolute_error: 0.0120\n",
      "\n",
      "Epoch 02962: loss did not improve\n",
      "Epoch 2963/10000\n",
      "2700/2700 [==============================] - 0s 31us/step - loss: 3.3053e-04 - mean_absolute_error: 0.0119\n",
      "\n",
      "Epoch 02963: loss did not improve\n",
      "Epoch 2964/10000\n",
      "2700/2700 [==============================] - 0s 29us/step - loss: 3.3374e-04 - mean_absolute_error: 0.0119\n",
      "\n",
      "Epoch 02964: loss did not improve\n",
      "Epoch 2965/10000\n",
      "2700/2700 [==============================] - 0s 28us/step - loss: 3.3488e-04 - mean_absolute_error: 0.0121\n",
      "\n",
      "Epoch 02965: loss did not improve\n",
      "Epoch 2966/10000\n",
      "2700/2700 [==============================] - 0s 42us/step - loss: 3.3074e-04 - mean_absolute_error: 0.0118\n",
      "\n",
      "Epoch 02966: loss did not improve\n",
      "Epoch 2967/10000\n",
      "2700/2700 [==============================] - 0s 32us/step - loss: 3.3075e-04 - mean_absolute_error: 0.0119\n",
      "\n",
      "Epoch 02967: loss did not improve\n",
      "Epoch 2968/10000\n",
      "2700/2700 [==============================] - 0s 36us/step - loss: 3.3232e-04 - mean_absolute_error: 0.0119\n",
      "\n",
      "Epoch 02968: loss did not improve\n",
      "Epoch 2969/10000\n",
      "2700/2700 [==============================] - 0s 33us/step - loss: 3.2925e-04 - mean_absolute_error: 0.0118\n",
      "\n",
      "Epoch 02969: loss did not improve\n",
      "Epoch 2970/10000\n",
      "2700/2700 [==============================] - 0s 34us/step - loss: 3.2980e-04 - mean_absolute_error: 0.0118\n",
      "\n",
      "Epoch 02970: loss did not improve\n",
      "Epoch 2971/10000\n",
      "2700/2700 [==============================] - 0s 38us/step - loss: 3.4143e-04 - mean_absolute_error: 0.0123\n",
      "\n",
      "Epoch 02971: loss did not improve\n",
      "Epoch 2972/10000\n",
      "2700/2700 [==============================] - 0s 26us/step - loss: 3.3091e-04 - mean_absolute_error: 0.0119\n",
      "\n",
      "Epoch 02972: loss did not improve\n",
      "Epoch 2973/10000\n",
      "2700/2700 [==============================] - 0s 29us/step - loss: 3.3101e-04 - mean_absolute_error: 0.0119\n",
      "\n",
      "Epoch 02973: loss did not improve\n",
      "Epoch 2974/10000\n",
      "2700/2700 [==============================] - 0s 27us/step - loss: 3.3036e-04 - mean_absolute_error: 0.0118\n",
      "\n",
      "Epoch 02974: loss did not improve\n",
      "Epoch 2975/10000\n",
      "2700/2700 [==============================] - 0s 34us/step - loss: 3.3288e-04 - mean_absolute_error: 0.0119\n",
      "\n",
      "Epoch 02975: loss did not improve\n",
      "Epoch 2976/10000\n",
      "2700/2700 [==============================] - 0s 29us/step - loss: 3.3150e-04 - mean_absolute_error: 0.0120\n",
      "\n",
      "Epoch 02976: loss did not improve\n",
      "Epoch 2977/10000\n",
      "2700/2700 [==============================] - 0s 31us/step - loss: 3.2604e-04 - mean_absolute_error: 0.0117\n",
      "\n",
      "Epoch 02977: loss improved from 0.00033 to 0.00033, saving model to keras/best_baseline_weights.hdf5\n",
      "Epoch 2978/10000\n",
      "2700/2700 [==============================] - 0s 34us/step - loss: 3.2944e-04 - mean_absolute_error: 0.0118\n",
      "\n",
      "Epoch 02978: loss did not improve\n",
      "Epoch 2979/10000\n",
      "2700/2700 [==============================] - 0s 29us/step - loss: 3.3138e-04 - mean_absolute_error: 0.0119\n",
      "\n",
      "Epoch 02979: loss did not improve\n",
      "Epoch 2980/10000\n",
      "2700/2700 [==============================] - 0s 35us/step - loss: 3.2746e-04 - mean_absolute_error: 0.0118\n",
      "\n",
      "Epoch 02980: loss did not improve\n",
      "Epoch 2981/10000\n",
      "2700/2700 [==============================] - 0s 33us/step - loss: 3.3170e-04 - mean_absolute_error: 0.0119\n",
      "\n",
      "Epoch 02981: loss did not improve\n",
      "Epoch 2982/10000\n",
      "2700/2700 [==============================] - 0s 35us/step - loss: 3.3404e-04 - mean_absolute_error: 0.0119\n",
      "\n",
      "Epoch 02982: loss did not improve\n",
      "Epoch 2983/10000\n",
      "2700/2700 [==============================] - 0s 29us/step - loss: 3.2801e-04 - mean_absolute_error: 0.0118\n",
      "\n",
      "Epoch 02983: loss did not improve\n",
      "Epoch 2984/10000\n",
      "2700/2700 [==============================] - 0s 32us/step - loss: 3.2974e-04 - mean_absolute_error: 0.0118\n",
      "\n",
      "Epoch 02984: loss did not improve\n",
      "Epoch 2985/10000\n",
      "2700/2700 [==============================] - 0s 31us/step - loss: 3.2864e-04 - mean_absolute_error: 0.0118\n",
      "\n",
      "Epoch 02985: loss did not improve\n",
      "Epoch 2986/10000\n",
      "2700/2700 [==============================] - 0s 28us/step - loss: 3.3127e-04 - mean_absolute_error: 0.0119\n",
      "\n",
      "Epoch 02986: loss did not improve\n",
      "Epoch 2987/10000\n",
      "2700/2700 [==============================] - 0s 29us/step - loss: 3.2820e-04 - mean_absolute_error: 0.0118\n",
      "\n",
      "Epoch 02987: loss did not improve\n",
      "Epoch 2988/10000\n",
      "2700/2700 [==============================] - 0s 28us/step - loss: 3.2816e-04 - mean_absolute_error: 0.0118\n",
      "\n",
      "Epoch 02988: loss did not improve\n",
      "Epoch 2989/10000\n",
      "2700/2700 [==============================] - 0s 28us/step - loss: 3.3364e-04 - mean_absolute_error: 0.0120\n",
      "\n",
      "Epoch 02989: loss did not improve\n",
      "Epoch 2990/10000\n",
      "2700/2700 [==============================] - 0s 28us/step - loss: 3.3124e-04 - mean_absolute_error: 0.0119\n",
      "\n",
      "Epoch 02990: loss did not improve\n",
      "Epoch 2991/10000\n",
      "2700/2700 [==============================] - 0s 27us/step - loss: 3.2785e-04 - mean_absolute_error: 0.0118\n",
      "\n",
      "Epoch 02991: loss did not improve\n",
      "Epoch 2992/10000\n",
      "2700/2700 [==============================] - 0s 30us/step - loss: 3.2886e-04 - mean_absolute_error: 0.0118\n",
      "\n",
      "Epoch 02992: loss did not improve\n",
      "Epoch 2993/10000\n",
      "2700/2700 [==============================] - 0s 33us/step - loss: 3.3096e-04 - mean_absolute_error: 0.0119\n",
      "\n",
      "Epoch 02993: loss did not improve\n",
      "Epoch 2994/10000\n",
      "2700/2700 [==============================] - 0s 36us/step - loss: 3.2787e-04 - mean_absolute_error: 0.0118\n",
      "\n",
      "Epoch 02994: loss did not improve\n",
      "Epoch 2995/10000\n",
      "2700/2700 [==============================] - 0s 31us/step - loss: 3.2949e-04 - mean_absolute_error: 0.0119\n",
      "\n",
      "Epoch 02995: loss did not improve\n",
      "Epoch 2996/10000\n",
      "2700/2700 [==============================] - 0s 28us/step - loss: 3.2907e-04 - mean_absolute_error: 0.0118\n",
      "\n",
      "Epoch 02996: loss did not improve\n",
      "Epoch 2997/10000\n",
      "2700/2700 [==============================] - 0s 30us/step - loss: 3.2863e-04 - mean_absolute_error: 0.0119\n",
      "\n",
      "Epoch 02997: loss did not improve\n",
      "Epoch 2998/10000\n",
      "2700/2700 [==============================] - 0s 33us/step - loss: 3.2528e-04 - mean_absolute_error: 0.0117\n",
      "\n",
      "Epoch 02998: loss improved from 0.00033 to 0.00033, saving model to keras/best_baseline_weights.hdf5\n",
      "Epoch 2999/10000\n",
      "2700/2700 [==============================] - 0s 28us/step - loss: 3.3340e-04 - mean_absolute_error: 0.0121\n",
      "\n",
      "Epoch 02999: loss did not improve\n",
      "Epoch 3000/10000\n",
      "2700/2700 [==============================] - 0s 28us/step - loss: 3.2744e-04 - mean_absolute_error: 0.0117\n",
      "\n",
      "Epoch 03000: loss did not improve\n",
      "Epoch 3001/10000\n",
      "2700/2700 [==============================] - 0s 29us/step - loss: 3.2889e-04 - mean_absolute_error: 0.0118\n",
      "\n",
      "Epoch 03001: loss did not improve\n",
      "Epoch 3002/10000\n",
      "2700/2700 [==============================] - 0s 27us/step - loss: 3.3017e-04 - mean_absolute_error: 0.0119\n",
      "\n",
      "Epoch 03002: loss did not improve\n",
      "Epoch 3003/10000\n",
      "2700/2700 [==============================] - 0s 29us/step - loss: 3.2675e-04 - mean_absolute_error: 0.0118\n",
      "\n",
      "Epoch 03003: loss did not improve\n",
      "Epoch 3004/10000\n",
      "2700/2700 [==============================] - 0s 29us/step - loss: 3.2818e-04 - mean_absolute_error: 0.0119\n",
      "\n",
      "Epoch 03004: loss did not improve\n",
      "Epoch 3005/10000\n",
      "2700/2700 [==============================] - 0s 29us/step - loss: 3.2861e-04 - mean_absolute_error: 0.0118\n",
      "\n",
      "Epoch 03005: loss did not improve\n",
      "Epoch 3006/10000\n",
      "2700/2700 [==============================] - 0s 28us/step - loss: 3.2841e-04 - mean_absolute_error: 0.0118\n",
      "\n",
      "Epoch 03006: loss did not improve\n",
      "Epoch 3007/10000\n",
      "2700/2700 [==============================] - 0s 25us/step - loss: 3.2541e-04 - mean_absolute_error: 0.0117\n",
      "\n",
      "Epoch 03007: loss did not improve\n",
      "Epoch 3008/10000\n",
      "2700/2700 [==============================] - 0s 32us/step - loss: 3.2760e-04 - mean_absolute_error: 0.0119\n",
      "\n",
      "Epoch 03008: loss did not improve\n",
      "Epoch 3009/10000\n",
      "2700/2700 [==============================] - 0s 29us/step - loss: 3.3145e-04 - mean_absolute_error: 0.0120\n",
      "\n",
      "Epoch 03009: loss did not improve\n",
      "Epoch 3010/10000\n",
      "2700/2700 [==============================] - 0s 29us/step - loss: 3.3641e-04 - mean_absolute_error: 0.0121\n",
      "\n",
      "Epoch 03010: loss did not improve\n",
      "Epoch 3011/10000\n",
      "2700/2700 [==============================] - 0s 27us/step - loss: 3.3490e-04 - mean_absolute_error: 0.0119\n",
      "\n",
      "Epoch 03011: loss did not improve\n",
      "Epoch 3012/10000\n",
      "2700/2700 [==============================] - 0s 28us/step - loss: 3.2657e-04 - mean_absolute_error: 0.0117\n",
      "\n",
      "Epoch 03012: loss did not improve\n",
      "Epoch 3013/10000\n",
      "2700/2700 [==============================] - 0s 28us/step - loss: 3.2461e-04 - mean_absolute_error: 0.0116\n",
      "\n",
      "Epoch 03013: loss improved from 0.00033 to 0.00032, saving model to keras/best_baseline_weights.hdf5\n",
      "Epoch 3014/10000\n",
      "2700/2700 [==============================] - 0s 30us/step - loss: 3.2402e-04 - mean_absolute_error: 0.0117\n",
      "\n",
      "Epoch 03014: loss improved from 0.00032 to 0.00032, saving model to keras/best_baseline_weights.hdf5\n",
      "Epoch 3015/10000\n",
      "2700/2700 [==============================] - 0s 30us/step - loss: 3.2352e-04 - mean_absolute_error: 0.0117\n",
      "\n",
      "Epoch 03015: loss improved from 0.00032 to 0.00032, saving model to keras/best_baseline_weights.hdf5\n",
      "Epoch 3016/10000\n",
      "2700/2700 [==============================] - 0s 28us/step - loss: 3.2523e-04 - mean_absolute_error: 0.0117\n",
      "\n",
      "Epoch 03016: loss did not improve\n",
      "Epoch 3017/10000\n",
      "2700/2700 [==============================] - 0s 28us/step - loss: 3.2344e-04 - mean_absolute_error: 0.0116\n",
      "\n",
      "Epoch 03017: loss improved from 0.00032 to 0.00032, saving model to keras/best_baseline_weights.hdf5\n",
      "Epoch 3018/10000\n",
      "2700/2700 [==============================] - 0s 28us/step - loss: 3.2322e-04 - mean_absolute_error: 0.0117\n",
      "\n",
      "Epoch 03018: loss improved from 0.00032 to 0.00032, saving model to keras/best_baseline_weights.hdf5\n",
      "Epoch 3019/10000\n",
      "2700/2700 [==============================] - 0s 28us/step - loss: 3.2810e-04 - mean_absolute_error: 0.0118\n",
      "\n",
      "Epoch 03019: loss did not improve\n",
      "Epoch 3020/10000\n",
      "2700/2700 [==============================] - 0s 29us/step - loss: 3.2422e-04 - mean_absolute_error: 0.0117\n",
      "\n",
      "Epoch 03020: loss did not improve\n",
      "Epoch 3021/10000\n",
      "2700/2700 [==============================] - 0s 30us/step - loss: 3.2885e-04 - mean_absolute_error: 0.0120\n",
      "\n",
      "Epoch 03021: loss did not improve\n",
      "Epoch 3022/10000\n",
      "2700/2700 [==============================] - 0s 27us/step - loss: 3.2484e-04 - mean_absolute_error: 0.0117\n",
      "\n",
      "Epoch 03022: loss did not improve\n",
      "Epoch 3023/10000\n",
      "2700/2700 [==============================] - 0s 26us/step - loss: 3.2454e-04 - mean_absolute_error: 0.0118\n",
      "\n",
      "Epoch 03023: loss did not improve\n",
      "Epoch 3024/10000\n",
      "2700/2700 [==============================] - 0s 29us/step - loss: 3.2486e-04 - mean_absolute_error: 0.0118\n",
      "\n",
      "Epoch 03024: loss did not improve\n",
      "Epoch 3025/10000\n",
      "2700/2700 [==============================] - 0s 37us/step - loss: 3.2491e-04 - mean_absolute_error: 0.0117\n",
      "\n",
      "Epoch 03025: loss did not improve\n",
      "Epoch 3026/10000\n",
      "2700/2700 [==============================] - 0s 33us/step - loss: 3.2467e-04 - mean_absolute_error: 0.0117\n",
      "\n",
      "Epoch 03026: loss did not improve\n",
      "Epoch 3027/10000\n",
      "2700/2700 [==============================] - 0s 29us/step - loss: 3.3047e-04 - mean_absolute_error: 0.0118\n",
      "\n",
      "Epoch 03027: loss did not improve\n",
      "Epoch 3028/10000\n",
      "2700/2700 [==============================] - 0s 29us/step - loss: 3.2841e-04 - mean_absolute_error: 0.0118\n",
      "\n",
      "Epoch 03028: loss did not improve\n",
      "Epoch 3029/10000\n",
      "2700/2700 [==============================] - 0s 29us/step - loss: 3.2787e-04 - mean_absolute_error: 0.0119\n",
      "\n",
      "Epoch 03029: loss did not improve\n",
      "Epoch 3030/10000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2700/2700 [==============================] - 0s 30us/step - loss: 3.2699e-04 - mean_absolute_error: 0.0118\n",
      "\n",
      "Epoch 03030: loss did not improve\n",
      "Epoch 3031/10000\n",
      "2700/2700 [==============================] - 0s 28us/step - loss: 3.2132e-04 - mean_absolute_error: 0.0116\n",
      "\n",
      "Epoch 03031: loss improved from 0.00032 to 0.00032, saving model to keras/best_baseline_weights.hdf5\n",
      "Epoch 3032/10000\n",
      "2700/2700 [==============================] - 0s 29us/step - loss: 3.2297e-04 - mean_absolute_error: 0.0117\n",
      "\n",
      "Epoch 03032: loss did not improve\n",
      "Epoch 3033/10000\n",
      "2700/2700 [==============================] - 0s 29us/step - loss: 3.2285e-04 - mean_absolute_error: 0.0117\n",
      "\n",
      "Epoch 03033: loss did not improve\n",
      "Epoch 3034/10000\n",
      "2700/2700 [==============================] - 0s 26us/step - loss: 3.2390e-04 - mean_absolute_error: 0.0117\n",
      "\n",
      "Epoch 03034: loss did not improve\n",
      "Epoch 3035/10000\n",
      "2700/2700 [==============================] - 0s 28us/step - loss: 3.2221e-04 - mean_absolute_error: 0.0116\n",
      "\n",
      "Epoch 03035: loss did not improve\n",
      "Epoch 3036/10000\n",
      "2700/2700 [==============================] - 0s 29us/step - loss: 3.2084e-04 - mean_absolute_error: 0.0116\n",
      "\n",
      "Epoch 03036: loss improved from 0.00032 to 0.00032, saving model to keras/best_baseline_weights.hdf5\n",
      "Epoch 3037/10000\n",
      "2700/2700 [==============================] - 0s 28us/step - loss: 3.2493e-04 - mean_absolute_error: 0.0117\n",
      "\n",
      "Epoch 03037: loss did not improve\n",
      "Epoch 3038/10000\n",
      "2700/2700 [==============================] - 0s 27us/step - loss: 3.2243e-04 - mean_absolute_error: 0.0116\n",
      "\n",
      "Epoch 03038: loss did not improve\n",
      "Epoch 3039/10000\n",
      "2700/2700 [==============================] - 0s 26us/step - loss: 3.2494e-04 - mean_absolute_error: 0.0118\n",
      "\n",
      "Epoch 03039: loss did not improve\n",
      "Epoch 3040/10000\n",
      "2700/2700 [==============================] - 0s 29us/step - loss: 3.1936e-04 - mean_absolute_error: 0.0115\n",
      "\n",
      "Epoch 03040: loss improved from 0.00032 to 0.00032, saving model to keras/best_baseline_weights.hdf5\n",
      "Epoch 3041/10000\n",
      "2700/2700 [==============================] - 0s 31us/step - loss: 3.3161e-04 - mean_absolute_error: 0.0119\n",
      "\n",
      "Epoch 03041: loss did not improve\n",
      "Epoch 3042/10000\n",
      "2700/2700 [==============================] - 0s 28us/step - loss: 3.2642e-04 - mean_absolute_error: 0.0117\n",
      "\n",
      "Epoch 03042: loss did not improve\n",
      "Epoch 3043/10000\n",
      "2700/2700 [==============================] - 0s 30us/step - loss: 3.2180e-04 - mean_absolute_error: 0.0116\n",
      "\n",
      "Epoch 03043: loss did not improve\n",
      "Epoch 3044/10000\n",
      "2700/2700 [==============================] - 0s 28us/step - loss: 3.2523e-04 - mean_absolute_error: 0.0118\n",
      "\n",
      "Epoch 03044: loss did not improve\n",
      "Epoch 3045/10000\n",
      "2700/2700 [==============================] - 0s 29us/step - loss: 3.2409e-04 - mean_absolute_error: 0.0117\n",
      "\n",
      "Epoch 03045: loss did not improve\n",
      "Epoch 3046/10000\n",
      "2700/2700 [==============================] - 0s 29us/step - loss: 3.2327e-04 - mean_absolute_error: 0.0116\n",
      "\n",
      "Epoch 03046: loss did not improve\n",
      "Epoch 3047/10000\n",
      "2700/2700 [==============================] - 0s 27us/step - loss: 3.2281e-04 - mean_absolute_error: 0.0116\n",
      "\n",
      "Epoch 03047: loss did not improve\n",
      "Epoch 3048/10000\n",
      "2700/2700 [==============================] - 0s 28us/step - loss: 3.2650e-04 - mean_absolute_error: 0.0118\n",
      "\n",
      "Epoch 03048: loss did not improve\n",
      "Epoch 3049/10000\n",
      "2700/2700 [==============================] - 0s 27us/step - loss: 3.2150e-04 - mean_absolute_error: 0.0116\n",
      "\n",
      "Epoch 03049: loss did not improve\n",
      "Epoch 3050/10000\n",
      "2700/2700 [==============================] - 0s 28us/step - loss: 3.2219e-04 - mean_absolute_error: 0.0117\n",
      "\n",
      "Epoch 03050: loss did not improve\n",
      "Epoch 3051/10000\n",
      "2700/2700 [==============================] - 0s 28us/step - loss: 3.2292e-04 - mean_absolute_error: 0.0117\n",
      "\n",
      "Epoch 03051: loss did not improve\n",
      "Epoch 3052/10000\n",
      "2700/2700 [==============================] - 0s 25us/step - loss: 3.1939e-04 - mean_absolute_error: 0.0115\n",
      "\n",
      "Epoch 03052: loss did not improve\n",
      "Epoch 3053/10000\n",
      "2700/2700 [==============================] - 0s 29us/step - loss: 3.1883e-04 - mean_absolute_error: 0.0116\n",
      "\n",
      "Epoch 03053: loss improved from 0.00032 to 0.00032, saving model to keras/best_baseline_weights.hdf5\n",
      "Epoch 3054/10000\n",
      "2700/2700 [==============================] - 0s 27us/step - loss: 3.2086e-04 - mean_absolute_error: 0.0116\n",
      "\n",
      "Epoch 03054: loss did not improve\n",
      "Epoch 3055/10000\n",
      "2700/2700 [==============================] - 0s 29us/step - loss: 3.2881e-04 - mean_absolute_error: 0.0119\n",
      "\n",
      "Epoch 03055: loss did not improve\n",
      "Epoch 3056/10000\n",
      "2700/2700 [==============================] - 0s 35us/step - loss: 3.2410e-04 - mean_absolute_error: 0.0116\n",
      "\n",
      "Epoch 03056: loss did not improve\n",
      "Epoch 3057/10000\n",
      "2700/2700 [==============================] - 0s 27us/step - loss: 3.2096e-04 - mean_absolute_error: 0.0116\n",
      "\n",
      "Epoch 03057: loss did not improve\n",
      "Epoch 3058/10000\n",
      "2700/2700 [==============================] - 0s 29us/step - loss: 3.2111e-04 - mean_absolute_error: 0.0116\n",
      "\n",
      "Epoch 03058: loss did not improve\n",
      "Epoch 3059/10000\n",
      "2700/2700 [==============================] - 0s 29us/step - loss: 3.2415e-04 - mean_absolute_error: 0.0117\n",
      "\n",
      "Epoch 03059: loss did not improve\n",
      "Epoch 3060/10000\n",
      "2700/2700 [==============================] - 0s 29us/step - loss: 3.2554e-04 - mean_absolute_error: 0.0118\n",
      "\n",
      "Epoch 03060: loss did not improve\n",
      "Epoch 3061/10000\n",
      "2700/2700 [==============================] - 0s 28us/step - loss: 3.2253e-04 - mean_absolute_error: 0.0116\n",
      "\n",
      "Epoch 03061: loss did not improve\n",
      "Epoch 3062/10000\n",
      "2700/2700 [==============================] - 0s 28us/step - loss: 3.2575e-04 - mean_absolute_error: 0.0118\n",
      "\n",
      "Epoch 03062: loss did not improve\n",
      "Epoch 3063/10000\n",
      "2700/2700 [==============================] - 0s 28us/step - loss: 3.1870e-04 - mean_absolute_error: 0.0116\n",
      "\n",
      "Epoch 03063: loss improved from 0.00032 to 0.00032, saving model to keras/best_baseline_weights.hdf5\n",
      "Epoch 3064/10000\n",
      "2700/2700 [==============================] - 0s 29us/step - loss: 3.2179e-04 - mean_absolute_error: 0.0117\n",
      "\n",
      "Epoch 03064: loss did not improve\n",
      "Epoch 3065/10000\n",
      "2700/2700 [==============================] - 0s 28us/step - loss: 3.2293e-04 - mean_absolute_error: 0.0117\n",
      "\n",
      "Epoch 03065: loss did not improve\n",
      "Epoch 3066/10000\n",
      "2700/2700 [==============================] - 0s 32us/step - loss: 3.2249e-04 - mean_absolute_error: 0.0117\n",
      "\n",
      "Epoch 03066: loss did not improve\n",
      "Epoch 3067/10000\n",
      "2700/2700 [==============================] - 0s 28us/step - loss: 3.2268e-04 - mean_absolute_error: 0.0118\n",
      "\n",
      "Epoch 03067: loss did not improve\n",
      "Epoch 3068/10000\n",
      "2700/2700 [==============================] - 0s 32us/step - loss: 3.1943e-04 - mean_absolute_error: 0.0116\n",
      "\n",
      "Epoch 03068: loss did not improve\n",
      "Epoch 3069/10000\n",
      "2700/2700 [==============================] - 0s 33us/step - loss: 3.2307e-04 - mean_absolute_error: 0.0118\n",
      "\n",
      "Epoch 03069: loss did not improve\n",
      "Epoch 3070/10000\n",
      "2700/2700 [==============================] - 0s 30us/step - loss: 3.1819e-04 - mean_absolute_error: 0.0116\n",
      "\n",
      "Epoch 03070: loss improved from 0.00032 to 0.00032, saving model to keras/best_baseline_weights.hdf5\n",
      "Epoch 3071/10000\n",
      "2700/2700 [==============================] - 0s 30us/step - loss: 3.2203e-04 - mean_absolute_error: 0.0117\n",
      "\n",
      "Epoch 03071: loss did not improve\n",
      "Epoch 3072/10000\n",
      "2700/2700 [==============================] - 0s 32us/step - loss: 3.1774e-04 - mean_absolute_error: 0.0115\n",
      "\n",
      "Epoch 03072: loss improved from 0.00032 to 0.00032, saving model to keras/best_baseline_weights.hdf5\n",
      "Epoch 3073/10000\n",
      "2700/2700 [==============================] - 0s 30us/step - loss: 3.1987e-04 - mean_absolute_error: 0.0117\n",
      "\n",
      "Epoch 03073: loss did not improve\n",
      "Epoch 3074/10000\n",
      "2700/2700 [==============================] - 0s 26us/step - loss: 3.2029e-04 - mean_absolute_error: 0.0116\n",
      "\n",
      "Epoch 03074: loss did not improve\n",
      "Epoch 3075/10000\n",
      "2700/2700 [==============================] - 0s 28us/step - loss: 3.2294e-04 - mean_absolute_error: 0.0118\n",
      "\n",
      "Epoch 03075: loss did not improve\n",
      "Epoch 3076/10000\n",
      "2700/2700 [==============================] - 0s 29us/step - loss: 3.1865e-04 - mean_absolute_error: 0.0116\n",
      "\n",
      "Epoch 03076: loss did not improve\n",
      "Epoch 3077/10000\n",
      "2700/2700 [==============================] - 0s 30us/step - loss: 3.2590e-04 - mean_absolute_error: 0.0118\n",
      "\n",
      "Epoch 03077: loss did not improve\n",
      "Epoch 3078/10000\n",
      "2700/2700 [==============================] - 0s 26us/step - loss: 3.2191e-04 - mean_absolute_error: 0.0116\n",
      "\n",
      "Epoch 03078: loss did not improve\n",
      "Epoch 3079/10000\n",
      "2700/2700 [==============================] - 0s 31us/step - loss: 3.1710e-04 - mean_absolute_error: 0.0115\n",
      "\n",
      "Epoch 03079: loss improved from 0.00032 to 0.00032, saving model to keras/best_baseline_weights.hdf5\n",
      "Epoch 3080/10000\n",
      "2700/2700 [==============================] - 0s 28us/step - loss: 3.2275e-04 - mean_absolute_error: 0.0117\n",
      "\n",
      "Epoch 03080: loss did not improve\n",
      "Epoch 3081/10000\n",
      "2700/2700 [==============================] - 0s 27us/step - loss: 3.1740e-04 - mean_absolute_error: 0.0116\n",
      "\n",
      "Epoch 03081: loss did not improve\n",
      "Epoch 3082/10000\n",
      "2700/2700 [==============================] - 0s 30us/step - loss: 3.1869e-04 - mean_absolute_error: 0.0116\n",
      "\n",
      "Epoch 03082: loss did not improve\n",
      "Epoch 3083/10000\n",
      "2700/2700 [==============================] - 0s 28us/step - loss: 3.1709e-04 - mean_absolute_error: 0.0116\n",
      "\n",
      "Epoch 03083: loss improved from 0.00032 to 0.00032, saving model to keras/best_baseline_weights.hdf5\n",
      "Epoch 3084/10000\n",
      "2700/2700 [==============================] - 0s 26us/step - loss: 3.2212e-04 - mean_absolute_error: 0.0117\n",
      "\n",
      "Epoch 03084: loss did not improve\n",
      "Epoch 3085/10000\n",
      "2700/2700 [==============================] - 0s 29us/step - loss: 3.1709e-04 - mean_absolute_error: 0.0116\n",
      "\n",
      "Epoch 03085: loss did not improve\n",
      "Epoch 3086/10000\n",
      "2700/2700 [==============================] - 0s 29us/step - loss: 3.2063e-04 - mean_absolute_error: 0.0117\n",
      "\n",
      "Epoch 03086: loss did not improve\n",
      "Epoch 3087/10000\n",
      "2700/2700 [==============================] - 0s 30us/step - loss: 3.1647e-04 - mean_absolute_error: 0.0115\n",
      "\n",
      "Epoch 03087: loss improved from 0.00032 to 0.00032, saving model to keras/best_baseline_weights.hdf5\n",
      "Epoch 3088/10000\n",
      "2700/2700 [==============================] - 0s 29us/step - loss: 3.1572e-04 - mean_absolute_error: 0.0115\n",
      "\n",
      "Epoch 03088: loss improved from 0.00032 to 0.00032, saving model to keras/best_baseline_weights.hdf5\n",
      "Epoch 3089/10000\n",
      "2700/2700 [==============================] - 0s 30us/step - loss: 3.1682e-04 - mean_absolute_error: 0.0116\n",
      "\n",
      "Epoch 03089: loss did not improve\n",
      "Epoch 3090/10000\n",
      "2700/2700 [==============================] - 0s 26us/step - loss: 3.1730e-04 - mean_absolute_error: 0.0115\n",
      "\n",
      "Epoch 03090: loss did not improve\n",
      "Epoch 3091/10000\n",
      "2700/2700 [==============================] - 0s 27us/step - loss: 3.2022e-04 - mean_absolute_error: 0.0117\n",
      "\n",
      "Epoch 03091: loss did not improve\n",
      "Epoch 3092/10000\n",
      "2700/2700 [==============================] - 0s 31us/step - loss: 3.1916e-04 - mean_absolute_error: 0.0118\n",
      "\n",
      "Epoch 03092: loss did not improve\n",
      "Epoch 3093/10000\n",
      "2700/2700 [==============================] - 0s 29us/step - loss: 3.2233e-04 - mean_absolute_error: 0.0117\n",
      "\n",
      "Epoch 03093: loss did not improve\n",
      "Epoch 3094/10000\n",
      "2700/2700 [==============================] - 0s 26us/step - loss: 3.1927e-04 - mean_absolute_error: 0.0116\n",
      "\n",
      "Epoch 03094: loss did not improve\n",
      "Epoch 3095/10000\n",
      "2700/2700 [==============================] - 0s 29us/step - loss: 3.1618e-04 - mean_absolute_error: 0.0115\n",
      "\n",
      "Epoch 03095: loss did not improve\n",
      "Epoch 3096/10000\n",
      "2700/2700 [==============================] - 0s 29us/step - loss: 3.1471e-04 - mean_absolute_error: 0.0115\n",
      "\n",
      "Epoch 03096: loss improved from 0.00032 to 0.00031, saving model to keras/best_baseline_weights.hdf5\n",
      "Epoch 3097/10000\n",
      "2700/2700 [==============================] - 0s 28us/step - loss: 3.2485e-04 - mean_absolute_error: 0.0118\n",
      "\n",
      "Epoch 03097: loss did not improve\n",
      "Epoch 3098/10000\n",
      "2700/2700 [==============================] - 0s 29us/step - loss: 3.2039e-04 - mean_absolute_error: 0.0117\n",
      "\n",
      "Epoch 03098: loss did not improve\n",
      "Epoch 3099/10000\n",
      "2700/2700 [==============================] - 0s 29us/step - loss: 3.1847e-04 - mean_absolute_error: 0.0116\n",
      "\n",
      "Epoch 03099: loss did not improve\n",
      "Epoch 3100/10000\n",
      "2700/2700 [==============================] - 0s 28us/step - loss: 3.1520e-04 - mean_absolute_error: 0.0115\n",
      "\n",
      "Epoch 03100: loss did not improve\n",
      "Epoch 3101/10000\n",
      "2700/2700 [==============================] - 0s 30us/step - loss: 3.1733e-04 - mean_absolute_error: 0.0116\n",
      "\n",
      "Epoch 03101: loss did not improve\n",
      "Epoch 3102/10000\n",
      "2700/2700 [==============================] - 0s 29us/step - loss: 3.1614e-04 - mean_absolute_error: 0.0116\n",
      "\n",
      "Epoch 03102: loss did not improve\n",
      "Epoch 3103/10000\n",
      "2700/2700 [==============================] - 0s 29us/step - loss: 3.1881e-04 - mean_absolute_error: 0.0116\n",
      "\n",
      "Epoch 03103: loss did not improve\n",
      "Epoch 3104/10000\n",
      "2700/2700 [==============================] - 0s 28us/step - loss: 3.2051e-04 - mean_absolute_error: 0.0116\n",
      "\n",
      "Epoch 03104: loss did not improve\n",
      "Epoch 3105/10000\n",
      "2700/2700 [==============================] - 0s 31us/step - loss: 3.1637e-04 - mean_absolute_error: 0.0116\n",
      "\n",
      "Epoch 03105: loss did not improve\n",
      "Epoch 3106/10000\n",
      "2700/2700 [==============================] - 0s 31us/step - loss: 3.2638e-04 - mean_absolute_error: 0.0119\n",
      "\n",
      "Epoch 03106: loss did not improve\n",
      "Epoch 3107/10000\n",
      "2700/2700 [==============================] - ETA: 0s - loss: 2.9504e-04 - mean_absolute_error: 0.011 - 0s 31us/step - loss: 3.2658e-04 - mean_absolute_error: 0.0119\n",
      "\n",
      "Epoch 03107: loss did not improve\n",
      "Epoch 3108/10000\n",
      "2700/2700 [==============================] - 0s 30us/step - loss: 3.1898e-04 - mean_absolute_error: 0.0116\n",
      "\n",
      "Epoch 03108: loss did not improve\n",
      "Epoch 3109/10000\n",
      "2700/2700 [==============================] - 0s 31us/step - loss: 3.1731e-04 - mean_absolute_error: 0.0116\n",
      "\n",
      "Epoch 03109: loss did not improve\n",
      "Epoch 3110/10000\n",
      "2700/2700 [==============================] - 0s 29us/step - loss: 3.2125e-04 - mean_absolute_error: 0.0118\n",
      "\n",
      "Epoch 03110: loss did not improve\n",
      "Epoch 3111/10000\n",
      "2700/2700 [==============================] - 0s 30us/step - loss: 3.1861e-04 - mean_absolute_error: 0.0116\n",
      "\n",
      "Epoch 03111: loss did not improve\n",
      "Epoch 3112/10000\n",
      "2700/2700 [==============================] - 0s 30us/step - loss: 3.1603e-04 - mean_absolute_error: 0.0115\n",
      "\n",
      "Epoch 03112: loss did not improve\n",
      "Epoch 3113/10000\n",
      "2700/2700 [==============================] - 0s 28us/step - loss: 3.1470e-04 - mean_absolute_error: 0.0115\n",
      "\n",
      "Epoch 03113: loss improved from 0.00031 to 0.00031, saving model to keras/best_baseline_weights.hdf5\n",
      "Epoch 3114/10000\n",
      "2700/2700 [==============================] - 0s 30us/step - loss: 3.1465e-04 - mean_absolute_error: 0.0115\n",
      "\n",
      "Epoch 03114: loss improved from 0.00031 to 0.00031, saving model to keras/best_baseline_weights.hdf5\n",
      "Epoch 3115/10000\n",
      "2700/2700 [==============================] - 0s 30us/step - loss: 3.1407e-04 - mean_absolute_error: 0.0114\n",
      "\n",
      "Epoch 03115: loss improved from 0.00031 to 0.00031, saving model to keras/best_baseline_weights.hdf5\n",
      "Epoch 3116/10000\n",
      "2700/2700 [==============================] - 0s 31us/step - loss: 3.1127e-04 - mean_absolute_error: 0.0114\n",
      "\n",
      "Epoch 03116: loss improved from 0.00031 to 0.00031, saving model to keras/best_baseline_weights.hdf5\n",
      "Epoch 3117/10000\n",
      "2700/2700 [==============================] - 0s 33us/step - loss: 3.1186e-04 - mean_absolute_error: 0.0114\n",
      "\n",
      "Epoch 03117: loss did not improve\n",
      "Epoch 3118/10000\n",
      "2700/2700 [==============================] - 0s 30us/step - loss: 3.1201e-04 - mean_absolute_error: 0.0114\n",
      "\n",
      "Epoch 03118: loss did not improve\n",
      "Epoch 3119/10000\n",
      "2700/2700 [==============================] - 0s 28us/step - loss: 3.2169e-04 - mean_absolute_error: 0.0117\n",
      "\n",
      "Epoch 03119: loss did not improve\n",
      "Epoch 3120/10000\n",
      "2700/2700 [==============================] - 0s 29us/step - loss: 3.1829e-04 - mean_absolute_error: 0.0117\n",
      "\n",
      "Epoch 03120: loss did not improve\n",
      "Epoch 3121/10000\n",
      "2700/2700 [==============================] - 0s 28us/step - loss: 3.1510e-04 - mean_absolute_error: 0.0115\n",
      "\n",
      "Epoch 03121: loss did not improve\n",
      "Epoch 3122/10000\n",
      "2700/2700 [==============================] - 0s 27us/step - loss: 3.1609e-04 - mean_absolute_error: 0.0116\n",
      "\n",
      "Epoch 03122: loss did not improve\n",
      "Epoch 3123/10000\n",
      "2700/2700 [==============================] - 0s 30us/step - loss: 3.1120e-04 - mean_absolute_error: 0.0114\n",
      "\n",
      "Epoch 03123: loss improved from 0.00031 to 0.00031, saving model to keras/best_baseline_weights.hdf5\n",
      "Epoch 3124/10000\n",
      "2700/2700 [==============================] - 0s 29us/step - loss: 3.1989e-04 - mean_absolute_error: 0.0117\n",
      "\n",
      "Epoch 03124: loss did not improve\n",
      "Epoch 3125/10000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2700/2700 [==============================] - 0s 28us/step - loss: 3.1758e-04 - mean_absolute_error: 0.0116\n",
      "\n",
      "Epoch 03125: loss did not improve\n",
      "Epoch 3126/10000\n",
      "2700/2700 [==============================] - 0s 29us/step - loss: 3.1349e-04 - mean_absolute_error: 0.0115\n",
      "\n",
      "Epoch 03126: loss did not improve\n",
      "Epoch 3127/10000\n",
      "2700/2700 [==============================] - 0s 26us/step - loss: 3.1477e-04 - mean_absolute_error: 0.0115\n",
      "\n",
      "Epoch 03127: loss did not improve\n",
      "Epoch 3128/10000\n",
      "2700/2700 [==============================] - 0s 29us/step - loss: 3.1220e-04 - mean_absolute_error: 0.0115\n",
      "\n",
      "Epoch 03128: loss did not improve\n",
      "Epoch 3129/10000\n",
      "2700/2700 [==============================] - 0s 30us/step - loss: 3.1674e-04 - mean_absolute_error: 0.0116\n",
      "\n",
      "Epoch 03129: loss did not improve\n",
      "Epoch 3130/10000\n",
      "2700/2700 [==============================] - 0s 28us/step - loss: 3.1736e-04 - mean_absolute_error: 0.0116\n",
      "\n",
      "Epoch 03130: loss did not improve\n",
      "Epoch 3131/10000\n",
      "2700/2700 [==============================] - 0s 26us/step - loss: 3.1655e-04 - mean_absolute_error: 0.0116\n",
      "\n",
      "Epoch 03131: loss did not improve\n",
      "Epoch 3132/10000\n",
      "2700/2700 [==============================] - 0s 27us/step - loss: 3.1480e-04 - mean_absolute_error: 0.0114\n",
      "\n",
      "Epoch 03132: loss did not improve\n",
      "Epoch 3133/10000\n",
      "2700/2700 [==============================] - 0s 32us/step - loss: 3.1830e-04 - mean_absolute_error: 0.0116\n",
      "\n",
      "Epoch 03133: loss did not improve\n",
      "Epoch 3134/10000\n",
      "2700/2700 [==============================] - 0s 28us/step - loss: 3.1330e-04 - mean_absolute_error: 0.0116\n",
      "\n",
      "Epoch 03134: loss did not improve\n",
      "Epoch 3135/10000\n",
      "2700/2700 [==============================] - 0s 28us/step - loss: 3.1231e-04 - mean_absolute_error: 0.0114\n",
      "\n",
      "Epoch 03135: loss did not improve\n",
      "Epoch 3136/10000\n",
      "2700/2700 [==============================] - 0s 28us/step - loss: 3.1359e-04 - mean_absolute_error: 0.0115\n",
      "\n",
      "Epoch 03136: loss did not improve\n",
      "Epoch 3137/10000\n",
      "2700/2700 [==============================] - 0s 31us/step - loss: 3.1221e-04 - mean_absolute_error: 0.0114\n",
      "\n",
      "Epoch 03137: loss did not improve\n",
      "Epoch 3138/10000\n",
      "2700/2700 [==============================] - 0s 29us/step - loss: 3.1060e-04 - mean_absolute_error: 0.0114\n",
      "\n",
      "Epoch 03138: loss improved from 0.00031 to 0.00031, saving model to keras/best_baseline_weights.hdf5\n",
      "Epoch 3139/10000\n",
      "2700/2700 [==============================] - 0s 30us/step - loss: 3.1106e-04 - mean_absolute_error: 0.0114\n",
      "\n",
      "Epoch 03139: loss did not improve\n",
      "Epoch 3140/10000\n",
      "2700/2700 [==============================] - 0s 28us/step - loss: 3.1601e-04 - mean_absolute_error: 0.0115\n",
      "\n",
      "Epoch 03140: loss did not improve\n",
      "Epoch 3141/10000\n",
      "2700/2700 [==============================] - 0s 26us/step - loss: 3.1011e-04 - mean_absolute_error: 0.0114\n",
      "\n",
      "Epoch 03141: loss improved from 0.00031 to 0.00031, saving model to keras/best_baseline_weights.hdf5\n",
      "Epoch 3142/10000\n",
      "2700/2700 [==============================] - 0s 28us/step - loss: 3.1423e-04 - mean_absolute_error: 0.0115\n",
      "\n",
      "Epoch 03142: loss did not improve\n",
      "Epoch 3143/10000\n",
      "2700/2700 [==============================] - 0s 31us/step - loss: 3.2113e-04 - mean_absolute_error: 0.0118\n",
      "\n",
      "Epoch 03143: loss did not improve\n",
      "Epoch 3144/10000\n",
      "2700/2700 [==============================] - 0s 33us/step - loss: 3.1093e-04 - mean_absolute_error: 0.0113\n",
      "\n",
      "Epoch 03144: loss did not improve\n",
      "Epoch 3145/10000\n",
      "2700/2700 [==============================] - 0s 31us/step - loss: 3.1177e-04 - mean_absolute_error: 0.0114\n",
      "\n",
      "Epoch 03145: loss did not improve\n",
      "Epoch 3146/10000\n",
      "2700/2700 [==============================] - 0s 30us/step - loss: 3.0831e-04 - mean_absolute_error: 0.0114\n",
      "\n",
      "Epoch 03146: loss improved from 0.00031 to 0.00031, saving model to keras/best_baseline_weights.hdf5\n",
      "Epoch 3147/10000\n",
      "2700/2700 [==============================] - 0s 31us/step - loss: 3.1128e-04 - mean_absolute_error: 0.0115\n",
      "\n",
      "Epoch 03147: loss did not improve\n",
      "Epoch 3148/10000\n",
      "2700/2700 [==============================] - 0s 28us/step - loss: 3.1242e-04 - mean_absolute_error: 0.0114\n",
      "\n",
      "Epoch 03148: loss did not improve\n",
      "Epoch 3149/10000\n",
      "2700/2700 [==============================] - 0s 30us/step - loss: 3.1020e-04 - mean_absolute_error: 0.0114\n",
      "\n",
      "Epoch 03149: loss did not improve\n",
      "Epoch 3150/10000\n",
      "2700/2700 [==============================] - 0s 32us/step - loss: 3.1157e-04 - mean_absolute_error: 0.0114\n",
      "\n",
      "Epoch 03150: loss did not improve\n",
      "Epoch 3151/10000\n",
      "2700/2700 [==============================] - 0s 30us/step - loss: 3.1243e-04 - mean_absolute_error: 0.0115\n",
      "\n",
      "Epoch 03151: loss did not improve\n",
      "Epoch 3152/10000\n",
      "2700/2700 [==============================] - 0s 28us/step - loss: 3.1501e-04 - mean_absolute_error: 0.0117\n",
      "\n",
      "Epoch 03152: loss did not improve\n",
      "Epoch 3153/10000\n",
      "2700/2700 [==============================] - 0s 27us/step - loss: 3.1164e-04 - mean_absolute_error: 0.0114\n",
      "\n",
      "Epoch 03153: loss did not improve\n",
      "Epoch 3154/10000\n",
      "2700/2700 [==============================] - 0s 28us/step - loss: 3.1829e-04 - mean_absolute_error: 0.0116\n",
      "\n",
      "Epoch 03154: loss did not improve\n",
      "Epoch 3155/10000\n",
      "2700/2700 [==============================] - 0s 28us/step - loss: 3.1171e-04 - mean_absolute_error: 0.0113\n",
      "\n",
      "Epoch 03155: loss did not improve\n",
      "Epoch 3156/10000\n",
      "2700/2700 [==============================] - 0s 29us/step - loss: 3.0987e-04 - mean_absolute_error: 0.0114\n",
      "\n",
      "Epoch 03156: loss did not improve\n",
      "Epoch 3157/10000\n",
      "2700/2700 [==============================] - 0s 31us/step - loss: 3.0912e-04 - mean_absolute_error: 0.0114\n",
      "\n",
      "Epoch 03157: loss did not improve\n",
      "Epoch 3158/10000\n",
      "2700/2700 [==============================] - 0s 28us/step - loss: 3.1083e-04 - mean_absolute_error: 0.0114\n",
      "\n",
      "Epoch 03158: loss did not improve\n",
      "Epoch 3159/10000\n",
      "2700/2700 [==============================] - 0s 28us/step - loss: 3.1146e-04 - mean_absolute_error: 0.0114\n",
      "\n",
      "Epoch 03159: loss did not improve\n",
      "Epoch 3160/10000\n",
      "2700/2700 [==============================] - 0s 29us/step - loss: 3.1146e-04 - mean_absolute_error: 0.0114\n",
      "\n",
      "Epoch 03160: loss did not improve\n",
      "Epoch 3161/10000\n",
      "2700/2700 [==============================] - 0s 41us/step - loss: 3.1080e-04 - mean_absolute_error: 0.0115\n",
      "\n",
      "Epoch 03161: loss did not improve\n",
      "Epoch 3162/10000\n",
      "2700/2700 [==============================] - 0s 30us/step - loss: 3.0998e-04 - mean_absolute_error: 0.0114\n",
      "\n",
      "Epoch 03162: loss did not improve\n",
      "Epoch 3163/10000\n",
      "2700/2700 [==============================] - 0s 40us/step - loss: 3.0679e-04 - mean_absolute_error: 0.0113\n",
      "\n",
      "Epoch 03163: loss improved from 0.00031 to 0.00031, saving model to keras/best_baseline_weights.hdf5\n",
      "Epoch 3164/10000\n",
      "2700/2700 [==============================] - 0s 31us/step - loss: 3.1026e-04 - mean_absolute_error: 0.0115\n",
      "\n",
      "Epoch 03164: loss did not improve\n",
      "Epoch 3165/10000\n",
      "2700/2700 [==============================] - 0s 30us/step - loss: 3.0800e-04 - mean_absolute_error: 0.0114\n",
      "\n",
      "Epoch 03165: loss did not improve\n",
      "Epoch 3166/10000\n",
      "2700/2700 [==============================] - 0s 27us/step - loss: 3.0941e-04 - mean_absolute_error: 0.0114\n",
      "\n",
      "Epoch 03166: loss did not improve\n",
      "Epoch 3167/10000\n",
      "2700/2700 [==============================] - 0s 38us/step - loss: 3.0655e-04 - mean_absolute_error: 0.0113\n",
      "\n",
      "Epoch 03167: loss improved from 0.00031 to 0.00031, saving model to keras/best_baseline_weights.hdf5\n",
      "Epoch 3168/10000\n",
      "2700/2700 [==============================] - 0s 37us/step - loss: 3.1006e-04 - mean_absolute_error: 0.0114\n",
      "\n",
      "Epoch 03168: loss did not improve\n",
      "Epoch 3169/10000\n",
      "2700/2700 [==============================] - 0s 36us/step - loss: 3.0431e-04 - mean_absolute_error: 0.0112\n",
      "\n",
      "Epoch 03169: loss improved from 0.00031 to 0.00030, saving model to keras/best_baseline_weights.hdf5\n",
      "Epoch 3170/10000\n",
      "2700/2700 [==============================] - 0s 31us/step - loss: 3.1037e-04 - mean_absolute_error: 0.0113\n",
      "\n",
      "Epoch 03170: loss did not improve\n",
      "Epoch 3171/10000\n",
      "2700/2700 [==============================] - 0s 27us/step - loss: 3.0776e-04 - mean_absolute_error: 0.0113\n",
      "\n",
      "Epoch 03171: loss did not improve\n",
      "Epoch 3172/10000\n",
      "2700/2700 [==============================] - 0s 30us/step - loss: 3.0813e-04 - mean_absolute_error: 0.0114\n",
      "\n",
      "Epoch 03172: loss did not improve\n",
      "Epoch 3173/10000\n",
      "2700/2700 [==============================] - 0s 31us/step - loss: 3.0666e-04 - mean_absolute_error: 0.0114\n",
      "\n",
      "Epoch 03173: loss did not improve\n",
      "Epoch 3174/10000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2700/2700 [==============================] - 0s 29us/step - loss: 3.0873e-04 - mean_absolute_error: 0.0113\n",
      "\n",
      "Epoch 03174: loss did not improve\n",
      "Epoch 3175/10000\n",
      "2700/2700 [==============================] - 0s 26us/step - loss: 3.0815e-04 - mean_absolute_error: 0.0114\n",
      "\n",
      "Epoch 03175: loss did not improve\n",
      "Epoch 3176/10000\n",
      "2700/2700 [==============================] - 0s 27us/step - loss: 3.0639e-04 - mean_absolute_error: 0.0113\n",
      "\n",
      "Epoch 03176: loss did not improve\n",
      "Epoch 3177/10000\n",
      "2700/2700 [==============================] - 0s 29us/step - loss: 3.0667e-04 - mean_absolute_error: 0.0113\n",
      "\n",
      "Epoch 03177: loss did not improve\n",
      "Epoch 3178/10000\n",
      "2700/2700 [==============================] - 0s 30us/step - loss: 3.1316e-04 - mean_absolute_error: 0.0115\n",
      "\n",
      "Epoch 03178: loss did not improve\n",
      "Epoch 3179/10000\n",
      "2700/2700 [==============================] - 0s 29us/step - loss: 3.0854e-04 - mean_absolute_error: 0.0115\n",
      "\n",
      "Epoch 03179: loss did not improve\n",
      "Epoch 3180/10000\n",
      "2700/2700 [==============================] - 0s 26us/step - loss: 3.0601e-04 - mean_absolute_error: 0.0112\n",
      "\n",
      "Epoch 03180: loss did not improve\n",
      "Epoch 3181/10000\n",
      "2700/2700 [==============================] - 0s 28us/step - loss: 3.1253e-04 - mean_absolute_error: 0.0115\n",
      "\n",
      "Epoch 03181: loss did not improve\n",
      "Epoch 3182/10000\n",
      "2700/2700 [==============================] - 0s 28us/step - loss: 3.0811e-04 - mean_absolute_error: 0.0113\n",
      "\n",
      "Epoch 03182: loss did not improve\n",
      "Epoch 3183/10000\n",
      "2700/2700 [==============================] - 0s 30us/step - loss: 3.0483e-04 - mean_absolute_error: 0.0112\n",
      "\n",
      "Epoch 03183: loss did not improve\n",
      "Epoch 3184/10000\n",
      "2700/2700 [==============================] - 0s 26us/step - loss: 3.0873e-04 - mean_absolute_error: 0.0113\n",
      "\n",
      "Epoch 03184: loss did not improve\n",
      "Epoch 3185/10000\n",
      "2700/2700 [==============================] - 0s 30us/step - loss: 3.0705e-04 - mean_absolute_error: 0.0113\n",
      "\n",
      "Epoch 03185: loss did not improve\n",
      "Epoch 3186/10000\n",
      "2700/2700 [==============================] - 0s 30us/step - loss: 3.0402e-04 - mean_absolute_error: 0.0112\n",
      "\n",
      "Epoch 03186: loss improved from 0.00030 to 0.00030, saving model to keras/best_baseline_weights.hdf5\n",
      "Epoch 3187/10000\n",
      "2700/2700 [==============================] - 0s 30us/step - loss: 3.0973e-04 - mean_absolute_error: 0.0115\n",
      "\n",
      "Epoch 03187: loss did not improve\n",
      "Epoch 3188/10000\n",
      "2700/2700 [==============================] - 0s 27us/step - loss: 3.0495e-04 - mean_absolute_error: 0.0113\n",
      "\n",
      "Epoch 03188: loss did not improve\n",
      "Epoch 3189/10000\n",
      "2700/2700 [==============================] - 0s 31us/step - loss: 3.1018e-04 - mean_absolute_error: 0.0114\n",
      "\n",
      "Epoch 03189: loss did not improve\n",
      "Epoch 3190/10000\n",
      "2700/2700 [==============================] - 0s 29us/step - loss: 3.0501e-04 - mean_absolute_error: 0.0113\n",
      "\n",
      "Epoch 03190: loss did not improve\n",
      "Epoch 3191/10000\n",
      "2700/2700 [==============================] - 0s 30us/step - loss: 3.0208e-04 - mean_absolute_error: 0.0111\n",
      "\n",
      "Epoch 03191: loss improved from 0.00030 to 0.00030, saving model to keras/best_baseline_weights.hdf5\n",
      "Epoch 3192/10000\n",
      "2700/2700 [==============================] - 0s 29us/step - loss: 3.0993e-04 - mean_absolute_error: 0.0115\n",
      "\n",
      "Epoch 03192: loss did not improve\n",
      "Epoch 3193/10000\n",
      "2700/2700 [==============================] - 0s 30us/step - loss: 3.0525e-04 - mean_absolute_error: 0.0113\n",
      "\n",
      "Epoch 03193: loss did not improve\n",
      "Epoch 3194/10000\n",
      "2700/2700 [==============================] - 0s 30us/step - loss: 3.0674e-04 - mean_absolute_error: 0.0113\n",
      "\n",
      "Epoch 03194: loss did not improve\n",
      "Epoch 3195/10000\n",
      "2700/2700 [==============================] - 0s 26us/step - loss: 3.0849e-04 - mean_absolute_error: 0.0114\n",
      "\n",
      "Epoch 03195: loss did not improve\n",
      "Epoch 3196/10000\n",
      "2700/2700 [==============================] - 0s 28us/step - loss: 3.0582e-04 - mean_absolute_error: 0.0113\n",
      "\n",
      "Epoch 03196: loss did not improve\n",
      "Epoch 3197/10000\n",
      "2700/2700 [==============================] - 0s 28us/step - loss: 3.0611e-04 - mean_absolute_error: 0.0112\n",
      "\n",
      "Epoch 03197: loss did not improve\n",
      "Epoch 3198/10000\n",
      "2700/2700 [==============================] - 0s 32us/step - loss: 3.0199e-04 - mean_absolute_error: 0.0112\n",
      "\n",
      "Epoch 03198: loss improved from 0.00030 to 0.00030, saving model to keras/best_baseline_weights.hdf5\n",
      "Epoch 3199/10000\n",
      "2700/2700 [==============================] - 0s 32us/step - loss: 3.0254e-04 - mean_absolute_error: 0.0111\n",
      "\n",
      "Epoch 03199: loss did not improve\n",
      "Epoch 3200/10000\n",
      "2700/2700 [==============================] - 0s 30us/step - loss: 3.0261e-04 - mean_absolute_error: 0.0112\n",
      "\n",
      "Epoch 03200: loss did not improve\n",
      "Epoch 3201/10000\n",
      "2700/2700 [==============================] - 0s 30us/step - loss: 3.0268e-04 - mean_absolute_error: 0.0112\n",
      "\n",
      "Epoch 03201: loss did not improve\n",
      "Epoch 3202/10000\n",
      "2700/2700 [==============================] - 0s 30us/step - loss: 3.0241e-04 - mean_absolute_error: 0.0112\n",
      "\n",
      "Epoch 03202: loss did not improve\n",
      "Epoch 3203/10000\n",
      "2700/2700 [==============================] - 0s 29us/step - loss: 3.0609e-04 - mean_absolute_error: 0.0112\n",
      "\n",
      "Epoch 03203: loss did not improve\n",
      "Epoch 3204/10000\n",
      "2700/2700 [==============================] - 0s 29us/step - loss: 3.0004e-04 - mean_absolute_error: 0.0112\n",
      "\n",
      "Epoch 03204: loss improved from 0.00030 to 0.00030, saving model to keras/best_baseline_weights.hdf5\n",
      "Epoch 3205/10000\n",
      "2700/2700 [==============================] - 0s 30us/step - loss: 2.9997e-04 - mean_absolute_error: 0.0111\n",
      "\n",
      "Epoch 03205: loss improved from 0.00030 to 0.00030, saving model to keras/best_baseline_weights.hdf5\n",
      "Epoch 3206/10000\n",
      "2700/2700 [==============================] - 0s 29us/step - loss: 3.0171e-04 - mean_absolute_error: 0.0113\n",
      "\n",
      "Epoch 03206: loss did not improve\n",
      "Epoch 3207/10000\n",
      "2700/2700 [==============================] - 0s 28us/step - loss: 2.9819e-04 - mean_absolute_error: 0.0111\n",
      "\n",
      "Epoch 03207: loss improved from 0.00030 to 0.00030, saving model to keras/best_baseline_weights.hdf5\n",
      "Epoch 3208/10000\n",
      "2700/2700 [==============================] - 0s 31us/step - loss: 3.0191e-04 - mean_absolute_error: 0.0112\n",
      "\n",
      "Epoch 03208: loss did not improve\n",
      "Epoch 3209/10000\n",
      "2700/2700 [==============================] - 0s 29us/step - loss: 3.0157e-04 - mean_absolute_error: 0.0111\n",
      "\n",
      "Epoch 03209: loss did not improve\n",
      "Epoch 3210/10000\n",
      "2700/2700 [==============================] - 0s 40us/step - loss: 2.9897e-04 - mean_absolute_error: 0.0111\n",
      "\n",
      "Epoch 03210: loss did not improve\n",
      "Epoch 3211/10000\n",
      "2700/2700 [==============================] - 0s 38us/step - loss: 3.0352e-04 - mean_absolute_error: 0.0112\n",
      "\n",
      "Epoch 03211: loss did not improve\n",
      "Epoch 3212/10000\n",
      "2700/2700 [==============================] - 0s 34us/step - loss: 3.0986e-04 - mean_absolute_error: 0.0115\n",
      "\n",
      "Epoch 03212: loss did not improve\n",
      "Epoch 3213/10000\n",
      "2700/2700 [==============================] - 0s 29us/step - loss: 3.0027e-04 - mean_absolute_error: 0.0111\n",
      "\n",
      "Epoch 03213: loss did not improve\n",
      "Epoch 3214/10000\n",
      "2700/2700 [==============================] - 0s 31us/step - loss: 3.0082e-04 - mean_absolute_error: 0.0111\n",
      "\n",
      "Epoch 03214: loss did not improve\n",
      "Epoch 3215/10000\n",
      "2700/2700 [==============================] - 0s 35us/step - loss: 3.1468e-04 - mean_absolute_error: 0.0116\n",
      "\n",
      "Epoch 03215: loss did not improve\n",
      "Epoch 3216/10000\n",
      "2700/2700 [==============================] - 0s 31us/step - loss: 3.0321e-04 - mean_absolute_error: 0.0112\n",
      "\n",
      "Epoch 03216: loss did not improve\n",
      "Epoch 3217/10000\n",
      "2700/2700 [==============================] - 0s 31us/step - loss: 3.0656e-04 - mean_absolute_error: 0.0113\n",
      "\n",
      "Epoch 03217: loss did not improve\n",
      "Epoch 3218/10000\n",
      "2700/2700 [==============================] - 0s 35us/step - loss: 2.9969e-04 - mean_absolute_error: 0.0111\n",
      "\n",
      "Epoch 03218: loss did not improve\n",
      "Epoch 3219/10000\n",
      "2700/2700 [==============================] - 0s 32us/step - loss: 2.9866e-04 - mean_absolute_error: 0.0111\n",
      "\n",
      "Epoch 03219: loss did not improve\n",
      "Epoch 3220/10000\n",
      "2700/2700 [==============================] - 0s 32us/step - loss: 3.0688e-04 - mean_absolute_error: 0.0114\n",
      "\n",
      "Epoch 03220: loss did not improve\n",
      "Epoch 3221/10000\n",
      "2700/2700 [==============================] - 0s 32us/step - loss: 3.0184e-04 - mean_absolute_error: 0.0113\n",
      "\n",
      "Epoch 03221: loss did not improve\n",
      "Epoch 3222/10000\n",
      "2700/2700 [==============================] - 0s 30us/step - loss: 3.0617e-04 - mean_absolute_error: 0.0114\n",
      "\n",
      "Epoch 03222: loss did not improve\n",
      "Epoch 3223/10000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2700/2700 [==============================] - 0s 27us/step - loss: 3.0136e-04 - mean_absolute_error: 0.0111\n",
      "\n",
      "Epoch 03223: loss did not improve\n",
      "Epoch 3224/10000\n",
      "2700/2700 [==============================] - 0s 28us/step - loss: 2.9801e-04 - mean_absolute_error: 0.0111\n",
      "\n",
      "Epoch 03224: loss improved from 0.00030 to 0.00030, saving model to keras/best_baseline_weights.hdf5\n",
      "Epoch 3225/10000\n",
      "2700/2700 [==============================] - 0s 30us/step - loss: 2.9965e-04 - mean_absolute_error: 0.0112\n",
      "\n",
      "Epoch 03225: loss did not improve\n",
      "Epoch 3226/10000\n",
      "2700/2700 [==============================] - 0s 29us/step - loss: 3.0161e-04 - mean_absolute_error: 0.0113\n",
      "\n",
      "Epoch 03226: loss did not improve\n",
      "Epoch 3227/10000\n",
      "2700/2700 [==============================] - 0s 28us/step - loss: 3.0504e-04 - mean_absolute_error: 0.0113\n",
      "\n",
      "Epoch 03227: loss did not improve\n",
      "Epoch 3228/10000\n",
      "2700/2700 [==============================] - 0s 26us/step - loss: 3.0219e-04 - mean_absolute_error: 0.0112\n",
      "\n",
      "Epoch 03228: loss did not improve\n",
      "Epoch 3229/10000\n",
      "2700/2700 [==============================] - 0s 30us/step - loss: 2.9868e-04 - mean_absolute_error: 0.0111\n",
      "\n",
      "Epoch 03229: loss did not improve\n",
      "Epoch 3230/10000\n",
      "2700/2700 [==============================] - 0s 29us/step - loss: 3.0288e-04 - mean_absolute_error: 0.0112\n",
      "\n",
      "Epoch 03230: loss did not improve\n",
      "Epoch 3231/10000\n",
      "2700/2700 [==============================] - 0s 30us/step - loss: 3.0127e-04 - mean_absolute_error: 0.0112\n",
      "\n",
      "Epoch 03231: loss did not improve\n",
      "Epoch 3232/10000\n",
      "2700/2700 [==============================] - 0s 28us/step - loss: 2.9984e-04 - mean_absolute_error: 0.0110\n",
      "\n",
      "Epoch 03232: loss did not improve\n",
      "Epoch 3233/10000\n",
      "2700/2700 [==============================] - 0s 26us/step - loss: 2.9949e-04 - mean_absolute_error: 0.0112\n",
      "\n",
      "Epoch 03233: loss did not improve\n",
      "Epoch 3234/10000\n",
      "2700/2700 [==============================] - 0s 31us/step - loss: 2.9774e-04 - mean_absolute_error: 0.0111\n",
      "\n",
      "Epoch 03234: loss improved from 0.00030 to 0.00030, saving model to keras/best_baseline_weights.hdf5\n",
      "Epoch 3235/10000\n",
      "2700/2700 [==============================] - 0s 30us/step - loss: 3.0104e-04 - mean_absolute_error: 0.0112\n",
      "\n",
      "Epoch 03235: loss did not improve\n",
      "Epoch 3236/10000\n",
      "2700/2700 [==============================] - 0s 31us/step - loss: 3.0037e-04 - mean_absolute_error: 0.0112\n",
      "\n",
      "Epoch 03236: loss did not improve\n",
      "Epoch 3237/10000\n",
      "2700/2700 [==============================] - 0s 28us/step - loss: 3.0643e-04 - mean_absolute_error: 0.0114\n",
      "\n",
      "Epoch 03237: loss did not improve\n",
      "Epoch 3238/10000\n",
      "2700/2700 [==============================] - 0s 27us/step - loss: 2.9782e-04 - mean_absolute_error: 0.0111\n",
      "\n",
      "Epoch 03238: loss did not improve\n",
      "Epoch 3239/10000\n",
      "2700/2700 [==============================] - 0s 30us/step - loss: 2.9901e-04 - mean_absolute_error: 0.0111\n",
      "\n",
      "Epoch 03239: loss did not improve\n",
      "Epoch 3240/10000\n",
      "2700/2700 [==============================] - 0s 28us/step - loss: 3.0003e-04 - mean_absolute_error: 0.0112\n",
      "\n",
      "Epoch 03240: loss did not improve\n",
      "Epoch 3241/10000\n",
      "2700/2700 [==============================] - 0s 31us/step - loss: 3.0158e-04 - mean_absolute_error: 0.0111\n",
      "\n",
      "Epoch 03241: loss did not improve\n",
      "Epoch 3242/10000\n",
      "2700/2700 [==============================] - 0s 28us/step - loss: 2.9508e-04 - mean_absolute_error: 0.0110\n",
      "\n",
      "Epoch 03242: loss improved from 0.00030 to 0.00030, saving model to keras/best_baseline_weights.hdf5\n",
      "Epoch 3243/10000\n",
      "2700/2700 [==============================] - 0s 28us/step - loss: 2.9640e-04 - mean_absolute_error: 0.0110\n",
      "\n",
      "Epoch 03243: loss did not improve\n",
      "Epoch 3244/10000\n",
      "2700/2700 [==============================] - 0s 28us/step - loss: 3.0363e-04 - mean_absolute_error: 0.0113\n",
      "\n",
      "Epoch 03244: loss did not improve\n",
      "Epoch 3245/10000\n",
      "2700/2700 [==============================] - 0s 28us/step - loss: 2.9826e-04 - mean_absolute_error: 0.0111\n",
      "\n",
      "Epoch 03245: loss did not improve\n",
      "Epoch 3246/10000\n",
      "2700/2700 [==============================] - 0s 29us/step - loss: 2.9438e-04 - mean_absolute_error: 0.0110\n",
      "\n",
      "Epoch 03246: loss improved from 0.00030 to 0.00029, saving model to keras/best_baseline_weights.hdf5\n",
      "Epoch 3247/10000\n",
      "2700/2700 [==============================] - 0s 31us/step - loss: 2.9603e-04 - mean_absolute_error: 0.0110\n",
      "\n",
      "Epoch 03247: loss did not improve\n",
      "Epoch 3248/10000\n",
      "2700/2700 [==============================] - 0s 31us/step - loss: 2.9298e-04 - mean_absolute_error: 0.0109\n",
      "\n",
      "Epoch 03248: loss improved from 0.00029 to 0.00029, saving model to keras/best_baseline_weights.hdf5\n",
      "Epoch 3249/10000\n",
      "2700/2700 [==============================] - 0s 29us/step - loss: 2.9776e-04 - mean_absolute_error: 0.0111\n",
      "\n",
      "Epoch 03249: loss did not improve\n",
      "Epoch 3250/10000\n",
      "2700/2700 [==============================] - 0s 27us/step - loss: 3.0043e-04 - mean_absolute_error: 0.0112\n",
      "\n",
      "Epoch 03250: loss did not improve\n",
      "Epoch 3251/10000\n",
      "2700/2700 [==============================] - 0s 31us/step - loss: 2.9538e-04 - mean_absolute_error: 0.0110\n",
      "\n",
      "Epoch 03251: loss did not improve\n",
      "Epoch 3252/10000\n",
      "2700/2700 [==============================] - 0s 30us/step - loss: 3.0014e-04 - mean_absolute_error: 0.0112\n",
      "\n",
      "Epoch 03252: loss did not improve\n",
      "Epoch 3253/10000\n",
      "2700/2700 [==============================] - 0s 34us/step - loss: 2.9531e-04 - mean_absolute_error: 0.0110\n",
      "\n",
      "Epoch 03253: loss did not improve\n",
      "Epoch 3254/10000\n",
      "2700/2700 [==============================] - 0s 28us/step - loss: 2.9458e-04 - mean_absolute_error: 0.0110\n",
      "\n",
      "Epoch 03254: loss did not improve\n",
      "Epoch 3255/10000\n",
      "2700/2700 [==============================] - 0s 27us/step - loss: 3.0001e-04 - mean_absolute_error: 0.0112\n",
      "\n",
      "Epoch 03255: loss did not improve\n",
      "Epoch 3256/10000\n",
      "2700/2700 [==============================] - 0s 29us/step - loss: 2.9251e-04 - mean_absolute_error: 0.0109\n",
      "\n",
      "Epoch 03256: loss improved from 0.00029 to 0.00029, saving model to keras/best_baseline_weights.hdf5\n",
      "Epoch 3257/10000\n",
      "2700/2700 [==============================] - 0s 28us/step - loss: 2.9636e-04 - mean_absolute_error: 0.0111\n",
      "\n",
      "Epoch 03257: loss did not improve\n",
      "Epoch 3258/10000\n",
      "2700/2700 [==============================] - 0s 31us/step - loss: 2.9404e-04 - mean_absolute_error: 0.0110\n",
      "\n",
      "Epoch 03258: loss did not improve\n",
      "Epoch 3259/10000\n",
      "2700/2700 [==============================] - 0s 30us/step - loss: 3.0411e-04 - mean_absolute_error: 0.0113\n",
      "\n",
      "Epoch 03259: loss did not improve\n",
      "Epoch 3260/10000\n",
      "2700/2700 [==============================] - 0s 27us/step - loss: 3.0328e-04 - mean_absolute_error: 0.0112\n",
      "\n",
      "Epoch 03260: loss did not improve\n",
      "Epoch 3261/10000\n",
      "2700/2700 [==============================] - 0s 29us/step - loss: 2.9465e-04 - mean_absolute_error: 0.0110\n",
      "\n",
      "Epoch 03261: loss did not improve\n",
      "Epoch 3262/10000\n",
      "2700/2700 [==============================] - 0s 30us/step - loss: 2.9091e-04 - mean_absolute_error: 0.0109\n",
      "\n",
      "Epoch 03262: loss improved from 0.00029 to 0.00029, saving model to keras/best_baseline_weights.hdf5\n",
      "Epoch 3263/10000\n",
      "2700/2700 [==============================] - 0s 32us/step - loss: 2.9648e-04 - mean_absolute_error: 0.0110\n",
      "\n",
      "Epoch 03263: loss did not improve\n",
      "Epoch 3264/10000\n",
      "2700/2700 [==============================] - 0s 28us/step - loss: 2.9858e-04 - mean_absolute_error: 0.0110\n",
      "\n",
      "Epoch 03264: loss did not improve\n",
      "Epoch 3265/10000\n",
      "2700/2700 [==============================] - 0s 29us/step - loss: 2.9159e-04 - mean_absolute_error: 0.0109\n",
      "\n",
      "Epoch 03265: loss did not improve\n",
      "Epoch 3266/10000\n",
      "2700/2700 [==============================] - 0s 30us/step - loss: 2.9179e-04 - mean_absolute_error: 0.0110\n",
      "\n",
      "Epoch 03266: loss did not improve\n",
      "Epoch 3267/10000\n",
      "2700/2700 [==============================] - 0s 28us/step - loss: 2.9317e-04 - mean_absolute_error: 0.0110\n",
      "\n",
      "Epoch 03267: loss did not improve\n",
      "Epoch 3268/10000\n",
      "2700/2700 [==============================] - 0s 29us/step - loss: 2.9483e-04 - mean_absolute_error: 0.0111\n",
      "\n",
      "Epoch 03268: loss did not improve\n",
      "Epoch 3269/10000\n",
      "2700/2700 [==============================] - 0s 31us/step - loss: 2.9763e-04 - mean_absolute_error: 0.0110\n",
      "\n",
      "Epoch 03269: loss did not improve\n",
      "Epoch 3270/10000\n",
      "2700/2700 [==============================] - 0s 27us/step - loss: 2.9556e-04 - mean_absolute_error: 0.0111\n",
      "\n",
      "Epoch 03270: loss did not improve\n",
      "Epoch 3271/10000\n",
      "2700/2700 [==============================] - 0s 30us/step - loss: 2.9250e-04 - mean_absolute_error: 0.0110\n",
      "\n",
      "Epoch 03271: loss did not improve\n",
      "Epoch 3272/10000\n",
      "2700/2700 [==============================] - 0s 27us/step - loss: 2.9539e-04 - mean_absolute_error: 0.0111\n",
      "\n",
      "Epoch 03272: loss did not improve\n",
      "Epoch 3273/10000\n",
      "2700/2700 [==============================] - 0s 30us/step - loss: 2.9478e-04 - mean_absolute_error: 0.0110\n",
      "\n",
      "Epoch 03273: loss did not improve\n",
      "Epoch 3274/10000\n",
      "2700/2700 [==============================] - 0s 29us/step - loss: 2.9032e-04 - mean_absolute_error: 0.0110\n",
      "\n",
      "Epoch 03274: loss improved from 0.00029 to 0.00029, saving model to keras/best_baseline_weights.hdf5\n",
      "Epoch 3275/10000\n",
      "2700/2700 [==============================] - 0s 29us/step - loss: 3.0052e-04 - mean_absolute_error: 0.0112\n",
      "\n",
      "Epoch 03275: loss did not improve\n",
      "Epoch 3276/10000\n",
      "2700/2700 [==============================] - 0s 28us/step - loss: 2.9359e-04 - mean_absolute_error: 0.0109\n",
      "\n",
      "Epoch 03276: loss did not improve\n",
      "Epoch 3277/10000\n",
      "2700/2700 [==============================] - 0s 30us/step - loss: 2.9415e-04 - mean_absolute_error: 0.0110\n",
      "\n",
      "Epoch 03277: loss did not improve\n",
      "Epoch 3278/10000\n",
      "2700/2700 [==============================] - 0s 30us/step - loss: 2.9703e-04 - mean_absolute_error: 0.0112\n",
      "\n",
      "Epoch 03278: loss did not improve\n",
      "Epoch 3279/10000\n",
      "2700/2700 [==============================] - 0s 29us/step - loss: 2.9440e-04 - mean_absolute_error: 0.0109\n",
      "\n",
      "Epoch 03279: loss did not improve\n",
      "Epoch 3280/10000\n",
      "2700/2700 [==============================] - 0s 28us/step - loss: 2.8804e-04 - mean_absolute_error: 0.0109\n",
      "\n",
      "Epoch 03280: loss improved from 0.00029 to 0.00029, saving model to keras/best_baseline_weights.hdf5\n",
      "Epoch 3281/10000\n",
      "2700/2700 [==============================] - 0s 30us/step - loss: 2.9075e-04 - mean_absolute_error: 0.0110\n",
      "\n",
      "Epoch 03281: loss did not improve\n",
      "Epoch 3282/10000\n",
      "2700/2700 [==============================] - 0s 30us/step - loss: 2.9455e-04 - mean_absolute_error: 0.0110\n",
      "\n",
      "Epoch 03282: loss did not improve\n",
      "Epoch 3283/10000\n",
      "2700/2700 [==============================] - 0s 29us/step - loss: 2.9290e-04 - mean_absolute_error: 0.0109\n",
      "\n",
      "Epoch 03283: loss did not improve\n",
      "Epoch 3284/10000\n",
      "2700/2700 [==============================] - 0s 28us/step - loss: 2.9452e-04 - mean_absolute_error: 0.0110\n",
      "\n",
      "Epoch 03284: loss did not improve\n",
      "Epoch 3285/10000\n",
      "2700/2700 [==============================] - 0s 26us/step - loss: 2.8751e-04 - mean_absolute_error: 0.0108\n",
      "\n",
      "Epoch 03285: loss improved from 0.00029 to 0.00029, saving model to keras/best_baseline_weights.hdf5\n",
      "Epoch 3286/10000\n",
      "2700/2700 [==============================] - 0s 27us/step - loss: 2.9232e-04 - mean_absolute_error: 0.0109\n",
      "\n",
      "Epoch 03286: loss did not improve\n",
      "Epoch 3287/10000\n",
      "2700/2700 [==============================] - 0s 29us/step - loss: 3.0376e-04 - mean_absolute_error: 0.0114\n",
      "\n",
      "Epoch 03287: loss did not improve\n",
      "Epoch 3288/10000\n",
      "2700/2700 [==============================] - 0s 31us/step - loss: 2.9241e-04 - mean_absolute_error: 0.0110\n",
      "\n",
      "Epoch 03288: loss did not improve\n",
      "Epoch 3289/10000\n",
      "2700/2700 [==============================] - 0s 31us/step - loss: 2.8845e-04 - mean_absolute_error: 0.0108\n",
      "\n",
      "Epoch 03289: loss did not improve\n",
      "Epoch 3290/10000\n",
      "2700/2700 [==============================] - 0s 29us/step - loss: 2.8832e-04 - mean_absolute_error: 0.0108\n",
      "\n",
      "Epoch 03290: loss did not improve\n",
      "Epoch 3291/10000\n",
      "2700/2700 [==============================] - 0s 26us/step - loss: 2.8792e-04 - mean_absolute_error: 0.0108\n",
      "\n",
      "Epoch 03291: loss did not improve\n",
      "Epoch 3292/10000\n",
      "2700/2700 [==============================] - 0s 30us/step - loss: 2.9200e-04 - mean_absolute_error: 0.0110\n",
      "\n",
      "Epoch 03292: loss did not improve\n",
      "Epoch 3293/10000\n",
      "2700/2700 [==============================] - 0s 30us/step - loss: 2.8954e-04 - mean_absolute_error: 0.0108\n",
      "\n",
      "Epoch 03293: loss did not improve\n",
      "Epoch 3294/10000\n",
      "2700/2700 [==============================] - 0s 27us/step - loss: 2.9386e-04 - mean_absolute_error: 0.0110\n",
      "\n",
      "Epoch 03294: loss did not improve\n",
      "Epoch 3295/10000\n",
      "2700/2700 [==============================] - 0s 28us/step - loss: 2.9322e-04 - mean_absolute_error: 0.0110\n",
      "\n",
      "Epoch 03295: loss did not improve\n",
      "Epoch 3296/10000\n",
      "2700/2700 [==============================] - 0s 29us/step - loss: 2.8431e-04 - mean_absolute_error: 0.0107\n",
      "\n",
      "Epoch 03296: loss improved from 0.00029 to 0.00028, saving model to keras/best_baseline_weights.hdf5\n",
      "Epoch 3297/10000\n",
      "2700/2700 [==============================] - 0s 30us/step - loss: 2.8945e-04 - mean_absolute_error: 0.0109\n",
      "\n",
      "Epoch 03297: loss did not improve\n",
      "Epoch 3298/10000\n",
      "2700/2700 [==============================] - 0s 29us/step - loss: 2.9144e-04 - mean_absolute_error: 0.0110\n",
      "\n",
      "Epoch 03298: loss did not improve\n",
      "Epoch 3299/10000\n",
      "2700/2700 [==============================] - 0s 28us/step - loss: 2.8699e-04 - mean_absolute_error: 0.0109\n",
      "\n",
      "Epoch 03299: loss did not improve\n",
      "Epoch 3300/10000\n",
      "2700/2700 [==============================] - 0s 29us/step - loss: 2.9021e-04 - mean_absolute_error: 0.0109\n",
      "\n",
      "Epoch 03300: loss did not improve\n",
      "Epoch 3301/10000\n",
      "2700/2700 [==============================] - 0s 28us/step - loss: 2.8488e-04 - mean_absolute_error: 0.0107\n",
      "\n",
      "Epoch 03301: loss did not improve\n",
      "Epoch 3302/10000\n",
      "2700/2700 [==============================] - 0s 29us/step - loss: 2.8756e-04 - mean_absolute_error: 0.0108\n",
      "\n",
      "Epoch 03302: loss did not improve\n",
      "Epoch 3303/10000\n",
      "2700/2700 [==============================] - 0s 28us/step - loss: 2.8677e-04 - mean_absolute_error: 0.0108\n",
      "\n",
      "Epoch 03303: loss did not improve\n",
      "Epoch 3304/10000\n",
      "2700/2700 [==============================] - 0s 27us/step - loss: 2.8482e-04 - mean_absolute_error: 0.0107\n",
      "\n",
      "Epoch 03304: loss did not improve\n",
      "Epoch 3305/10000\n",
      "2700/2700 [==============================] - 0s 30us/step - loss: 2.9094e-04 - mean_absolute_error: 0.0110\n",
      "\n",
      "Epoch 03305: loss did not improve\n",
      "Epoch 3306/10000\n",
      "2700/2700 [==============================] - 0s 30us/step - loss: 2.8795e-04 - mean_absolute_error: 0.0108\n",
      "\n",
      "Epoch 03306: loss did not improve\n",
      "Epoch 3307/10000\n",
      "2700/2700 [==============================] - 0s 34us/step - loss: 2.9094e-04 - mean_absolute_error: 0.0110\n",
      "\n",
      "Epoch 03307: loss did not improve\n",
      "Epoch 3308/10000\n",
      "2700/2700 [==============================] - 0s 32us/step - loss: 2.8527e-04 - mean_absolute_error: 0.0108\n",
      "\n",
      "Epoch 03308: loss did not improve\n",
      "Epoch 3309/10000\n",
      "2700/2700 [==============================] - 0s 29us/step - loss: 2.9109e-04 - mean_absolute_error: 0.0109\n",
      "\n",
      "Epoch 03309: loss did not improve\n",
      "Epoch 3310/10000\n",
      "2700/2700 [==============================] - 0s 34us/step - loss: 2.8917e-04 - mean_absolute_error: 0.0108\n",
      "\n",
      "Epoch 03310: loss did not improve\n",
      "Epoch 3311/10000\n",
      "2700/2700 [==============================] - 0s 29us/step - loss: 2.9090e-04 - mean_absolute_error: 0.0109\n",
      "\n",
      "Epoch 03311: loss did not improve\n",
      "Epoch 3312/10000\n",
      "2700/2700 [==============================] - 0s 37us/step - loss: 2.9184e-04 - mean_absolute_error: 0.0110\n",
      "\n",
      "Epoch 03312: loss did not improve\n",
      "Epoch 3313/10000\n",
      "2700/2700 [==============================] - 0s 31us/step - loss: 2.8430e-04 - mean_absolute_error: 0.0107\n",
      "\n",
      "Epoch 03313: loss improved from 0.00028 to 0.00028, saving model to keras/best_baseline_weights.hdf5\n",
      "Epoch 3314/10000\n",
      "2700/2700 [==============================] - 0s 35us/step - loss: 2.8380e-04 - mean_absolute_error: 0.0107\n",
      "\n",
      "Epoch 03314: loss improved from 0.00028 to 0.00028, saving model to keras/best_baseline_weights.hdf5\n",
      "Epoch 3315/10000\n",
      "2700/2700 [==============================] - 0s 29us/step - loss: 2.8392e-04 - mean_absolute_error: 0.0108\n",
      "\n",
      "Epoch 03315: loss did not improve\n",
      "Epoch 3316/10000\n",
      "2700/2700 [==============================] - 0s 31us/step - loss: 2.9286e-04 - mean_absolute_error: 0.0110\n",
      "\n",
      "Epoch 03316: loss did not improve\n",
      "Epoch 3317/10000\n",
      "2700/2700 [==============================] - 0s 29us/step - loss: 2.8486e-04 - mean_absolute_error: 0.0108\n",
      "\n",
      "Epoch 03317: loss did not improve\n",
      "Epoch 3318/10000\n",
      "2700/2700 [==============================] - 0s 29us/step - loss: 2.8248e-04 - mean_absolute_error: 0.0107\n",
      "\n",
      "Epoch 03318: loss improved from 0.00028 to 0.00028, saving model to keras/best_baseline_weights.hdf5\n",
      "Epoch 3319/10000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2700/2700 [==============================] - 0s 35us/step - loss: 2.8323e-04 - mean_absolute_error: 0.0107\n",
      "\n",
      "Epoch 03319: loss did not improve\n",
      "Epoch 3320/10000\n",
      "2700/2700 [==============================] - 0s 31us/step - loss: 2.8994e-04 - mean_absolute_error: 0.0109\n",
      "\n",
      "Epoch 03320: loss did not improve\n",
      "Epoch 3321/10000\n",
      "2700/2700 [==============================] - 0s 28us/step - loss: 2.8808e-04 - mean_absolute_error: 0.0109\n",
      "\n",
      "Epoch 03321: loss did not improve\n",
      "Epoch 3322/10000\n",
      "2700/2700 [==============================] - 0s 28us/step - loss: 2.8492e-04 - mean_absolute_error: 0.0107\n",
      "\n",
      "Epoch 03322: loss did not improve\n",
      "Epoch 3323/10000\n",
      "2700/2700 [==============================] - 0s 28us/step - loss: 2.8655e-04 - mean_absolute_error: 0.0109\n",
      "\n",
      "Epoch 03323: loss did not improve\n",
      "Epoch 3324/10000\n",
      "2700/2700 [==============================] - 0s 28us/step - loss: 2.8328e-04 - mean_absolute_error: 0.0108\n",
      "\n",
      "Epoch 03324: loss did not improve\n",
      "Epoch 3325/10000\n",
      "2700/2700 [==============================] - 0s 27us/step - loss: 2.8716e-04 - mean_absolute_error: 0.0108\n",
      "\n",
      "Epoch 03325: loss did not improve\n",
      "Epoch 3326/10000\n",
      "2700/2700 [==============================] - 0s 31us/step - loss: 2.8389e-04 - mean_absolute_error: 0.0107\n",
      "\n",
      "Epoch 03326: loss did not improve\n",
      "Epoch 3327/10000\n",
      "2700/2700 [==============================] - 0s 30us/step - loss: 2.8599e-04 - mean_absolute_error: 0.0108\n",
      "\n",
      "Epoch 03327: loss did not improve\n",
      "Epoch 3328/10000\n",
      "2700/2700 [==============================] - 0s 34us/step - loss: 2.8682e-04 - mean_absolute_error: 0.0108\n",
      "\n",
      "Epoch 03328: loss did not improve\n",
      "Epoch 3329/10000\n",
      "2700/2700 [==============================] - 0s 28us/step - loss: 2.8112e-04 - mean_absolute_error: 0.0106\n",
      "\n",
      "Epoch 03329: loss improved from 0.00028 to 0.00028, saving model to keras/best_baseline_weights.hdf5\n",
      "Epoch 3330/10000\n",
      "2700/2700 [==============================] - 0s 31us/step - loss: 2.8426e-04 - mean_absolute_error: 0.0107\n",
      "\n",
      "Epoch 03330: loss did not improve\n",
      "Epoch 3331/10000\n",
      "2700/2700 [==============================] - 0s 37us/step - loss: 2.8296e-04 - mean_absolute_error: 0.0107\n",
      "\n",
      "Epoch 03331: loss did not improve\n",
      "Epoch 3332/10000\n",
      "2700/2700 [==============================] - 0s 30us/step - loss: 2.7936e-04 - mean_absolute_error: 0.0106\n",
      "\n",
      "Epoch 03332: loss improved from 0.00028 to 0.00028, saving model to keras/best_baseline_weights.hdf5\n",
      "Epoch 3333/10000\n",
      "2700/2700 [==============================] - 0s 27us/step - loss: 2.8112e-04 - mean_absolute_error: 0.0106\n",
      "\n",
      "Epoch 03333: loss did not improve\n",
      "Epoch 3334/10000\n",
      "2700/2700 [==============================] - 0s 29us/step - loss: 2.8201e-04 - mean_absolute_error: 0.0107\n",
      "\n",
      "Epoch 03334: loss did not improve\n",
      "Epoch 3335/10000\n",
      "2700/2700 [==============================] - 0s 33us/step - loss: 2.7925e-04 - mean_absolute_error: 0.0107\n",
      "\n",
      "Epoch 03335: loss improved from 0.00028 to 0.00028, saving model to keras/best_baseline_weights.hdf5\n",
      "Epoch 3336/10000\n",
      "2700/2700 [==============================] - 0s 31us/step - loss: 2.8091e-04 - mean_absolute_error: 0.0106\n",
      "\n",
      "Epoch 03336: loss did not improve\n",
      "Epoch 3337/10000\n",
      "2700/2700 [==============================] - 0s 29us/step - loss: 2.8630e-04 - mean_absolute_error: 0.0108\n",
      "\n",
      "Epoch 03337: loss did not improve\n",
      "Epoch 3338/10000\n",
      "2700/2700 [==============================] - 0s 30us/step - loss: 2.7804e-04 - mean_absolute_error: 0.0106\n",
      "\n",
      "Epoch 03338: loss improved from 0.00028 to 0.00028, saving model to keras/best_baseline_weights.hdf5\n",
      "Epoch 3339/10000\n",
      "2700/2700 [==============================] - 0s 29us/step - loss: 2.8287e-04 - mean_absolute_error: 0.0107\n",
      "\n",
      "Epoch 03339: loss did not improve\n",
      "Epoch 3340/10000\n",
      "2700/2700 [==============================] - 0s 28us/step - loss: 2.8108e-04 - mean_absolute_error: 0.0106\n",
      "\n",
      "Epoch 03340: loss did not improve\n",
      "Epoch 3341/10000\n",
      "2700/2700 [==============================] - 0s 29us/step - loss: 2.8231e-04 - mean_absolute_error: 0.0107\n",
      "\n",
      "Epoch 03341: loss did not improve\n",
      "Epoch 3342/10000\n",
      "2700/2700 [==============================] - 0s 28us/step - loss: 2.8766e-04 - mean_absolute_error: 0.0108\n",
      "\n",
      "Epoch 03342: loss did not improve\n",
      "Epoch 3343/10000\n",
      "2700/2700 [==============================] - 0s 29us/step - loss: 2.8190e-04 - mean_absolute_error: 0.0107\n",
      "\n",
      "Epoch 03343: loss did not improve\n",
      "Epoch 3344/10000\n",
      "2700/2700 [==============================] - 0s 26us/step - loss: 2.7858e-04 - mean_absolute_error: 0.0106\n",
      "\n",
      "Epoch 03344: loss did not improve\n",
      "Epoch 3345/10000\n",
      "2700/2700 [==============================] - 0s 28us/step - loss: 2.7746e-04 - mean_absolute_error: 0.0106\n",
      "\n",
      "Epoch 03345: loss improved from 0.00028 to 0.00028, saving model to keras/best_baseline_weights.hdf5\n",
      "Epoch 3346/10000\n",
      "2700/2700 [==============================] - 0s 32us/step - loss: 2.7698e-04 - mean_absolute_error: 0.0105\n",
      "\n",
      "Epoch 03346: loss improved from 0.00028 to 0.00028, saving model to keras/best_baseline_weights.hdf5\n",
      "Epoch 3347/10000\n",
      "2700/2700 [==============================] - 0s 36us/step - loss: 2.8524e-04 - mean_absolute_error: 0.0108\n",
      "\n",
      "Epoch 03347: loss did not improve\n",
      "Epoch 3348/10000\n",
      "2700/2700 [==============================] - 0s 34us/step - loss: 2.8007e-04 - mean_absolute_error: 0.0107\n",
      "\n",
      "Epoch 03348: loss did not improve\n",
      "Epoch 3349/10000\n",
      "2700/2700 [==============================] - 0s 30us/step - loss: 2.8254e-04 - mean_absolute_error: 0.0107\n",
      "\n",
      "Epoch 03349: loss did not improve\n",
      "Epoch 3350/10000\n",
      "2700/2700 [==============================] - 0s 34us/step - loss: 2.7978e-04 - mean_absolute_error: 0.0106\n",
      "\n",
      "Epoch 03350: loss did not improve\n",
      "Epoch 3351/10000\n",
      "2700/2700 [==============================] - 0s 35us/step - loss: 2.7710e-04 - mean_absolute_error: 0.0106\n",
      "\n",
      "Epoch 03351: loss did not improve\n",
      "Epoch 3352/10000\n",
      "2700/2700 [==============================] - 0s 33us/step - loss: 2.7784e-04 - mean_absolute_error: 0.0104\n",
      "\n",
      "Epoch 03352: loss did not improve\n",
      "Epoch 3353/10000\n",
      "2700/2700 [==============================] - 0s 33us/step - loss: 2.8014e-04 - mean_absolute_error: 0.0107\n",
      "\n",
      "Epoch 03353: loss did not improve\n",
      "Epoch 3354/10000\n",
      "2700/2700 [==============================] - 0s 46us/step - loss: 2.9104e-04 - mean_absolute_error: 0.0111\n",
      "\n",
      "Epoch 03354: loss did not improve\n",
      "Epoch 3355/10000\n",
      "2700/2700 [==============================] - 0s 34us/step - loss: 2.7813e-04 - mean_absolute_error: 0.0106\n",
      "\n",
      "Epoch 03355: loss did not improve\n",
      "Epoch 3356/10000\n",
      "2700/2700 [==============================] - 0s 30us/step - loss: 2.7774e-04 - mean_absolute_error: 0.0105\n",
      "\n",
      "Epoch 03356: loss did not improve\n",
      "Epoch 3357/10000\n",
      "2700/2700 [==============================] - 0s 27us/step - loss: 2.8084e-04 - mean_absolute_error: 0.0108\n",
      "\n",
      "Epoch 03357: loss did not improve\n",
      "Epoch 3358/10000\n",
      "2700/2700 [==============================] - 0s 36us/step - loss: 2.8203e-04 - mean_absolute_error: 0.0106\n",
      "\n",
      "Epoch 03358: loss did not improve\n",
      "Epoch 3359/10000\n",
      "2700/2700 [==============================] - 0s 30us/step - loss: 2.7762e-04 - mean_absolute_error: 0.0106\n",
      "\n",
      "Epoch 03359: loss did not improve\n",
      "Epoch 3360/10000\n",
      "2700/2700 [==============================] - 0s 30us/step - loss: 2.7994e-04 - mean_absolute_error: 0.0106\n",
      "\n",
      "Epoch 03360: loss did not improve\n",
      "Epoch 3361/10000\n",
      "2700/2700 [==============================] - 0s 29us/step - loss: 2.8736e-04 - mean_absolute_error: 0.0108\n",
      "\n",
      "Epoch 03361: loss did not improve\n",
      "Epoch 3362/10000\n",
      "2700/2700 [==============================] - 0s 33us/step - loss: 2.7746e-04 - mean_absolute_error: 0.0105\n",
      "\n",
      "Epoch 03362: loss did not improve\n",
      "Epoch 3363/10000\n",
      "2700/2700 [==============================] - 0s 28us/step - loss: 2.7708e-04 - mean_absolute_error: 0.0106\n",
      "\n",
      "Epoch 03363: loss did not improve\n",
      "Epoch 3364/10000\n",
      "2700/2700 [==============================] - 0s 30us/step - loss: 2.7680e-04 - mean_absolute_error: 0.0105\n",
      "\n",
      "Epoch 03364: loss improved from 0.00028 to 0.00028, saving model to keras/best_baseline_weights.hdf5\n",
      "Epoch 3365/10000\n",
      "2700/2700 [==============================] - 0s 28us/step - loss: 2.7451e-04 - mean_absolute_error: 0.0104\n",
      "\n",
      "Epoch 03365: loss improved from 0.00028 to 0.00027, saving model to keras/best_baseline_weights.hdf5\n",
      "Epoch 3366/10000\n",
      "2700/2700 [==============================] - 0s 30us/step - loss: 2.7450e-04 - mean_absolute_error: 0.0104\n",
      "\n",
      "Epoch 03366: loss improved from 0.00027 to 0.00027, saving model to keras/best_baseline_weights.hdf5\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 3367/10000\n",
      "2700/2700 [==============================] - 0s 31us/step - loss: 2.7493e-04 - mean_absolute_error: 0.0105\n",
      "\n",
      "Epoch 03367: loss did not improve\n",
      "Epoch 3368/10000\n",
      "2700/2700 [==============================] - 0s 29us/step - loss: 2.7403e-04 - mean_absolute_error: 0.0105\n",
      "\n",
      "Epoch 03368: loss improved from 0.00027 to 0.00027, saving model to keras/best_baseline_weights.hdf5\n",
      "Epoch 3369/10000\n",
      "2700/2700 [==============================] - 0s 30us/step - loss: 2.7350e-04 - mean_absolute_error: 0.0104\n",
      "\n",
      "Epoch 03369: loss improved from 0.00027 to 0.00027, saving model to keras/best_baseline_weights.hdf5\n",
      "Epoch 3370/10000\n",
      "2700/2700 [==============================] - 0s 29us/step - loss: 2.7535e-04 - mean_absolute_error: 0.0104\n",
      "\n",
      "Epoch 03370: loss did not improve\n",
      "Epoch 3371/10000\n",
      "2700/2700 [==============================] - 0s 27us/step - loss: 2.7593e-04 - mean_absolute_error: 0.0106\n",
      "\n",
      "Epoch 03371: loss did not improve\n",
      "Epoch 3372/10000\n",
      "2700/2700 [==============================] - 0s 28us/step - loss: 2.7750e-04 - mean_absolute_error: 0.0105\n",
      "\n",
      "Epoch 03372: loss did not improve\n",
      "Epoch 3373/10000\n",
      "2700/2700 [==============================] - 0s 29us/step - loss: 2.7690e-04 - mean_absolute_error: 0.0105\n",
      "\n",
      "Epoch 03373: loss did not improve\n",
      "Epoch 3374/10000\n",
      "2700/2700 [==============================] - 0s 28us/step - loss: 2.7405e-04 - mean_absolute_error: 0.0105\n",
      "\n",
      "Epoch 03374: loss did not improve\n",
      "Epoch 3375/10000\n",
      "2700/2700 [==============================] - 0s 28us/step - loss: 2.8263e-04 - mean_absolute_error: 0.0107\n",
      "\n",
      "Epoch 03375: loss did not improve\n",
      "Epoch 3376/10000\n",
      "2700/2700 [==============================] - 0s 28us/step - loss: 2.7708e-04 - mean_absolute_error: 0.0105\n",
      "\n",
      "Epoch 03376: loss did not improve\n",
      "Epoch 3377/10000\n",
      "2700/2700 [==============================] - 0s 29us/step - loss: 2.7275e-04 - mean_absolute_error: 0.0104\n",
      "\n",
      "Epoch 03377: loss improved from 0.00027 to 0.00027, saving model to keras/best_baseline_weights.hdf5\n",
      "Epoch 3378/10000\n",
      "2700/2700 [==============================] - 0s 32us/step - loss: 2.7594e-04 - mean_absolute_error: 0.0105\n",
      "\n",
      "Epoch 03378: loss did not improve\n",
      "Epoch 3379/10000\n",
      "2700/2700 [==============================] - 0s 30us/step - loss: 2.7263e-04 - mean_absolute_error: 0.0104\n",
      "\n",
      "Epoch 03379: loss improved from 0.00027 to 0.00027, saving model to keras/best_baseline_weights.hdf5\n",
      "Epoch 3380/10000\n",
      "2700/2700 [==============================] - 0s 29us/step - loss: 2.7972e-04 - mean_absolute_error: 0.0106\n",
      "\n",
      "Epoch 03380: loss did not improve\n",
      "Epoch 3381/10000\n",
      "2700/2700 [==============================] - 0s 29us/step - loss: 2.7385e-04 - mean_absolute_error: 0.0104\n",
      "\n",
      "Epoch 03381: loss did not improve\n",
      "Epoch 3382/10000\n",
      "2700/2700 [==============================] - 0s 27us/step - loss: 2.7369e-04 - mean_absolute_error: 0.0104\n",
      "\n",
      "Epoch 03382: loss did not improve\n",
      "Epoch 3383/10000\n",
      "2700/2700 [==============================] - 0s 28us/step - loss: 2.7303e-04 - mean_absolute_error: 0.0103\n",
      "\n",
      "Epoch 03383: loss did not improve\n",
      "Epoch 3384/10000\n",
      "2700/2700 [==============================] - 0s 28us/step - loss: 2.7613e-04 - mean_absolute_error: 0.0107\n",
      "\n",
      "Epoch 03384: loss did not improve\n",
      "Epoch 3385/10000\n",
      "2700/2700 [==============================] - 0s 30us/step - loss: 2.8174e-04 - mean_absolute_error: 0.0108\n",
      "\n",
      "Epoch 03385: loss did not improve\n",
      "Epoch 3386/10000\n",
      "2700/2700 [==============================] - 0s 28us/step - loss: 2.7276e-04 - mean_absolute_error: 0.0105\n",
      "\n",
      "Epoch 03386: loss did not improve\n",
      "Epoch 3387/10000\n",
      "2700/2700 [==============================] - 0s 31us/step - loss: 2.7353e-04 - mean_absolute_error: 0.0105\n",
      "\n",
      "Epoch 03387: loss did not improve\n",
      "Epoch 3388/10000\n",
      "2700/2700 [==============================] - 0s 38us/step - loss: 2.7563e-04 - mean_absolute_error: 0.0105\n",
      "\n",
      "Epoch 03388: loss did not improve\n",
      "Epoch 3389/10000\n",
      "2700/2700 [==============================] - 0s 28us/step - loss: 2.7923e-04 - mean_absolute_error: 0.0106\n",
      "\n",
      "Epoch 03389: loss did not improve\n",
      "Epoch 3390/10000\n",
      "2700/2700 [==============================] - 0s 35us/step - loss: 2.7895e-04 - mean_absolute_error: 0.0107\n",
      "\n",
      "Epoch 03390: loss did not improve\n",
      "Epoch 3391/10000\n",
      "2700/2700 [==============================] - 0s 28us/step - loss: 2.7123e-04 - mean_absolute_error: 0.0104\n",
      "\n",
      "Epoch 03391: loss improved from 0.00027 to 0.00027, saving model to keras/best_baseline_weights.hdf5\n",
      "Epoch 3392/10000\n",
      "2700/2700 [==============================] - 0s 35us/step - loss: 2.7173e-04 - mean_absolute_error: 0.0104\n",
      "\n",
      "Epoch 03392: loss did not improve\n",
      "Epoch 3393/10000\n",
      "2700/2700 [==============================] - 0s 30us/step - loss: 2.7171e-04 - mean_absolute_error: 0.0104\n",
      "\n",
      "Epoch 03393: loss did not improve\n",
      "Epoch 3394/10000\n",
      "2700/2700 [==============================] - 0s 28us/step - loss: 2.6986e-04 - mean_absolute_error: 0.0103\n",
      "\n",
      "Epoch 03394: loss improved from 0.00027 to 0.00027, saving model to keras/best_baseline_weights.hdf5\n",
      "Epoch 3395/10000\n",
      "2700/2700 [==============================] - 0s 29us/step - loss: 2.7047e-04 - mean_absolute_error: 0.0104\n",
      "\n",
      "Epoch 03395: loss did not improve\n",
      "Epoch 3396/10000\n",
      "2700/2700 [==============================] - 0s 47us/step - loss: 2.6884e-04 - mean_absolute_error: 0.0103\n",
      "\n",
      "Epoch 03396: loss improved from 0.00027 to 0.00027, saving model to keras/best_baseline_weights.hdf5\n",
      "Epoch 3397/10000\n",
      "2700/2700 [==============================] - 0s 34us/step - loss: 2.7126e-04 - mean_absolute_error: 0.0104\n",
      "\n",
      "Epoch 03397: loss did not improve\n",
      "Epoch 3398/10000\n",
      "2700/2700 [==============================] - 0s 29us/step - loss: 2.7464e-04 - mean_absolute_error: 0.0104\n",
      "\n",
      "Epoch 03398: loss did not improve\n",
      "Epoch 3399/10000\n",
      "2700/2700 [==============================] - 0s 29us/step - loss: 2.7067e-04 - mean_absolute_error: 0.0104\n",
      "\n",
      "Epoch 03399: loss did not improve\n",
      "Epoch 3400/10000\n",
      "2700/2700 [==============================] - 0s 28us/step - loss: 2.7017e-04 - mean_absolute_error: 0.0103\n",
      "\n",
      "Epoch 03400: loss did not improve\n",
      "Epoch 3401/10000\n",
      "2700/2700 [==============================] - 0s 33us/step - loss: 2.7213e-04 - mean_absolute_error: 0.0105\n",
      "\n",
      "Epoch 03401: loss did not improve\n",
      "Epoch 3402/10000\n",
      "2700/2700 [==============================] - 0s 30us/step - loss: 2.7244e-04 - mean_absolute_error: 0.0105\n",
      "\n",
      "Epoch 03402: loss did not improve\n",
      "Epoch 3403/10000\n",
      "2700/2700 [==============================] - 0s 30us/step - loss: 2.7388e-04 - mean_absolute_error: 0.0105\n",
      "\n",
      "Epoch 03403: loss did not improve\n",
      "Epoch 3404/10000\n",
      "2700/2700 [==============================] - 0s 29us/step - loss: 2.6975e-04 - mean_absolute_error: 0.0102\n",
      "\n",
      "Epoch 03404: loss did not improve\n",
      "Epoch 3405/10000\n",
      "2700/2700 [==============================] - 0s 29us/step - loss: 2.6851e-04 - mean_absolute_error: 0.0103\n",
      "\n",
      "Epoch 03405: loss improved from 0.00027 to 0.00027, saving model to keras/best_baseline_weights.hdf5\n",
      "Epoch 3406/10000\n",
      "2700/2700 [==============================] - 0s 31us/step - loss: 2.7166e-04 - mean_absolute_error: 0.0103\n",
      "\n",
      "Epoch 03406: loss did not improve\n",
      "Epoch 3407/10000\n",
      "2700/2700 [==============================] - 0s 28us/step - loss: 2.7213e-04 - mean_absolute_error: 0.0105\n",
      "\n",
      "Epoch 03407: loss did not improve\n",
      "Epoch 3408/10000\n",
      "2700/2700 [==============================] - 0s 28us/step - loss: 2.6626e-04 - mean_absolute_error: 0.0102\n",
      "\n",
      "Epoch 03408: loss improved from 0.00027 to 0.00027, saving model to keras/best_baseline_weights.hdf5\n",
      "Epoch 3409/10000\n",
      "2700/2700 [==============================] - 0s 30us/step - loss: 2.6769e-04 - mean_absolute_error: 0.0103\n",
      "\n",
      "Epoch 03409: loss did not improve\n",
      "Epoch 3410/10000\n",
      "2700/2700 [==============================] - 0s 27us/step - loss: 2.6745e-04 - mean_absolute_error: 0.0103\n",
      "\n",
      "Epoch 03410: loss did not improve\n",
      "Epoch 3411/10000\n",
      "2700/2700 [==============================] - 0s 29us/step - loss: 2.7398e-04 - mean_absolute_error: 0.0106\n",
      "\n",
      "Epoch 03411: loss did not improve\n",
      "Epoch 3412/10000\n",
      "2700/2700 [==============================] - 0s 28us/step - loss: 2.7368e-04 - mean_absolute_error: 0.0105\n",
      "\n",
      "Epoch 03412: loss did not improve\n",
      "Epoch 3413/10000\n",
      "2700/2700 [==============================] - 0s 30us/step - loss: 2.6993e-04 - mean_absolute_error: 0.0103\n",
      "\n",
      "Epoch 03413: loss did not improve\n",
      "Epoch 3414/10000\n",
      "2700/2700 [==============================] - 0s 32us/step - loss: 2.6755e-04 - mean_absolute_error: 0.0104\n",
      "\n",
      "Epoch 03414: loss did not improve\n",
      "Epoch 3415/10000\n",
      "2700/2700 [==============================] - 0s 29us/step - loss: 2.7315e-04 - mean_absolute_error: 0.0105\n",
      "\n",
      "Epoch 03415: loss did not improve\n",
      "Epoch 3416/10000\n",
      "2700/2700 [==============================] - 0s 31us/step - loss: 2.6609e-04 - mean_absolute_error: 0.0101\n",
      "\n",
      "Epoch 03416: loss improved from 0.00027 to 0.00027, saving model to keras/best_baseline_weights.hdf5\n",
      "Epoch 3417/10000\n",
      "2700/2700 [==============================] - 0s 32us/step - loss: 2.7236e-04 - mean_absolute_error: 0.0105\n",
      "\n",
      "Epoch 03417: loss did not improve\n",
      "Epoch 3418/10000\n",
      "2700/2700 [==============================] - 0s 29us/step - loss: 2.7001e-04 - mean_absolute_error: 0.0104\n",
      "\n",
      "Epoch 03418: loss did not improve\n",
      "Epoch 3419/10000\n",
      "2700/2700 [==============================] - 0s 31us/step - loss: 2.6723e-04 - mean_absolute_error: 0.0104\n",
      "\n",
      "Epoch 03419: loss did not improve\n",
      "Epoch 3420/10000\n",
      "2700/2700 [==============================] - 0s 29us/step - loss: 2.6913e-04 - mean_absolute_error: 0.0103\n",
      "\n",
      "Epoch 03420: loss did not improve\n",
      "Epoch 3421/10000\n",
      "2700/2700 [==============================] - 0s 26us/step - loss: 2.6441e-04 - mean_absolute_error: 0.0102\n",
      "\n",
      "Epoch 03421: loss improved from 0.00027 to 0.00026, saving model to keras/best_baseline_weights.hdf5\n",
      "Epoch 3422/10000\n",
      "2700/2700 [==============================] - 0s 30us/step - loss: 2.6661e-04 - mean_absolute_error: 0.0103\n",
      "\n",
      "Epoch 03422: loss did not improve\n",
      "Epoch 3423/10000\n",
      "2700/2700 [==============================] - 0s 28us/step - loss: 2.6830e-04 - mean_absolute_error: 0.0103\n",
      "\n",
      "Epoch 03423: loss did not improve\n",
      "Epoch 3424/10000\n",
      "2700/2700 [==============================] - 0s 29us/step - loss: 2.6568e-04 - mean_absolute_error: 0.0102\n",
      "\n",
      "Epoch 03424: loss did not improve\n",
      "Epoch 3425/10000\n",
      "2700/2700 [==============================] - 0s 30us/step - loss: 2.6537e-04 - mean_absolute_error: 0.0102\n",
      "\n",
      "Epoch 03425: loss did not improve\n",
      "Epoch 3426/10000\n",
      "2700/2700 [==============================] - 0s 28us/step - loss: 2.7132e-04 - mean_absolute_error: 0.0104\n",
      "\n",
      "Epoch 03426: loss did not improve\n",
      "Epoch 3427/10000\n",
      "2700/2700 [==============================] - 0s 27us/step - loss: 2.6837e-04 - mean_absolute_error: 0.0102\n",
      "\n",
      "Epoch 03427: loss did not improve\n",
      "Epoch 3428/10000\n",
      "2700/2700 [==============================] - 0s 29us/step - loss: 2.6745e-04 - mean_absolute_error: 0.0103\n",
      "\n",
      "Epoch 03428: loss did not improve\n",
      "Epoch 3429/10000\n",
      "2700/2700 [==============================] - 0s 28us/step - loss: 2.6445e-04 - mean_absolute_error: 0.0102\n",
      "\n",
      "Epoch 03429: loss did not improve\n",
      "Epoch 3430/10000\n",
      "2700/2700 [==============================] - 0s 27us/step - loss: 2.6730e-04 - mean_absolute_error: 0.0103\n",
      "\n",
      "Epoch 03430: loss did not improve\n",
      "Epoch 3431/10000\n",
      "2700/2700 [==============================] - 0s 29us/step - loss: 2.6356e-04 - mean_absolute_error: 0.0101\n",
      "\n",
      "Epoch 03431: loss improved from 0.00026 to 0.00026, saving model to keras/best_baseline_weights.hdf5\n",
      "Epoch 3432/10000\n",
      "2700/2700 [==============================] - 0s 29us/step - loss: 2.6403e-04 - mean_absolute_error: 0.0102\n",
      "\n",
      "Epoch 03432: loss did not improve\n",
      "Epoch 3433/10000\n",
      "2700/2700 [==============================] - 0s 32us/step - loss: 2.6468e-04 - mean_absolute_error: 0.0102\n",
      "\n",
      "Epoch 03433: loss did not improve\n",
      "Epoch 3434/10000\n",
      "2700/2700 [==============================] - 0s 28us/step - loss: 2.6374e-04 - mean_absolute_error: 0.0102\n",
      "\n",
      "Epoch 03434: loss did not improve\n",
      "Epoch 3435/10000\n",
      "2700/2700 [==============================] - 0s 27us/step - loss: 2.6741e-04 - mean_absolute_error: 0.0103\n",
      "\n",
      "Epoch 03435: loss did not improve\n",
      "Epoch 3436/10000\n",
      "2700/2700 [==============================] - 0s 29us/step - loss: 2.6825e-04 - mean_absolute_error: 0.0104\n",
      "\n",
      "Epoch 03436: loss did not improve\n",
      "Epoch 3437/10000\n",
      "2700/2700 [==============================] - 0s 31us/step - loss: 2.6625e-04 - mean_absolute_error: 0.0103\n",
      "\n",
      "Epoch 03437: loss did not improve\n",
      "Epoch 3438/10000\n",
      "2700/2700 [==============================] - 0s 28us/step - loss: 2.6189e-04 - mean_absolute_error: 0.0100\n",
      "\n",
      "Epoch 03438: loss improved from 0.00026 to 0.00026, saving model to keras/best_baseline_weights.hdf5\n",
      "Epoch 3439/10000\n",
      "2700/2700 [==============================] - 0s 31us/step - loss: 2.6477e-04 - mean_absolute_error: 0.0102\n",
      "\n",
      "Epoch 03439: loss did not improve\n",
      "Epoch 3440/10000\n",
      "2700/2700 [==============================] - 0s 26us/step - loss: 2.6484e-04 - mean_absolute_error: 0.0102\n",
      "\n",
      "Epoch 03440: loss did not improve\n",
      "Epoch 3441/10000\n",
      "2700/2700 [==============================] - 0s 28us/step - loss: 2.6613e-04 - mean_absolute_error: 0.0103\n",
      "\n",
      "Epoch 03441: loss did not improve\n",
      "Epoch 3442/10000\n",
      "2700/2700 [==============================] - 0s 30us/step - loss: 2.6057e-04 - mean_absolute_error: 0.0101\n",
      "\n",
      "Epoch 03442: loss improved from 0.00026 to 0.00026, saving model to keras/best_baseline_weights.hdf5\n",
      "Epoch 3443/10000\n",
      "2700/2700 [==============================] - 0s 29us/step - loss: 2.6365e-04 - mean_absolute_error: 0.0101\n",
      "\n",
      "Epoch 03443: loss did not improve\n",
      "Epoch 3444/10000\n",
      "2700/2700 [==============================] - 0s 27us/step - loss: 2.6537e-04 - mean_absolute_error: 0.0102\n",
      "\n",
      "Epoch 03444: loss did not improve\n",
      "Epoch 3445/10000\n",
      "2700/2700 [==============================] - 0s 29us/step - loss: 2.6581e-04 - mean_absolute_error: 0.0102\n",
      "\n",
      "Epoch 03445: loss did not improve\n",
      "Epoch 3446/10000\n",
      "2700/2700 [==============================] - 0s 30us/step - loss: 2.6218e-04 - mean_absolute_error: 0.0101\n",
      "\n",
      "Epoch 03446: loss did not improve\n",
      "Epoch 3447/10000\n",
      "2700/2700 [==============================] - 0s 30us/step - loss: 2.6216e-04 - mean_absolute_error: 0.0101\n",
      "\n",
      "Epoch 03447: loss did not improve\n",
      "Epoch 3448/10000\n",
      "2700/2700 [==============================] - 0s 30us/step - loss: 2.6244e-04 - mean_absolute_error: 0.0101\n",
      "\n",
      "Epoch 03448: loss did not improve\n",
      "Epoch 3449/10000\n",
      "2700/2700 [==============================] - 0s 31us/step - loss: 2.6390e-04 - mean_absolute_error: 0.0102\n",
      "\n",
      "Epoch 03449: loss did not improve\n",
      "Epoch 3450/10000\n",
      "2700/2700 [==============================] - 0s 29us/step - loss: 2.6518e-04 - mean_absolute_error: 0.0103\n",
      "\n",
      "Epoch 03450: loss did not improve\n",
      "Epoch 3451/10000\n",
      "2700/2700 [==============================] - 0s 28us/step - loss: 2.6673e-04 - mean_absolute_error: 0.0103\n",
      "\n",
      "Epoch 03451: loss did not improve\n",
      "Epoch 3452/10000\n",
      "2700/2700 [==============================] - 0s 29us/step - loss: 2.6132e-04 - mean_absolute_error: 0.0100\n",
      "\n",
      "Epoch 03452: loss did not improve\n",
      "Epoch 3453/10000\n",
      "2700/2700 [==============================] - 0s 30us/step - loss: 2.5946e-04 - mean_absolute_error: 0.0101\n",
      "\n",
      "Epoch 03453: loss improved from 0.00026 to 0.00026, saving model to keras/best_baseline_weights.hdf5\n",
      "Epoch 3454/10000\n",
      "2700/2700 [==============================] - 0s 31us/step - loss: 2.5895e-04 - mean_absolute_error: 0.0100\n",
      "\n",
      "Epoch 03454: loss improved from 0.00026 to 0.00026, saving model to keras/best_baseline_weights.hdf5\n",
      "Epoch 3455/10000\n",
      "2700/2700 [==============================] - 0s 32us/step - loss: 2.6358e-04 - mean_absolute_error: 0.0103\n",
      "\n",
      "Epoch 03455: loss did not improve\n",
      "Epoch 3456/10000\n",
      "2700/2700 [==============================] - 0s 28us/step - loss: 2.5970e-04 - mean_absolute_error: 0.0100\n",
      "\n",
      "Epoch 03456: loss did not improve\n",
      "Epoch 3457/10000\n",
      "2700/2700 [==============================] - 0s 29us/step - loss: 2.6276e-04 - mean_absolute_error: 0.0101\n",
      "\n",
      "Epoch 03457: loss did not improve\n",
      "Epoch 3458/10000\n",
      "2700/2700 [==============================] - 0s 30us/step - loss: 2.6341e-04 - mean_absolute_error: 0.0102\n",
      "\n",
      "Epoch 03458: loss did not improve\n",
      "Epoch 3459/10000\n",
      "2700/2700 [==============================] - 0s 28us/step - loss: 2.5968e-04 - mean_absolute_error: 0.0101\n",
      "\n",
      "Epoch 03459: loss did not improve\n",
      "Epoch 3460/10000\n",
      "2700/2700 [==============================] - 0s 28us/step - loss: 2.5918e-04 - mean_absolute_error: 0.0100\n",
      "\n",
      "Epoch 03460: loss did not improve\n",
      "Epoch 3461/10000\n",
      "2700/2700 [==============================] - 0s 31us/step - loss: 2.6943e-04 - mean_absolute_error: 0.0104\n",
      "\n",
      "Epoch 03461: loss did not improve\n",
      "Epoch 3462/10000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2700/2700 [==============================] - 0s 30us/step - loss: 2.5908e-04 - mean_absolute_error: 0.0100\n",
      "\n",
      "Epoch 03462: loss did not improve\n",
      "Epoch 3463/10000\n",
      "2700/2700 [==============================] - 0s 31us/step - loss: 2.6117e-04 - mean_absolute_error: 0.0101\n",
      "\n",
      "Epoch 03463: loss did not improve\n",
      "Epoch 3464/10000\n",
      "2700/2700 [==============================] - 0s 29us/step - loss: 2.6198e-04 - mean_absolute_error: 0.0101\n",
      "\n",
      "Epoch 03464: loss did not improve\n",
      "Epoch 3465/10000\n",
      "2700/2700 [==============================] - 0s 30us/step - loss: 2.6157e-04 - mean_absolute_error: 0.0101\n",
      "\n",
      "Epoch 03465: loss did not improve\n",
      "Epoch 3466/10000\n",
      "2700/2700 [==============================] - 0s 29us/step - loss: 2.5876e-04 - mean_absolute_error: 0.0100\n",
      "\n",
      "Epoch 03466: loss improved from 0.00026 to 0.00026, saving model to keras/best_baseline_weights.hdf5\n",
      "Epoch 3467/10000\n",
      "2700/2700 [==============================] - 0s 30us/step - loss: 2.5784e-04 - mean_absolute_error: 0.0100\n",
      "\n",
      "Epoch 03467: loss improved from 0.00026 to 0.00026, saving model to keras/best_baseline_weights.hdf5\n",
      "Epoch 3468/10000\n",
      "2700/2700 [==============================] - 0s 32us/step - loss: 2.6327e-04 - mean_absolute_error: 0.0102\n",
      "\n",
      "Epoch 03468: loss did not improve\n",
      "Epoch 3469/10000\n",
      "2700/2700 [==============================] - 0s 27us/step - loss: 2.6093e-04 - mean_absolute_error: 0.0102\n",
      "\n",
      "Epoch 03469: loss did not improve\n",
      "Epoch 3470/10000\n",
      "2700/2700 [==============================] - 0s 30us/step - loss: 2.6289e-04 - mean_absolute_error: 0.0102\n",
      "\n",
      "Epoch 03470: loss did not improve\n",
      "Epoch 3471/10000\n",
      "2700/2700 [==============================] - 0s 31us/step - loss: 2.5736e-04 - mean_absolute_error: 0.0100\n",
      "\n",
      "Epoch 03471: loss improved from 0.00026 to 0.00026, saving model to keras/best_baseline_weights.hdf5\n",
      "Epoch 3472/10000\n",
      "2700/2700 [==============================] - 0s 29us/step - loss: 2.6186e-04 - mean_absolute_error: 0.0102\n",
      "\n",
      "Epoch 03472: loss did not improve\n",
      "Epoch 3473/10000\n",
      "2700/2700 [==============================] - 0s 31us/step - loss: 2.5501e-04 - mean_absolute_error: 0.0099\n",
      "\n",
      "Epoch 03473: loss improved from 0.00026 to 0.00026, saving model to keras/best_baseline_weights.hdf5\n",
      "Epoch 3474/10000\n",
      "2700/2700 [==============================] - 0s 30us/step - loss: 2.5668e-04 - mean_absolute_error: 0.0099\n",
      "\n",
      "Epoch 03474: loss did not improve\n",
      "Epoch 3475/10000\n",
      "2700/2700 [==============================] - 0s 27us/step - loss: 2.5386e-04 - mean_absolute_error: 0.0099\n",
      "\n",
      "Epoch 03475: loss improved from 0.00026 to 0.00025, saving model to keras/best_baseline_weights.hdf5\n",
      "Epoch 3476/10000\n",
      "2700/2700 [==============================] - 0s 29us/step - loss: 2.5781e-04 - mean_absolute_error: 0.0099\n",
      "\n",
      "Epoch 03476: loss did not improve\n",
      "Epoch 3477/10000\n",
      "2700/2700 [==============================] - 0s 30us/step - loss: 2.5902e-04 - mean_absolute_error: 0.0101\n",
      "\n",
      "Epoch 03477: loss did not improve\n",
      "Epoch 3478/10000\n",
      "2700/2700 [==============================] - 0s 30us/step - loss: 2.5663e-04 - mean_absolute_error: 0.0100\n",
      "\n",
      "Epoch 03478: loss did not improve\n",
      "Epoch 3479/10000\n",
      "2700/2700 [==============================] - 0s 30us/step - loss: 2.5805e-04 - mean_absolute_error: 0.0101\n",
      "\n",
      "Epoch 03479: loss did not improve\n",
      "Epoch 3480/10000\n",
      "2700/2700 [==============================] - 0s 28us/step - loss: 2.5760e-04 - mean_absolute_error: 0.0101\n",
      "\n",
      "Epoch 03480: loss did not improve\n",
      "Epoch 3481/10000\n",
      "2700/2700 [==============================] - 0s 29us/step - loss: 2.5939e-04 - mean_absolute_error: 0.0101\n",
      "\n",
      "Epoch 03481: loss did not improve\n",
      "Epoch 3482/10000\n",
      "2700/2700 [==============================] - 0s 30us/step - loss: 2.5460e-04 - mean_absolute_error: 0.0098\n",
      "\n",
      "Epoch 03482: loss did not improve\n",
      "Epoch 3483/10000\n",
      "2700/2700 [==============================] - 0s 29us/step - loss: 2.5589e-04 - mean_absolute_error: 0.0100\n",
      "\n",
      "Epoch 03483: loss did not improve\n",
      "Epoch 3484/10000\n",
      "2700/2700 [==============================] - 0s 27us/step - loss: 2.5788e-04 - mean_absolute_error: 0.0101\n",
      "\n",
      "Epoch 03484: loss did not improve\n",
      "Epoch 3485/10000\n",
      "2700/2700 [==============================] - 0s 27us/step - loss: 2.5848e-04 - mean_absolute_error: 0.0101\n",
      "\n",
      "Epoch 03485: loss did not improve\n",
      "Epoch 3486/10000\n",
      "2700/2700 [==============================] - 0s 30us/step - loss: 2.5581e-04 - mean_absolute_error: 0.0100\n",
      "\n",
      "Epoch 03486: loss did not improve\n",
      "Epoch 3487/10000\n",
      "2700/2700 [==============================] - 0s 30us/step - loss: 2.5704e-04 - mean_absolute_error: 0.0099\n",
      "\n",
      "Epoch 03487: loss did not improve\n",
      "Epoch 3488/10000\n",
      "2700/2700 [==============================] - 0s 28us/step - loss: 2.5555e-04 - mean_absolute_error: 0.0100\n",
      "\n",
      "Epoch 03488: loss did not improve\n",
      "Epoch 3489/10000\n",
      "2700/2700 [==============================] - 0s 31us/step - loss: 2.5404e-04 - mean_absolute_error: 0.0099\n",
      "\n",
      "Epoch 03489: loss did not improve\n",
      "Epoch 3490/10000\n",
      "2700/2700 [==============================] - 0s 26us/step - loss: 2.5462e-04 - mean_absolute_error: 0.0099\n",
      "\n",
      "Epoch 03490: loss did not improve\n",
      "Epoch 3491/10000\n",
      "2700/2700 [==============================] - 0s 29us/step - loss: 2.5650e-04 - mean_absolute_error: 0.0100\n",
      "\n",
      "Epoch 03491: loss did not improve\n",
      "Epoch 3492/10000\n",
      "2700/2700 [==============================] - 0s 28us/step - loss: 2.5805e-04 - mean_absolute_error: 0.0100\n",
      "\n",
      "Epoch 03492: loss did not improve\n",
      "Epoch 3493/10000\n",
      "2700/2700 [==============================] - 0s 28us/step - loss: 2.5605e-04 - mean_absolute_error: 0.0099\n",
      "\n",
      "Epoch 03493: loss did not improve\n",
      "Epoch 3494/10000\n",
      "2700/2700 [==============================] - 0s 25us/step - loss: 2.5924e-04 - mean_absolute_error: 0.0103\n",
      "\n",
      "Epoch 03494: loss did not improve\n",
      "Epoch 3495/10000\n",
      "2700/2700 [==============================] - 0s 31us/step - loss: 2.5497e-04 - mean_absolute_error: 0.0099\n",
      "\n",
      "Epoch 03495: loss did not improve\n",
      "Epoch 3496/10000\n",
      "2700/2700 [==============================] - 0s 36us/step - loss: 2.5157e-04 - mean_absolute_error: 0.0098\n",
      "\n",
      "Epoch 03496: loss improved from 0.00025 to 0.00025, saving model to keras/best_baseline_weights.hdf5\n",
      "Epoch 3497/10000\n",
      "2700/2700 [==============================] - 0s 29us/step - loss: 2.5364e-04 - mean_absolute_error: 0.0100\n",
      "\n",
      "Epoch 03497: loss did not improve\n",
      "Epoch 3498/10000\n",
      "2700/2700 [==============================] - 0s 27us/step - loss: 2.5742e-04 - mean_absolute_error: 0.0101\n",
      "\n",
      "Epoch 03498: loss did not improve\n",
      "Epoch 3499/10000\n",
      "2700/2700 [==============================] - 0s 30us/step - loss: 2.5187e-04 - mean_absolute_error: 0.0098\n",
      "\n",
      "Epoch 03499: loss did not improve\n",
      "Epoch 3500/10000\n",
      "2700/2700 [==============================] - 0s 27us/step - loss: 2.5494e-04 - mean_absolute_error: 0.0099\n",
      "\n",
      "Epoch 03500: loss did not improve\n",
      "Epoch 3501/10000\n",
      "2700/2700 [==============================] - 0s 32us/step - loss: 2.5331e-04 - mean_absolute_error: 0.0099\n",
      "\n",
      "Epoch 03501: loss did not improve\n",
      "Epoch 3502/10000\n",
      "2700/2700 [==============================] - 0s 30us/step - loss: 2.5268e-04 - mean_absolute_error: 0.0098\n",
      "\n",
      "Epoch 03502: loss did not improve\n",
      "Epoch 3503/10000\n",
      "2700/2700 [==============================] - 0s 33us/step - loss: 2.5295e-04 - mean_absolute_error: 0.0099\n",
      "\n",
      "Epoch 03503: loss did not improve\n",
      "Epoch 3504/10000\n",
      "2700/2700 [==============================] - 0s 28us/step - loss: 2.5533e-04 - mean_absolute_error: 0.0100\n",
      "\n",
      "Epoch 03504: loss did not improve\n",
      "Epoch 3505/10000\n",
      "2700/2700 [==============================] - 0s 28us/step - loss: 2.5331e-04 - mean_absolute_error: 0.0099\n",
      "\n",
      "Epoch 03505: loss did not improve\n",
      "Epoch 3506/10000\n",
      "2700/2700 [==============================] - 0s 29us/step - loss: 2.5322e-04 - mean_absolute_error: 0.0099\n",
      "\n",
      "Epoch 03506: loss did not improve\n",
      "Epoch 3507/10000\n",
      "2700/2700 [==============================] - 0s 28us/step - loss: 2.5932e-04 - mean_absolute_error: 0.0102\n",
      "\n",
      "Epoch 03507: loss did not improve\n",
      "Epoch 3508/10000\n",
      "2700/2700 [==============================] - 0s 34us/step - loss: 2.5637e-04 - mean_absolute_error: 0.0100\n",
      "\n",
      "Epoch 03508: loss did not improve\n",
      "Epoch 3509/10000\n",
      "2700/2700 [==============================] - 0s 28us/step - loss: 2.4938e-04 - mean_absolute_error: 0.0098\n",
      "\n",
      "Epoch 03509: loss improved from 0.00025 to 0.00025, saving model to keras/best_baseline_weights.hdf5\n",
      "Epoch 3510/10000\n",
      "2700/2700 [==============================] - 0s 33us/step - loss: 2.5480e-04 - mean_absolute_error: 0.0101\n",
      "\n",
      "Epoch 03510: loss did not improve\n",
      "Epoch 3511/10000\n",
      "2700/2700 [==============================] - 0s 25us/step - loss: 2.5137e-04 - mean_absolute_error: 0.0099\n",
      "\n",
      "Epoch 03511: loss did not improve\n",
      "Epoch 3512/10000\n",
      "2700/2700 [==============================] - 0s 29us/step - loss: 2.5052e-04 - mean_absolute_error: 0.0098\n",
      "\n",
      "Epoch 03512: loss did not improve\n",
      "Epoch 3513/10000\n",
      "2700/2700 [==============================] - 0s 28us/step - loss: 2.4891e-04 - mean_absolute_error: 0.0098\n",
      "\n",
      "Epoch 03513: loss improved from 0.00025 to 0.00025, saving model to keras/best_baseline_weights.hdf5\n",
      "Epoch 3514/10000\n",
      "2700/2700 [==============================] - 0s 30us/step - loss: 2.4920e-04 - mean_absolute_error: 0.0098\n",
      "\n",
      "Epoch 03514: loss did not improve\n",
      "Epoch 3515/10000\n",
      "2700/2700 [==============================] - 0s 28us/step - loss: 2.5261e-04 - mean_absolute_error: 0.0099\n",
      "\n",
      "Epoch 03515: loss did not improve\n",
      "Epoch 3516/10000\n",
      "2700/2700 [==============================] - 0s 26us/step - loss: 2.4901e-04 - mean_absolute_error: 0.0098\n",
      "\n",
      "Epoch 03516: loss did not improve\n",
      "Epoch 3517/10000\n",
      "2700/2700 [==============================] - 0s 30us/step - loss: 2.5211e-04 - mean_absolute_error: 0.0099\n",
      "\n",
      "Epoch 03517: loss did not improve\n",
      "Epoch 3518/10000\n",
      "2700/2700 [==============================] - 0s 28us/step - loss: 2.4983e-04 - mean_absolute_error: 0.0098\n",
      "\n",
      "Epoch 03518: loss did not improve\n",
      "Epoch 3519/10000\n",
      "2700/2700 [==============================] - 0s 32us/step - loss: 2.5425e-04 - mean_absolute_error: 0.0101\n",
      "\n",
      "Epoch 03519: loss did not improve\n",
      "Epoch 3520/10000\n",
      "2700/2700 [==============================] - 0s 31us/step - loss: 2.4935e-04 - mean_absolute_error: 0.0098\n",
      "\n",
      "Epoch 03520: loss did not improve\n",
      "Epoch 3521/10000\n",
      "2700/2700 [==============================] - 0s 30us/step - loss: 2.5279e-04 - mean_absolute_error: 0.0099\n",
      "\n",
      "Epoch 03521: loss did not improve\n",
      "Epoch 3522/10000\n",
      "2700/2700 [==============================] - 0s 29us/step - loss: 2.5328e-04 - mean_absolute_error: 0.0099\n",
      "\n",
      "Epoch 03522: loss did not improve\n",
      "Epoch 3523/10000\n",
      "2700/2700 [==============================] - 0s 34us/step - loss: 2.5045e-04 - mean_absolute_error: 0.0099\n",
      "\n",
      "Epoch 03523: loss did not improve\n",
      "Epoch 3524/10000\n",
      "2700/2700 [==============================] - 0s 29us/step - loss: 2.5163e-04 - mean_absolute_error: 0.0098\n",
      "\n",
      "Epoch 03524: loss did not improve\n",
      "Epoch 3525/10000\n",
      "2700/2700 [==============================] - 0s 33us/step - loss: 2.5152e-04 - mean_absolute_error: 0.0099\n",
      "\n",
      "Epoch 03525: loss did not improve\n",
      "Epoch 3526/10000\n",
      "2700/2700 [==============================] - 0s 28us/step - loss: 2.5212e-04 - mean_absolute_error: 0.0099\n",
      "\n",
      "Epoch 03526: loss did not improve\n",
      "Epoch 3527/10000\n",
      "2700/2700 [==============================] - 0s 36us/step - loss: 2.5043e-04 - mean_absolute_error: 0.0098\n",
      "\n",
      "Epoch 03527: loss did not improve\n",
      "Epoch 3528/10000\n",
      "2700/2700 [==============================] - 0s 26us/step - loss: 2.4906e-04 - mean_absolute_error: 0.0098\n",
      "\n",
      "Epoch 03528: loss did not improve\n",
      "Epoch 3529/10000\n",
      "2700/2700 [==============================] - 0s 31us/step - loss: 2.5064e-04 - mean_absolute_error: 0.0098\n",
      "\n",
      "Epoch 03529: loss did not improve\n",
      "Epoch 3530/10000\n",
      "2700/2700 [==============================] - 0s 28us/step - loss: 2.4752e-04 - mean_absolute_error: 0.0097\n",
      "\n",
      "Epoch 03530: loss improved from 0.00025 to 0.00025, saving model to keras/best_baseline_weights.hdf5\n",
      "Epoch 3531/10000\n",
      "2700/2700 [==============================] - 0s 30us/step - loss: 2.4635e-04 - mean_absolute_error: 0.0097\n",
      "\n",
      "Epoch 03531: loss improved from 0.00025 to 0.00025, saving model to keras/best_baseline_weights.hdf5\n",
      "Epoch 3532/10000\n",
      "2700/2700 [==============================] - 0s 30us/step - loss: 2.4901e-04 - mean_absolute_error: 0.0098\n",
      "\n",
      "Epoch 03532: loss did not improve\n",
      "Epoch 3533/10000\n",
      "2700/2700 [==============================] - 0s 28us/step - loss: 2.4971e-04 - mean_absolute_error: 0.0098\n",
      "\n",
      "Epoch 03533: loss did not improve\n",
      "Epoch 3534/10000\n",
      "2700/2700 [==============================] - 0s 26us/step - loss: 2.4999e-04 - mean_absolute_error: 0.0100\n",
      "\n",
      "Epoch 03534: loss did not improve\n",
      "Epoch 3535/10000\n",
      "2700/2700 [==============================] - 0s 28us/step - loss: 2.5432e-04 - mean_absolute_error: 0.0100\n",
      "\n",
      "Epoch 03535: loss did not improve\n",
      "Epoch 3536/10000\n",
      "2700/2700 [==============================] - 0s 31us/step - loss: 2.5058e-04 - mean_absolute_error: 0.0099\n",
      "\n",
      "Epoch 03536: loss did not improve\n",
      "Epoch 3537/10000\n",
      "2700/2700 [==============================] - 0s 30us/step - loss: 2.4853e-04 - mean_absolute_error: 0.0098\n",
      "\n",
      "Epoch 03537: loss did not improve\n",
      "Epoch 3538/10000\n",
      "2700/2700 [==============================] - 0s 30us/step - loss: 2.4921e-04 - mean_absolute_error: 0.0098\n",
      "\n",
      "Epoch 03538: loss did not improve\n",
      "Epoch 3539/10000\n",
      "2700/2700 [==============================] - 0s 29us/step - loss: 2.4510e-04 - mean_absolute_error: 0.0097\n",
      "\n",
      "Epoch 03539: loss improved from 0.00025 to 0.00025, saving model to keras/best_baseline_weights.hdf5\n",
      "Epoch 3540/10000\n",
      "2700/2700 [==============================] - 0s 32us/step - loss: 2.4535e-04 - mean_absolute_error: 0.0097\n",
      "\n",
      "Epoch 03540: loss did not improve\n",
      "Epoch 3541/10000\n",
      "2700/2700 [==============================] - 0s 30us/step - loss: 2.4912e-04 - mean_absolute_error: 0.0098\n",
      "\n",
      "Epoch 03541: loss did not improve\n",
      "Epoch 3542/10000\n",
      "2700/2700 [==============================] - 0s 28us/step - loss: 2.4653e-04 - mean_absolute_error: 0.0097\n",
      "\n",
      "Epoch 03542: loss did not improve\n",
      "Epoch 3543/10000\n",
      "2700/2700 [==============================] - 0s 27us/step - loss: 2.4904e-04 - mean_absolute_error: 0.0099\n",
      "\n",
      "Epoch 03543: loss did not improve\n",
      "Epoch 3544/10000\n",
      "2700/2700 [==============================] - 0s 33us/step - loss: 2.4650e-04 - mean_absolute_error: 0.0097\n",
      "\n",
      "Epoch 03544: loss did not improve\n",
      "Epoch 3545/10000\n",
      "2700/2700 [==============================] - 0s 30us/step - loss: 2.5140e-04 - mean_absolute_error: 0.0100\n",
      "\n",
      "Epoch 03545: loss did not improve\n",
      "Epoch 3546/10000\n",
      "2700/2700 [==============================] - 0s 34us/step - loss: 2.4513e-04 - mean_absolute_error: 0.0096\n",
      "\n",
      "Epoch 03546: loss did not improve\n",
      "Epoch 3547/10000\n",
      "2700/2700 [==============================] - 0s 28us/step - loss: 2.4303e-04 - mean_absolute_error: 0.0097\n",
      "\n",
      "Epoch 03547: loss improved from 0.00025 to 0.00024, saving model to keras/best_baseline_weights.hdf5\n",
      "Epoch 3548/10000\n",
      "2700/2700 [==============================] - 0s 34us/step - loss: 2.4911e-04 - mean_absolute_error: 0.0098\n",
      "\n",
      "Epoch 03548: loss did not improve\n",
      "Epoch 3549/10000\n",
      "2700/2700 [==============================] - 0s 29us/step - loss: 2.4709e-04 - mean_absolute_error: 0.0098\n",
      "\n",
      "Epoch 03549: loss did not improve\n",
      "Epoch 3550/10000\n",
      "2700/2700 [==============================] - 0s 32us/step - loss: 2.4565e-04 - mean_absolute_error: 0.0097\n",
      "\n",
      "Epoch 03550: loss did not improve\n",
      "Epoch 3551/10000\n",
      "2700/2700 [==============================] - 0s 27us/step - loss: 2.4468e-04 - mean_absolute_error: 0.0097\n",
      "\n",
      "Epoch 03551: loss did not improve\n",
      "Epoch 3552/10000\n",
      "2700/2700 [==============================] - 0s 33us/step - loss: 2.4438e-04 - mean_absolute_error: 0.0096\n",
      "\n",
      "Epoch 03552: loss did not improve\n",
      "Epoch 3553/10000\n",
      "2700/2700 [==============================] - 0s 32us/step - loss: 2.4818e-04 - mean_absolute_error: 0.0097\n",
      "\n",
      "Epoch 03553: loss did not improve\n",
      "Epoch 3554/10000\n",
      "2700/2700 [==============================] - 0s 30us/step - loss: 2.4586e-04 - mean_absolute_error: 0.0097\n",
      "\n",
      "Epoch 03554: loss did not improve\n",
      "Epoch 3555/10000\n",
      "2700/2700 [==============================] - 0s 34us/step - loss: 2.4367e-04 - mean_absolute_error: 0.0096\n",
      "\n",
      "Epoch 03555: loss did not improve\n",
      "Epoch 3556/10000\n",
      "2700/2700 [==============================] - 0s 41us/step - loss: 2.4656e-04 - mean_absolute_error: 0.0097\n",
      "\n",
      "Epoch 03556: loss did not improve\n",
      "Epoch 3557/10000\n",
      "2700/2700 [==============================] - 0s 33us/step - loss: 2.4353e-04 - mean_absolute_error: 0.0096\n",
      "\n",
      "Epoch 03557: loss did not improve\n",
      "Epoch 3558/10000\n",
      "2700/2700 [==============================] - 0s 29us/step - loss: 2.4284e-04 - mean_absolute_error: 0.0095\n",
      "\n",
      "Epoch 03558: loss improved from 0.00024 to 0.00024, saving model to keras/best_baseline_weights.hdf5\n",
      "Epoch 3559/10000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2700/2700 [==============================] - 0s 31us/step - loss: 2.4224e-04 - mean_absolute_error: 0.0096\n",
      "\n",
      "Epoch 03559: loss improved from 0.00024 to 0.00024, saving model to keras/best_baseline_weights.hdf5\n",
      "Epoch 3560/10000\n",
      "2700/2700 [==============================] - 0s 28us/step - loss: 2.4516e-04 - mean_absolute_error: 0.0097\n",
      "\n",
      "Epoch 03560: loss did not improve\n",
      "Epoch 3561/10000\n",
      "2700/2700 [==============================] - 0s 32us/step - loss: 2.4321e-04 - mean_absolute_error: 0.0095\n",
      "\n",
      "Epoch 03561: loss did not improve\n",
      "Epoch 3562/10000\n",
      "2700/2700 [==============================] - 0s 30us/step - loss: 2.4485e-04 - mean_absolute_error: 0.0096\n",
      "\n",
      "Epoch 03562: loss did not improve\n",
      "Epoch 3563/10000\n",
      "2700/2700 [==============================] - 0s 29us/step - loss: 2.4393e-04 - mean_absolute_error: 0.0097\n",
      "\n",
      "Epoch 03563: loss did not improve\n",
      "Epoch 3564/10000\n",
      "2700/2700 [==============================] - 0s 30us/step - loss: 2.4131e-04 - mean_absolute_error: 0.0095\n",
      "\n",
      "Epoch 03564: loss improved from 0.00024 to 0.00024, saving model to keras/best_baseline_weights.hdf5\n",
      "Epoch 3565/10000\n",
      "2700/2700 [==============================] - 0s 30us/step - loss: 2.4188e-04 - mean_absolute_error: 0.0095\n",
      "\n",
      "Epoch 03565: loss did not improve\n",
      "Epoch 3566/10000\n",
      "2700/2700 [==============================] - 0s 35us/step - loss: 2.4706e-04 - mean_absolute_error: 0.0098\n",
      "\n",
      "Epoch 03566: loss did not improve\n",
      "Epoch 3567/10000\n",
      "2700/2700 [==============================] - 0s 27us/step - loss: 2.4672e-04 - mean_absolute_error: 0.0097\n",
      "\n",
      "Epoch 03567: loss did not improve\n",
      "Epoch 3568/10000\n",
      "2700/2700 [==============================] - 0s 32us/step - loss: 2.4170e-04 - mean_absolute_error: 0.0096\n",
      "\n",
      "Epoch 03568: loss did not improve\n",
      "Epoch 3569/10000\n",
      "2700/2700 [==============================] - 0s 34us/step - loss: 2.4459e-04 - mean_absolute_error: 0.0097\n",
      "\n",
      "Epoch 03569: loss did not improve\n",
      "Epoch 3570/10000\n",
      "2700/2700 [==============================] - 0s 37us/step - loss: 2.4301e-04 - mean_absolute_error: 0.0096\n",
      "\n",
      "Epoch 03570: loss did not improve\n",
      "Epoch 3571/10000\n",
      "2700/2700 [==============================] - 0s 35us/step - loss: 2.4036e-04 - mean_absolute_error: 0.0095\n",
      "\n",
      "Epoch 03571: loss improved from 0.00024 to 0.00024, saving model to keras/best_baseline_weights.hdf5\n",
      "Epoch 3572/10000\n",
      "2700/2700 [==============================] - 0s 29us/step - loss: 2.4510e-04 - mean_absolute_error: 0.0097\n",
      "\n",
      "Epoch 03572: loss did not improve\n",
      "Epoch 3573/10000\n",
      "2700/2700 [==============================] - 0s 29us/step - loss: 2.4455e-04 - mean_absolute_error: 0.0097\n",
      "\n",
      "Epoch 03573: loss did not improve\n",
      "Epoch 3574/10000\n",
      "2700/2700 [==============================] - 0s 28us/step - loss: 2.4346e-04 - mean_absolute_error: 0.0096\n",
      "\n",
      "Epoch 03574: loss did not improve\n",
      "Epoch 3575/10000\n",
      "2700/2700 [==============================] - 0s 28us/step - loss: 2.4087e-04 - mean_absolute_error: 0.0096\n",
      "\n",
      "Epoch 03575: loss did not improve\n",
      "Epoch 3576/10000\n",
      "2700/2700 [==============================] - 0s 36us/step - loss: 2.4240e-04 - mean_absolute_error: 0.0095\n",
      "\n",
      "Epoch 03576: loss did not improve\n",
      "Epoch 3577/10000\n",
      "2700/2700 [==============================] - 0s 29us/step - loss: 2.4324e-04 - mean_absolute_error: 0.0096\n",
      "\n",
      "Epoch 03577: loss did not improve\n",
      "Epoch 3578/10000\n",
      "2700/2700 [==============================] - 0s 29us/step - loss: 2.4249e-04 - mean_absolute_error: 0.0097\n",
      "\n",
      "Epoch 03578: loss did not improve\n",
      "Epoch 3579/10000\n",
      "2700/2700 [==============================] - 0s 31us/step - loss: 2.4293e-04 - mean_absolute_error: 0.0097\n",
      "\n",
      "Epoch 03579: loss did not improve\n",
      "Epoch 3580/10000\n",
      "2700/2700 [==============================] - 0s 25us/step - loss: 2.4370e-04 - mean_absolute_error: 0.0097\n",
      "\n",
      "Epoch 03580: loss did not improve\n",
      "Epoch 3581/10000\n",
      "2700/2700 [==============================] - 0s 27us/step - loss: 2.3985e-04 - mean_absolute_error: 0.0096\n",
      "\n",
      "Epoch 03581: loss improved from 0.00024 to 0.00024, saving model to keras/best_baseline_weights.hdf5\n",
      "Epoch 3582/10000\n",
      "2700/2700 [==============================] - 0s 29us/step - loss: 2.4235e-04 - mean_absolute_error: 0.0097\n",
      "\n",
      "Epoch 03582: loss did not improve\n",
      "Epoch 3583/10000\n",
      "2700/2700 [==============================] - 0s 29us/step - loss: 2.4030e-04 - mean_absolute_error: 0.0095\n",
      "\n",
      "Epoch 03583: loss did not improve\n",
      "Epoch 3584/10000\n",
      "2700/2700 [==============================] - 0s 29us/step - loss: 2.4103e-04 - mean_absolute_error: 0.0096\n",
      "\n",
      "Epoch 03584: loss did not improve\n",
      "Epoch 3585/10000\n",
      "2700/2700 [==============================] - 0s 30us/step - loss: 2.4537e-04 - mean_absolute_error: 0.0098\n",
      "\n",
      "Epoch 03585: loss did not improve\n",
      "Epoch 3586/10000\n",
      "2700/2700 [==============================] - 0s 28us/step - loss: 2.4306e-04 - mean_absolute_error: 0.0097\n",
      "\n",
      "Epoch 03586: loss did not improve\n",
      "Epoch 3587/10000\n",
      "2700/2700 [==============================] - 0s 27us/step - loss: 2.4097e-04 - mean_absolute_error: 0.0095\n",
      "\n",
      "Epoch 03587: loss did not improve\n",
      "Epoch 3588/10000\n",
      "2700/2700 [==============================] - 0s 30us/step - loss: 2.5287e-04 - mean_absolute_error: 0.0102\n",
      "\n",
      "Epoch 03588: loss did not improve\n",
      "Epoch 3589/10000\n",
      "2700/2700 [==============================] - 0s 27us/step - loss: 2.3994e-04 - mean_absolute_error: 0.0095\n",
      "\n",
      "Epoch 03589: loss did not improve\n",
      "Epoch 3590/10000\n",
      "2700/2700 [==============================] - 0s 26us/step - loss: 2.4130e-04 - mean_absolute_error: 0.0096\n",
      "\n",
      "Epoch 03590: loss did not improve\n",
      "Epoch 3591/10000\n",
      "2700/2700 [==============================] - 0s 29us/step - loss: 2.3845e-04 - mean_absolute_error: 0.0095\n",
      "\n",
      "Epoch 03591: loss improved from 0.00024 to 0.00024, saving model to keras/best_baseline_weights.hdf5\n",
      "Epoch 3592/10000\n",
      "2700/2700 [==============================] - 0s 29us/step - loss: 2.3924e-04 - mean_absolute_error: 0.0095\n",
      "\n",
      "Epoch 03592: loss did not improve\n",
      "Epoch 3593/10000\n",
      "2700/2700 [==============================] - 0s 28us/step - loss: 2.3632e-04 - mean_absolute_error: 0.0094\n",
      "\n",
      "Epoch 03593: loss improved from 0.00024 to 0.00024, saving model to keras/best_baseline_weights.hdf5\n",
      "Epoch 3594/10000\n",
      "2700/2700 [==============================] - 0s 26us/step - loss: 2.4212e-04 - mean_absolute_error: 0.0097\n",
      "\n",
      "Epoch 03594: loss did not improve\n",
      "Epoch 3595/10000\n",
      "2700/2700 [==============================] - 0s 27us/step - loss: 2.3789e-04 - mean_absolute_error: 0.0095\n",
      "\n",
      "Epoch 03595: loss did not improve\n",
      "Epoch 3596/10000\n",
      "2700/2700 [==============================] - 0s 28us/step - loss: 2.4281e-04 - mean_absolute_error: 0.0097\n",
      "\n",
      "Epoch 03596: loss did not improve\n",
      "Epoch 3597/10000\n",
      "2700/2700 [==============================] - 0s 29us/step - loss: 2.3938e-04 - mean_absolute_error: 0.0095\n",
      "\n",
      "Epoch 03597: loss did not improve\n",
      "Epoch 3598/10000\n",
      "2700/2700 [==============================] - 0s 30us/step - loss: 2.4435e-04 - mean_absolute_error: 0.0097\n",
      "\n",
      "Epoch 03598: loss did not improve\n",
      "Epoch 3599/10000\n",
      "2700/2700 [==============================] - 0s 26us/step - loss: 2.4289e-04 - mean_absolute_error: 0.0097\n",
      "\n",
      "Epoch 03599: loss did not improve\n",
      "Epoch 3600/10000\n",
      "2700/2700 [==============================] - 0s 29us/step - loss: 2.3740e-04 - mean_absolute_error: 0.0095\n",
      "\n",
      "Epoch 03600: loss did not improve\n",
      "Epoch 3601/10000\n",
      "2700/2700 [==============================] - 0s 30us/step - loss: 2.3948e-04 - mean_absolute_error: 0.0095\n",
      "\n",
      "Epoch 03601: loss did not improve\n",
      "Epoch 3602/10000\n",
      "2700/2700 [==============================] - 0s 30us/step - loss: 2.3796e-04 - mean_absolute_error: 0.0095\n",
      "\n",
      "Epoch 03602: loss did not improve\n",
      "Epoch 3603/10000\n",
      "2700/2700 [==============================] - 0s 30us/step - loss: 2.4078e-04 - mean_absolute_error: 0.0095\n",
      "\n",
      "Epoch 03603: loss did not improve\n",
      "Epoch 3604/10000\n",
      "2700/2700 [==============================] - 0s 27us/step - loss: 2.3948e-04 - mean_absolute_error: 0.0096\n",
      "\n",
      "Epoch 03604: loss did not improve\n",
      "Epoch 3605/10000\n",
      "2700/2700 [==============================] - 0s 29us/step - loss: 2.3707e-04 - mean_absolute_error: 0.0095\n",
      "\n",
      "Epoch 03605: loss did not improve\n",
      "Epoch 3606/10000\n",
      "2700/2700 [==============================] - 0s 30us/step - loss: 2.3880e-04 - mean_absolute_error: 0.0095\n",
      "\n",
      "Epoch 03606: loss did not improve\n",
      "Epoch 3607/10000\n",
      "2700/2700 [==============================] - 0s 30us/step - loss: 2.3789e-04 - mean_absolute_error: 0.0095\n",
      "\n",
      "Epoch 03607: loss did not improve\n",
      "Epoch 3608/10000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2700/2700 [==============================] - 0s 25us/step - loss: 2.3725e-04 - mean_absolute_error: 0.0094\n",
      "\n",
      "Epoch 03608: loss did not improve\n",
      "Epoch 3609/10000\n",
      "2700/2700 [==============================] - 0s 29us/step - loss: 2.4047e-04 - mean_absolute_error: 0.0096\n",
      "\n",
      "Epoch 03609: loss did not improve\n",
      "Epoch 3610/10000\n",
      "2700/2700 [==============================] - 0s 31us/step - loss: 2.3995e-04 - mean_absolute_error: 0.0097\n",
      "\n",
      "Epoch 03610: loss did not improve\n",
      "Epoch 3611/10000\n",
      "2700/2700 [==============================] - 0s 30us/step - loss: 2.3844e-04 - mean_absolute_error: 0.0096\n",
      "\n",
      "Epoch 03611: loss did not improve\n",
      "Epoch 3612/10000\n",
      "2700/2700 [==============================] - 0s 29us/step - loss: 2.4551e-04 - mean_absolute_error: 0.0099\n",
      "\n",
      "Epoch 03612: loss did not improve\n",
      "Epoch 3613/10000\n",
      "2700/2700 [==============================] - 0s 27us/step - loss: 2.3828e-04 - mean_absolute_error: 0.0096\n",
      "\n",
      "Epoch 03613: loss did not improve\n",
      "Epoch 3614/10000\n",
      "2700/2700 [==============================] - 0s 28us/step - loss: 2.3761e-04 - mean_absolute_error: 0.0095\n",
      "\n",
      "Epoch 03614: loss did not improve\n",
      "Epoch 3615/10000\n",
      "2700/2700 [==============================] - 0s 30us/step - loss: 2.3671e-04 - mean_absolute_error: 0.0095\n",
      "\n",
      "Epoch 03615: loss did not improve\n",
      "Epoch 3616/10000\n",
      "2700/2700 [==============================] - 0s 28us/step - loss: 2.3561e-04 - mean_absolute_error: 0.0094\n",
      "\n",
      "Epoch 03616: loss improved from 0.00024 to 0.00024, saving model to keras/best_baseline_weights.hdf5\n",
      "Epoch 3617/10000\n",
      "2700/2700 [==============================] - 0s 30us/step - loss: 2.3553e-04 - mean_absolute_error: 0.0095\n",
      "\n",
      "Epoch 03617: loss improved from 0.00024 to 0.00024, saving model to keras/best_baseline_weights.hdf5\n",
      "Epoch 3618/10000\n",
      "2700/2700 [==============================] - 0s 28us/step - loss: 2.3820e-04 - mean_absolute_error: 0.0095\n",
      "\n",
      "Epoch 03618: loss did not improve\n",
      "Epoch 3619/10000\n",
      "2700/2700 [==============================] - 0s 27us/step - loss: 2.3787e-04 - mean_absolute_error: 0.0095\n",
      "\n",
      "Epoch 03619: loss did not improve\n",
      "Epoch 3620/10000\n",
      "2700/2700 [==============================] - 0s 29us/step - loss: 2.3622e-04 - mean_absolute_error: 0.0095: 0s - loss: 2.2032e-04 - mean_absolute_error: 0.009\n",
      "\n",
      "Epoch 03620: loss did not improve\n",
      "Epoch 3621/10000\n",
      "2700/2700 [==============================] - 0s 28us/step - loss: 2.3541e-04 - mean_absolute_error: 0.0095\n",
      "\n",
      "Epoch 03621: loss improved from 0.00024 to 0.00024, saving model to keras/best_baseline_weights.hdf5\n",
      "Epoch 3622/10000\n",
      "2700/2700 [==============================] - 0s 27us/step - loss: 2.4109e-04 - mean_absolute_error: 0.0097\n",
      "\n",
      "Epoch 03622: loss did not improve\n",
      "Epoch 3623/10000\n",
      "2700/2700 [==============================] - 0s 28us/step - loss: 2.3447e-04 - mean_absolute_error: 0.0094\n",
      "\n",
      "Epoch 03623: loss improved from 0.00024 to 0.00023, saving model to keras/best_baseline_weights.hdf5\n",
      "Epoch 3624/10000\n",
      "2700/2700 [==============================] - 0s 30us/step - loss: 2.3392e-04 - mean_absolute_error: 0.0093\n",
      "\n",
      "Epoch 03624: loss improved from 0.00023 to 0.00023, saving model to keras/best_baseline_weights.hdf5\n",
      "Epoch 3625/10000\n",
      "2700/2700 [==============================] - 0s 28us/step - loss: 2.3410e-04 - mean_absolute_error: 0.0094\n",
      "\n",
      "Epoch 03625: loss did not improve\n",
      "Epoch 3626/10000\n",
      "2700/2700 [==============================] - 0s 29us/step - loss: 2.3500e-04 - mean_absolute_error: 0.0093\n",
      "\n",
      "Epoch 03626: loss did not improve\n",
      "Epoch 3627/10000\n",
      "2700/2700 [==============================] - 0s 28us/step - loss: 2.3852e-04 - mean_absolute_error: 0.0095\n",
      "\n",
      "Epoch 03627: loss did not improve\n",
      "Epoch 3628/10000\n",
      "2700/2700 [==============================] - 0s 28us/step - loss: 2.3673e-04 - mean_absolute_error: 0.0095\n",
      "\n",
      "Epoch 03628: loss did not improve\n",
      "Epoch 3629/10000\n",
      "2700/2700 [==============================] - 0s 26us/step - loss: 2.3824e-04 - mean_absolute_error: 0.0096\n",
      "\n",
      "Epoch 03629: loss did not improve\n",
      "Epoch 3630/10000\n",
      "2700/2700 [==============================] - 0s 28us/step - loss: 2.3922e-04 - mean_absolute_error: 0.0097\n",
      "\n",
      "Epoch 03630: loss did not improve\n",
      "Epoch 3631/10000\n",
      "2700/2700 [==============================] - 0s 28us/step - loss: 2.3391e-04 - mean_absolute_error: 0.0093\n",
      "\n",
      "Epoch 03631: loss improved from 0.00023 to 0.00023, saving model to keras/best_baseline_weights.hdf5\n",
      "Epoch 3632/10000\n",
      "2700/2700 [==============================] - 0s 28us/step - loss: 2.3377e-04 - mean_absolute_error: 0.0093\n",
      "\n",
      "Epoch 03632: loss improved from 0.00023 to 0.00023, saving model to keras/best_baseline_weights.hdf5\n",
      "Epoch 3633/10000\n",
      "2700/2700 [==============================] - 0s 29us/step - loss: 2.3263e-04 - mean_absolute_error: 0.0093\n",
      "\n",
      "Epoch 03633: loss improved from 0.00023 to 0.00023, saving model to keras/best_baseline_weights.hdf5\n",
      "Epoch 3634/10000\n",
      "2700/2700 [==============================] - 0s 27us/step - loss: 2.3725e-04 - mean_absolute_error: 0.0095\n",
      "\n",
      "Epoch 03634: loss did not improve\n",
      "Epoch 3635/10000\n",
      "2700/2700 [==============================] - 0s 27us/step - loss: 2.3921e-04 - mean_absolute_error: 0.0096\n",
      "\n",
      "Epoch 03635: loss did not improve\n",
      "Epoch 3636/10000\n",
      "2700/2700 [==============================] - 0s 31us/step - loss: 2.3476e-04 - mean_absolute_error: 0.0094\n",
      "\n",
      "Epoch 03636: loss did not improve\n",
      "Epoch 3637/10000\n",
      "2700/2700 [==============================] - 0s 32us/step - loss: 2.3420e-04 - mean_absolute_error: 0.0094\n",
      "\n",
      "Epoch 03637: loss did not improve\n",
      "Epoch 3638/10000\n",
      "2700/2700 [==============================] - 0s 31us/step - loss: 2.3159e-04 - mean_absolute_error: 0.0093\n",
      "\n",
      "Epoch 03638: loss improved from 0.00023 to 0.00023, saving model to keras/best_baseline_weights.hdf5\n",
      "Epoch 3639/10000\n",
      "2700/2700 [==============================] - 0s 29us/step - loss: 2.3369e-04 - mean_absolute_error: 0.0095\n",
      "\n",
      "Epoch 03639: loss did not improve\n",
      "Epoch 3640/10000\n",
      "2700/2700 [==============================] - 0s 37us/step - loss: 2.3279e-04 - mean_absolute_error: 0.0093\n",
      "\n",
      "Epoch 03640: loss did not improve\n",
      "Epoch 3641/10000\n",
      "2700/2700 [==============================] - 0s 27us/step - loss: 2.4009e-04 - mean_absolute_error: 0.0096\n",
      "\n",
      "Epoch 03641: loss did not improve\n",
      "Epoch 3642/10000\n",
      "2700/2700 [==============================] - 0s 31us/step - loss: 2.3627e-04 - mean_absolute_error: 0.0095\n",
      "\n",
      "Epoch 03642: loss did not improve\n",
      "Epoch 3643/10000\n",
      "2700/2700 [==============================] - 0s 32us/step - loss: 2.3055e-04 - mean_absolute_error: 0.0092\n",
      "\n",
      "Epoch 03643: loss improved from 0.00023 to 0.00023, saving model to keras/best_baseline_weights.hdf5\n",
      "Epoch 3644/10000\n",
      "2700/2700 [==============================] - 0s 32us/step - loss: 2.3357e-04 - mean_absolute_error: 0.0094\n",
      "\n",
      "Epoch 03644: loss did not improve\n",
      "Epoch 3645/10000\n",
      "2700/2700 [==============================] - 0s 28us/step - loss: 2.3321e-04 - mean_absolute_error: 0.0094\n",
      "\n",
      "Epoch 03645: loss did not improve\n",
      "Epoch 3646/10000\n",
      "2700/2700 [==============================] - 0s 31us/step - loss: 2.3120e-04 - mean_absolute_error: 0.0093\n",
      "\n",
      "Epoch 03646: loss did not improve\n",
      "Epoch 3647/10000\n",
      "2700/2700 [==============================] - 0s 28us/step - loss: 2.3298e-04 - mean_absolute_error: 0.0094\n",
      "\n",
      "Epoch 03647: loss did not improve\n",
      "Epoch 3648/10000\n",
      "2700/2700 [==============================] - 0s 28us/step - loss: 2.3119e-04 - mean_absolute_error: 0.0093\n",
      "\n",
      "Epoch 03648: loss did not improve\n",
      "Epoch 3649/10000\n",
      "2700/2700 [==============================] - 0s 30us/step - loss: 2.3084e-04 - mean_absolute_error: 0.0093\n",
      "\n",
      "Epoch 03649: loss did not improve\n",
      "Epoch 3650/10000\n",
      "2700/2700 [==============================] - 0s 30us/step - loss: 2.3236e-04 - mean_absolute_error: 0.0093\n",
      "\n",
      "Epoch 03650: loss did not improve\n",
      "Epoch 3651/10000\n",
      "2700/2700 [==============================] - 0s 34us/step - loss: 2.3001e-04 - mean_absolute_error: 0.0093\n",
      "\n",
      "Epoch 03651: loss improved from 0.00023 to 0.00023, saving model to keras/best_baseline_weights.hdf5\n",
      "Epoch 3652/10000\n",
      "2700/2700 [==============================] - 0s 30us/step - loss: 2.2992e-04 - mean_absolute_error: 0.0093\n",
      "\n",
      "Epoch 03652: loss improved from 0.00023 to 0.00023, saving model to keras/best_baseline_weights.hdf5\n",
      "Epoch 3653/10000\n",
      "2700/2700 [==============================] - 0s 29us/step - loss: 2.3489e-04 - mean_absolute_error: 0.0095\n",
      "\n",
      "Epoch 03653: loss did not improve\n",
      "Epoch 3654/10000\n",
      "2700/2700 [==============================] - 0s 27us/step - loss: 2.3381e-04 - mean_absolute_error: 0.0094\n",
      "\n",
      "Epoch 03654: loss did not improve\n",
      "Epoch 3655/10000\n",
      "2700/2700 [==============================] - 0s 29us/step - loss: 2.3216e-04 - mean_absolute_error: 0.0093\n",
      "\n",
      "Epoch 03655: loss did not improve\n",
      "Epoch 3656/10000\n",
      "2700/2700 [==============================] - 0s 31us/step - loss: 2.2991e-04 - mean_absolute_error: 0.0093\n",
      "\n",
      "Epoch 03656: loss improved from 0.00023 to 0.00023, saving model to keras/best_baseline_weights.hdf5\n",
      "Epoch 3657/10000\n",
      "2700/2700 [==============================] - 0s 30us/step - loss: 2.3328e-04 - mean_absolute_error: 0.0094\n",
      "\n",
      "Epoch 03657: loss did not improve\n",
      "Epoch 3658/10000\n",
      "2700/2700 [==============================] - 0s 28us/step - loss: 2.3138e-04 - mean_absolute_error: 0.0093\n",
      "\n",
      "Epoch 03658: loss did not improve\n",
      "Epoch 3659/10000\n",
      "2700/2700 [==============================] - 0s 30us/step - loss: 2.3567e-04 - mean_absolute_error: 0.0095\n",
      "\n",
      "Epoch 03659: loss did not improve\n",
      "Epoch 3660/10000\n",
      "2700/2700 [==============================] - 0s 28us/step - loss: 2.3087e-04 - mean_absolute_error: 0.0093\n",
      "\n",
      "Epoch 03660: loss did not improve\n",
      "Epoch 3661/10000\n",
      "2700/2700 [==============================] - 0s 27us/step - loss: 2.3013e-04 - mean_absolute_error: 0.0092\n",
      "\n",
      "Epoch 03661: loss did not improve\n",
      "Epoch 3662/10000\n",
      "2700/2700 [==============================] - 0s 29us/step - loss: 2.2840e-04 - mean_absolute_error: 0.0093\n",
      "\n",
      "Epoch 03662: loss improved from 0.00023 to 0.00023, saving model to keras/best_baseline_weights.hdf5\n",
      "Epoch 3663/10000\n",
      "2700/2700 [==============================] - 0s 30us/step - loss: 2.3473e-04 - mean_absolute_error: 0.0094\n",
      "\n",
      "Epoch 03663: loss did not improve\n",
      "Epoch 3664/10000\n",
      "2700/2700 [==============================] - 0s 27us/step - loss: 2.2917e-04 - mean_absolute_error: 0.0092\n",
      "\n",
      "Epoch 03664: loss did not improve\n",
      "Epoch 3665/10000\n",
      "2700/2700 [==============================] - 0s 31us/step - loss: 2.3122e-04 - mean_absolute_error: 0.0093\n",
      "\n",
      "Epoch 03665: loss did not improve\n",
      "Epoch 3666/10000\n",
      "2700/2700 [==============================] - 0s 29us/step - loss: 2.2859e-04 - mean_absolute_error: 0.0092\n",
      "\n",
      "Epoch 03666: loss did not improve\n",
      "Epoch 3667/10000\n",
      "2700/2700 [==============================] - 0s 32us/step - loss: 2.3391e-04 - mean_absolute_error: 0.0096\n",
      "\n",
      "Epoch 03667: loss did not improve\n",
      "Epoch 3668/10000\n",
      "2700/2700 [==============================] - 0s 31us/step - loss: 2.2914e-04 - mean_absolute_error: 0.0092\n",
      "\n",
      "Epoch 03668: loss did not improve\n",
      "Epoch 3669/10000\n",
      "2700/2700 [==============================] - 0s 28us/step - loss: 2.3047e-04 - mean_absolute_error: 0.0093\n",
      "\n",
      "Epoch 03669: loss did not improve\n",
      "Epoch 3670/10000\n",
      "2700/2700 [==============================] - 0s 28us/step - loss: 2.3110e-04 - mean_absolute_error: 0.0094\n",
      "\n",
      "Epoch 03670: loss did not improve\n",
      "Epoch 3671/10000\n",
      "2700/2700 [==============================] - 0s 32us/step - loss: 2.3748e-04 - mean_absolute_error: 0.0096\n",
      "\n",
      "Epoch 03671: loss did not improve\n",
      "Epoch 3672/10000\n",
      "2700/2700 [==============================] - 0s 30us/step - loss: 2.3096e-04 - mean_absolute_error: 0.0093\n",
      "\n",
      "Epoch 03672: loss did not improve\n",
      "Epoch 3673/10000\n",
      "2700/2700 [==============================] - 0s 30us/step - loss: 2.2851e-04 - mean_absolute_error: 0.0093\n",
      "\n",
      "Epoch 03673: loss did not improve\n",
      "Epoch 3674/10000\n",
      "2700/2700 [==============================] - 0s 28us/step - loss: 2.3214e-04 - mean_absolute_error: 0.0095\n",
      "\n",
      "Epoch 03674: loss did not improve\n",
      "Epoch 3675/10000\n",
      "2700/2700 [==============================] - 0s 29us/step - loss: 2.3083e-04 - mean_absolute_error: 0.0093\n",
      "\n",
      "Epoch 03675: loss did not improve\n",
      "Epoch 3676/10000\n",
      "2700/2700 [==============================] - 0s 29us/step - loss: 2.3327e-04 - mean_absolute_error: 0.0094\n",
      "\n",
      "Epoch 03676: loss did not improve\n",
      "Epoch 3677/10000\n",
      "2700/2700 [==============================] - 0s 28us/step - loss: 2.2786e-04 - mean_absolute_error: 0.0092\n",
      "\n",
      "Epoch 03677: loss improved from 0.00023 to 0.00023, saving model to keras/best_baseline_weights.hdf5\n",
      "Epoch 3678/10000\n",
      "2700/2700 [==============================] - 0s 29us/step - loss: 2.2692e-04 - mean_absolute_error: 0.0092\n",
      "\n",
      "Epoch 03678: loss improved from 0.00023 to 0.00023, saving model to keras/best_baseline_weights.hdf5\n",
      "Epoch 3679/10000\n",
      "2700/2700 [==============================] - 0s 29us/step - loss: 2.2818e-04 - mean_absolute_error: 0.0093\n",
      "\n",
      "Epoch 03679: loss did not improve\n",
      "Epoch 3680/10000\n",
      "2700/2700 [==============================] - 0s 30us/step - loss: 2.3137e-04 - mean_absolute_error: 0.0094\n",
      "\n",
      "Epoch 03680: loss did not improve\n",
      "Epoch 3681/10000\n",
      "2700/2700 [==============================] - 0s 32us/step - loss: 2.2837e-04 - mean_absolute_error: 0.0094\n",
      "\n",
      "Epoch 03681: loss did not improve\n",
      "Epoch 3682/10000\n",
      "2700/2700 [==============================] - 0s 30us/step - loss: 2.2545e-04 - mean_absolute_error: 0.0091\n",
      "\n",
      "Epoch 03682: loss improved from 0.00023 to 0.00023, saving model to keras/best_baseline_weights.hdf5\n",
      "Epoch 3683/10000\n",
      "2700/2700 [==============================] - 0s 29us/step - loss: 2.2727e-04 - mean_absolute_error: 0.0092\n",
      "\n",
      "Epoch 03683: loss did not improve\n",
      "Epoch 3684/10000\n",
      "2700/2700 [==============================] - 0s 28us/step - loss: 2.2994e-04 - mean_absolute_error: 0.0093\n",
      "\n",
      "Epoch 03684: loss did not improve\n",
      "Epoch 3685/10000\n",
      "2700/2700 [==============================] - 0s 31us/step - loss: 2.2784e-04 - mean_absolute_error: 0.0093\n",
      "\n",
      "Epoch 03685: loss did not improve\n",
      "Epoch 3686/10000\n",
      "2700/2700 [==============================] - 0s 28us/step - loss: 2.2889e-04 - mean_absolute_error: 0.0092\n",
      "\n",
      "Epoch 03686: loss did not improve\n",
      "Epoch 3687/10000\n",
      "2700/2700 [==============================] - 0s 30us/step - loss: 2.2665e-04 - mean_absolute_error: 0.0092\n",
      "\n",
      "Epoch 03687: loss did not improve\n",
      "Epoch 3688/10000\n",
      "2700/2700 [==============================] - 0s 29us/step - loss: 2.2889e-04 - mean_absolute_error: 0.0094\n",
      "\n",
      "Epoch 03688: loss did not improve\n",
      "Epoch 3689/10000\n",
      "2700/2700 [==============================] - 0s 31us/step - loss: 2.2827e-04 - mean_absolute_error: 0.0093\n",
      "\n",
      "Epoch 03689: loss did not improve\n",
      "Epoch 3690/10000\n",
      "2700/2700 [==============================] - 0s 28us/step - loss: 2.2822e-04 - mean_absolute_error: 0.0092\n",
      "\n",
      "Epoch 03690: loss did not improve\n",
      "Epoch 3691/10000\n",
      "2700/2700 [==============================] - 0s 27us/step - loss: 2.2688e-04 - mean_absolute_error: 0.0092\n",
      "\n",
      "Epoch 03691: loss did not improve\n",
      "Epoch 3692/10000\n",
      "2700/2700 [==============================] - 0s 29us/step - loss: 2.2717e-04 - mean_absolute_error: 0.0093\n",
      "\n",
      "Epoch 03692: loss did not improve\n",
      "Epoch 3693/10000\n",
      "2700/2700 [==============================] - 0s 29us/step - loss: 2.2632e-04 - mean_absolute_error: 0.0091\n",
      "\n",
      "Epoch 03693: loss did not improve\n",
      "Epoch 3694/10000\n",
      "2700/2700 [==============================] - 0s 29us/step - loss: 2.2605e-04 - mean_absolute_error: 0.0093\n",
      "\n",
      "Epoch 03694: loss did not improve\n",
      "Epoch 3695/10000\n",
      "2700/2700 [==============================] - 0s 33us/step - loss: 2.3134e-04 - mean_absolute_error: 0.0094\n",
      "\n",
      "Epoch 03695: loss did not improve\n",
      "Epoch 3696/10000\n",
      "2700/2700 [==============================] - 0s 34us/step - loss: 2.2791e-04 - mean_absolute_error: 0.0093\n",
      "\n",
      "Epoch 03696: loss did not improve\n",
      "Epoch 3697/10000\n",
      "2700/2700 [==============================] - 0s 37us/step - loss: 2.2619e-04 - mean_absolute_error: 0.0092\n",
      "\n",
      "Epoch 03697: loss did not improve\n",
      "Epoch 3698/10000\n",
      "2700/2700 [==============================] - 0s 39us/step - loss: 2.2599e-04 - mean_absolute_error: 0.0092\n",
      "\n",
      "Epoch 03698: loss did not improve\n",
      "Epoch 3699/10000\n",
      "2700/2700 [==============================] - 0s 36us/step - loss: 2.2732e-04 - mean_absolute_error: 0.0093\n",
      "\n",
      "Epoch 03699: loss did not improve\n",
      "Epoch 3700/10000\n",
      "2700/2700 [==============================] - 0s 30us/step - loss: 2.2558e-04 - mean_absolute_error: 0.0091\n",
      "\n",
      "Epoch 03700: loss did not improve\n",
      "Epoch 3701/10000\n",
      "2700/2700 [==============================] - 0s 30us/step - loss: 2.2809e-04 - mean_absolute_error: 0.0093\n",
      "\n",
      "Epoch 03701: loss did not improve\n",
      "Epoch 3702/10000\n",
      "2700/2700 [==============================] - 0s 30us/step - loss: 2.2661e-04 - mean_absolute_error: 0.0092\n",
      "\n",
      "Epoch 03702: loss did not improve\n",
      "Epoch 3703/10000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2700/2700 [==============================] - 0s 33us/step - loss: 2.3801e-04 - mean_absolute_error: 0.0098\n",
      "\n",
      "Epoch 03703: loss did not improve\n",
      "Epoch 3704/10000\n",
      "2700/2700 [==============================] - 0s 31us/step - loss: 2.2668e-04 - mean_absolute_error: 0.0093\n",
      "\n",
      "Epoch 03704: loss did not improve\n",
      "Epoch 3705/10000\n",
      "2700/2700 [==============================] - 0s 32us/step - loss: 2.2773e-04 - mean_absolute_error: 0.0093\n",
      "\n",
      "Epoch 03705: loss did not improve\n",
      "Epoch 3706/10000\n",
      "2700/2700 [==============================] - 0s 27us/step - loss: 2.2702e-04 - mean_absolute_error: 0.0092\n",
      "\n",
      "Epoch 03706: loss did not improve\n",
      "Epoch 3707/10000\n",
      "2700/2700 [==============================] - 0s 30us/step - loss: 2.2501e-04 - mean_absolute_error: 0.0092\n",
      "\n",
      "Epoch 03707: loss improved from 0.00023 to 0.00023, saving model to keras/best_baseline_weights.hdf5\n",
      "Epoch 3708/10000\n",
      "2700/2700 [==============================] - 0s 29us/step - loss: 2.2602e-04 - mean_absolute_error: 0.0092\n",
      "\n",
      "Epoch 03708: loss did not improve\n",
      "Epoch 3709/10000\n",
      "2700/2700 [==============================] - 0s 29us/step - loss: 2.2513e-04 - mean_absolute_error: 0.0092\n",
      "\n",
      "Epoch 03709: loss did not improve\n",
      "Epoch 3710/10000\n",
      "2700/2700 [==============================] - 0s 29us/step - loss: 2.2589e-04 - mean_absolute_error: 0.0092\n",
      "\n",
      "Epoch 03710: loss did not improve\n",
      "Epoch 3711/10000\n",
      "2700/2700 [==============================] - 0s 26us/step - loss: 2.2889e-04 - mean_absolute_error: 0.0093\n",
      "\n",
      "Epoch 03711: loss did not improve\n",
      "Epoch 3712/10000\n",
      "2700/2700 [==============================] - 0s 29us/step - loss: 2.2626e-04 - mean_absolute_error: 0.0093\n",
      "\n",
      "Epoch 03712: loss did not improve\n",
      "Epoch 3713/10000\n",
      "2700/2700 [==============================] - 0s 28us/step - loss: 2.2355e-04 - mean_absolute_error: 0.0091\n",
      "\n",
      "Epoch 03713: loss improved from 0.00023 to 0.00022, saving model to keras/best_baseline_weights.hdf5\n",
      "Epoch 3714/10000\n",
      "2700/2700 [==============================] - 0s 29us/step - loss: 2.2659e-04 - mean_absolute_error: 0.0093\n",
      "\n",
      "Epoch 03714: loss did not improve\n",
      "Epoch 3715/10000\n",
      "2700/2700 [==============================] - 0s 29us/step - loss: 2.3098e-04 - mean_absolute_error: 0.0094\n",
      "\n",
      "Epoch 03715: loss did not improve\n",
      "Epoch 3716/10000\n",
      "2700/2700 [==============================] - 0s 27us/step - loss: 2.2185e-04 - mean_absolute_error: 0.0091\n",
      "\n",
      "Epoch 03716: loss improved from 0.00022 to 0.00022, saving model to keras/best_baseline_weights.hdf5\n",
      "Epoch 3717/10000\n",
      "2700/2700 [==============================] - 0s 29us/step - loss: 2.2464e-04 - mean_absolute_error: 0.0092\n",
      "\n",
      "Epoch 03717: loss did not improve\n",
      "Epoch 3718/10000\n",
      "2700/2700 [==============================] - 0s 38us/step - loss: 2.2151e-04 - mean_absolute_error: 0.0091\n",
      "\n",
      "Epoch 03718: loss improved from 0.00022 to 0.00022, saving model to keras/best_baseline_weights.hdf5\n",
      "Epoch 3719/10000\n",
      "2700/2700 [==============================] - 0s 34us/step - loss: 2.2477e-04 - mean_absolute_error: 0.0091\n",
      "\n",
      "Epoch 03719: loss did not improve\n",
      "Epoch 3720/10000\n",
      "2700/2700 [==============================] - 0s 31us/step - loss: 2.2501e-04 - mean_absolute_error: 0.0092\n",
      "\n",
      "Epoch 03720: loss did not improve\n",
      "Epoch 3721/10000\n",
      "2700/2700 [==============================] - 0s 37us/step - loss: 2.2373e-04 - mean_absolute_error: 0.0092\n",
      "\n",
      "Epoch 03721: loss did not improve\n",
      "Epoch 3722/10000\n",
      "2700/2700 [==============================] - 0s 33us/step - loss: 2.2554e-04 - mean_absolute_error: 0.0092\n",
      "\n",
      "Epoch 03722: loss did not improve\n",
      "Epoch 3723/10000\n",
      "2700/2700 [==============================] - 0s 29us/step - loss: 2.2327e-04 - mean_absolute_error: 0.0092\n",
      "\n",
      "Epoch 03723: loss did not improve\n",
      "Epoch 3724/10000\n",
      "2700/2700 [==============================] - 0s 36us/step - loss: 2.2284e-04 - mean_absolute_error: 0.0092\n",
      "\n",
      "Epoch 03724: loss did not improve\n",
      "Epoch 3725/10000\n",
      "2700/2700 [==============================] - 0s 35us/step - loss: 2.2083e-04 - mean_absolute_error: 0.0090\n",
      "\n",
      "Epoch 03725: loss improved from 0.00022 to 0.00022, saving model to keras/best_baseline_weights.hdf5\n",
      "Epoch 3726/10000\n",
      "2700/2700 [==============================] - 0s 33us/step - loss: 2.2352e-04 - mean_absolute_error: 0.0092\n",
      "\n",
      "Epoch 03726: loss did not improve\n",
      "Epoch 3727/10000\n",
      "2700/2700 [==============================] - 0s 28us/step - loss: 2.2093e-04 - mean_absolute_error: 0.0090\n",
      "\n",
      "Epoch 03727: loss did not improve\n",
      "Epoch 3728/10000\n",
      "2700/2700 [==============================] - 0s 33us/step - loss: 2.2206e-04 - mean_absolute_error: 0.0091\n",
      "\n",
      "Epoch 03728: loss did not improve\n",
      "Epoch 3729/10000\n",
      "2700/2700 [==============================] - 0s 33us/step - loss: 2.2006e-04 - mean_absolute_error: 0.0090\n",
      "\n",
      "Epoch 03729: loss improved from 0.00022 to 0.00022, saving model to keras/best_baseline_weights.hdf5\n",
      "Epoch 3730/10000\n",
      "2700/2700 [==============================] - 0s 38us/step - loss: 2.2066e-04 - mean_absolute_error: 0.0090\n",
      "\n",
      "Epoch 03730: loss did not improve\n",
      "Epoch 3731/10000\n",
      "2700/2700 [==============================] - 0s 32us/step - loss: 2.2265e-04 - mean_absolute_error: 0.0091\n",
      "\n",
      "Epoch 03731: loss did not improve\n",
      "Epoch 3732/10000\n",
      "2700/2700 [==============================] - 0s 29us/step - loss: 2.1946e-04 - mean_absolute_error: 0.0090\n",
      "\n",
      "Epoch 03732: loss improved from 0.00022 to 0.00022, saving model to keras/best_baseline_weights.hdf5\n",
      "Epoch 3733/10000\n",
      "2700/2700 [==============================] - 0s 33us/step - loss: 2.2162e-04 - mean_absolute_error: 0.0091\n",
      "\n",
      "Epoch 03733: loss did not improve\n",
      "Epoch 3734/10000\n",
      "2700/2700 [==============================] - 0s 31us/step - loss: 2.2293e-04 - mean_absolute_error: 0.0091\n",
      "\n",
      "Epoch 03734: loss did not improve\n",
      "Epoch 3735/10000\n",
      "2700/2700 [==============================] - 0s 30us/step - loss: 2.2325e-04 - mean_absolute_error: 0.0092\n",
      "\n",
      "Epoch 03735: loss did not improve\n",
      "Epoch 3736/10000\n",
      "2700/2700 [==============================] - 0s 32us/step - loss: 2.2071e-04 - mean_absolute_error: 0.0090\n",
      "\n",
      "Epoch 03736: loss did not improve\n",
      "Epoch 3737/10000\n",
      "2700/2700 [==============================] - 0s 32us/step - loss: 2.2488e-04 - mean_absolute_error: 0.0092\n",
      "\n",
      "Epoch 03737: loss did not improve\n",
      "Epoch 3738/10000\n",
      "2700/2700 [==============================] - 0s 34us/step - loss: 2.2189e-04 - mean_absolute_error: 0.0092\n",
      "\n",
      "Epoch 03738: loss did not improve\n",
      "Epoch 3739/10000\n",
      "2700/2700 [==============================] - 0s 31us/step - loss: 2.2363e-04 - mean_absolute_error: 0.0092\n",
      "\n",
      "Epoch 03739: loss did not improve\n",
      "Epoch 3740/10000\n",
      "2700/2700 [==============================] - 0s 37us/step - loss: 2.2106e-04 - mean_absolute_error: 0.0091\n",
      "\n",
      "Epoch 03740: loss did not improve\n",
      "Epoch 3741/10000\n",
      "2700/2700 [==============================] - 0s 36us/step - loss: 2.2021e-04 - mean_absolute_error: 0.0090\n",
      "\n",
      "Epoch 03741: loss did not improve\n",
      "Epoch 3742/10000\n",
      "2700/2700 [==============================] - 0s 36us/step - loss: 2.2245e-04 - mean_absolute_error: 0.0091\n",
      "\n",
      "Epoch 03742: loss did not improve\n",
      "Epoch 3743/10000\n",
      "2700/2700 [==============================] - 0s 33us/step - loss: 2.1848e-04 - mean_absolute_error: 0.0090\n",
      "\n",
      "Epoch 03743: loss improved from 0.00022 to 0.00022, saving model to keras/best_baseline_weights.hdf5\n",
      "Epoch 3744/10000\n",
      "2700/2700 [==============================] - 0s 34us/step - loss: 2.2088e-04 - mean_absolute_error: 0.0091\n",
      "\n",
      "Epoch 03744: loss did not improve\n",
      "Epoch 3745/10000\n",
      "2700/2700 [==============================] - 0s 30us/step - loss: 2.2223e-04 - mean_absolute_error: 0.0092\n",
      "\n",
      "Epoch 03745: loss did not improve\n",
      "Epoch 3746/10000\n",
      "2700/2700 [==============================] - 0s 32us/step - loss: 2.1989e-04 - mean_absolute_error: 0.0090\n",
      "\n",
      "Epoch 03746: loss did not improve\n",
      "Epoch 3747/10000\n",
      "2700/2700 [==============================] - 0s 33us/step - loss: 2.2249e-04 - mean_absolute_error: 0.0091\n",
      "\n",
      "Epoch 03747: loss did not improve\n",
      "Epoch 3748/10000\n",
      "2700/2700 [==============================] - 0s 34us/step - loss: 2.1998e-04 - mean_absolute_error: 0.0091\n",
      "\n",
      "Epoch 03748: loss did not improve\n",
      "Epoch 3749/10000\n",
      "2700/2700 [==============================] - 0s 34us/step - loss: 2.2128e-04 - mean_absolute_error: 0.0091\n",
      "\n",
      "Epoch 03749: loss did not improve\n",
      "Epoch 3750/10000\n",
      "2700/2700 [==============================] - 0s 33us/step - loss: 2.2681e-04 - mean_absolute_error: 0.0094\n",
      "\n",
      "Epoch 03750: loss did not improve\n",
      "Epoch 3751/10000\n",
      "2700/2700 [==============================] - 0s 34us/step - loss: 2.2031e-04 - mean_absolute_error: 0.0090\n",
      "\n",
      "Epoch 03751: loss did not improve\n",
      "Epoch 3752/10000\n",
      "2700/2700 [==============================] - 0s 30us/step - loss: 2.2062e-04 - mean_absolute_error: 0.0091\n",
      "\n",
      "Epoch 03752: loss did not improve\n",
      "Epoch 3753/10000\n",
      "2700/2700 [==============================] - 0s 27us/step - loss: 2.1796e-04 - mean_absolute_error: 0.0089\n",
      "\n",
      "Epoch 03753: loss improved from 0.00022 to 0.00022, saving model to keras/best_baseline_weights.hdf5\n",
      "Epoch 3754/10000\n",
      "2700/2700 [==============================] - 0s 28us/step - loss: 2.1990e-04 - mean_absolute_error: 0.0091\n",
      "\n",
      "Epoch 03754: loss did not improve\n",
      "Epoch 3755/10000\n",
      "2700/2700 [==============================] - 0s 33us/step - loss: 2.1959e-04 - mean_absolute_error: 0.0090\n",
      "\n",
      "Epoch 03755: loss did not improve\n",
      "Epoch 3756/10000\n",
      "2700/2700 [==============================] - 0s 29us/step - loss: 2.1753e-04 - mean_absolute_error: 0.0090\n",
      "\n",
      "Epoch 03756: loss improved from 0.00022 to 0.00022, saving model to keras/best_baseline_weights.hdf5\n",
      "Epoch 3757/10000\n",
      "2700/2700 [==============================] - 0s 30us/step - loss: 2.1904e-04 - mean_absolute_error: 0.0092\n",
      "\n",
      "Epoch 03757: loss did not improve\n",
      "Epoch 3758/10000\n",
      "2700/2700 [==============================] - 0s 28us/step - loss: 2.1844e-04 - mean_absolute_error: 0.0090\n",
      "\n",
      "Epoch 03758: loss did not improve\n",
      "Epoch 3759/10000\n",
      "2700/2700 [==============================] - 0s 29us/step - loss: 2.1956e-04 - mean_absolute_error: 0.0091\n",
      "\n",
      "Epoch 03759: loss did not improve\n",
      "Epoch 3760/10000\n",
      "2700/2700 [==============================] - 0s 31us/step - loss: 2.1823e-04 - mean_absolute_error: 0.0090\n",
      "\n",
      "Epoch 03760: loss did not improve\n",
      "Epoch 3761/10000\n",
      "2700/2700 [==============================] - 0s 30us/step - loss: 2.1948e-04 - mean_absolute_error: 0.0090\n",
      "\n",
      "Epoch 03761: loss did not improve\n",
      "Epoch 3762/10000\n",
      "2700/2700 [==============================] - 0s 32us/step - loss: 2.2464e-04 - mean_absolute_error: 0.0093\n",
      "\n",
      "Epoch 03762: loss did not improve\n",
      "Epoch 3763/10000\n",
      "2700/2700 [==============================] - 0s 32us/step - loss: 2.1669e-04 - mean_absolute_error: 0.0090\n",
      "\n",
      "Epoch 03763: loss improved from 0.00022 to 0.00022, saving model to keras/best_baseline_weights.hdf5\n",
      "Epoch 3764/10000\n",
      "2700/2700 [==============================] - 0s 34us/step - loss: 2.1847e-04 - mean_absolute_error: 0.0091\n",
      "\n",
      "Epoch 03764: loss did not improve\n",
      "Epoch 3765/10000\n",
      "2700/2700 [==============================] - 0s 28us/step - loss: 2.1942e-04 - mean_absolute_error: 0.0091\n",
      "\n",
      "Epoch 03765: loss did not improve\n",
      "Epoch 3766/10000\n",
      "2700/2700 [==============================] - 0s 30us/step - loss: 2.2012e-04 - mean_absolute_error: 0.0091\n",
      "\n",
      "Epoch 03766: loss did not improve\n",
      "Epoch 3767/10000\n",
      "2700/2700 [==============================] - 0s 29us/step - loss: 2.1938e-04 - mean_absolute_error: 0.0091\n",
      "\n",
      "Epoch 03767: loss did not improve\n",
      "Epoch 3768/10000\n",
      "2700/2700 [==============================] - 0s 32us/step - loss: 2.1840e-04 - mean_absolute_error: 0.0090\n",
      "\n",
      "Epoch 03768: loss did not improve\n",
      "Epoch 3769/10000\n",
      "2700/2700 [==============================] - 0s 32us/step - loss: 2.2059e-04 - mean_absolute_error: 0.0091\n",
      "\n",
      "Epoch 03769: loss did not improve\n",
      "Epoch 3770/10000\n",
      "2700/2700 [==============================] - 0s 30us/step - loss: 2.2463e-04 - mean_absolute_error: 0.0093\n",
      "\n",
      "Epoch 03770: loss did not improve\n",
      "Epoch 3771/10000\n",
      "2700/2700 [==============================] - 0s 32us/step - loss: 2.1640e-04 - mean_absolute_error: 0.0090\n",
      "\n",
      "Epoch 03771: loss improved from 0.00022 to 0.00022, saving model to keras/best_baseline_weights.hdf5\n",
      "Epoch 3772/10000\n",
      "2700/2700 [==============================] - 0s 31us/step - loss: 2.2512e-04 - mean_absolute_error: 0.0094\n",
      "\n",
      "Epoch 03772: loss did not improve\n",
      "Epoch 3773/10000\n",
      "2700/2700 [==============================] - 0s 31us/step - loss: 2.2103e-04 - mean_absolute_error: 0.0092\n",
      "\n",
      "Epoch 03773: loss did not improve\n",
      "Epoch 3774/10000\n",
      "2700/2700 [==============================] - 0s 29us/step - loss: 2.1443e-04 - mean_absolute_error: 0.0089\n",
      "\n",
      "Epoch 03774: loss improved from 0.00022 to 0.00021, saving model to keras/best_baseline_weights.hdf5\n",
      "Epoch 3775/10000\n",
      "2700/2700 [==============================] - 0s 31us/step - loss: 2.1714e-04 - mean_absolute_error: 0.0091\n",
      "\n",
      "Epoch 03775: loss did not improve\n",
      "Epoch 3776/10000\n",
      "2700/2700 [==============================] - 0s 31us/step - loss: 2.1430e-04 - mean_absolute_error: 0.0089\n",
      "\n",
      "Epoch 03776: loss improved from 0.00021 to 0.00021, saving model to keras/best_baseline_weights.hdf5\n",
      "Epoch 3777/10000\n",
      "2700/2700 [==============================] - 0s 31us/step - loss: 2.1561e-04 - mean_absolute_error: 0.0090\n",
      "\n",
      "Epoch 03777: loss did not improve\n",
      "Epoch 3778/10000\n",
      "2700/2700 [==============================] - 0s 33us/step - loss: 2.1453e-04 - mean_absolute_error: 0.0088\n",
      "\n",
      "Epoch 03778: loss did not improve\n",
      "Epoch 3779/10000\n",
      "2700/2700 [==============================] - 0s 29us/step - loss: 2.1868e-04 - mean_absolute_error: 0.0091\n",
      "\n",
      "Epoch 03779: loss did not improve\n",
      "Epoch 3780/10000\n",
      "2700/2700 [==============================] - 0s 28us/step - loss: 2.1928e-04 - mean_absolute_error: 0.0092\n",
      "\n",
      "Epoch 03780: loss did not improve\n",
      "Epoch 3781/10000\n",
      "2700/2700 [==============================] - 0s 28us/step - loss: 2.2073e-04 - mean_absolute_error: 0.0091\n",
      "\n",
      "Epoch 03781: loss did not improve\n",
      "Epoch 3782/10000\n",
      "2700/2700 [==============================] - 0s 30us/step - loss: 2.2247e-04 - mean_absolute_error: 0.0092\n",
      "\n",
      "Epoch 03782: loss did not improve\n",
      "Epoch 3783/10000\n",
      "2700/2700 [==============================] - 0s 29us/step - loss: 2.1594e-04 - mean_absolute_error: 0.0090\n",
      "\n",
      "Epoch 03783: loss did not improve\n",
      "Epoch 3784/10000\n",
      "2700/2700 [==============================] - 0s 30us/step - loss: 2.1542e-04 - mean_absolute_error: 0.0089\n",
      "\n",
      "Epoch 03784: loss did not improve\n",
      "Epoch 3785/10000\n",
      "2700/2700 [==============================] - 0s 30us/step - loss: 2.1347e-04 - mean_absolute_error: 0.0089\n",
      "\n",
      "Epoch 03785: loss improved from 0.00021 to 0.00021, saving model to keras/best_baseline_weights.hdf5\n",
      "Epoch 3786/10000\n",
      "2700/2700 [==============================] - 0s 35us/step - loss: 2.1401e-04 - mean_absolute_error: 0.0089\n",
      "\n",
      "Epoch 03786: loss did not improve\n",
      "Epoch 3787/10000\n",
      "2700/2700 [==============================] - 0s 29us/step - loss: 2.1752e-04 - mean_absolute_error: 0.0091\n",
      "\n",
      "Epoch 03787: loss did not improve\n",
      "Epoch 3788/10000\n",
      "2700/2700 [==============================] - 0s 32us/step - loss: 2.1506e-04 - mean_absolute_error: 0.0090\n",
      "\n",
      "Epoch 03788: loss did not improve\n",
      "Epoch 3789/10000\n",
      "2700/2700 [==============================] - 0s 34us/step - loss: 2.1932e-04 - mean_absolute_error: 0.0092\n",
      "\n",
      "Epoch 03789: loss did not improve\n",
      "Epoch 3790/10000\n",
      "2700/2700 [==============================] - 0s 28us/step - loss: 2.1632e-04 - mean_absolute_error: 0.0091\n",
      "\n",
      "Epoch 03790: loss did not improve\n",
      "Epoch 3791/10000\n",
      "2700/2700 [==============================] - 0s 30us/step - loss: 2.1454e-04 - mean_absolute_error: 0.0089\n",
      "\n",
      "Epoch 03791: loss did not improve\n",
      "Epoch 3792/10000\n",
      "2700/2700 [==============================] - 0s 31us/step - loss: 2.1594e-04 - mean_absolute_error: 0.0090\n",
      "\n",
      "Epoch 03792: loss did not improve\n",
      "Epoch 3793/10000\n",
      "2700/2700 [==============================] - 0s 28us/step - loss: 2.1999e-04 - mean_absolute_error: 0.0091\n",
      "\n",
      "Epoch 03793: loss did not improve\n",
      "Epoch 3794/10000\n",
      "2700/2700 [==============================] - 0s 34us/step - loss: 2.2060e-04 - mean_absolute_error: 0.0093\n",
      "\n",
      "Epoch 03794: loss did not improve\n",
      "Epoch 3795/10000\n",
      "2700/2700 [==============================] - 0s 29us/step - loss: 2.1452e-04 - mean_absolute_error: 0.0089\n",
      "\n",
      "Epoch 03795: loss did not improve\n",
      "Epoch 3796/10000\n",
      "2700/2700 [==============================] - 0s 30us/step - loss: 2.1568e-04 - mean_absolute_error: 0.0090\n",
      "\n",
      "Epoch 03796: loss did not improve\n",
      "Epoch 3797/10000\n",
      "2700/2700 [==============================] - 0s 30us/step - loss: 2.1502e-04 - mean_absolute_error: 0.0091\n",
      "\n",
      "Epoch 03797: loss did not improve\n",
      "Epoch 3798/10000\n",
      "2700/2700 [==============================] - 0s 31us/step - loss: 2.1584e-04 - mean_absolute_error: 0.0089\n",
      "\n",
      "Epoch 03798: loss did not improve\n",
      "Epoch 3799/10000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2700/2700 [==============================] - 0s 28us/step - loss: 2.1771e-04 - mean_absolute_error: 0.0091\n",
      "\n",
      "Epoch 03799: loss did not improve\n",
      "Epoch 3800/10000\n",
      "2700/2700 [==============================] - 0s 30us/step - loss: 2.1575e-04 - mean_absolute_error: 0.0090\n",
      "\n",
      "Epoch 03800: loss did not improve\n",
      "Epoch 3801/10000\n",
      "2700/2700 [==============================] - 0s 30us/step - loss: 2.1593e-04 - mean_absolute_error: 0.0090\n",
      "\n",
      "Epoch 03801: loss did not improve\n",
      "Epoch 3802/10000\n",
      "2700/2700 [==============================] - 0s 28us/step - loss: 2.1711e-04 - mean_absolute_error: 0.0091\n",
      "\n",
      "Epoch 03802: loss did not improve\n",
      "Epoch 3803/10000\n",
      "2700/2700 [==============================] - 0s 26us/step - loss: 2.1483e-04 - mean_absolute_error: 0.0090\n",
      "\n",
      "Epoch 03803: loss did not improve\n",
      "Epoch 3804/10000\n",
      "2700/2700 [==============================] - 0s 29us/step - loss: 2.1125e-04 - mean_absolute_error: 0.0088\n",
      "\n",
      "Epoch 03804: loss improved from 0.00021 to 0.00021, saving model to keras/best_baseline_weights.hdf5\n",
      "Epoch 3805/10000\n",
      "2700/2700 [==============================] - 0s 30us/step - loss: 2.1452e-04 - mean_absolute_error: 0.0090\n",
      "\n",
      "Epoch 03805: loss did not improve\n",
      "Epoch 3806/10000\n",
      "2700/2700 [==============================] - 0s 28us/step - loss: 2.1395e-04 - mean_absolute_error: 0.0089\n",
      "\n",
      "Epoch 03806: loss did not improve\n",
      "Epoch 3807/10000\n",
      "2700/2700 [==============================] - 0s 28us/step - loss: 2.1302e-04 - mean_absolute_error: 0.0089\n",
      "\n",
      "Epoch 03807: loss did not improve\n",
      "Epoch 3808/10000\n",
      "2700/2700 [==============================] - 0s 31us/step - loss: 2.1722e-04 - mean_absolute_error: 0.0092\n",
      "\n",
      "Epoch 03808: loss did not improve\n",
      "Epoch 3809/10000\n",
      "2700/2700 [==============================] - 0s 29us/step - loss: 2.1259e-04 - mean_absolute_error: 0.0089\n",
      "\n",
      "Epoch 03809: loss did not improve\n",
      "Epoch 3810/10000\n",
      "2700/2700 [==============================] - 0s 37us/step - loss: 2.1542e-04 - mean_absolute_error: 0.0090\n",
      "\n",
      "Epoch 03810: loss did not improve\n",
      "Epoch 3811/10000\n",
      "2700/2700 [==============================] - 0s 36us/step - loss: 2.1622e-04 - mean_absolute_error: 0.0090\n",
      "\n",
      "Epoch 03811: loss did not improve\n",
      "Epoch 3812/10000\n",
      "2700/2700 [==============================] - 0s 30us/step - loss: 2.1486e-04 - mean_absolute_error: 0.0091\n",
      "\n",
      "Epoch 03812: loss did not improve\n",
      "Epoch 3813/10000\n",
      "2700/2700 [==============================] - 0s 33us/step - loss: 2.1705e-04 - mean_absolute_error: 0.0091\n",
      "\n",
      "Epoch 03813: loss did not improve\n",
      "Epoch 3814/10000\n",
      "2700/2700 [==============================] - 0s 31us/step - loss: 2.1379e-04 - mean_absolute_error: 0.0089\n",
      "\n",
      "Epoch 03814: loss did not improve\n",
      "Epoch 3815/10000\n",
      "2700/2700 [==============================] - 0s 30us/step - loss: 2.1500e-04 - mean_absolute_error: 0.0090\n",
      "\n",
      "Epoch 03815: loss did not improve\n",
      "Epoch 3816/10000\n",
      "2700/2700 [==============================] - 0s 32us/step - loss: 2.1390e-04 - mean_absolute_error: 0.0089\n",
      "\n",
      "Epoch 03816: loss did not improve\n",
      "Epoch 3817/10000\n",
      "2700/2700 [==============================] - 0s 31us/step - loss: 2.1317e-04 - mean_absolute_error: 0.0090\n",
      "\n",
      "Epoch 03817: loss did not improve\n",
      "Epoch 3818/10000\n",
      "2700/2700 [==============================] - 0s 30us/step - loss: 2.1303e-04 - mean_absolute_error: 0.0089\n",
      "\n",
      "Epoch 03818: loss did not improve\n",
      "Epoch 3819/10000\n",
      "2700/2700 [==============================] - 0s 33us/step - loss: 2.1383e-04 - mean_absolute_error: 0.0090\n",
      "\n",
      "Epoch 03819: loss did not improve\n",
      "Epoch 3820/10000\n",
      "2700/2700 [==============================] - 0s 30us/step - loss: 2.1106e-04 - mean_absolute_error: 0.0089\n",
      "\n",
      "Epoch 03820: loss improved from 0.00021 to 0.00021, saving model to keras/best_baseline_weights.hdf5\n",
      "Epoch 3821/10000\n",
      "2700/2700 [==============================] - 0s 41us/step - loss: 2.1267e-04 - mean_absolute_error: 0.0090\n",
      "\n",
      "Epoch 03821: loss did not improve\n",
      "Epoch 3822/10000\n",
      "2700/2700 [==============================] - 0s 34us/step - loss: 2.1408e-04 - mean_absolute_error: 0.0090\n",
      "\n",
      "Epoch 03822: loss did not improve\n",
      "Epoch 3823/10000\n",
      "2700/2700 [==============================] - 0s 34us/step - loss: 2.1144e-04 - mean_absolute_error: 0.0089\n",
      "\n",
      "Epoch 03823: loss did not improve\n",
      "Epoch 3824/10000\n",
      "2700/2700 [==============================] - 0s 35us/step - loss: 2.0897e-04 - mean_absolute_error: 0.0088\n",
      "\n",
      "Epoch 03824: loss improved from 0.00021 to 0.00021, saving model to keras/best_baseline_weights.hdf5\n",
      "Epoch 3825/10000\n",
      "2700/2700 [==============================] - 0s 35us/step - loss: 2.1503e-04 - mean_absolute_error: 0.0090\n",
      "\n",
      "Epoch 03825: loss did not improve\n",
      "Epoch 3826/10000\n",
      "2700/2700 [==============================] - 0s 31us/step - loss: 2.1015e-04 - mean_absolute_error: 0.0089\n",
      "\n",
      "Epoch 03826: loss did not improve\n",
      "Epoch 3827/10000\n",
      "2700/2700 [==============================] - 0s 33us/step - loss: 2.1121e-04 - mean_absolute_error: 0.0089\n",
      "\n",
      "Epoch 03827: loss did not improve\n",
      "Epoch 3828/10000\n",
      "2700/2700 [==============================] - 0s 36us/step - loss: 2.1037e-04 - mean_absolute_error: 0.0089\n",
      "\n",
      "Epoch 03828: loss did not improve\n",
      "Epoch 3829/10000\n",
      "2700/2700 [==============================] - 0s 49us/step - loss: 2.1188e-04 - mean_absolute_error: 0.0088\n",
      "\n",
      "Epoch 03829: loss did not improve\n",
      "Epoch 3830/10000\n",
      "2700/2700 [==============================] - 0s 39us/step - loss: 2.1671e-04 - mean_absolute_error: 0.0092\n",
      "\n",
      "Epoch 03830: loss did not improve\n",
      "Epoch 3831/10000\n",
      "2700/2700 [==============================] - 0s 39us/step - loss: 2.1360e-04 - mean_absolute_error: 0.0090\n",
      "\n",
      "Epoch 03831: loss did not improve\n",
      "Epoch 3832/10000\n",
      "2700/2700 [==============================] - 0s 41us/step - loss: 2.1278e-04 - mean_absolute_error: 0.0090\n",
      "\n",
      "Epoch 03832: loss did not improve\n",
      "Epoch 3833/10000\n",
      "2700/2700 [==============================] - 0s 32us/step - loss: 2.0882e-04 - mean_absolute_error: 0.0088\n",
      "\n",
      "Epoch 03833: loss improved from 0.00021 to 0.00021, saving model to keras/best_baseline_weights.hdf5\n",
      "Epoch 3834/10000\n",
      "2700/2700 [==============================] - 0s 33us/step - loss: 2.1045e-04 - mean_absolute_error: 0.0089\n",
      "\n",
      "Epoch 03834: loss did not improve\n",
      "Epoch 3835/10000\n",
      "2700/2700 [==============================] - 0s 32us/step - loss: 2.1185e-04 - mean_absolute_error: 0.0088\n",
      "\n",
      "Epoch 03835: loss did not improve\n",
      "Epoch 3836/10000\n",
      "2700/2700 [==============================] - 0s 30us/step - loss: 2.0981e-04 - mean_absolute_error: 0.0089\n",
      "\n",
      "Epoch 03836: loss did not improve\n",
      "Epoch 3837/10000\n",
      "2700/2700 [==============================] - 0s 29us/step - loss: 2.1117e-04 - mean_absolute_error: 0.0089\n",
      "\n",
      "Epoch 03837: loss did not improve\n",
      "Epoch 3838/10000\n",
      "2700/2700 [==============================] - 0s 36us/step - loss: 2.1422e-04 - mean_absolute_error: 0.0091\n",
      "\n",
      "Epoch 03838: loss did not improve\n",
      "Epoch 3839/10000\n",
      "2700/2700 [==============================] - 0s 36us/step - loss: 2.0799e-04 - mean_absolute_error: 0.0087\n",
      "\n",
      "Epoch 03839: loss improved from 0.00021 to 0.00021, saving model to keras/best_baseline_weights.hdf5\n",
      "Epoch 3840/10000\n",
      "2700/2700 [==============================] - 0s 41us/step - loss: 2.0966e-04 - mean_absolute_error: 0.0088\n",
      "\n",
      "Epoch 03840: loss did not improve\n",
      "Epoch 3841/10000\n",
      "2700/2700 [==============================] - 0s 31us/step - loss: 2.1226e-04 - mean_absolute_error: 0.0090\n",
      "\n",
      "Epoch 03841: loss did not improve\n",
      "Epoch 3842/10000\n",
      "2700/2700 [==============================] - 0s 32us/step - loss: 2.0851e-04 - mean_absolute_error: 0.0088\n",
      "\n",
      "Epoch 03842: loss did not improve\n",
      "Epoch 3843/10000\n",
      "2700/2700 [==============================] - 0s 31us/step - loss: 2.0848e-04 - mean_absolute_error: 0.0087\n",
      "\n",
      "Epoch 03843: loss did not improve\n",
      "Epoch 3844/10000\n",
      "2700/2700 [==============================] - 0s 32us/step - loss: 2.1112e-04 - mean_absolute_error: 0.0089\n",
      "\n",
      "Epoch 03844: loss did not improve\n",
      "Epoch 3845/10000\n",
      "2700/2700 [==============================] - 0s 33us/step - loss: 2.1176e-04 - mean_absolute_error: 0.0089\n",
      "\n",
      "Epoch 03845: loss did not improve\n",
      "Epoch 3846/10000\n",
      "2700/2700 [==============================] - 0s 33us/step - loss: 2.1110e-04 - mean_absolute_error: 0.0090\n",
      "\n",
      "Epoch 03846: loss did not improve\n",
      "Epoch 3847/10000\n",
      "2700/2700 [==============================] - 0s 31us/step - loss: 2.0747e-04 - mean_absolute_error: 0.0088\n",
      "\n",
      "Epoch 03847: loss improved from 0.00021 to 0.00021, saving model to keras/best_baseline_weights.hdf5\n",
      "Epoch 3848/10000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2700/2700 [==============================] - 0s 33us/step - loss: 2.1085e-04 - mean_absolute_error: 0.0089\n",
      "\n",
      "Epoch 03848: loss did not improve\n",
      "Epoch 3849/10000\n",
      "2700/2700 [==============================] - 0s 27us/step - loss: 2.0608e-04 - mean_absolute_error: 0.0087\n",
      "\n",
      "Epoch 03849: loss improved from 0.00021 to 0.00021, saving model to keras/best_baseline_weights.hdf5\n",
      "Epoch 3850/10000\n",
      "2700/2700 [==============================] - 0s 26us/step - loss: 2.1104e-04 - mean_absolute_error: 0.0089\n",
      "\n",
      "Epoch 03850: loss did not improve\n",
      "Epoch 3851/10000\n",
      "2700/2700 [==============================] - 0s 29us/step - loss: 2.1103e-04 - mean_absolute_error: 0.0089\n",
      "\n",
      "Epoch 03851: loss did not improve\n",
      "Epoch 3852/10000\n",
      "2700/2700 [==============================] - 0s 28us/step - loss: 2.1242e-04 - mean_absolute_error: 0.0090\n",
      "\n",
      "Epoch 03852: loss did not improve\n",
      "Epoch 3853/10000\n",
      "2700/2700 [==============================] - 0s 30us/step - loss: 2.1587e-04 - mean_absolute_error: 0.0092\n",
      "\n",
      "Epoch 03853: loss did not improve\n",
      "Epoch 3854/10000\n",
      "2700/2700 [==============================] - 0s 33us/step - loss: 2.0664e-04 - mean_absolute_error: 0.0087\n",
      "\n",
      "Epoch 03854: loss did not improve\n",
      "Epoch 3855/10000\n",
      "2700/2700 [==============================] - 0s 27us/step - loss: 2.0768e-04 - mean_absolute_error: 0.0087\n",
      "\n",
      "Epoch 03855: loss did not improve\n",
      "Epoch 3856/10000\n",
      "2700/2700 [==============================] - 0s 31us/step - loss: 2.1363e-04 - mean_absolute_error: 0.0091\n",
      "\n",
      "Epoch 03856: loss did not improve\n",
      "Epoch 3857/10000\n",
      "2700/2700 [==============================] - 0s 28us/step - loss: 2.0750e-04 - mean_absolute_error: 0.0089\n",
      "\n",
      "Epoch 03857: loss did not improve\n",
      "Epoch 3858/10000\n",
      "2700/2700 [==============================] - 0s 29us/step - loss: 2.1243e-04 - mean_absolute_error: 0.0090\n",
      "\n",
      "Epoch 03858: loss did not improve\n",
      "Epoch 3859/10000\n",
      "2700/2700 [==============================] - 0s 28us/step - loss: 2.0949e-04 - mean_absolute_error: 0.0089\n",
      "\n",
      "Epoch 03859: loss did not improve\n",
      "Epoch 3860/10000\n",
      "2700/2700 [==============================] - 0s 27us/step - loss: 2.1229e-04 - mean_absolute_error: 0.0090\n",
      "\n",
      "Epoch 03860: loss did not improve\n",
      "Epoch 3861/10000\n",
      "2700/2700 [==============================] - 0s 28us/step - loss: 2.1653e-04 - mean_absolute_error: 0.0091\n",
      "\n",
      "Epoch 03861: loss did not improve\n",
      "Epoch 3862/10000\n",
      "2700/2700 [==============================] - 0s 28us/step - loss: 2.0912e-04 - mean_absolute_error: 0.0088\n",
      "\n",
      "Epoch 03862: loss did not improve\n",
      "Epoch 3863/10000\n",
      "2700/2700 [==============================] - 0s 27us/step - loss: 2.0490e-04 - mean_absolute_error: 0.0087\n",
      "\n",
      "Epoch 03863: loss improved from 0.00021 to 0.00020, saving model to keras/best_baseline_weights.hdf5\n",
      "Epoch 3864/10000\n",
      "2700/2700 [==============================] - 0s 27us/step - loss: 2.0932e-04 - mean_absolute_error: 0.0089\n",
      "\n",
      "Epoch 03864: loss did not improve\n",
      "Epoch 3865/10000\n",
      "2700/2700 [==============================] - 0s 30us/step - loss: 2.0910e-04 - mean_absolute_error: 0.0089\n",
      "\n",
      "Epoch 03865: loss did not improve\n",
      "Epoch 3866/10000\n",
      "2700/2700 [==============================] - 0s 30us/step - loss: 2.0681e-04 - mean_absolute_error: 0.0088\n",
      "\n",
      "Epoch 03866: loss did not improve\n",
      "Epoch 3867/10000\n",
      "2700/2700 [==============================] - 0s 28us/step - loss: 2.0861e-04 - mean_absolute_error: 0.0088\n",
      "\n",
      "Epoch 03867: loss did not improve\n",
      "Epoch 3868/10000\n",
      "2700/2700 [==============================] - 0s 28us/step - loss: 2.0777e-04 - mean_absolute_error: 0.0089\n",
      "\n",
      "Epoch 03868: loss did not improve\n",
      "Epoch 3869/10000\n",
      "2700/2700 [==============================] - 0s 27us/step - loss: 2.0672e-04 - mean_absolute_error: 0.0088\n",
      "\n",
      "Epoch 03869: loss did not improve\n",
      "Epoch 3870/10000\n",
      "2700/2700 [==============================] - 0s 30us/step - loss: 2.0880e-04 - mean_absolute_error: 0.0089\n",
      "\n",
      "Epoch 03870: loss did not improve\n",
      "Epoch 3871/10000\n",
      "2700/2700 [==============================] - 0s 28us/step - loss: 2.0732e-04 - mean_absolute_error: 0.0088\n",
      "\n",
      "Epoch 03871: loss did not improve\n",
      "Epoch 3872/10000\n",
      "2700/2700 [==============================] - 0s 28us/step - loss: 2.0510e-04 - mean_absolute_error: 0.0087\n",
      "\n",
      "Epoch 03872: loss did not improve\n",
      "Epoch 3873/10000\n",
      "2700/2700 [==============================] - 0s 27us/step - loss: 2.0599e-04 - mean_absolute_error: 0.0088\n",
      "\n",
      "Epoch 03873: loss did not improve\n",
      "Epoch 3874/10000\n",
      "2700/2700 [==============================] - 0s 28us/step - loss: 2.0534e-04 - mean_absolute_error: 0.0087\n",
      "\n",
      "Epoch 03874: loss did not improve\n",
      "Epoch 3875/10000\n",
      "2700/2700 [==============================] - 0s 28us/step - loss: 2.0504e-04 - mean_absolute_error: 0.0088\n",
      "\n",
      "Epoch 03875: loss did not improve\n",
      "Epoch 3876/10000\n",
      "2700/2700 [==============================] - 0s 29us/step - loss: 2.0460e-04 - mean_absolute_error: 0.0087\n",
      "\n",
      "Epoch 03876: loss improved from 0.00020 to 0.00020, saving model to keras/best_baseline_weights.hdf5\n",
      "Epoch 3877/10000\n",
      "2700/2700 [==============================] - 0s 29us/step - loss: 2.0470e-04 - mean_absolute_error: 0.0087\n",
      "\n",
      "Epoch 03877: loss did not improve\n",
      "Epoch 3878/10000\n",
      "2700/2700 [==============================] - 0s 27us/step - loss: 2.0446e-04 - mean_absolute_error: 0.0087\n",
      "\n",
      "Epoch 03878: loss improved from 0.00020 to 0.00020, saving model to keras/best_baseline_weights.hdf5\n",
      "Epoch 3879/10000\n",
      "2700/2700 [==============================] - 0s 30us/step - loss: 2.0841e-04 - mean_absolute_error: 0.0088\n",
      "\n",
      "Epoch 03879: loss did not improve\n",
      "Epoch 3880/10000\n",
      "2700/2700 [==============================] - 0s 30us/step - loss: 2.0456e-04 - mean_absolute_error: 0.0087\n",
      "\n",
      "Epoch 03880: loss did not improve\n",
      "Epoch 3881/10000\n",
      "2700/2700 [==============================] - 0s 31us/step - loss: 2.0635e-04 - mean_absolute_error: 0.0087\n",
      "\n",
      "Epoch 03881: loss did not improve\n",
      "Epoch 3882/10000\n",
      "2700/2700 [==============================] - 0s 31us/step - loss: 2.0580e-04 - mean_absolute_error: 0.0087\n",
      "\n",
      "Epoch 03882: loss did not improve\n",
      "Epoch 3883/10000\n",
      "2700/2700 [==============================] - 0s 28us/step - loss: 2.0508e-04 - mean_absolute_error: 0.0088\n",
      "\n",
      "Epoch 03883: loss did not improve\n",
      "Epoch 3884/10000\n",
      "2700/2700 [==============================] - 0s 26us/step - loss: 2.0896e-04 - mean_absolute_error: 0.0090\n",
      "\n",
      "Epoch 03884: loss did not improve\n",
      "Epoch 3885/10000\n",
      "2700/2700 [==============================] - 0s 28us/step - loss: 2.0316e-04 - mean_absolute_error: 0.0087\n",
      "\n",
      "Epoch 03885: loss improved from 0.00020 to 0.00020, saving model to keras/best_baseline_weights.hdf5\n",
      "Epoch 3886/10000\n",
      "2700/2700 [==============================] - 0s 29us/step - loss: 2.0517e-04 - mean_absolute_error: 0.0088\n",
      "\n",
      "Epoch 03886: loss did not improve\n",
      "Epoch 3887/10000\n",
      "2700/2700 [==============================] - 0s 27us/step - loss: 2.0275e-04 - mean_absolute_error: 0.0087\n",
      "\n",
      "Epoch 03887: loss improved from 0.00020 to 0.00020, saving model to keras/best_baseline_weights.hdf5\n",
      "Epoch 3888/10000\n",
      "2700/2700 [==============================] - 0s 29us/step - loss: 2.1088e-04 - mean_absolute_error: 0.0091\n",
      "\n",
      "Epoch 03888: loss did not improve\n",
      "Epoch 3889/10000\n",
      "2700/2700 [==============================] - 0s 29us/step - loss: 2.0559e-04 - mean_absolute_error: 0.0088\n",
      "\n",
      "Epoch 03889: loss did not improve\n",
      "Epoch 3890/10000\n",
      "2700/2700 [==============================] - 0s 29us/step - loss: 2.0390e-04 - mean_absolute_error: 0.0087\n",
      "\n",
      "Epoch 03890: loss did not improve\n",
      "Epoch 3891/10000\n",
      "2700/2700 [==============================] - 0s 30us/step - loss: 2.0564e-04 - mean_absolute_error: 0.0088\n",
      "\n",
      "Epoch 03891: loss did not improve\n",
      "Epoch 3892/10000\n",
      "2700/2700 [==============================] - 0s 29us/step - loss: 2.0247e-04 - mean_absolute_error: 0.0087\n",
      "\n",
      "Epoch 03892: loss improved from 0.00020 to 0.00020, saving model to keras/best_baseline_weights.hdf5\n",
      "Epoch 3893/10000\n",
      "2700/2700 [==============================] - 0s 28us/step - loss: 2.0528e-04 - mean_absolute_error: 0.0088\n",
      "\n",
      "Epoch 03893: loss did not improve\n",
      "Epoch 3894/10000\n",
      "2700/2700 [==============================] - 0s 38us/step - loss: 2.0698e-04 - mean_absolute_error: 0.0088\n",
      "\n",
      "Epoch 03894: loss did not improve\n",
      "Epoch 3895/10000\n",
      "2700/2700 [==============================] - 0s 36us/step - loss: 2.0654e-04 - mean_absolute_error: 0.0089\n",
      "\n",
      "Epoch 03895: loss did not improve\n",
      "Epoch 3896/10000\n",
      "2700/2700 [==============================] - 0s 37us/step - loss: 2.0418e-04 - mean_absolute_error: 0.0087\n",
      "\n",
      "Epoch 03896: loss did not improve\n",
      "Epoch 3897/10000\n",
      "2700/2700 [==============================] - 0s 34us/step - loss: 2.0339e-04 - mean_absolute_error: 0.0087\n",
      "\n",
      "Epoch 03897: loss did not improve\n",
      "Epoch 3898/10000\n",
      "2700/2700 [==============================] - 0s 27us/step - loss: 2.0418e-04 - mean_absolute_error: 0.0087\n",
      "\n",
      "Epoch 03898: loss did not improve\n",
      "Epoch 3899/10000\n",
      "2700/2700 [==============================] - 0s 30us/step - loss: 2.0419e-04 - mean_absolute_error: 0.0087\n",
      "\n",
      "Epoch 03899: loss did not improve\n",
      "Epoch 3900/10000\n",
      "2700/2700 [==============================] - 0s 29us/step - loss: 2.0621e-04 - mean_absolute_error: 0.0088\n",
      "\n",
      "Epoch 03900: loss did not improve\n",
      "Epoch 3901/10000\n",
      "2700/2700 [==============================] - 0s 28us/step - loss: 2.0669e-04 - mean_absolute_error: 0.0089\n",
      "\n",
      "Epoch 03901: loss did not improve\n",
      "Epoch 3902/10000\n",
      "2700/2700 [==============================] - 0s 28us/step - loss: 2.0372e-04 - mean_absolute_error: 0.0087\n",
      "\n",
      "Epoch 03902: loss did not improve\n",
      "Epoch 3903/10000\n",
      "2700/2700 [==============================] - 0s 27us/step - loss: 2.0125e-04 - mean_absolute_error: 0.0086\n",
      "\n",
      "Epoch 03903: loss improved from 0.00020 to 0.00020, saving model to keras/best_baseline_weights.hdf5\n",
      "Epoch 3904/10000\n",
      "2700/2700 [==============================] - 0s 44us/step - loss: 2.0937e-04 - mean_absolute_error: 0.0091\n",
      "\n",
      "Epoch 03904: loss did not improve\n",
      "Epoch 3905/10000\n",
      "2700/2700 [==============================] - 0s 44us/step - loss: 2.0441e-04 - mean_absolute_error: 0.0088\n",
      "\n",
      "Epoch 03905: loss did not improve\n",
      "Epoch 3906/10000\n",
      "2700/2700 [==============================] - 0s 42us/step - loss: 2.0194e-04 - mean_absolute_error: 0.0087\n",
      "\n",
      "Epoch 03906: loss did not improve\n",
      "Epoch 3907/10000\n",
      "2700/2700 [==============================] - 0s 35us/step - loss: 2.0314e-04 - mean_absolute_error: 0.0087\n",
      "\n",
      "Epoch 03907: loss did not improve\n",
      "Epoch 3908/10000\n",
      "2700/2700 [==============================] - 0s 32us/step - loss: 1.9971e-04 - mean_absolute_error: 0.0086\n",
      "\n",
      "Epoch 03908: loss improved from 0.00020 to 0.00020, saving model to keras/best_baseline_weights.hdf5\n",
      "Epoch 3909/10000\n",
      "2700/2700 [==============================] - 0s 33us/step - loss: 2.0087e-04 - mean_absolute_error: 0.0086\n",
      "\n",
      "Epoch 03909: loss did not improve\n",
      "Epoch 3910/10000\n",
      "2700/2700 [==============================] - 0s 28us/step - loss: 2.0368e-04 - mean_absolute_error: 0.0087\n",
      "\n",
      "Epoch 03910: loss did not improve\n",
      "Epoch 3911/10000\n",
      "2700/2700 [==============================] - 0s 28us/step - loss: 2.0314e-04 - mean_absolute_error: 0.0087\n",
      "\n",
      "Epoch 03911: loss did not improve\n",
      "Epoch 3912/10000\n",
      "2700/2700 [==============================] - 0s 28us/step - loss: 2.0194e-04 - mean_absolute_error: 0.0088\n",
      "\n",
      "Epoch 03912: loss did not improve\n",
      "Epoch 3913/10000\n",
      "2700/2700 [==============================] - 0s 30us/step - loss: 2.0494e-04 - mean_absolute_error: 0.0087\n",
      "\n",
      "Epoch 03913: loss did not improve\n",
      "Epoch 3914/10000\n",
      "2700/2700 [==============================] - 0s 30us/step - loss: 2.0165e-04 - mean_absolute_error: 0.0087\n",
      "\n",
      "Epoch 03914: loss did not improve\n",
      "Epoch 3915/10000\n",
      "2700/2700 [==============================] - 0s 30us/step - loss: 2.0234e-04 - mean_absolute_error: 0.0087\n",
      "\n",
      "Epoch 03915: loss did not improve\n",
      "Epoch 3916/10000\n",
      "2700/2700 [==============================] - 0s 29us/step - loss: 2.0318e-04 - mean_absolute_error: 0.0087\n",
      "\n",
      "Epoch 03916: loss did not improve\n",
      "Epoch 3917/10000\n",
      "2700/2700 [==============================] - 0s 45us/step - loss: 2.0217e-04 - mean_absolute_error: 0.0087\n",
      "\n",
      "Epoch 03917: loss did not improve\n",
      "Epoch 3918/10000\n",
      "2700/2700 [==============================] - 0s 38us/step - loss: 2.0741e-04 - mean_absolute_error: 0.0090\n",
      "\n",
      "Epoch 03918: loss did not improve\n",
      "Epoch 3919/10000\n",
      "2700/2700 [==============================] - 0s 32us/step - loss: 2.0162e-04 - mean_absolute_error: 0.0087\n",
      "\n",
      "Epoch 03919: loss did not improve\n",
      "Epoch 3920/10000\n",
      "2700/2700 [==============================] - 0s 36us/step - loss: 2.0327e-04 - mean_absolute_error: 0.0087\n",
      "\n",
      "Epoch 03920: loss did not improve\n",
      "Epoch 3921/10000\n",
      "2700/2700 [==============================] - 0s 36us/step - loss: 2.0328e-04 - mean_absolute_error: 0.0087\n",
      "\n",
      "Epoch 03921: loss did not improve\n",
      "Epoch 3922/10000\n",
      "2700/2700 [==============================] - 0s 36us/step - loss: 2.0291e-04 - mean_absolute_error: 0.0088\n",
      "\n",
      "Epoch 03922: loss did not improve\n",
      "Epoch 3923/10000\n",
      "2700/2700 [==============================] - 0s 29us/step - loss: 2.0374e-04 - mean_absolute_error: 0.0088\n",
      "\n",
      "Epoch 03923: loss did not improve\n",
      "Epoch 3924/10000\n",
      "2700/2700 [==============================] - 0s 32us/step - loss: 1.9849e-04 - mean_absolute_error: 0.0085\n",
      "\n",
      "Epoch 03924: loss improved from 0.00020 to 0.00020, saving model to keras/best_baseline_weights.hdf5\n",
      "Epoch 3925/10000\n",
      "2700/2700 [==============================] - 0s 29us/step - loss: 2.0377e-04 - mean_absolute_error: 0.0089\n",
      "\n",
      "Epoch 03925: loss did not improve\n",
      "Epoch 3926/10000\n",
      "2700/2700 [==============================] - 0s 29us/step - loss: 2.0447e-04 - mean_absolute_error: 0.0088\n",
      "\n",
      "Epoch 03926: loss did not improve\n",
      "Epoch 3927/10000\n",
      "2700/2700 [==============================] - 0s 29us/step - loss: 2.0194e-04 - mean_absolute_error: 0.0087\n",
      "\n",
      "Epoch 03927: loss did not improve\n",
      "Epoch 3928/10000\n",
      "2700/2700 [==============================] - 0s 28us/step - loss: 1.9806e-04 - mean_absolute_error: 0.0085\n",
      "\n",
      "Epoch 03928: loss improved from 0.00020 to 0.00020, saving model to keras/best_baseline_weights.hdf5\n",
      "Epoch 3929/10000\n",
      "2700/2700 [==============================] - 0s 32us/step - loss: 2.0225e-04 - mean_absolute_error: 0.0087\n",
      "\n",
      "Epoch 03929: loss did not improve\n",
      "Epoch 3930/10000\n",
      "2700/2700 [==============================] - 0s 30us/step - loss: 2.0067e-04 - mean_absolute_error: 0.0086\n",
      "\n",
      "Epoch 03930: loss did not improve\n",
      "Epoch 3931/10000\n",
      "2700/2700 [==============================] - 0s 41us/step - loss: 2.0249e-04 - mean_absolute_error: 0.0087\n",
      "\n",
      "Epoch 03931: loss did not improve\n",
      "Epoch 3932/10000\n",
      "2700/2700 [==============================] - 0s 47us/step - loss: 1.9737e-04 - mean_absolute_error: 0.0085\n",
      "\n",
      "Epoch 03932: loss improved from 0.00020 to 0.00020, saving model to keras/best_baseline_weights.hdf5\n",
      "Epoch 3933/10000\n",
      "2700/2700 [==============================] - 0s 43us/step - loss: 2.0221e-04 - mean_absolute_error: 0.0088\n",
      "\n",
      "Epoch 03933: loss did not improve\n",
      "Epoch 3934/10000\n",
      "2700/2700 [==============================] - 0s 46us/step - loss: 1.9759e-04 - mean_absolute_error: 0.0085\n",
      "\n",
      "Epoch 03934: loss did not improve\n",
      "Epoch 3935/10000\n",
      "2700/2700 [==============================] - 0s 42us/step - loss: 1.9972e-04 - mean_absolute_error: 0.0087\n",
      "\n",
      "Epoch 03935: loss did not improve\n",
      "Epoch 3936/10000\n",
      "2700/2700 [==============================] - 0s 44us/step - loss: 1.9962e-04 - mean_absolute_error: 0.0086\n",
      "\n",
      "Epoch 03936: loss did not improve\n",
      "Epoch 3937/10000\n",
      "2700/2700 [==============================] - 0s 43us/step - loss: 2.0114e-04 - mean_absolute_error: 0.0086\n",
      "\n",
      "Epoch 03937: loss did not improve\n",
      "Epoch 3938/10000\n",
      "2700/2700 [==============================] - 0s 35us/step - loss: 1.9803e-04 - mean_absolute_error: 0.0086\n",
      "\n",
      "Epoch 03938: loss did not improve\n",
      "Epoch 3939/10000\n",
      "2700/2700 [==============================] - 0s 32us/step - loss: 1.9956e-04 - mean_absolute_error: 0.0086\n",
      "\n",
      "Epoch 03939: loss did not improve\n",
      "Epoch 3940/10000\n",
      "2700/2700 [==============================] - 0s 31us/step - loss: 1.9711e-04 - mean_absolute_error: 0.0085\n",
      "\n",
      "Epoch 03940: loss improved from 0.00020 to 0.00020, saving model to keras/best_baseline_weights.hdf5\n",
      "Epoch 3941/10000\n",
      "2700/2700 [==============================] - 0s 30us/step - loss: 2.0151e-04 - mean_absolute_error: 0.0088\n",
      "\n",
      "Epoch 03941: loss did not improve\n",
      "Epoch 3942/10000\n",
      "2700/2700 [==============================] - 0s 34us/step - loss: 1.9894e-04 - mean_absolute_error: 0.0087\n",
      "\n",
      "Epoch 03942: loss did not improve\n",
      "Epoch 3943/10000\n",
      "2700/2700 [==============================] - 0s 49us/step - loss: 2.0399e-04 - mean_absolute_error: 0.0088\n",
      "\n",
      "Epoch 03943: loss did not improve\n",
      "Epoch 3944/10000\n",
      "2700/2700 [==============================] - 0s 40us/step - loss: 2.0507e-04 - mean_absolute_error: 0.0089\n",
      "\n",
      "Epoch 03944: loss did not improve\n",
      "Epoch 3945/10000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2700/2700 [==============================] - 0s 46us/step - loss: 2.0377e-04 - mean_absolute_error: 0.0088\n",
      "\n",
      "Epoch 03945: loss did not improve\n",
      "Epoch 3946/10000\n",
      "2700/2700 [==============================] - 0s 43us/step - loss: 2.0062e-04 - mean_absolute_error: 0.0087\n",
      "\n",
      "Epoch 03946: loss did not improve\n",
      "Epoch 3947/10000\n",
      "2700/2700 [==============================] - 0s 35us/step - loss: 2.0155e-04 - mean_absolute_error: 0.0088\n",
      "\n",
      "Epoch 03947: loss did not improve\n",
      "Epoch 3948/10000\n",
      "2700/2700 [==============================] - 0s 34us/step - loss: 1.9884e-04 - mean_absolute_error: 0.0086\n",
      "\n",
      "Epoch 03948: loss did not improve\n",
      "Epoch 3949/10000\n",
      "2700/2700 [==============================] - 0s 34us/step - loss: 1.9734e-04 - mean_absolute_error: 0.0085\n",
      "\n",
      "Epoch 03949: loss did not improve\n",
      "Epoch 3950/10000\n",
      "2700/2700 [==============================] - 0s 38us/step - loss: 1.9764e-04 - mean_absolute_error: 0.0085\n",
      "\n",
      "Epoch 03950: loss did not improve\n",
      "Epoch 3951/10000\n",
      "2700/2700 [==============================] - 0s 35us/step - loss: 1.9791e-04 - mean_absolute_error: 0.0086\n",
      "\n",
      "Epoch 03951: loss did not improve\n",
      "Epoch 3952/10000\n",
      "2700/2700 [==============================] - 0s 41us/step - loss: 1.9984e-04 - mean_absolute_error: 0.0087\n",
      "\n",
      "Epoch 03952: loss did not improve\n",
      "Epoch 3953/10000\n",
      "2700/2700 [==============================] - 0s 30us/step - loss: 2.0036e-04 - mean_absolute_error: 0.0087\n",
      "\n",
      "Epoch 03953: loss did not improve\n",
      "Epoch 3954/10000\n",
      "2700/2700 [==============================] - 0s 29us/step - loss: 1.9788e-04 - mean_absolute_error: 0.0085\n",
      "\n",
      "Epoch 03954: loss did not improve\n",
      "Epoch 3955/10000\n",
      "2700/2700 [==============================] - 0s 27us/step - loss: 1.9598e-04 - mean_absolute_error: 0.0085\n",
      "\n",
      "Epoch 03955: loss improved from 0.00020 to 0.00020, saving model to keras/best_baseline_weights.hdf5\n",
      "Epoch 3956/10000\n",
      "2700/2700 [==============================] - 0s 30us/step - loss: 2.0215e-04 - mean_absolute_error: 0.0088\n",
      "\n",
      "Epoch 03956: loss did not improve\n",
      "Epoch 3957/10000\n",
      "2700/2700 [==============================] - 0s 29us/step - loss: 1.9971e-04 - mean_absolute_error: 0.0087\n",
      "\n",
      "Epoch 03957: loss did not improve\n",
      "Epoch 3958/10000\n",
      "2700/2700 [==============================] - 0s 29us/step - loss: 1.9429e-04 - mean_absolute_error: 0.0084\n",
      "\n",
      "Epoch 03958: loss improved from 0.00020 to 0.00019, saving model to keras/best_baseline_weights.hdf5\n",
      "Epoch 3959/10000\n",
      "2700/2700 [==============================] - 0s 29us/step - loss: 1.9775e-04 - mean_absolute_error: 0.0085\n",
      "\n",
      "Epoch 03959: loss did not improve\n",
      "Epoch 3960/10000\n",
      "2700/2700 [==============================] - 0s 27us/step - loss: 2.0252e-04 - mean_absolute_error: 0.0088\n",
      "\n",
      "Epoch 03960: loss did not improve\n",
      "Epoch 3961/10000\n",
      "2700/2700 [==============================] - 0s 29us/step - loss: 1.9542e-04 - mean_absolute_error: 0.0085\n",
      "\n",
      "Epoch 03961: loss did not improve\n",
      "Epoch 3962/10000\n",
      "2700/2700 [==============================] - 0s 44us/step - loss: 1.9606e-04 - mean_absolute_error: 0.0085\n",
      "\n",
      "Epoch 03962: loss did not improve\n",
      "Epoch 3963/10000\n",
      "2700/2700 [==============================] - 0s 44us/step - loss: 1.9575e-04 - mean_absolute_error: 0.0086\n",
      "\n",
      "Epoch 03963: loss did not improve\n",
      "Epoch 3964/10000\n",
      "2700/2700 [==============================] - 0s 44us/step - loss: 1.9627e-04 - mean_absolute_error: 0.0085\n",
      "\n",
      "Epoch 03964: loss did not improve\n",
      "Epoch 3965/10000\n",
      "2700/2700 [==============================] - 0s 40us/step - loss: 1.9561e-04 - mean_absolute_error: 0.0084\n",
      "\n",
      "Epoch 03965: loss did not improve\n",
      "Epoch 3966/10000\n",
      "2700/2700 [==============================] - 0s 36us/step - loss: 1.9755e-04 - mean_absolute_error: 0.0086\n",
      "\n",
      "Epoch 03966: loss did not improve\n",
      "Epoch 3967/10000\n",
      "2700/2700 [==============================] - 0s 35us/step - loss: 1.9704e-04 - mean_absolute_error: 0.0086\n",
      "\n",
      "Epoch 03967: loss did not improve\n",
      "Epoch 3968/10000\n",
      "2700/2700 [==============================] - 0s 42us/step - loss: 1.9623e-04 - mean_absolute_error: 0.0085\n",
      "\n",
      "Epoch 03968: loss did not improve\n",
      "Epoch 3969/10000\n",
      "2700/2700 [==============================] - 0s 42us/step - loss: 1.9712e-04 - mean_absolute_error: 0.0086\n",
      "\n",
      "Epoch 03969: loss did not improve\n",
      "Epoch 3970/10000\n",
      "2700/2700 [==============================] - 0s 36us/step - loss: 1.9521e-04 - mean_absolute_error: 0.0084\n",
      "\n",
      "Epoch 03970: loss did not improve\n",
      "Epoch 3971/10000\n",
      "2700/2700 [==============================] - 0s 37us/step - loss: 1.9638e-04 - mean_absolute_error: 0.0085\n",
      "\n",
      "Epoch 03971: loss did not improve\n",
      "Epoch 3972/10000\n",
      "2700/2700 [==============================] - 0s 36us/step - loss: 1.9352e-04 - mean_absolute_error: 0.0085\n",
      "\n",
      "Epoch 03972: loss improved from 0.00019 to 0.00019, saving model to keras/best_baseline_weights.hdf5\n",
      "Epoch 3973/10000\n",
      "2700/2700 [==============================] - 0s 36us/step - loss: 1.9561e-04 - mean_absolute_error: 0.0085\n",
      "\n",
      "Epoch 03973: loss did not improve\n",
      "Epoch 3974/10000\n",
      "2700/2700 [==============================] - 0s 28us/step - loss: 1.9713e-04 - mean_absolute_error: 0.0087\n",
      "\n",
      "Epoch 03974: loss did not improve\n",
      "Epoch 3975/10000\n",
      "2700/2700 [==============================] - 0s 37us/step - loss: 1.9659e-04 - mean_absolute_error: 0.0086\n",
      "\n",
      "Epoch 03975: loss did not improve\n",
      "Epoch 3976/10000\n",
      "2700/2700 [==============================] - 0s 31us/step - loss: 1.9452e-04 - mean_absolute_error: 0.0085\n",
      "\n",
      "Epoch 03976: loss did not improve\n",
      "Epoch 3977/10000\n",
      "2700/2700 [==============================] - 0s 35us/step - loss: 1.9950e-04 - mean_absolute_error: 0.0087\n",
      "\n",
      "Epoch 03977: loss did not improve\n",
      "Epoch 3978/10000\n",
      "2700/2700 [==============================] - 0s 31us/step - loss: 2.0510e-04 - mean_absolute_error: 0.0093\n",
      "\n",
      "Epoch 03978: loss did not improve\n",
      "Epoch 3979/10000\n",
      "2700/2700 [==============================] - 0s 29us/step - loss: 1.9365e-04 - mean_absolute_error: 0.0084\n",
      "\n",
      "Epoch 03979: loss did not improve\n",
      "Epoch 3980/10000\n",
      "2700/2700 [==============================] - 0s 31us/step - loss: 1.9526e-04 - mean_absolute_error: 0.0085\n",
      "\n",
      "Epoch 03980: loss did not improve\n",
      "Epoch 3981/10000\n",
      "2700/2700 [==============================] - 0s 32us/step - loss: 1.9456e-04 - mean_absolute_error: 0.0085\n",
      "\n",
      "Epoch 03981: loss did not improve\n",
      "Epoch 3982/10000\n",
      "2700/2700 [==============================] - 0s 33us/step - loss: 1.9750e-04 - mean_absolute_error: 0.0086\n",
      "\n",
      "Epoch 03982: loss did not improve\n",
      "Epoch 3983/10000\n",
      "2700/2700 [==============================] - 0s 36us/step - loss: 1.9602e-04 - mean_absolute_error: 0.0085\n",
      "\n",
      "Epoch 03983: loss did not improve\n",
      "Epoch 3984/10000\n",
      "2700/2700 [==============================] - 0s 30us/step - loss: 1.9217e-04 - mean_absolute_error: 0.0084\n",
      "\n",
      "Epoch 03984: loss improved from 0.00019 to 0.00019, saving model to keras/best_baseline_weights.hdf5\n",
      "Epoch 3985/10000\n",
      "2700/2700 [==============================] - 0s 33us/step - loss: 1.9661e-04 - mean_absolute_error: 0.0085\n",
      "\n",
      "Epoch 03985: loss did not improve\n",
      "Epoch 3986/10000\n",
      "2700/2700 [==============================] - 0s 33us/step - loss: 1.9558e-04 - mean_absolute_error: 0.0086\n",
      "\n",
      "Epoch 03986: loss did not improve\n",
      "Epoch 3987/10000\n",
      "2700/2700 [==============================] - 0s 33us/step - loss: 1.9359e-04 - mean_absolute_error: 0.0085\n",
      "\n",
      "Epoch 03987: loss did not improve\n",
      "Epoch 3988/10000\n",
      "2700/2700 [==============================] - 0s 27us/step - loss: 1.9196e-04 - mean_absolute_error: 0.0084\n",
      "\n",
      "Epoch 03988: loss improved from 0.00019 to 0.00019, saving model to keras/best_baseline_weights.hdf5\n",
      "Epoch 3989/10000\n",
      "2700/2700 [==============================] - 0s 33us/step - loss: 1.9340e-04 - mean_absolute_error: 0.0084\n",
      "\n",
      "Epoch 03989: loss did not improve\n",
      "Epoch 3990/10000\n",
      "2700/2700 [==============================] - 0s 30us/step - loss: 1.9218e-04 - mean_absolute_error: 0.0084\n",
      "\n",
      "Epoch 03990: loss did not improve\n",
      "Epoch 3991/10000\n",
      "2700/2700 [==============================] - 0s 35us/step - loss: 1.9336e-04 - mean_absolute_error: 0.0084\n",
      "\n",
      "Epoch 03991: loss did not improve\n",
      "Epoch 3992/10000\n",
      "2700/2700 [==============================] - 0s 33us/step - loss: 1.9401e-04 - mean_absolute_error: 0.0086\n",
      "\n",
      "Epoch 03992: loss did not improve\n",
      "Epoch 3993/10000\n",
      "2700/2700 [==============================] - 0s 31us/step - loss: 1.9374e-04 - mean_absolute_error: 0.0084\n",
      "\n",
      "Epoch 03993: loss did not improve\n",
      "Epoch 3994/10000\n",
      "2700/2700 [==============================] - 0s 38us/step - loss: 1.9315e-04 - mean_absolute_error: 0.0084\n",
      "\n",
      "Epoch 03994: loss did not improve\n",
      "Epoch 3995/10000\n",
      "2700/2700 [==============================] - 0s 30us/step - loss: 1.9593e-04 - mean_absolute_error: 0.0086\n",
      "\n",
      "Epoch 03995: loss did not improve\n",
      "Epoch 3996/10000\n",
      "2700/2700 [==============================] - 0s 31us/step - loss: 1.9530e-04 - mean_absolute_error: 0.0085\n",
      "\n",
      "Epoch 03996: loss did not improve\n",
      "Epoch 3997/10000\n",
      "2700/2700 [==============================] - 0s 31us/step - loss: 1.9190e-04 - mean_absolute_error: 0.0084\n",
      "\n",
      "Epoch 03997: loss improved from 0.00019 to 0.00019, saving model to keras/best_baseline_weights.hdf5\n",
      "Epoch 3998/10000\n",
      "2700/2700 [==============================] - 0s 29us/step - loss: 1.9348e-04 - mean_absolute_error: 0.0086\n",
      "\n",
      "Epoch 03998: loss did not improve\n",
      "Epoch 3999/10000\n",
      "2700/2700 [==============================] - 0s 27us/step - loss: 1.9349e-04 - mean_absolute_error: 0.0085\n",
      "\n",
      "Epoch 03999: loss did not improve\n",
      "Epoch 4000/10000\n",
      "2700/2700 [==============================] - 0s 28us/step - loss: 1.9633e-04 - mean_absolute_error: 0.0087\n",
      "\n",
      "Epoch 04000: loss did not improve\n",
      "Epoch 4001/10000\n",
      "2700/2700 [==============================] - 0s 31us/step - loss: 1.9085e-04 - mean_absolute_error: 0.0084\n",
      "\n",
      "Epoch 04001: loss improved from 0.00019 to 0.00019, saving model to keras/best_baseline_weights.hdf5\n",
      "Epoch 4002/10000\n",
      "2700/2700 [==============================] - 0s 27us/step - loss: 1.9329e-04 - mean_absolute_error: 0.0085\n",
      "\n",
      "Epoch 04002: loss did not improve\n",
      "Epoch 4003/10000\n",
      "2700/2700 [==============================] - 0s 29us/step - loss: 2.0074e-04 - mean_absolute_error: 0.0088\n",
      "\n",
      "Epoch 04003: loss did not improve\n",
      "Epoch 4004/10000\n",
      "2700/2700 [==============================] - 0s 30us/step - loss: 1.9224e-04 - mean_absolute_error: 0.0084\n",
      "\n",
      "Epoch 04004: loss did not improve\n",
      "Epoch 4005/10000\n",
      "2700/2700 [==============================] - 0s 30us/step - loss: 1.9349e-04 - mean_absolute_error: 0.0085\n",
      "\n",
      "Epoch 04005: loss did not improve\n",
      "Epoch 4006/10000\n",
      "2700/2700 [==============================] - 0s 28us/step - loss: 1.9088e-04 - mean_absolute_error: 0.0084\n",
      "\n",
      "Epoch 04006: loss did not improve\n",
      "Epoch 4007/10000\n",
      "2700/2700 [==============================] - 0s 29us/step - loss: 1.9521e-04 - mean_absolute_error: 0.0086\n",
      "\n",
      "Epoch 04007: loss did not improve\n",
      "Epoch 4008/10000\n",
      "2700/2700 [==============================] - 0s 27us/step - loss: 1.9121e-04 - mean_absolute_error: 0.0084\n",
      "\n",
      "Epoch 04008: loss did not improve\n",
      "Epoch 4009/10000\n",
      "2700/2700 [==============================] - 0s 30us/step - loss: 1.9465e-04 - mean_absolute_error: 0.0086\n",
      "\n",
      "Epoch 04009: loss did not improve\n",
      "Epoch 4010/10000\n",
      "2700/2700 [==============================] - 0s 28us/step - loss: 1.9283e-04 - mean_absolute_error: 0.0085\n",
      "\n",
      "Epoch 04010: loss did not improve\n",
      "Epoch 4011/10000\n",
      "2700/2700 [==============================] - 0s 30us/step - loss: 1.8967e-04 - mean_absolute_error: 0.0083\n",
      "\n",
      "Epoch 04011: loss improved from 0.00019 to 0.00019, saving model to keras/best_baseline_weights.hdf5\n",
      "Epoch 4012/10000\n",
      "2700/2700 [==============================] - 0s 28us/step - loss: 1.8944e-04 - mean_absolute_error: 0.0084\n",
      "\n",
      "Epoch 04012: loss improved from 0.00019 to 0.00019, saving model to keras/best_baseline_weights.hdf5\n",
      "Epoch 4013/10000\n",
      "2700/2700 [==============================] - 0s 32us/step - loss: 1.9073e-04 - mean_absolute_error: 0.0083\n",
      "\n",
      "Epoch 04013: loss did not improve\n",
      "Epoch 4014/10000\n",
      "2700/2700 [==============================] - 0s 27us/step - loss: 1.9120e-04 - mean_absolute_error: 0.0085\n",
      "\n",
      "Epoch 04014: loss did not improve\n",
      "Epoch 4015/10000\n",
      "2700/2700 [==============================] - 0s 30us/step - loss: 1.8927e-04 - mean_absolute_error: 0.0084\n",
      "\n",
      "Epoch 04015: loss improved from 0.00019 to 0.00019, saving model to keras/best_baseline_weights.hdf5\n",
      "Epoch 4016/10000\n",
      "2700/2700 [==============================] - 0s 29us/step - loss: 1.9300e-04 - mean_absolute_error: 0.0085\n",
      "\n",
      "Epoch 04016: loss did not improve\n",
      "Epoch 4017/10000\n",
      "2700/2700 [==============================] - 0s 28us/step - loss: 1.9180e-04 - mean_absolute_error: 0.0086\n",
      "\n",
      "Epoch 04017: loss did not improve\n",
      "Epoch 4018/10000\n",
      "2700/2700 [==============================] - 0s 31us/step - loss: 1.9324e-04 - mean_absolute_error: 0.0085\n",
      "\n",
      "Epoch 04018: loss did not improve\n",
      "Epoch 4019/10000\n",
      "2700/2700 [==============================] - 0s 30us/step - loss: 1.9125e-04 - mean_absolute_error: 0.0085\n",
      "\n",
      "Epoch 04019: loss did not improve\n",
      "Epoch 4020/10000\n",
      "2700/2700 [==============================] - 0s 32us/step - loss: 1.9220e-04 - mean_absolute_error: 0.0084\n",
      "\n",
      "Epoch 04020: loss did not improve\n",
      "Epoch 4021/10000\n",
      "2700/2700 [==============================] - 0s 30us/step - loss: 1.8864e-04 - mean_absolute_error: 0.0083\n",
      "\n",
      "Epoch 04021: loss improved from 0.00019 to 0.00019, saving model to keras/best_baseline_weights.hdf5\n",
      "Epoch 4022/10000\n",
      "2700/2700 [==============================] - 0s 29us/step - loss: 1.9218e-04 - mean_absolute_error: 0.0085\n",
      "\n",
      "Epoch 04022: loss did not improve\n",
      "Epoch 4023/10000\n",
      "2700/2700 [==============================] - 0s 29us/step - loss: 1.9076e-04 - mean_absolute_error: 0.0085\n",
      "\n",
      "Epoch 04023: loss did not improve\n",
      "Epoch 4024/10000\n",
      "2700/2700 [==============================] - 0s 29us/step - loss: 1.9019e-04 - mean_absolute_error: 0.0083\n",
      "\n",
      "Epoch 04024: loss did not improve\n",
      "Epoch 4025/10000\n",
      "2700/2700 [==============================] - 0s 29us/step - loss: 1.9220e-04 - mean_absolute_error: 0.0085\n",
      "\n",
      "Epoch 04025: loss did not improve\n",
      "Epoch 4026/10000\n",
      "2700/2700 [==============================] - 0s 31us/step - loss: 1.9068e-04 - mean_absolute_error: 0.0084\n",
      "\n",
      "Epoch 04026: loss did not improve\n",
      "Epoch 4027/10000\n",
      "2700/2700 [==============================] - 0s 30us/step - loss: 1.9003e-04 - mean_absolute_error: 0.0084\n",
      "\n",
      "Epoch 04027: loss did not improve\n",
      "Epoch 4028/10000\n",
      "2700/2700 [==============================] - 0s 32us/step - loss: 1.8695e-04 - mean_absolute_error: 0.0082\n",
      "\n",
      "Epoch 04028: loss improved from 0.00019 to 0.00019, saving model to keras/best_baseline_weights.hdf5\n",
      "Epoch 4029/10000\n",
      "2700/2700 [==============================] - 0s 29us/step - loss: 1.8821e-04 - mean_absolute_error: 0.0083\n",
      "\n",
      "Epoch 04029: loss did not improve\n",
      "Epoch 4030/10000\n",
      "2700/2700 [==============================] - 0s 30us/step - loss: 1.8955e-04 - mean_absolute_error: 0.0084\n",
      "\n",
      "Epoch 04030: loss did not improve\n",
      "Epoch 4031/10000\n",
      "2700/2700 [==============================] - 0s 27us/step - loss: 1.9228e-04 - mean_absolute_error: 0.0085\n",
      "\n",
      "Epoch 04031: loss did not improve\n",
      "Epoch 4032/10000\n",
      "2700/2700 [==============================] - 0s 29us/step - loss: 1.9282e-04 - mean_absolute_error: 0.0085\n",
      "\n",
      "Epoch 04032: loss did not improve\n",
      "Epoch 4033/10000\n",
      "2700/2700 [==============================] - 0s 32us/step - loss: 1.8760e-04 - mean_absolute_error: 0.0083\n",
      "\n",
      "Epoch 04033: loss did not improve\n",
      "Epoch 4034/10000\n",
      "2700/2700 [==============================] - 0s 29us/step - loss: 1.8945e-04 - mean_absolute_error: 0.0084\n",
      "\n",
      "Epoch 04034: loss did not improve\n",
      "Epoch 4035/10000\n",
      "2700/2700 [==============================] - 0s 29us/step - loss: 1.8847e-04 - mean_absolute_error: 0.0083\n",
      "\n",
      "Epoch 04035: loss did not improve\n",
      "Epoch 4036/10000\n",
      "2700/2700 [==============================] - 0s 26us/step - loss: 1.8774e-04 - mean_absolute_error: 0.0083\n",
      "\n",
      "Epoch 04036: loss did not improve\n",
      "Epoch 4037/10000\n",
      "2700/2700 [==============================] - 0s 30us/step - loss: 1.9006e-04 - mean_absolute_error: 0.0084\n",
      "\n",
      "Epoch 04037: loss did not improve\n",
      "Epoch 4038/10000\n",
      "2700/2700 [==============================] - 0s 39us/step - loss: 1.8751e-04 - mean_absolute_error: 0.0083\n",
      "\n",
      "Epoch 04038: loss did not improve\n",
      "Epoch 4039/10000\n",
      "2700/2700 [==============================] - 0s 43us/step - loss: 1.9115e-04 - mean_absolute_error: 0.0085\n",
      "\n",
      "Epoch 04039: loss did not improve\n",
      "Epoch 4040/10000\n",
      "2700/2700 [==============================] - 0s 42us/step - loss: 1.8877e-04 - mean_absolute_error: 0.0083\n",
      "\n",
      "Epoch 04040: loss did not improve\n",
      "Epoch 4041/10000\n",
      "2700/2700 [==============================] - 0s 35us/step - loss: 1.8800e-04 - mean_absolute_error: 0.0084\n",
      "\n",
      "Epoch 04041: loss did not improve\n",
      "Epoch 4042/10000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2700/2700 [==============================] - 0s 28us/step - loss: 1.8743e-04 - mean_absolute_error: 0.0082\n",
      "\n",
      "Epoch 04042: loss did not improve\n",
      "Epoch 4043/10000\n",
      "2700/2700 [==============================] - 0s 29us/step - loss: 1.8973e-04 - mean_absolute_error: 0.0084\n",
      "\n",
      "Epoch 04043: loss did not improve\n",
      "Epoch 4044/10000\n",
      "2700/2700 [==============================] - 0s 31us/step - loss: 1.8796e-04 - mean_absolute_error: 0.0083\n",
      "\n",
      "Epoch 04044: loss did not improve\n",
      "Epoch 4045/10000\n",
      "2700/2700 [==============================] - 0s 30us/step - loss: 1.8723e-04 - mean_absolute_error: 0.0082\n",
      "\n",
      "Epoch 04045: loss did not improve\n",
      "Epoch 4046/10000\n",
      "2700/2700 [==============================] - 0s 32us/step - loss: 1.8748e-04 - mean_absolute_error: 0.0083\n",
      "\n",
      "Epoch 04046: loss did not improve\n",
      "Epoch 4047/10000\n",
      "2700/2700 [==============================] - 0s 29us/step - loss: 1.9135e-04 - mean_absolute_error: 0.0086\n",
      "\n",
      "Epoch 04047: loss did not improve\n",
      "Epoch 4048/10000\n",
      "2700/2700 [==============================] - 0s 28us/step - loss: 1.8748e-04 - mean_absolute_error: 0.0084\n",
      "\n",
      "Epoch 04048: loss did not improve\n",
      "Epoch 4049/10000\n",
      "2700/2700 [==============================] - 0s 30us/step - loss: 1.8773e-04 - mean_absolute_error: 0.0083\n",
      "\n",
      "Epoch 04049: loss did not improve\n",
      "Epoch 4050/10000\n",
      "2700/2700 [==============================] - 0s 27us/step - loss: 1.8995e-04 - mean_absolute_error: 0.0084\n",
      "\n",
      "Epoch 04050: loss did not improve\n",
      "Epoch 4051/10000\n",
      "2700/2700 [==============================] - 0s 25us/step - loss: 1.8738e-04 - mean_absolute_error: 0.0084\n",
      "\n",
      "Epoch 04051: loss did not improve\n",
      "Epoch 4052/10000\n",
      "2700/2700 [==============================] - 0s 30us/step - loss: 1.9036e-04 - mean_absolute_error: 0.0084\n",
      "\n",
      "Epoch 04052: loss did not improve\n",
      "Epoch 4053/10000\n",
      "2700/2700 [==============================] - 0s 31us/step - loss: 1.8635e-04 - mean_absolute_error: 0.0083\n",
      "\n",
      "Epoch 04053: loss improved from 0.00019 to 0.00019, saving model to keras/best_baseline_weights.hdf5\n",
      "Epoch 4054/10000\n",
      "2700/2700 [==============================] - 0s 30us/step - loss: 1.9829e-04 - mean_absolute_error: 0.0089\n",
      "\n",
      "Epoch 04054: loss did not improve\n",
      "Epoch 4055/10000\n",
      "2700/2700 [==============================] - 0s 30us/step - loss: 1.8799e-04 - mean_absolute_error: 0.0083\n",
      "\n",
      "Epoch 04055: loss did not improve\n",
      "Epoch 4056/10000\n",
      "2700/2700 [==============================] - 0s 29us/step - loss: 1.8588e-04 - mean_absolute_error: 0.0082\n",
      "\n",
      "Epoch 04056: loss improved from 0.00019 to 0.00019, saving model to keras/best_baseline_weights.hdf5\n",
      "Epoch 4057/10000\n",
      "2700/2700 [==============================] - 0s 26us/step - loss: 1.8549e-04 - mean_absolute_error: 0.0083\n",
      "\n",
      "Epoch 04057: loss improved from 0.00019 to 0.00019, saving model to keras/best_baseline_weights.hdf5\n",
      "Epoch 4058/10000\n",
      "2700/2700 [==============================] - 0s 28us/step - loss: 1.8766e-04 - mean_absolute_error: 0.0083\n",
      "\n",
      "Epoch 04058: loss did not improve\n",
      "Epoch 4059/10000\n",
      "2700/2700 [==============================] - 0s 33us/step - loss: 1.8698e-04 - mean_absolute_error: 0.0083\n",
      "\n",
      "Epoch 04059: loss did not improve\n",
      "Epoch 4060/10000\n",
      "2700/2700 [==============================] - 0s 36us/step - loss: 1.8348e-04 - mean_absolute_error: 0.0081\n",
      "\n",
      "Epoch 04060: loss improved from 0.00019 to 0.00018, saving model to keras/best_baseline_weights.hdf5\n",
      "Epoch 4061/10000\n",
      "2700/2700 [==============================] - 0s 36us/step - loss: 1.8628e-04 - mean_absolute_error: 0.0083\n",
      "\n",
      "Epoch 04061: loss did not improve\n",
      "Epoch 4062/10000\n",
      "2700/2700 [==============================] - 0s 36us/step - loss: 1.9027e-04 - mean_absolute_error: 0.0084\n",
      "\n",
      "Epoch 04062: loss did not improve\n",
      "Epoch 4063/10000\n",
      "2700/2700 [==============================] - 0s 34us/step - loss: 1.8731e-04 - mean_absolute_error: 0.0083: 0s - loss: 1.7567e-04 - mean_absolute_error: 0.008\n",
      "\n",
      "Epoch 04063: loss did not improve\n",
      "Epoch 4064/10000\n",
      "2700/2700 [==============================] - 0s 28us/step - loss: 1.8697e-04 - mean_absolute_error: 0.0084\n",
      "\n",
      "Epoch 04064: loss did not improve\n",
      "Epoch 4065/10000\n",
      "2700/2700 [==============================] - 0s 28us/step - loss: 1.8478e-04 - mean_absolute_error: 0.0083\n",
      "\n",
      "Epoch 04065: loss did not improve\n",
      "Epoch 4066/10000\n",
      "2700/2700 [==============================] - 0s 28us/step - loss: 1.8528e-04 - mean_absolute_error: 0.0082\n",
      "\n",
      "Epoch 04066: loss did not improve\n",
      "Epoch 4067/10000\n",
      "2700/2700 [==============================] - 0s 27us/step - loss: 1.8853e-04 - mean_absolute_error: 0.0084\n",
      "\n",
      "Epoch 04067: loss did not improve\n",
      "Epoch 4068/10000\n",
      "2700/2700 [==============================] - 0s 29us/step - loss: 1.8557e-04 - mean_absolute_error: 0.0082\n",
      "\n",
      "Epoch 04068: loss did not improve\n",
      "Epoch 4069/10000\n",
      "2700/2700 [==============================] - 0s 29us/step - loss: 1.8479e-04 - mean_absolute_error: 0.0082\n",
      "\n",
      "Epoch 04069: loss did not improve\n",
      "Epoch 4070/10000\n",
      "2700/2700 [==============================] - 0s 31us/step - loss: 1.8422e-04 - mean_absolute_error: 0.0082\n",
      "\n",
      "Epoch 04070: loss did not improve\n",
      "Epoch 4071/10000\n",
      "2700/2700 [==============================] - 0s 28us/step - loss: 1.8724e-04 - mean_absolute_error: 0.0084\n",
      "\n",
      "Epoch 04071: loss did not improve\n",
      "Epoch 4072/10000\n",
      "2700/2700 [==============================] - 0s 28us/step - loss: 1.8378e-04 - mean_absolute_error: 0.0082\n",
      "\n",
      "Epoch 04072: loss did not improve\n",
      "Epoch 4073/10000\n",
      "2700/2700 [==============================] - 0s 30us/step - loss: 1.8613e-04 - mean_absolute_error: 0.0083\n",
      "\n",
      "Epoch 04073: loss did not improve\n",
      "Epoch 4074/10000\n",
      "2700/2700 [==============================] - 0s 26us/step - loss: 1.8209e-04 - mean_absolute_error: 0.0081\n",
      "\n",
      "Epoch 04074: loss improved from 0.00018 to 0.00018, saving model to keras/best_baseline_weights.hdf5\n",
      "Epoch 4075/10000\n",
      "2700/2700 [==============================] - 0s 30us/step - loss: 1.8499e-04 - mean_absolute_error: 0.0081\n",
      "\n",
      "Epoch 04075: loss did not improve\n",
      "Epoch 4076/10000\n",
      "2700/2700 [==============================] - ETA: 0s - loss: 1.6421e-04 - mean_absolute_error: 0.008 - 0s 30us/step - loss: 1.8281e-04 - mean_absolute_error: 0.0082\n",
      "\n",
      "Epoch 04076: loss did not improve\n",
      "Epoch 4077/10000\n",
      "2700/2700 [==============================] - 0s 28us/step - loss: 1.8417e-04 - mean_absolute_error: 0.0083\n",
      "\n",
      "Epoch 04077: loss did not improve\n",
      "Epoch 4078/10000\n",
      "2700/2700 [==============================] - 0s 29us/step - loss: 1.8567e-04 - mean_absolute_error: 0.0083\n",
      "\n",
      "Epoch 04078: loss did not improve\n",
      "Epoch 4079/10000\n",
      "2700/2700 [==============================] - 0s 28us/step - loss: 1.8563e-04 - mean_absolute_error: 0.0083\n",
      "\n",
      "Epoch 04079: loss did not improve\n",
      "Epoch 4080/10000\n",
      "2700/2700 [==============================] - 0s 29us/step - loss: 1.8414e-04 - mean_absolute_error: 0.0082\n",
      "\n",
      "Epoch 04080: loss did not improve\n",
      "Epoch 4081/10000\n",
      "2700/2700 [==============================] - 0s 28us/step - loss: 1.8327e-04 - mean_absolute_error: 0.0082\n",
      "\n",
      "Epoch 04081: loss did not improve\n",
      "Epoch 4082/10000\n",
      "2700/2700 [==============================] - 0s 32us/step - loss: 1.8442e-04 - mean_absolute_error: 0.0082\n",
      "\n",
      "Epoch 04082: loss did not improve\n",
      "Epoch 4083/10000\n",
      "2700/2700 [==============================] - 0s 29us/step - loss: 1.9678e-04 - mean_absolute_error: 0.0088\n",
      "\n",
      "Epoch 04083: loss did not improve\n",
      "Epoch 4084/10000\n",
      "2700/2700 [==============================] - 0s 31us/step - loss: 1.8363e-04 - mean_absolute_error: 0.0083\n",
      "\n",
      "Epoch 04084: loss did not improve\n",
      "Epoch 4085/10000\n",
      "2700/2700 [==============================] - 0s 31us/step - loss: 1.8555e-04 - mean_absolute_error: 0.0084\n",
      "\n",
      "Epoch 04085: loss did not improve\n",
      "Epoch 4086/10000\n",
      "2700/2700 [==============================] - 0s 29us/step - loss: 1.8598e-04 - mean_absolute_error: 0.0083\n",
      "\n",
      "Epoch 04086: loss did not improve\n",
      "Epoch 4087/10000\n",
      "2700/2700 [==============================] - 0s 30us/step - loss: 1.8142e-04 - mean_absolute_error: 0.0081\n",
      "\n",
      "Epoch 04087: loss improved from 0.00018 to 0.00018, saving model to keras/best_baseline_weights.hdf5\n",
      "Epoch 4088/10000\n",
      "2700/2700 [==============================] - 0s 32us/step - loss: 1.8127e-04 - mean_absolute_error: 0.0081\n",
      "\n",
      "Epoch 04088: loss improved from 0.00018 to 0.00018, saving model to keras/best_baseline_weights.hdf5\n",
      "Epoch 4089/10000\n",
      "2700/2700 [==============================] - 0s 33us/step - loss: 1.8341e-04 - mean_absolute_error: 0.0081\n",
      "\n",
      "Epoch 04089: loss did not improve\n",
      "Epoch 4090/10000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2700/2700 [==============================] - 0s 30us/step - loss: 1.8284e-04 - mean_absolute_error: 0.0081\n",
      "\n",
      "Epoch 04090: loss did not improve\n",
      "Epoch 4091/10000\n",
      "2700/2700 [==============================] - 0s 32us/step - loss: 1.8937e-04 - mean_absolute_error: 0.0085\n",
      "\n",
      "Epoch 04091: loss did not improve\n",
      "Epoch 4092/10000\n",
      "2700/2700 [==============================] - 0s 30us/step - loss: 1.8292e-04 - mean_absolute_error: 0.0082\n",
      "\n",
      "Epoch 04092: loss did not improve\n",
      "Epoch 4093/10000\n",
      "2700/2700 [==============================] - 0s 29us/step - loss: 1.8251e-04 - mean_absolute_error: 0.0081\n",
      "\n",
      "Epoch 04093: loss did not improve\n",
      "Epoch 4094/10000\n",
      "2700/2700 [==============================] - 0s 28us/step - loss: 1.8960e-04 - mean_absolute_error: 0.0085\n",
      "\n",
      "Epoch 04094: loss did not improve\n",
      "Epoch 4095/10000\n",
      "2700/2700 [==============================] - 0s 28us/step - loss: 1.8704e-04 - mean_absolute_error: 0.0084\n",
      "\n",
      "Epoch 04095: loss did not improve\n",
      "Epoch 4096/10000\n",
      "2700/2700 [==============================] - 0s 30us/step - loss: 1.8432e-04 - mean_absolute_error: 0.0083\n",
      "\n",
      "Epoch 04096: loss did not improve\n",
      "Epoch 4097/10000\n",
      "2700/2700 [==============================] - 0s 29us/step - loss: 1.8054e-04 - mean_absolute_error: 0.0081\n",
      "\n",
      "Epoch 04097: loss improved from 0.00018 to 0.00018, saving model to keras/best_baseline_weights.hdf5\n",
      "Epoch 4098/10000\n",
      "2700/2700 [==============================] - 0s 29us/step - loss: 1.8235e-04 - mean_absolute_error: 0.0082\n",
      "\n",
      "Epoch 04098: loss did not improve\n",
      "Epoch 4099/10000\n",
      "2700/2700 [==============================] - 0s 29us/step - loss: 1.8219e-04 - mean_absolute_error: 0.0081\n",
      "\n",
      "Epoch 04099: loss did not improve\n",
      "Epoch 4100/10000\n",
      "2700/2700 [==============================] - 0s 27us/step - loss: 1.8376e-04 - mean_absolute_error: 0.0083\n",
      "\n",
      "Epoch 04100: loss did not improve\n",
      "Epoch 4101/10000\n",
      "2700/2700 [==============================] - 0s 30us/step - loss: 1.8195e-04 - mean_absolute_error: 0.0082\n",
      "\n",
      "Epoch 04101: loss did not improve\n",
      "Epoch 4102/10000\n",
      "2700/2700 [==============================] - 0s 28us/step - loss: 1.8316e-04 - mean_absolute_error: 0.0083\n",
      "\n",
      "Epoch 04102: loss did not improve\n",
      "Epoch 4103/10000\n",
      "2700/2700 [==============================] - 0s 28us/step - loss: 1.8787e-04 - mean_absolute_error: 0.0083\n",
      "\n",
      "Epoch 04103: loss did not improve\n",
      "Epoch 4104/10000\n",
      "2700/2700 [==============================] - 0s 28us/step - loss: 1.7895e-04 - mean_absolute_error: 0.0080\n",
      "\n",
      "Epoch 04104: loss improved from 0.00018 to 0.00018, saving model to keras/best_baseline_weights.hdf5\n",
      "Epoch 4105/10000\n",
      "2700/2700 [==============================] - 0s 29us/step - loss: 1.8018e-04 - mean_absolute_error: 0.0080\n",
      "\n",
      "Epoch 04105: loss did not improve\n",
      "Epoch 4106/10000\n",
      "2700/2700 [==============================] - 0s 30us/step - loss: 1.8379e-04 - mean_absolute_error: 0.0083\n",
      "\n",
      "Epoch 04106: loss did not improve\n",
      "Epoch 4107/10000\n",
      "2700/2700 [==============================] - 0s 28us/step - loss: 1.8083e-04 - mean_absolute_error: 0.0080\n",
      "\n",
      "Epoch 04107: loss did not improve\n",
      "Epoch 4108/10000\n",
      "2700/2700 [==============================] - 0s 28us/step - loss: 1.8207e-04 - mean_absolute_error: 0.0081\n",
      "\n",
      "Epoch 04108: loss did not improve\n",
      "Epoch 4109/10000\n",
      "2700/2700 [==============================] - 0s 30us/step - loss: 1.8207e-04 - mean_absolute_error: 0.0082\n",
      "\n",
      "Epoch 04109: loss did not improve\n",
      "Epoch 4110/10000\n",
      "2700/2700 [==============================] - 0s 30us/step - loss: 1.8346e-04 - mean_absolute_error: 0.0083\n",
      "\n",
      "Epoch 04110: loss did not improve\n",
      "Epoch 4111/10000\n",
      "2700/2700 [==============================] - 0s 30us/step - loss: 1.8003e-04 - mean_absolute_error: 0.0081\n",
      "\n",
      "Epoch 04111: loss did not improve\n",
      "Epoch 4112/10000\n",
      "2700/2700 [==============================] - 0s 30us/step - loss: 1.8381e-04 - mean_absolute_error: 0.0084\n",
      "\n",
      "Epoch 04112: loss did not improve\n",
      "Epoch 4113/10000\n",
      "2700/2700 [==============================] - 0s 28us/step - loss: 1.8118e-04 - mean_absolute_error: 0.0081\n",
      "\n",
      "Epoch 04113: loss did not improve\n",
      "Epoch 4114/10000\n",
      "2700/2700 [==============================] - 0s 29us/step - loss: 1.8093e-04 - mean_absolute_error: 0.0082\n",
      "\n",
      "Epoch 04114: loss did not improve\n",
      "Epoch 4115/10000\n",
      "2700/2700 [==============================] - 0s 29us/step - loss: 1.8801e-04 - mean_absolute_error: 0.0084\n",
      "\n",
      "Epoch 04115: loss did not improve\n",
      "Epoch 4116/10000\n",
      "2700/2700 [==============================] - 0s 31us/step - loss: 1.8095e-04 - mean_absolute_error: 0.0081\n",
      "\n",
      "Epoch 04116: loss did not improve\n",
      "Epoch 4117/10000\n",
      "2700/2700 [==============================] - 0s 31us/step - loss: 1.8068e-04 - mean_absolute_error: 0.0081\n",
      "\n",
      "Epoch 04117: loss did not improve\n",
      "Epoch 4118/10000\n",
      "2700/2700 [==============================] - 0s 28us/step - loss: 1.8363e-04 - mean_absolute_error: 0.0083\n",
      "\n",
      "Epoch 04118: loss did not improve\n",
      "Epoch 4119/10000\n",
      "2700/2700 [==============================] - 0s 26us/step - loss: 1.8038e-04 - mean_absolute_error: 0.0081\n",
      "\n",
      "Epoch 04119: loss did not improve\n",
      "Epoch 4120/10000\n",
      "2700/2700 [==============================] - 0s 29us/step - loss: 1.8647e-04 - mean_absolute_error: 0.0086\n",
      "\n",
      "Epoch 04120: loss did not improve\n",
      "Epoch 4121/10000\n",
      "2700/2700 [==============================] - 0s 28us/step - loss: 1.7977e-04 - mean_absolute_error: 0.0080\n",
      "\n",
      "Epoch 04121: loss did not improve\n",
      "Epoch 4122/10000\n",
      "2700/2700 [==============================] - 0s 29us/step - loss: 1.7772e-04 - mean_absolute_error: 0.0080\n",
      "\n",
      "Epoch 04122: loss improved from 0.00018 to 0.00018, saving model to keras/best_baseline_weights.hdf5\n",
      "Epoch 4123/10000\n",
      "2700/2700 [==============================] - 0s 26us/step - loss: 1.8249e-04 - mean_absolute_error: 0.0083\n",
      "\n",
      "Epoch 04123: loss did not improve\n",
      "Epoch 4124/10000\n",
      "2700/2700 [==============================] - 0s 31us/step - loss: 1.8034e-04 - mean_absolute_error: 0.0082\n",
      "\n",
      "Epoch 04124: loss did not improve\n",
      "Epoch 4125/10000\n",
      "2700/2700 [==============================] - 0s 30us/step - loss: 1.8202e-04 - mean_absolute_error: 0.0082\n",
      "\n",
      "Epoch 04125: loss did not improve\n",
      "Epoch 4126/10000\n",
      "2700/2700 [==============================] - 0s 29us/step - loss: 1.8500e-04 - mean_absolute_error: 0.0085\n",
      "\n",
      "Epoch 04126: loss did not improve\n",
      "Epoch 4127/10000\n",
      "2700/2700 [==============================] - 0s 29us/step - loss: 1.7932e-04 - mean_absolute_error: 0.0081\n",
      "\n",
      "Epoch 04127: loss did not improve\n",
      "Epoch 4128/10000\n",
      "2700/2700 [==============================] - 0s 26us/step - loss: 1.7692e-04 - mean_absolute_error: 0.0080\n",
      "\n",
      "Epoch 04128: loss improved from 0.00018 to 0.00018, saving model to keras/best_baseline_weights.hdf5\n",
      "Epoch 4129/10000\n",
      "2700/2700 [==============================] - 0s 29us/step - loss: 1.8111e-04 - mean_absolute_error: 0.0082\n",
      "\n",
      "Epoch 04129: loss did not improve\n",
      "Epoch 4130/10000\n",
      "2700/2700 [==============================] - 0s 30us/step - loss: 1.8040e-04 - mean_absolute_error: 0.0080\n",
      "\n",
      "Epoch 04130: loss did not improve\n",
      "Epoch 4131/10000\n",
      "2700/2700 [==============================] - 0s 27us/step - loss: 1.8275e-04 - mean_absolute_error: 0.0083\n",
      "\n",
      "Epoch 04131: loss did not improve\n",
      "Epoch 4132/10000\n",
      "2700/2700 [==============================] - 0s 29us/step - loss: 1.7952e-04 - mean_absolute_error: 0.0081\n",
      "\n",
      "Epoch 04132: loss did not improve\n",
      "Epoch 4133/10000\n",
      "2700/2700 [==============================] - 0s 29us/step - loss: 1.7860e-04 - mean_absolute_error: 0.0080\n",
      "\n",
      "Epoch 04133: loss did not improve\n",
      "Epoch 4134/10000\n",
      "2700/2700 [==============================] - 0s 29us/step - loss: 1.8153e-04 - mean_absolute_error: 0.0083\n",
      "\n",
      "Epoch 04134: loss did not improve\n",
      "Epoch 4135/10000\n",
      "2700/2700 [==============================] - 0s 29us/step - loss: 1.7763e-04 - mean_absolute_error: 0.0080\n",
      "\n",
      "Epoch 04135: loss did not improve\n",
      "Epoch 4136/10000\n",
      "2700/2700 [==============================] - 0s 27us/step - loss: 1.7733e-04 - mean_absolute_error: 0.0080\n",
      "\n",
      "Epoch 04136: loss did not improve\n",
      "Epoch 4137/10000\n",
      "2700/2700 [==============================] - 0s 29us/step - loss: 1.7908e-04 - mean_absolute_error: 0.0082\n",
      "\n",
      "Epoch 04137: loss did not improve\n",
      "Epoch 4138/10000\n",
      "2700/2700 [==============================] - 0s 27us/step - loss: 1.7702e-04 - mean_absolute_error: 0.0080\n",
      "\n",
      "Epoch 04138: loss did not improve\n",
      "Epoch 4139/10000\n",
      "2700/2700 [==============================] - 0s 28us/step - loss: 1.7714e-04 - mean_absolute_error: 0.0080\n",
      "\n",
      "Epoch 04139: loss did not improve\n",
      "Epoch 4140/10000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2700/2700 [==============================] - 0s 29us/step - loss: 1.7644e-04 - mean_absolute_error: 0.0079\n",
      "\n",
      "Epoch 04140: loss improved from 0.00018 to 0.00018, saving model to keras/best_baseline_weights.hdf5\n",
      "Epoch 4141/10000\n",
      "2700/2700 [==============================] - 0s 29us/step - loss: 1.7888e-04 - mean_absolute_error: 0.0081\n",
      "\n",
      "Epoch 04141: loss did not improve\n",
      "Epoch 4142/10000\n",
      "2700/2700 [==============================] - 0s 27us/step - loss: 1.7640e-04 - mean_absolute_error: 0.0079\n",
      "\n",
      "Epoch 04142: loss improved from 0.00018 to 0.00018, saving model to keras/best_baseline_weights.hdf5\n",
      "Epoch 4143/10000\n",
      "2700/2700 [==============================] - 0s 27us/step - loss: 1.7672e-04 - mean_absolute_error: 0.0080\n",
      "\n",
      "Epoch 04143: loss did not improve\n",
      "Epoch 4144/10000\n",
      "2700/2700 [==============================] - 0s 30us/step - loss: 1.8544e-04 - mean_absolute_error: 0.0085\n",
      "\n",
      "Epoch 04144: loss did not improve\n",
      "Epoch 4145/10000\n",
      "2700/2700 [==============================] - 0s 29us/step - loss: 1.8268e-04 - mean_absolute_error: 0.0083\n",
      "\n",
      "Epoch 04145: loss did not improve\n",
      "Epoch 4146/10000\n",
      "2700/2700 [==============================] - 0s 28us/step - loss: 1.7543e-04 - mean_absolute_error: 0.0079\n",
      "\n",
      "Epoch 04146: loss improved from 0.00018 to 0.00018, saving model to keras/best_baseline_weights.hdf5\n",
      "Epoch 4147/10000\n",
      "2700/2700 [==============================] - 0s 26us/step - loss: 1.7703e-04 - mean_absolute_error: 0.0080\n",
      "\n",
      "Epoch 04147: loss did not improve\n",
      "Epoch 4148/10000\n",
      "2700/2700 [==============================] - 0s 28us/step - loss: 1.7666e-04 - mean_absolute_error: 0.0079\n",
      "\n",
      "Epoch 04148: loss did not improve\n",
      "Epoch 4149/10000\n",
      "2700/2700 [==============================] - 0s 29us/step - loss: 1.7652e-04 - mean_absolute_error: 0.0080\n",
      "\n",
      "Epoch 04149: loss did not improve\n",
      "Epoch 4150/10000\n",
      "2700/2700 [==============================] - 0s 28us/step - loss: 1.7621e-04 - mean_absolute_error: 0.0080\n",
      "\n",
      "Epoch 04150: loss did not improve\n",
      "Epoch 4151/10000\n",
      "2700/2700 [==============================] - 0s 34us/step - loss: 1.7687e-04 - mean_absolute_error: 0.0080\n",
      "\n",
      "Epoch 04151: loss did not improve\n",
      "Epoch 4152/10000\n",
      "2700/2700 [==============================] - 0s 29us/step - loss: 1.7764e-04 - mean_absolute_error: 0.0081\n",
      "\n",
      "Epoch 04152: loss did not improve\n",
      "Epoch 4153/10000\n",
      "2700/2700 [==============================] - 0s 30us/step - loss: 1.7889e-04 - mean_absolute_error: 0.0081\n",
      "\n",
      "Epoch 04153: loss did not improve\n",
      "Epoch 4154/10000\n",
      "2700/2700 [==============================] - 0s 29us/step - loss: 1.7599e-04 - mean_absolute_error: 0.0080\n",
      "\n",
      "Epoch 04154: loss did not improve\n",
      "Epoch 4155/10000\n",
      "2700/2700 [==============================] - 0s 30us/step - loss: 1.7624e-04 - mean_absolute_error: 0.0080\n",
      "\n",
      "Epoch 04155: loss did not improve\n",
      "Epoch 4156/10000\n",
      "2700/2700 [==============================] - 0s 32us/step - loss: 1.7496e-04 - mean_absolute_error: 0.0079\n",
      "\n",
      "Epoch 04156: loss improved from 0.00018 to 0.00017, saving model to keras/best_baseline_weights.hdf5\n",
      "Epoch 4157/10000\n",
      "2700/2700 [==============================] - 0s 32us/step - loss: 1.7740e-04 - mean_absolute_error: 0.0079\n",
      "\n",
      "Epoch 04157: loss did not improve\n",
      "Epoch 4158/10000\n",
      "2700/2700 [==============================] - 0s 28us/step - loss: 1.7760e-04 - mean_absolute_error: 0.0080\n",
      "\n",
      "Epoch 04158: loss did not improve\n",
      "Epoch 4159/10000\n",
      "2700/2700 [==============================] - 0s 28us/step - loss: 1.7640e-04 - mean_absolute_error: 0.0080\n",
      "\n",
      "Epoch 04159: loss did not improve\n",
      "Epoch 4160/10000\n",
      "2700/2700 [==============================] - 0s 42us/step - loss: 1.7802e-04 - mean_absolute_error: 0.0081\n",
      "\n",
      "Epoch 04160: loss did not improve\n",
      "Epoch 4161/10000\n",
      "2700/2700 [==============================] - 0s 30us/step - loss: 1.7501e-04 - mean_absolute_error: 0.0080\n",
      "\n",
      "Epoch 04161: loss did not improve\n",
      "Epoch 4162/10000\n",
      "2700/2700 [==============================] - 0s 30us/step - loss: 1.7658e-04 - mean_absolute_error: 0.0080\n",
      "\n",
      "Epoch 04162: loss did not improve\n",
      "Epoch 4163/10000\n",
      "2700/2700 [==============================] - 0s 32us/step - loss: 1.7427e-04 - mean_absolute_error: 0.0079\n",
      "\n",
      "Epoch 04163: loss improved from 0.00017 to 0.00017, saving model to keras/best_baseline_weights.hdf5\n",
      "Epoch 4164/10000\n",
      "2700/2700 [==============================] - 0s 31us/step - loss: 1.7554e-04 - mean_absolute_error: 0.0080\n",
      "\n",
      "Epoch 04164: loss did not improve\n",
      "Epoch 4165/10000\n",
      "2700/2700 [==============================] - 0s 29us/step - loss: 1.7464e-04 - mean_absolute_error: 0.0079\n",
      "\n",
      "Epoch 04165: loss did not improve\n",
      "Epoch 4166/10000\n",
      "2700/2700 [==============================] - 0s 30us/step - loss: 1.7563e-04 - mean_absolute_error: 0.0079\n",
      "\n",
      "Epoch 04166: loss did not improve\n",
      "Epoch 4167/10000\n",
      "2700/2700 [==============================] - 0s 29us/step - loss: 1.7756e-04 - mean_absolute_error: 0.0080\n",
      "\n",
      "Epoch 04167: loss did not improve\n",
      "Epoch 4168/10000\n",
      "2700/2700 [==============================] - 0s 30us/step - loss: 1.7473e-04 - mean_absolute_error: 0.0079\n",
      "\n",
      "Epoch 04168: loss did not improve\n",
      "Epoch 4169/10000\n",
      "2700/2700 [==============================] - 0s 28us/step - loss: 1.7673e-04 - mean_absolute_error: 0.0082\n",
      "\n",
      "Epoch 04169: loss did not improve\n",
      "Epoch 4170/10000\n",
      "2700/2700 [==============================] - 0s 32us/step - loss: 1.7395e-04 - mean_absolute_error: 0.0078\n",
      "\n",
      "Epoch 04170: loss improved from 0.00017 to 0.00017, saving model to keras/best_baseline_weights.hdf5\n",
      "Epoch 4171/10000\n",
      "2700/2700 [==============================] - 0s 45us/step - loss: 1.7387e-04 - mean_absolute_error: 0.0078\n",
      "\n",
      "Epoch 04171: loss improved from 0.00017 to 0.00017, saving model to keras/best_baseline_weights.hdf5\n",
      "Epoch 4172/10000\n",
      "2700/2700 [==============================] - 0s 30us/step - loss: 1.7645e-04 - mean_absolute_error: 0.0080\n",
      "\n",
      "Epoch 04172: loss did not improve\n",
      "Epoch 4173/10000\n",
      "2700/2700 [==============================] - 0s 28us/step - loss: 1.7649e-04 - mean_absolute_error: 0.0081\n",
      "\n",
      "Epoch 04173: loss did not improve\n",
      "Epoch 4174/10000\n",
      "2700/2700 [==============================] - 0s 30us/step - loss: 1.7970e-04 - mean_absolute_error: 0.0082\n",
      "\n",
      "Epoch 04174: loss did not improve\n",
      "Epoch 4175/10000\n",
      "2700/2700 [==============================] - 0s 40us/step - loss: 1.7465e-04 - mean_absolute_error: 0.0080\n",
      "\n",
      "Epoch 04175: loss did not improve\n",
      "Epoch 4176/10000\n",
      "2700/2700 [==============================] - 0s 36us/step - loss: 1.7512e-04 - mean_absolute_error: 0.0079\n",
      "\n",
      "Epoch 04176: loss did not improve\n",
      "Epoch 4177/10000\n",
      "2700/2700 [==============================] - 0s 27us/step - loss: 1.8098e-04 - mean_absolute_error: 0.0083\n",
      "\n",
      "Epoch 04177: loss did not improve\n",
      "Epoch 4178/10000\n",
      "2700/2700 [==============================] - 0s 31us/step - loss: 1.7623e-04 - mean_absolute_error: 0.0081\n",
      "\n",
      "Epoch 04178: loss did not improve\n",
      "Epoch 4179/10000\n",
      "2700/2700 [==============================] - 0s 31us/step - loss: 1.7352e-04 - mean_absolute_error: 0.0078\n",
      "\n",
      "Epoch 04179: loss improved from 0.00017 to 0.00017, saving model to keras/best_baseline_weights.hdf5\n",
      "Epoch 4180/10000\n",
      "2700/2700 [==============================] - 0s 44us/step - loss: 1.7856e-04 - mean_absolute_error: 0.0082\n",
      "\n",
      "Epoch 04180: loss did not improve\n",
      "Epoch 4181/10000\n",
      "2700/2700 [==============================] - 0s 34us/step - loss: 1.7476e-04 - mean_absolute_error: 0.0079\n",
      "\n",
      "Epoch 04181: loss did not improve\n",
      "Epoch 4182/10000\n",
      "2700/2700 [==============================] - 0s 30us/step - loss: 1.7254e-04 - mean_absolute_error: 0.0079\n",
      "\n",
      "Epoch 04182: loss improved from 0.00017 to 0.00017, saving model to keras/best_baseline_weights.hdf5\n",
      "Epoch 4183/10000\n",
      "2700/2700 [==============================] - 0s 32us/step - loss: 1.7353e-04 - mean_absolute_error: 0.0079\n",
      "\n",
      "Epoch 04183: loss did not improve\n",
      "Epoch 4184/10000\n",
      "2700/2700 [==============================] - 0s 30us/step - loss: 1.7178e-04 - mean_absolute_error: 0.0077\n",
      "\n",
      "Epoch 04184: loss improved from 0.00017 to 0.00017, saving model to keras/best_baseline_weights.hdf5\n",
      "Epoch 4185/10000\n",
      "2700/2700 [==============================] - 0s 27us/step - loss: 1.7545e-04 - mean_absolute_error: 0.0080\n",
      "\n",
      "Epoch 04185: loss did not improve\n",
      "Epoch 4186/10000\n",
      "2700/2700 [==============================] - 0s 29us/step - loss: 1.7345e-04 - mean_absolute_error: 0.0079\n",
      "\n",
      "Epoch 04186: loss did not improve\n",
      "Epoch 4187/10000\n",
      "2700/2700 [==============================] - 0s 27us/step - loss: 1.7224e-04 - mean_absolute_error: 0.0079\n",
      "\n",
      "Epoch 04187: loss did not improve\n",
      "Epoch 4188/10000\n",
      "2700/2700 [==============================] - 0s 27us/step - loss: 1.7403e-04 - mean_absolute_error: 0.0078\n",
      "\n",
      "Epoch 04188: loss did not improve\n",
      "Epoch 4189/10000\n",
      "2700/2700 [==============================] - 0s 27us/step - loss: 1.7179e-04 - mean_absolute_error: 0.0078\n",
      "\n",
      "Epoch 04189: loss did not improve\n",
      "Epoch 4190/10000\n",
      "2700/2700 [==============================] - 0s 29us/step - loss: 1.7296e-04 - mean_absolute_error: 0.0079\n",
      "\n",
      "Epoch 04190: loss did not improve\n",
      "Epoch 4191/10000\n",
      "2700/2700 [==============================] - 0s 26us/step - loss: 1.7354e-04 - mean_absolute_error: 0.0079\n",
      "\n",
      "Epoch 04191: loss did not improve\n",
      "Epoch 4192/10000\n",
      "2700/2700 [==============================] - 0s 28us/step - loss: 1.7209e-04 - mean_absolute_error: 0.0078\n",
      "\n",
      "Epoch 04192: loss did not improve\n",
      "Epoch 4193/10000\n",
      "2700/2700 [==============================] - 0s 29us/step - loss: 1.7474e-04 - mean_absolute_error: 0.0079\n",
      "\n",
      "Epoch 04193: loss did not improve\n",
      "Epoch 4194/10000\n",
      "2700/2700 [==============================] - 0s 28us/step - loss: 1.7323e-04 - mean_absolute_error: 0.0079\n",
      "\n",
      "Epoch 04194: loss did not improve\n",
      "Epoch 4195/10000\n",
      "2700/2700 [==============================] - 0s 31us/step - loss: 1.7792e-04 - mean_absolute_error: 0.0083\n",
      "\n",
      "Epoch 04195: loss did not improve\n",
      "Epoch 4196/10000\n",
      "2700/2700 [==============================] - 0s 28us/step - loss: 1.7394e-04 - mean_absolute_error: 0.0080\n",
      "\n",
      "Epoch 04196: loss did not improve\n",
      "Epoch 4197/10000\n",
      "2700/2700 [==============================] - 0s 29us/step - loss: 1.7619e-04 - mean_absolute_error: 0.0080\n",
      "\n",
      "Epoch 04197: loss did not improve\n",
      "Epoch 4198/10000\n",
      "2700/2700 [==============================] - 0s 27us/step - loss: 1.7718e-04 - mean_absolute_error: 0.0080\n",
      "\n",
      "Epoch 04198: loss did not improve\n",
      "Epoch 4199/10000\n",
      "2700/2700 [==============================] - 0s 27us/step - loss: 1.7400e-04 - mean_absolute_error: 0.0079\n",
      "\n",
      "Epoch 04199: loss did not improve\n",
      "Epoch 4200/10000\n",
      "2700/2700 [==============================] - 0s 28us/step - loss: 1.7410e-04 - mean_absolute_error: 0.0079\n",
      "\n",
      "Epoch 04200: loss did not improve\n",
      "Epoch 4201/10000\n",
      "2700/2700 [==============================] - 0s 28us/step - loss: 1.7106e-04 - mean_absolute_error: 0.0077\n",
      "\n",
      "Epoch 04201: loss improved from 0.00017 to 0.00017, saving model to keras/best_baseline_weights.hdf5\n",
      "Epoch 4202/10000\n",
      "2700/2700 [==============================] - 0s 30us/step - loss: 1.7387e-04 - mean_absolute_error: 0.0080\n",
      "\n",
      "Epoch 04202: loss did not improve\n",
      "Epoch 4203/10000\n",
      "2700/2700 [==============================] - 0s 28us/step - loss: 1.7456e-04 - mean_absolute_error: 0.0080\n",
      "\n",
      "Epoch 04203: loss did not improve\n",
      "Epoch 4204/10000\n",
      "2700/2700 [==============================] - 0s 28us/step - loss: 1.7195e-04 - mean_absolute_error: 0.0079\n",
      "\n",
      "Epoch 04204: loss did not improve\n",
      "Epoch 4205/10000\n",
      "2700/2700 [==============================] - 0s 29us/step - loss: 1.7167e-04 - mean_absolute_error: 0.0080\n",
      "\n",
      "Epoch 04205: loss did not improve\n",
      "Epoch 4206/10000\n",
      "2700/2700 [==============================] - 0s 30us/step - loss: 1.7191e-04 - mean_absolute_error: 0.0078\n",
      "\n",
      "Epoch 04206: loss did not improve\n",
      "Epoch 4207/10000\n",
      "2700/2700 [==============================] - 0s 30us/step - loss: 1.7266e-04 - mean_absolute_error: 0.0078\n",
      "\n",
      "Epoch 04207: loss did not improve\n",
      "Epoch 4208/10000\n",
      "2700/2700 [==============================] - 0s 29us/step - loss: 1.7732e-04 - mean_absolute_error: 0.0081\n",
      "\n",
      "Epoch 04208: loss did not improve\n",
      "Epoch 4209/10000\n",
      "2700/2700 [==============================] - 0s 27us/step - loss: 1.7076e-04 - mean_absolute_error: 0.0077\n",
      "\n",
      "Epoch 04209: loss improved from 0.00017 to 0.00017, saving model to keras/best_baseline_weights.hdf5\n",
      "Epoch 4210/10000\n",
      "2700/2700 [==============================] - 0s 28us/step - loss: 1.8155e-04 - mean_absolute_error: 0.0085\n",
      "\n",
      "Epoch 04210: loss did not improve\n",
      "Epoch 4211/10000\n",
      "2700/2700 [==============================] - 0s 28us/step - loss: 1.7036e-04 - mean_absolute_error: 0.0077\n",
      "\n",
      "Epoch 04211: loss improved from 0.00017 to 0.00017, saving model to keras/best_baseline_weights.hdf5\n",
      "Epoch 4212/10000\n",
      "2700/2700 [==============================] - 0s 31us/step - loss: 1.7157e-04 - mean_absolute_error: 0.0079\n",
      "\n",
      "Epoch 04212: loss did not improve\n",
      "Epoch 4213/10000\n",
      "2700/2700 [==============================] - 0s 30us/step - loss: 1.7300e-04 - mean_absolute_error: 0.0080\n",
      "\n",
      "Epoch 04213: loss did not improve\n",
      "Epoch 4214/10000\n",
      "2700/2700 [==============================] - 0s 34us/step - loss: 1.7307e-04 - mean_absolute_error: 0.0080\n",
      "\n",
      "Epoch 04214: loss did not improve\n",
      "Epoch 4215/10000\n",
      "2700/2700 [==============================] - 0s 37us/step - loss: 1.7642e-04 - mean_absolute_error: 0.0081\n",
      "\n",
      "Epoch 04215: loss did not improve\n",
      "Epoch 4216/10000\n",
      "2700/2700 [==============================] - 0s 40us/step - loss: 1.7064e-04 - mean_absolute_error: 0.0078\n",
      "\n",
      "Epoch 04216: loss did not improve\n",
      "Epoch 4217/10000\n",
      "2700/2700 [==============================] - 0s 37us/step - loss: 1.7106e-04 - mean_absolute_error: 0.0078\n",
      "\n",
      "Epoch 04217: loss did not improve\n",
      "Epoch 4218/10000\n",
      "2700/2700 [==============================] - 0s 28us/step - loss: 1.7304e-04 - mean_absolute_error: 0.0079\n",
      "\n",
      "Epoch 04218: loss did not improve\n",
      "Epoch 4219/10000\n",
      "2700/2700 [==============================] - 0s 29us/step - loss: 1.7120e-04 - mean_absolute_error: 0.0080\n",
      "\n",
      "Epoch 04219: loss did not improve\n",
      "Epoch 4220/10000\n",
      "2700/2700 [==============================] - 0s 29us/step - loss: 1.7220e-04 - mean_absolute_error: 0.0080\n",
      "\n",
      "Epoch 04220: loss did not improve\n",
      "Epoch 4221/10000\n",
      "2700/2700 [==============================] - 0s 30us/step - loss: 1.8000e-04 - mean_absolute_error: 0.0083\n",
      "\n",
      "Epoch 04221: loss did not improve\n",
      "Epoch 4222/10000\n",
      "2700/2700 [==============================] - 0s 29us/step - loss: 1.7005e-04 - mean_absolute_error: 0.0078\n",
      "\n",
      "Epoch 04222: loss improved from 0.00017 to 0.00017, saving model to keras/best_baseline_weights.hdf5\n",
      "Epoch 4223/10000\n",
      "2700/2700 [==============================] - 0s 26us/step - loss: 1.7055e-04 - mean_absolute_error: 0.0078\n",
      "\n",
      "Epoch 04223: loss did not improve\n",
      "Epoch 4224/10000\n",
      "2700/2700 [==============================] - 0s 27us/step - loss: 1.6935e-04 - mean_absolute_error: 0.0077\n",
      "\n",
      "Epoch 04224: loss improved from 0.00017 to 0.00017, saving model to keras/best_baseline_weights.hdf5\n",
      "Epoch 4225/10000\n",
      "2700/2700 [==============================] - 0s 28us/step - loss: 1.6879e-04 - mean_absolute_error: 0.0077\n",
      "\n",
      "Epoch 04225: loss improved from 0.00017 to 0.00017, saving model to keras/best_baseline_weights.hdf5\n",
      "Epoch 4226/10000\n",
      "2700/2700 [==============================] - 0s 28us/step - loss: 1.7072e-04 - mean_absolute_error: 0.0078\n",
      "\n",
      "Epoch 04226: loss did not improve\n",
      "Epoch 4227/10000\n",
      "2700/2700 [==============================] - 0s 31us/step - loss: 1.7368e-04 - mean_absolute_error: 0.0080\n",
      "\n",
      "Epoch 04227: loss did not improve\n",
      "Epoch 4228/10000\n",
      "2700/2700 [==============================] - 0s 34us/step - loss: 1.6935e-04 - mean_absolute_error: 0.0078\n",
      "\n",
      "Epoch 04228: loss did not improve\n",
      "Epoch 4229/10000\n",
      "2700/2700 [==============================] - 0s 37us/step - loss: 1.7047e-04 - mean_absolute_error: 0.0079\n",
      "\n",
      "Epoch 04229: loss did not improve\n",
      "Epoch 4230/10000\n",
      "2700/2700 [==============================] - 0s 36us/step - loss: 1.6961e-04 - mean_absolute_error: 0.0077\n",
      "\n",
      "Epoch 04230: loss did not improve\n",
      "Epoch 4231/10000\n",
      "2700/2700 [==============================] - 0s 33us/step - loss: 1.6893e-04 - mean_absolute_error: 0.0077\n",
      "\n",
      "Epoch 04231: loss did not improve\n",
      "Epoch 4232/10000\n",
      "2700/2700 [==============================] - 0s 28us/step - loss: 1.7396e-04 - mean_absolute_error: 0.0081\n",
      "\n",
      "Epoch 04232: loss did not improve\n",
      "Epoch 4233/10000\n",
      "2700/2700 [==============================] - 0s 30us/step - loss: 1.7178e-04 - mean_absolute_error: 0.0078\n",
      "\n",
      "Epoch 04233: loss did not improve\n",
      "Epoch 4234/10000\n",
      "2700/2700 [==============================] - 0s 30us/step - loss: 1.6645e-04 - mean_absolute_error: 0.0076\n",
      "\n",
      "Epoch 04234: loss improved from 0.00017 to 0.00017, saving model to keras/best_baseline_weights.hdf5\n",
      "Epoch 4235/10000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2700/2700 [==============================] - 0s 29us/step - loss: 1.6952e-04 - mean_absolute_error: 0.0078\n",
      "\n",
      "Epoch 04235: loss did not improve\n",
      "Epoch 4236/10000\n",
      "2700/2700 [==============================] - 0s 28us/step - loss: 1.7188e-04 - mean_absolute_error: 0.0079\n",
      "\n",
      "Epoch 04236: loss did not improve\n",
      "Epoch 4237/10000\n",
      "2700/2700 [==============================] - 0s 27us/step - loss: 1.7092e-04 - mean_absolute_error: 0.0079\n",
      "\n",
      "Epoch 04237: loss did not improve\n",
      "Epoch 4238/10000\n",
      "2700/2700 [==============================] - 0s 28us/step - loss: 1.6972e-04 - mean_absolute_error: 0.0078\n",
      "\n",
      "Epoch 04238: loss did not improve\n",
      "Epoch 4239/10000\n",
      "2700/2700 [==============================] - 0s 29us/step - loss: 1.7202e-04 - mean_absolute_error: 0.0080\n",
      "\n",
      "Epoch 04239: loss did not improve\n",
      "Epoch 4240/10000\n",
      "2700/2700 [==============================] - 0s 27us/step - loss: 1.6789e-04 - mean_absolute_error: 0.0077\n",
      "\n",
      "Epoch 04240: loss did not improve\n",
      "Epoch 4241/10000\n",
      "2700/2700 [==============================] - 0s 26us/step - loss: 1.6704e-04 - mean_absolute_error: 0.0077\n",
      "\n",
      "Epoch 04241: loss did not improve\n",
      "Epoch 4242/10000\n",
      "2700/2700 [==============================] - 0s 32us/step - loss: 1.7369e-04 - mean_absolute_error: 0.0081\n",
      "\n",
      "Epoch 04242: loss did not improve\n",
      "Epoch 4243/10000\n",
      "2700/2700 [==============================] - 0s 35us/step - loss: 1.6989e-04 - mean_absolute_error: 0.0078\n",
      "\n",
      "Epoch 04243: loss did not improve\n",
      "Epoch 4244/10000\n",
      "2700/2700 [==============================] - 0s 35us/step - loss: 1.6793e-04 - mean_absolute_error: 0.0078\n",
      "\n",
      "Epoch 04244: loss did not improve\n",
      "Epoch 4245/10000\n",
      "2700/2700 [==============================] - 0s 34us/step - loss: 1.6774e-04 - mean_absolute_error: 0.0078\n",
      "\n",
      "Epoch 04245: loss did not improve\n",
      "Epoch 4246/10000\n",
      "2700/2700 [==============================] - 0s 35us/step - loss: 1.6798e-04 - mean_absolute_error: 0.0078\n",
      "\n",
      "Epoch 04246: loss did not improve\n",
      "Epoch 4247/10000\n",
      "2700/2700 [==============================] - 0s 28us/step - loss: 1.7180e-04 - mean_absolute_error: 0.0080\n",
      "\n",
      "Epoch 04247: loss did not improve\n",
      "Epoch 4248/10000\n",
      "2700/2700 [==============================] - 0s 29us/step - loss: 1.7283e-04 - mean_absolute_error: 0.0080\n",
      "\n",
      "Epoch 04248: loss did not improve\n",
      "Epoch 4249/10000\n",
      "2700/2700 [==============================] - 0s 28us/step - loss: 1.7227e-04 - mean_absolute_error: 0.0080\n",
      "\n",
      "Epoch 04249: loss did not improve\n",
      "Epoch 4250/10000\n",
      "2700/2700 [==============================] - 0s 30us/step - loss: 1.7029e-04 - mean_absolute_error: 0.0078\n",
      "\n",
      "Epoch 04250: loss did not improve\n",
      "Epoch 4251/10000\n",
      "2700/2700 [==============================] - 0s 28us/step - loss: 1.7244e-04 - mean_absolute_error: 0.0081\n",
      "\n",
      "Epoch 04251: loss did not improve\n",
      "Epoch 4252/10000\n",
      "2700/2700 [==============================] - 0s 29us/step - loss: 1.7109e-04 - mean_absolute_error: 0.0079\n",
      "\n",
      "Epoch 04252: loss did not improve\n",
      "Epoch 4253/10000\n",
      "2700/2700 [==============================] - 0s 27us/step - loss: 1.6617e-04 - mean_absolute_error: 0.0076\n",
      "\n",
      "Epoch 04253: loss improved from 0.00017 to 0.00017, saving model to keras/best_baseline_weights.hdf5\n",
      "Epoch 4254/10000\n",
      "2700/2700 [==============================] - 0s 28us/step - loss: 1.6794e-04 - mean_absolute_error: 0.0077\n",
      "\n",
      "Epoch 04254: loss did not improve\n",
      "Epoch 4255/10000\n",
      "2700/2700 [==============================] - 0s 32us/step - loss: 1.6758e-04 - mean_absolute_error: 0.0077\n",
      "\n",
      "Epoch 04255: loss did not improve\n",
      "Epoch 4256/10000\n",
      "2700/2700 [==============================] - 0s 37us/step - loss: 1.7116e-04 - mean_absolute_error: 0.0080\n",
      "\n",
      "Epoch 04256: loss did not improve\n",
      "Epoch 4257/10000\n",
      "2700/2700 [==============================] - 0s 37us/step - loss: 1.6976e-04 - mean_absolute_error: 0.0079\n",
      "\n",
      "Epoch 04257: loss did not improve\n",
      "Epoch 4258/10000\n",
      "2700/2700 [==============================] - 0s 37us/step - loss: 1.6947e-04 - mean_absolute_error: 0.0079\n",
      "\n",
      "Epoch 04258: loss did not improve\n",
      "Epoch 4259/10000\n",
      "2700/2700 [==============================] - 0s 37us/step - loss: 1.7039e-04 - mean_absolute_error: 0.0078\n",
      "\n",
      "Epoch 04259: loss did not improve\n",
      "Epoch 4260/10000\n",
      "2700/2700 [==============================] - 0s 32us/step - loss: 1.7914e-04 - mean_absolute_error: 0.0085\n",
      "\n",
      "Epoch 04260: loss did not improve\n",
      "Epoch 4261/10000\n",
      "2700/2700 [==============================] - 0s 32us/step - loss: 1.6498e-04 - mean_absolute_error: 0.0076\n",
      "\n",
      "Epoch 04261: loss improved from 0.00017 to 0.00016, saving model to keras/best_baseline_weights.hdf5\n",
      "Epoch 4262/10000\n",
      "2700/2700 [==============================] - 0s 29us/step - loss: 1.7124e-04 - mean_absolute_error: 0.0080\n",
      "\n",
      "Epoch 04262: loss did not improve\n",
      "Epoch 4263/10000\n",
      "2700/2700 [==============================] - 0s 29us/step - loss: 1.6649e-04 - mean_absolute_error: 0.0076\n",
      "\n",
      "Epoch 04263: loss did not improve\n",
      "Epoch 4264/10000\n",
      "2700/2700 [==============================] - 0s 27us/step - loss: 1.6972e-04 - mean_absolute_error: 0.0078\n",
      "\n",
      "Epoch 04264: loss did not improve\n",
      "Epoch 4265/10000\n",
      "2700/2700 [==============================] - 0s 27us/step - loss: 1.6708e-04 - mean_absolute_error: 0.0077\n",
      "\n",
      "Epoch 04265: loss did not improve\n",
      "Epoch 4266/10000\n",
      "2700/2700 [==============================] - 0s 27us/step - loss: 1.6596e-04 - mean_absolute_error: 0.0077\n",
      "\n",
      "Epoch 04266: loss did not improve\n",
      "Epoch 4267/10000\n",
      "2700/2700 [==============================] - 0s 28us/step - loss: 1.6629e-04 - mean_absolute_error: 0.0076\n",
      "\n",
      "Epoch 04267: loss did not improve\n",
      "Epoch 4268/10000\n",
      "2700/2700 [==============================] - 0s 27us/step - loss: 1.6722e-04 - mean_absolute_error: 0.0078\n",
      "\n",
      "Epoch 04268: loss did not improve\n",
      "Epoch 4269/10000\n",
      "2700/2700 [==============================] - 0s 29us/step - loss: 1.6735e-04 - mean_absolute_error: 0.0077\n",
      "\n",
      "Epoch 04269: loss did not improve\n",
      "Epoch 4270/10000\n",
      "2700/2700 [==============================] - 0s 31us/step - loss: 1.6645e-04 - mean_absolute_error: 0.0077\n",
      "\n",
      "Epoch 04270: loss did not improve\n",
      "Epoch 4271/10000\n",
      "2700/2700 [==============================] - 0s 29us/step - loss: 1.6682e-04 - mean_absolute_error: 0.0077\n",
      "\n",
      "Epoch 04271: loss did not improve\n",
      "Epoch 4272/10000\n",
      "2700/2700 [==============================] - 0s 26us/step - loss: 1.6756e-04 - mean_absolute_error: 0.0077\n",
      "\n",
      "Epoch 04272: loss did not improve\n",
      "Epoch 4273/10000\n",
      "2700/2700 [==============================] - 0s 27us/step - loss: 1.6491e-04 - mean_absolute_error: 0.0076\n",
      "\n",
      "Epoch 04273: loss improved from 0.00016 to 0.00016, saving model to keras/best_baseline_weights.hdf5\n",
      "Epoch 4274/10000\n",
      "2700/2700 [==============================] - 0s 29us/step - loss: 1.6664e-04 - mean_absolute_error: 0.0077\n",
      "\n",
      "Epoch 04274: loss did not improve\n",
      "Epoch 4275/10000\n",
      "2700/2700 [==============================] - 0s 28us/step - loss: 1.6617e-04 - mean_absolute_error: 0.0078\n",
      "\n",
      "Epoch 04275: loss did not improve\n",
      "Epoch 4276/10000\n",
      "2700/2700 [==============================] - 0s 29us/step - loss: 1.6543e-04 - mean_absolute_error: 0.0076\n",
      "\n",
      "Epoch 04276: loss did not improve\n",
      "Epoch 4277/10000\n",
      "2700/2700 [==============================] - 0s 28us/step - loss: 1.6741e-04 - mean_absolute_error: 0.0077\n",
      "\n",
      "Epoch 04277: loss did not improve\n",
      "Epoch 4278/10000\n",
      "2700/2700 [==============================] - 0s 28us/step - loss: 1.6858e-04 - mean_absolute_error: 0.0078\n",
      "\n",
      "Epoch 04278: loss did not improve\n",
      "Epoch 4279/10000\n",
      "2700/2700 [==============================] - 0s 31us/step - loss: 1.6742e-04 - mean_absolute_error: 0.0077\n",
      "\n",
      "Epoch 04279: loss did not improve\n",
      "Epoch 4280/10000\n",
      "2700/2700 [==============================] - 0s 31us/step - loss: 1.6412e-04 - mean_absolute_error: 0.0076\n",
      "\n",
      "Epoch 04280: loss improved from 0.00016 to 0.00016, saving model to keras/best_baseline_weights.hdf5\n",
      "Epoch 4281/10000\n",
      "2700/2700 [==============================] - 0s 31us/step - loss: 1.6786e-04 - mean_absolute_error: 0.0079\n",
      "\n",
      "Epoch 04281: loss did not improve\n",
      "Epoch 4282/10000\n",
      "2700/2700 [==============================] - 0s 32us/step - loss: 1.6674e-04 - mean_absolute_error: 0.0077\n",
      "\n",
      "Epoch 04282: loss did not improve\n",
      "Epoch 4283/10000\n",
      "2700/2700 [==============================] - 0s 27us/step - loss: 1.6789e-04 - mean_absolute_error: 0.0078\n",
      "\n",
      "Epoch 04283: loss did not improve\n",
      "Epoch 4284/10000\n",
      "2700/2700 [==============================] - 0s 29us/step - loss: 1.6498e-04 - mean_absolute_error: 0.0077\n",
      "\n",
      "Epoch 04284: loss did not improve\n",
      "Epoch 4285/10000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2700/2700 [==============================] - 0s 28us/step - loss: 1.6625e-04 - mean_absolute_error: 0.0077\n",
      "\n",
      "Epoch 04285: loss did not improve\n",
      "Epoch 4286/10000\n",
      "2700/2700 [==============================] - 0s 28us/step - loss: 1.6650e-04 - mean_absolute_error: 0.0077\n",
      "\n",
      "Epoch 04286: loss did not improve\n",
      "Epoch 4287/10000\n",
      "2700/2700 [==============================] - 0s 29us/step - loss: 1.6472e-04 - mean_absolute_error: 0.0076\n",
      "\n",
      "Epoch 04287: loss did not improve\n",
      "Epoch 4288/10000\n",
      "2700/2700 [==============================] - 0s 27us/step - loss: 1.6781e-04 - mean_absolute_error: 0.0078\n",
      "\n",
      "Epoch 04288: loss did not improve\n",
      "Epoch 4289/10000\n",
      "2700/2700 [==============================] - 0s 28us/step - loss: 1.6489e-04 - mean_absolute_error: 0.0076\n",
      "\n",
      "Epoch 04289: loss did not improve\n",
      "Epoch 4290/10000\n",
      "2700/2700 [==============================] - 0s 29us/step - loss: 1.6805e-04 - mean_absolute_error: 0.0078\n",
      "\n",
      "Epoch 04290: loss did not improve\n",
      "Epoch 4291/10000\n",
      "2700/2700 [==============================] - 0s 27us/step - loss: 1.6355e-04 - mean_absolute_error: 0.0075\n",
      "\n",
      "Epoch 04291: loss improved from 0.00016 to 0.00016, saving model to keras/best_baseline_weights.hdf5\n",
      "Epoch 4292/10000\n",
      "2700/2700 [==============================] - 0s 26us/step - loss: 1.6564e-04 - mean_absolute_error: 0.0077\n",
      "\n",
      "Epoch 04292: loss did not improve\n",
      "Epoch 4293/10000\n",
      "2700/2700 [==============================] - 0s 28us/step - loss: 1.6329e-04 - mean_absolute_error: 0.0076\n",
      "\n",
      "Epoch 04293: loss improved from 0.00016 to 0.00016, saving model to keras/best_baseline_weights.hdf5\n",
      "Epoch 4294/10000\n",
      "2700/2700 [==============================] - 0s 28us/step - loss: 1.6667e-04 - mean_absolute_error: 0.0078\n",
      "\n",
      "Epoch 04294: loss did not improve\n",
      "Epoch 4295/10000\n",
      "2700/2700 [==============================] - 0s 29us/step - loss: 1.6420e-04 - mean_absolute_error: 0.0075\n",
      "\n",
      "Epoch 04295: loss did not improve\n",
      "Epoch 4296/10000\n",
      "2700/2700 [==============================] - 0s 27us/step - loss: 1.6461e-04 - mean_absolute_error: 0.0076\n",
      "\n",
      "Epoch 04296: loss did not improve\n",
      "Epoch 4297/10000\n",
      "2700/2700 [==============================] - 0s 29us/step - loss: 1.6542e-04 - mean_absolute_error: 0.0076\n",
      "\n",
      "Epoch 04297: loss did not improve\n",
      "Epoch 4298/10000\n",
      "2700/2700 [==============================] - 0s 29us/step - loss: 1.6696e-04 - mean_absolute_error: 0.0077\n",
      "\n",
      "Epoch 04298: loss did not improve\n",
      "Epoch 4299/10000\n",
      "2700/2700 [==============================] - 0s 27us/step - loss: 1.6752e-04 - mean_absolute_error: 0.0077\n",
      "\n",
      "Epoch 04299: loss did not improve\n",
      "Epoch 4300/10000\n",
      "2700/2700 [==============================] - 0s 30us/step - loss: 1.6301e-04 - mean_absolute_error: 0.0075\n",
      "\n",
      "Epoch 04300: loss improved from 0.00016 to 0.00016, saving model to keras/best_baseline_weights.hdf5\n",
      "Epoch 4301/10000\n",
      "2700/2700 [==============================] - 0s 29us/step - loss: 1.6641e-04 - mean_absolute_error: 0.0078\n",
      "\n",
      "Epoch 04301: loss did not improve\n",
      "Epoch 4302/10000\n",
      "2700/2700 [==============================] - 0s 35us/step - loss: 1.6591e-04 - mean_absolute_error: 0.0079\n",
      "\n",
      "Epoch 04302: loss did not improve\n",
      "Epoch 4303/10000\n",
      "2700/2700 [==============================] - 0s 37us/step - loss: 1.6719e-04 - mean_absolute_error: 0.0079\n",
      "\n",
      "Epoch 04303: loss did not improve\n",
      "Epoch 4304/10000\n",
      "2700/2700 [==============================] - 0s 37us/step - loss: 1.6681e-04 - mean_absolute_error: 0.0078\n",
      "\n",
      "Epoch 04304: loss did not improve\n",
      "Epoch 4305/10000\n",
      "2700/2700 [==============================] - 0s 32us/step - loss: 1.6639e-04 - mean_absolute_error: 0.0078\n",
      "\n",
      "Epoch 04305: loss did not improve\n",
      "Epoch 4306/10000\n",
      "2700/2700 [==============================] - 0s 30us/step - loss: 1.6520e-04 - mean_absolute_error: 0.0077\n",
      "\n",
      "Epoch 04306: loss did not improve\n",
      "Epoch 4307/10000\n",
      "2700/2700 [==============================] - 0s 30us/step - loss: 1.6297e-04 - mean_absolute_error: 0.0075\n",
      "\n",
      "Epoch 04307: loss improved from 0.00016 to 0.00016, saving model to keras/best_baseline_weights.hdf5\n",
      "Epoch 4308/10000\n",
      "2700/2700 [==============================] - 0s 29us/step - loss: 1.6583e-04 - mean_absolute_error: 0.0078\n",
      "\n",
      "Epoch 04308: loss did not improve\n",
      "Epoch 4309/10000\n",
      "2700/2700 [==============================] - 0s 31us/step - loss: 1.6310e-04 - mean_absolute_error: 0.0076\n",
      "\n",
      "Epoch 04309: loss did not improve\n",
      "Epoch 4310/10000\n",
      "2700/2700 [==============================] - 0s 30us/step - loss: 1.6173e-04 - mean_absolute_error: 0.0075\n",
      "\n",
      "Epoch 04310: loss improved from 0.00016 to 0.00016, saving model to keras/best_baseline_weights.hdf5\n",
      "Epoch 4311/10000\n",
      "2700/2700 [==============================] - 0s 26us/step - loss: 1.6288e-04 - mean_absolute_error: 0.0075\n",
      "\n",
      "Epoch 04311: loss did not improve\n",
      "Epoch 4312/10000\n",
      "2700/2700 [==============================] - 0s 28us/step - loss: 1.6275e-04 - mean_absolute_error: 0.0075\n",
      "\n",
      "Epoch 04312: loss did not improve\n",
      "Epoch 4313/10000\n",
      "2700/2700 [==============================] - 0s 28us/step - loss: 1.6249e-04 - mean_absolute_error: 0.0075\n",
      "\n",
      "Epoch 04313: loss did not improve\n",
      "Epoch 4314/10000\n",
      "2700/2700 [==============================] - 0s 28us/step - loss: 1.6162e-04 - mean_absolute_error: 0.0075\n",
      "\n",
      "Epoch 04314: loss improved from 0.00016 to 0.00016, saving model to keras/best_baseline_weights.hdf5\n",
      "Epoch 4315/10000\n",
      "2700/2700 [==============================] - 0s 29us/step - loss: 1.6510e-04 - mean_absolute_error: 0.0078\n",
      "\n",
      "Epoch 04315: loss did not improve\n",
      "Epoch 4316/10000\n",
      "2700/2700 [==============================] - 0s 25us/step - loss: 1.6207e-04 - mean_absolute_error: 0.0076\n",
      "\n",
      "Epoch 04316: loss did not improve\n",
      "Epoch 4317/10000\n",
      "2700/2700 [==============================] - 0s 28us/step - loss: 1.6504e-04 - mean_absolute_error: 0.0077\n",
      "\n",
      "Epoch 04317: loss did not improve\n",
      "Epoch 4318/10000\n",
      "2700/2700 [==============================] - 0s 28us/step - loss: 1.6566e-04 - mean_absolute_error: 0.0078\n",
      "\n",
      "Epoch 04318: loss did not improve\n",
      "Epoch 4319/10000\n",
      "2700/2700 [==============================] - 0s 27us/step - loss: 1.6332e-04 - mean_absolute_error: 0.0076\n",
      "\n",
      "Epoch 04319: loss did not improve\n",
      "Epoch 4320/10000\n",
      "2700/2700 [==============================] - 0s 27us/step - loss: 1.6160e-04 - mean_absolute_error: 0.0075\n",
      "\n",
      "Epoch 04320: loss improved from 0.00016 to 0.00016, saving model to keras/best_baseline_weights.hdf5\n",
      "Epoch 4321/10000\n",
      "2700/2700 [==============================] - 0s 28us/step - loss: 1.6179e-04 - mean_absolute_error: 0.0074\n",
      "\n",
      "Epoch 04321: loss did not improve\n",
      "Epoch 4322/10000\n",
      "2700/2700 [==============================] - 0s 29us/step - loss: 1.6700e-04 - mean_absolute_error: 0.0079\n",
      "\n",
      "Epoch 04322: loss did not improve\n",
      "Epoch 4323/10000\n",
      "2700/2700 [==============================] - 0s 29us/step - loss: 1.6300e-04 - mean_absolute_error: 0.0076\n",
      "\n",
      "Epoch 04323: loss did not improve\n",
      "Epoch 4324/10000\n",
      "2700/2700 [==============================] - 0s 29us/step - loss: 1.6376e-04 - mean_absolute_error: 0.0076\n",
      "\n",
      "Epoch 04324: loss did not improve\n",
      "Epoch 4325/10000\n",
      "2700/2700 [==============================] - 0s 27us/step - loss: 1.6271e-04 - mean_absolute_error: 0.0075\n",
      "\n",
      "Epoch 04325: loss did not improve\n",
      "Epoch 4326/10000\n",
      "2700/2700 [==============================] - 0s 28us/step - loss: 1.6308e-04 - mean_absolute_error: 0.0076\n",
      "\n",
      "Epoch 04326: loss did not improve\n",
      "Epoch 4327/10000\n",
      "2700/2700 [==============================] - 0s 30us/step - loss: 1.6336e-04 - mean_absolute_error: 0.0077\n",
      "\n",
      "Epoch 04327: loss did not improve\n",
      "Epoch 4328/10000\n",
      "2700/2700 [==============================] - 0s 28us/step - loss: 1.6301e-04 - mean_absolute_error: 0.0075\n",
      "\n",
      "Epoch 04328: loss did not improve\n",
      "Epoch 4329/10000\n",
      "2700/2700 [==============================] - 0s 27us/step - loss: 1.6144e-04 - mean_absolute_error: 0.0075\n",
      "\n",
      "Epoch 04329: loss improved from 0.00016 to 0.00016, saving model to keras/best_baseline_weights.hdf5\n",
      "Epoch 4330/10000\n",
      "2700/2700 [==============================] - 0s 27us/step - loss: 1.6177e-04 - mean_absolute_error: 0.0075\n",
      "\n",
      "Epoch 04330: loss did not improve\n",
      "Epoch 4331/10000\n",
      "2700/2700 [==============================] - 0s 29us/step - loss: 1.6285e-04 - mean_absolute_error: 0.0076\n",
      "\n",
      "Epoch 04331: loss did not improve\n",
      "Epoch 4332/10000\n",
      "2700/2700 [==============================] - 0s 29us/step - loss: 1.6422e-04 - mean_absolute_error: 0.0077\n",
      "\n",
      "Epoch 04332: loss did not improve\n",
      "Epoch 4333/10000\n",
      "2700/2700 [==============================] - 0s 28us/step - loss: 1.6205e-04 - mean_absolute_error: 0.0075\n",
      "\n",
      "Epoch 04333: loss did not improve\n",
      "Epoch 4334/10000\n",
      "2700/2700 [==============================] - 0s 28us/step - loss: 1.6273e-04 - mean_absolute_error: 0.0076\n",
      "\n",
      "Epoch 04334: loss did not improve\n",
      "Epoch 4335/10000\n",
      "2700/2700 [==============================] - 0s 29us/step - loss: 1.6731e-04 - mean_absolute_error: 0.0078\n",
      "\n",
      "Epoch 04335: loss did not improve\n",
      "Epoch 4336/10000\n",
      "2700/2700 [==============================] - 0s 28us/step - loss: 1.6126e-04 - mean_absolute_error: 0.0076\n",
      "\n",
      "Epoch 04336: loss improved from 0.00016 to 0.00016, saving model to keras/best_baseline_weights.hdf5\n",
      "Epoch 4337/10000\n",
      "2700/2700 [==============================] - 0s 34us/step - loss: 1.6743e-04 - mean_absolute_error: 0.0080\n",
      "\n",
      "Epoch 04337: loss did not improve\n",
      "Epoch 4338/10000\n",
      "2700/2700 [==============================] - 0s 29us/step - loss: 1.6162e-04 - mean_absolute_error: 0.0075\n",
      "\n",
      "Epoch 04338: loss did not improve\n",
      "Epoch 4339/10000\n",
      "2700/2700 [==============================] - 0s 26us/step - loss: 1.6179e-04 - mean_absolute_error: 0.0075\n",
      "\n",
      "Epoch 04339: loss did not improve\n",
      "Epoch 4340/10000\n",
      "2700/2700 [==============================] - 0s 27us/step - loss: 1.6048e-04 - mean_absolute_error: 0.0075\n",
      "\n",
      "Epoch 04340: loss improved from 0.00016 to 0.00016, saving model to keras/best_baseline_weights.hdf5\n",
      "Epoch 4341/10000\n",
      "2700/2700 [==============================] - 0s 28us/step - loss: 1.6325e-04 - mean_absolute_error: 0.0077\n",
      "\n",
      "Epoch 04341: loss did not improve\n",
      "Epoch 4342/10000\n",
      "2700/2700 [==============================] - 0s 28us/step - loss: 1.6212e-04 - mean_absolute_error: 0.0075\n",
      "\n",
      "Epoch 04342: loss did not improve\n",
      "Epoch 4343/10000\n",
      "2700/2700 [==============================] - 0s 27us/step - loss: 1.6389e-04 - mean_absolute_error: 0.0077\n",
      "\n",
      "Epoch 04343: loss did not improve\n",
      "Epoch 4344/10000\n",
      "2700/2700 [==============================] - 0s 28us/step - loss: 1.6478e-04 - mean_absolute_error: 0.0078\n",
      "\n",
      "Epoch 04344: loss did not improve\n",
      "Epoch 4345/10000\n",
      "2700/2700 [==============================] - 0s 28us/step - loss: 1.6348e-04 - mean_absolute_error: 0.0077\n",
      "\n",
      "Epoch 04345: loss did not improve\n",
      "Epoch 4346/10000\n",
      "2700/2700 [==============================] - 0s 28us/step - loss: 1.6177e-04 - mean_absolute_error: 0.0075\n",
      "\n",
      "Epoch 04346: loss did not improve\n",
      "Epoch 4347/10000\n",
      "2700/2700 [==============================] - 0s 28us/step - loss: 1.6237e-04 - mean_absolute_error: 0.0076\n",
      "\n",
      "Epoch 04347: loss did not improve\n",
      "Epoch 4348/10000\n",
      "2700/2700 [==============================] - 0s 26us/step - loss: 1.6008e-04 - mean_absolute_error: 0.0076\n",
      "\n",
      "Epoch 04348: loss improved from 0.00016 to 0.00016, saving model to keras/best_baseline_weights.hdf5\n",
      "Epoch 4349/10000\n",
      "2700/2700 [==============================] - 0s 29us/step - loss: 1.6027e-04 - mean_absolute_error: 0.0075\n",
      "\n",
      "Epoch 04349: loss did not improve\n",
      "Epoch 4350/10000\n",
      "2700/2700 [==============================] - 0s 30us/step - loss: 1.6146e-04 - mean_absolute_error: 0.0075\n",
      "\n",
      "Epoch 04350: loss did not improve\n",
      "Epoch 4351/10000\n",
      "2700/2700 [==============================] - 0s 29us/step - loss: 1.6223e-04 - mean_absolute_error: 0.0077\n",
      "\n",
      "Epoch 04351: loss did not improve\n",
      "Epoch 4352/10000\n",
      "2700/2700 [==============================] - 0s 28us/step - loss: 1.6067e-04 - mean_absolute_error: 0.0074\n",
      "\n",
      "Epoch 04352: loss did not improve\n",
      "Epoch 4353/10000\n",
      "2700/2700 [==============================] - 0s 27us/step - loss: 1.6166e-04 - mean_absolute_error: 0.0076\n",
      "\n",
      "Epoch 04353: loss did not improve\n",
      "Epoch 4354/10000\n",
      "2700/2700 [==============================] - 0s 29us/step - loss: 1.6396e-04 - mean_absolute_error: 0.0077\n",
      "\n",
      "Epoch 04354: loss did not improve\n",
      "Epoch 4355/10000\n",
      "2700/2700 [==============================] - 0s 28us/step - loss: 1.6112e-04 - mean_absolute_error: 0.0076\n",
      "\n",
      "Epoch 04355: loss did not improve\n",
      "Epoch 4356/10000\n",
      "2700/2700 [==============================] - 0s 28us/step - loss: 1.6106e-04 - mean_absolute_error: 0.0075\n",
      "\n",
      "Epoch 04356: loss did not improve\n",
      "Epoch 4357/10000\n",
      "2700/2700 [==============================] - 0s 26us/step - loss: 1.6087e-04 - mean_absolute_error: 0.0075\n",
      "\n",
      "Epoch 04357: loss did not improve\n",
      "Epoch 4358/10000\n",
      "2700/2700 [==============================] - 0s 29us/step - loss: 1.6202e-04 - mean_absolute_error: 0.0076\n",
      "\n",
      "Epoch 04358: loss did not improve\n",
      "Epoch 4359/10000\n",
      "2700/2700 [==============================] - 0s 29us/step - loss: 1.6284e-04 - mean_absolute_error: 0.0076\n",
      "\n",
      "Epoch 04359: loss did not improve\n",
      "Epoch 4360/10000\n",
      "2700/2700 [==============================] - 0s 27us/step - loss: 1.6279e-04 - mean_absolute_error: 0.0077\n",
      "\n",
      "Epoch 04360: loss did not improve\n",
      "Epoch 4361/10000\n",
      "2700/2700 [==============================] - 0s 26us/step - loss: 1.5938e-04 - mean_absolute_error: 0.0075\n",
      "\n",
      "Epoch 04361: loss improved from 0.00016 to 0.00016, saving model to keras/best_baseline_weights.hdf5\n",
      "Epoch 4362/10000\n",
      "2700/2700 [==============================] - 0s 29us/step - loss: 1.6034e-04 - mean_absolute_error: 0.0075\n",
      "\n",
      "Epoch 04362: loss did not improve\n",
      "Epoch 4363/10000\n",
      "2700/2700 [==============================] - 0s 30us/step - loss: 1.6138e-04 - mean_absolute_error: 0.0075\n",
      "\n",
      "Epoch 04363: loss did not improve\n",
      "Epoch 4364/10000\n",
      "2700/2700 [==============================] - 0s 29us/step - loss: 1.6312e-04 - mean_absolute_error: 0.0076\n",
      "\n",
      "Epoch 04364: loss did not improve\n",
      "Epoch 4365/10000\n",
      "2700/2700 [==============================] - 0s 27us/step - loss: 1.6241e-04 - mean_absolute_error: 0.0077\n",
      "\n",
      "Epoch 04365: loss did not improve\n",
      "Epoch 4366/10000\n",
      "2700/2700 [==============================] - 0s 27us/step - loss: 1.5911e-04 - mean_absolute_error: 0.0074\n",
      "\n",
      "Epoch 04366: loss improved from 0.00016 to 0.00016, saving model to keras/best_baseline_weights.hdf5\n",
      "Epoch 4367/10000\n",
      "2700/2700 [==============================] - 0s 29us/step - loss: 1.6126e-04 - mean_absolute_error: 0.0076\n",
      "\n",
      "Epoch 04367: loss did not improve\n",
      "Epoch 4368/10000\n",
      "2700/2700 [==============================] - 0s 29us/step - loss: 1.5909e-04 - mean_absolute_error: 0.0075\n",
      "\n",
      "Epoch 04368: loss improved from 0.00016 to 0.00016, saving model to keras/best_baseline_weights.hdf5\n",
      "Epoch 4369/10000\n",
      "2700/2700 [==============================] - 0s 29us/step - loss: 1.6124e-04 - mean_absolute_error: 0.0076\n",
      "\n",
      "Epoch 04369: loss did not improve\n",
      "Epoch 4370/10000\n",
      "2700/2700 [==============================] - 0s 28us/step - loss: 1.5968e-04 - mean_absolute_error: 0.0074\n",
      "\n",
      "Epoch 04370: loss did not improve\n",
      "Epoch 4371/10000\n",
      "2700/2700 [==============================] - 0s 30us/step - loss: 1.6660e-04 - mean_absolute_error: 0.0079\n",
      "\n",
      "Epoch 04371: loss did not improve\n",
      "Epoch 4372/10000\n",
      "2700/2700 [==============================] - 0s 26us/step - loss: 1.6459e-04 - mean_absolute_error: 0.0079\n",
      "\n",
      "Epoch 04372: loss did not improve\n",
      "Epoch 4373/10000\n",
      "2700/2700 [==============================] - 0s 30us/step - loss: 1.6149e-04 - mean_absolute_error: 0.0075\n",
      "\n",
      "Epoch 04373: loss did not improve\n",
      "Epoch 4374/10000\n",
      "2700/2700 [==============================] - 0s 31us/step - loss: 1.5936e-04 - mean_absolute_error: 0.0074\n",
      "\n",
      "Epoch 04374: loss did not improve\n",
      "Epoch 4375/10000\n",
      "2700/2700 [==============================] - 0s 30us/step - loss: 1.5929e-04 - mean_absolute_error: 0.0075\n",
      "\n",
      "Epoch 04375: loss did not improve\n",
      "Epoch 4376/10000\n",
      "2700/2700 [==============================] - 0s 29us/step - loss: 1.5772e-04 - mean_absolute_error: 0.0073\n",
      "\n",
      "Epoch 04376: loss improved from 0.00016 to 0.00016, saving model to keras/best_baseline_weights.hdf5\n",
      "Epoch 4377/10000\n",
      "2700/2700 [==============================] - 0s 28us/step - loss: 1.5971e-04 - mean_absolute_error: 0.0075\n",
      "\n",
      "Epoch 04377: loss did not improve\n",
      "Epoch 4378/10000\n",
      "2700/2700 [==============================] - 0s 30us/step - loss: 1.6214e-04 - mean_absolute_error: 0.0077\n",
      "\n",
      "Epoch 04378: loss did not improve\n",
      "Epoch 4379/10000\n",
      "2700/2700 [==============================] - 0s 30us/step - loss: 1.6317e-04 - mean_absolute_error: 0.0077\n",
      "\n",
      "Epoch 04379: loss did not improve\n",
      "Epoch 4380/10000\n",
      "2700/2700 [==============================] - 0s 28us/step - loss: 1.6379e-04 - mean_absolute_error: 0.0078\n",
      "\n",
      "Epoch 04380: loss did not improve\n",
      "Epoch 4381/10000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2700/2700 [==============================] - 0s 30us/step - loss: 1.6148e-04 - mean_absolute_error: 0.0075\n",
      "\n",
      "Epoch 04381: loss did not improve\n",
      "Epoch 4382/10000\n",
      "2700/2700 [==============================] - 0s 26us/step - loss: 1.6181e-04 - mean_absolute_error: 0.0077\n",
      "\n",
      "Epoch 04382: loss did not improve\n",
      "Epoch 4383/10000\n",
      "2700/2700 [==============================] - 0s 27us/step - loss: 1.5825e-04 - mean_absolute_error: 0.0074\n",
      "\n",
      "Epoch 04383: loss did not improve\n",
      "Epoch 4384/10000\n",
      "2700/2700 [==============================] - 0s 29us/step - loss: 1.5735e-04 - mean_absolute_error: 0.0074\n",
      "\n",
      "Epoch 04384: loss improved from 0.00016 to 0.00016, saving model to keras/best_baseline_weights.hdf5\n",
      "Epoch 4385/10000\n",
      "2700/2700 [==============================] - 0s 28us/step - loss: 1.5785e-04 - mean_absolute_error: 0.0074\n",
      "\n",
      "Epoch 04385: loss did not improve\n",
      "Epoch 4386/10000\n",
      "2700/2700 [==============================] - 0s 26us/step - loss: 1.6225e-04 - mean_absolute_error: 0.0076\n",
      "\n",
      "Epoch 04386: loss did not improve\n",
      "Epoch 4387/10000\n",
      "2700/2700 [==============================] - 0s 29us/step - loss: 1.5882e-04 - mean_absolute_error: 0.0075\n",
      "\n",
      "Epoch 04387: loss did not improve\n",
      "Epoch 4388/10000\n",
      "2700/2700 [==============================] - 0s 28us/step - loss: 1.6056e-04 - mean_absolute_error: 0.0076\n",
      "\n",
      "Epoch 04388: loss did not improve\n",
      "Epoch 4389/10000\n",
      "2700/2700 [==============================] - 0s 29us/step - loss: 1.5806e-04 - mean_absolute_error: 0.0074\n",
      "\n",
      "Epoch 04389: loss did not improve\n",
      "Epoch 4390/10000\n",
      "2700/2700 [==============================] - 0s 26us/step - loss: 1.5813e-04 - mean_absolute_error: 0.0075\n",
      "\n",
      "Epoch 04390: loss did not improve\n",
      "Epoch 4391/10000\n",
      "2700/2700 [==============================] - 0s 27us/step - loss: 1.6418e-04 - mean_absolute_error: 0.0078\n",
      "\n",
      "Epoch 04391: loss did not improve\n",
      "Epoch 4392/10000\n",
      "2700/2700 [==============================] - 0s 31us/step - loss: 1.5678e-04 - mean_absolute_error: 0.0073\n",
      "\n",
      "Epoch 04392: loss improved from 0.00016 to 0.00016, saving model to keras/best_baseline_weights.hdf5\n",
      "Epoch 4393/10000\n",
      "2700/2700 [==============================] - 0s 29us/step - loss: 1.5934e-04 - mean_absolute_error: 0.0075\n",
      "\n",
      "Epoch 04393: loss did not improve\n",
      "Epoch 4394/10000\n",
      "2700/2700 [==============================] - 0s 29us/step - loss: 1.6048e-04 - mean_absolute_error: 0.0075\n",
      "\n",
      "Epoch 04394: loss did not improve\n",
      "Epoch 4395/10000\n",
      "2700/2700 [==============================] - 0s 26us/step - loss: 1.5766e-04 - mean_absolute_error: 0.0074\n",
      "\n",
      "Epoch 04395: loss did not improve\n",
      "Epoch 4396/10000\n",
      "2700/2700 [==============================] - 0s 29us/step - loss: 1.5855e-04 - mean_absolute_error: 0.0074\n",
      "\n",
      "Epoch 04396: loss did not improve\n",
      "Epoch 4397/10000\n",
      "2700/2700 [==============================] - 0s 28us/step - loss: 1.5902e-04 - mean_absolute_error: 0.0076\n",
      "\n",
      "Epoch 04397: loss did not improve\n",
      "Epoch 4398/10000\n",
      "2700/2700 [==============================] - 0s 28us/step - loss: 1.6514e-04 - mean_absolute_error: 0.0079\n",
      "\n",
      "Epoch 04398: loss did not improve\n",
      "Epoch 4399/10000\n",
      "2700/2700 [==============================] - 0s 27us/step - loss: 1.5815e-04 - mean_absolute_error: 0.0075\n",
      "\n",
      "Epoch 04399: loss did not improve\n",
      "Epoch 4400/10000\n",
      "2700/2700 [==============================] - 0s 28us/step - loss: 1.5843e-04 - mean_absolute_error: 0.0074\n",
      "\n",
      "Epoch 04400: loss did not improve\n",
      "Epoch 4401/10000\n",
      "2700/2700 [==============================] - 0s 29us/step - loss: 1.5641e-04 - mean_absolute_error: 0.0073\n",
      "\n",
      "Epoch 04401: loss improved from 0.00016 to 0.00016, saving model to keras/best_baseline_weights.hdf5\n",
      "Epoch 4402/10000\n",
      "2700/2700 [==============================] - 0s 30us/step - loss: 1.5758e-04 - mean_absolute_error: 0.0074\n",
      "\n",
      "Epoch 04402: loss did not improve\n",
      "Epoch 4403/10000\n",
      "2700/2700 [==============================] - 0s 30us/step - loss: 1.5721e-04 - mean_absolute_error: 0.0073\n",
      "\n",
      "Epoch 04403: loss did not improve\n",
      "Epoch 4404/10000\n",
      "2700/2700 [==============================] - 0s 27us/step - loss: 1.5570e-04 - mean_absolute_error: 0.0073\n",
      "\n",
      "Epoch 04404: loss improved from 0.00016 to 0.00016, saving model to keras/best_baseline_weights.hdf5\n",
      "Epoch 4405/10000\n",
      "2700/2700 [==============================] - 0s 28us/step - loss: 1.5655e-04 - mean_absolute_error: 0.0074\n",
      "\n",
      "Epoch 04405: loss did not improve\n",
      "Epoch 4406/10000\n",
      "2700/2700 [==============================] - 0s 30us/step - loss: 1.5753e-04 - mean_absolute_error: 0.0074\n",
      "\n",
      "Epoch 04406: loss did not improve\n",
      "Epoch 4407/10000\n",
      "2700/2700 [==============================] - 0s 29us/step - loss: 1.5759e-04 - mean_absolute_error: 0.0074\n",
      "\n",
      "Epoch 04407: loss did not improve\n",
      "Epoch 4408/10000\n",
      "2700/2700 [==============================] - 0s 28us/step - loss: 1.5858e-04 - mean_absolute_error: 0.0075\n",
      "\n",
      "Epoch 04408: loss did not improve\n",
      "Epoch 4409/10000\n",
      "2700/2700 [==============================] - 0s 27us/step - loss: 1.6026e-04 - mean_absolute_error: 0.0076\n",
      "\n",
      "Epoch 04409: loss did not improve\n",
      "Epoch 4410/10000\n",
      "2700/2700 [==============================] - 0s 28us/step - loss: 1.6278e-04 - mean_absolute_error: 0.0079\n",
      "\n",
      "Epoch 04410: loss did not improve\n",
      "Epoch 4411/10000\n",
      "2700/2700 [==============================] - 0s 28us/step - loss: 1.5724e-04 - mean_absolute_error: 0.0073\n",
      "\n",
      "Epoch 04411: loss did not improve\n",
      "Epoch 4412/10000\n",
      "2700/2700 [==============================] - 0s 27us/step - loss: 1.5877e-04 - mean_absolute_error: 0.0075\n",
      "\n",
      "Epoch 04412: loss did not improve\n",
      "Epoch 4413/10000\n",
      "2700/2700 [==============================] - 0s 27us/step - loss: 1.6179e-04 - mean_absolute_error: 0.0077\n",
      "\n",
      "Epoch 04413: loss did not improve\n",
      "Epoch 4414/10000\n",
      "2700/2700 [==============================] - 0s 28us/step - loss: 1.6010e-04 - mean_absolute_error: 0.0077\n",
      "\n",
      "Epoch 04414: loss did not improve\n",
      "Epoch 4415/10000\n",
      "2700/2700 [==============================] - 0s 27us/step - loss: 1.5789e-04 - mean_absolute_error: 0.0075\n",
      "\n",
      "Epoch 04415: loss did not improve\n",
      "Epoch 4416/10000\n",
      "2700/2700 [==============================] - 0s 29us/step - loss: 1.5584e-04 - mean_absolute_error: 0.0072\n",
      "\n",
      "Epoch 04416: loss did not improve\n",
      "Epoch 4417/10000\n",
      "2700/2700 [==============================] - 0s 25us/step - loss: 1.5997e-04 - mean_absolute_error: 0.0075\n",
      "\n",
      "Epoch 04417: loss did not improve\n",
      "Epoch 4418/10000\n",
      "2700/2700 [==============================] - 0s 27us/step - loss: 1.5792e-04 - mean_absolute_error: 0.0076\n",
      "\n",
      "Epoch 04418: loss did not improve\n",
      "Epoch 4419/10000\n",
      "2700/2700 [==============================] - 0s 29us/step - loss: 1.5541e-04 - mean_absolute_error: 0.0073\n",
      "\n",
      "Epoch 04419: loss improved from 0.00016 to 0.00016, saving model to keras/best_baseline_weights.hdf5\n",
      "Epoch 4420/10000\n",
      "2700/2700 [==============================] - 0s 29us/step - loss: 1.6377e-04 - mean_absolute_error: 0.0080\n",
      "\n",
      "Epoch 04420: loss did not improve\n",
      "Epoch 4421/10000\n",
      "2700/2700 [==============================] - 0s 27us/step - loss: 1.5715e-04 - mean_absolute_error: 0.0074\n",
      "\n",
      "Epoch 04421: loss did not improve\n",
      "Epoch 4422/10000\n",
      "2700/2700 [==============================] - 0s 26us/step - loss: 1.5575e-04 - mean_absolute_error: 0.0072\n",
      "\n",
      "Epoch 04422: loss did not improve\n",
      "Epoch 4423/10000\n",
      "2700/2700 [==============================] - 0s 29us/step - loss: 1.5712e-04 - mean_absolute_error: 0.0074: 0s - loss: 1.4295e-04 - mean_absolute_error: 0.007\n",
      "\n",
      "Epoch 04423: loss did not improve\n",
      "Epoch 4424/10000\n",
      "2700/2700 [==============================] - 0s 29us/step - loss: 1.6570e-04 - mean_absolute_error: 0.0081\n",
      "\n",
      "Epoch 04424: loss did not improve\n",
      "Epoch 4425/10000\n",
      "2700/2700 [==============================] - 0s 27us/step - loss: 1.5804e-04 - mean_absolute_error: 0.0075\n",
      "\n",
      "Epoch 04425: loss did not improve\n",
      "Epoch 4426/10000\n",
      "2700/2700 [==============================] - 0s 28us/step - loss: 1.5844e-04 - mean_absolute_error: 0.0075\n",
      "\n",
      "Epoch 04426: loss did not improve\n",
      "Epoch 4427/10000\n",
      "2700/2700 [==============================] - 0s 30us/step - loss: 1.5578e-04 - mean_absolute_error: 0.0073\n",
      "\n",
      "Epoch 04427: loss did not improve\n",
      "Epoch 4428/10000\n",
      "2700/2700 [==============================] - 0s 28us/step - loss: 1.5712e-04 - mean_absolute_error: 0.0074\n",
      "\n",
      "Epoch 04428: loss did not improve\n",
      "Epoch 4429/10000\n",
      "2700/2700 [==============================] - 0s 28us/step - loss: 1.5886e-04 - mean_absolute_error: 0.0076\n",
      "\n",
      "Epoch 04429: loss did not improve\n",
      "Epoch 4430/10000\n",
      "2700/2700 [==============================] - 0s 30us/step - loss: 1.5430e-04 - mean_absolute_error: 0.0072\n",
      "\n",
      "Epoch 04430: loss improved from 0.00016 to 0.00015, saving model to keras/best_baseline_weights.hdf5\n",
      "Epoch 4431/10000\n",
      "2700/2700 [==============================] - 0s 26us/step - loss: 1.5936e-04 - mean_absolute_error: 0.0076\n",
      "\n",
      "Epoch 04431: loss did not improve\n",
      "Epoch 4432/10000\n",
      "2700/2700 [==============================] - 0s 29us/step - loss: 1.5736e-04 - mean_absolute_error: 0.0075\n",
      "\n",
      "Epoch 04432: loss did not improve\n",
      "Epoch 4433/10000\n",
      "2700/2700 [==============================] - 0s 29us/step - loss: 1.5579e-04 - mean_absolute_error: 0.0074\n",
      "\n",
      "Epoch 04433: loss did not improve\n",
      "Epoch 4434/10000\n",
      "2700/2700 [==============================] - 0s 28us/step - loss: 1.5438e-04 - mean_absolute_error: 0.0072\n",
      "\n",
      "Epoch 04434: loss did not improve\n",
      "Epoch 4435/10000\n",
      "2700/2700 [==============================] - 0s 26us/step - loss: 1.5641e-04 - mean_absolute_error: 0.0074\n",
      "\n",
      "Epoch 04435: loss did not improve\n",
      "Epoch 4436/10000\n",
      "2700/2700 [==============================] - 0s 29us/step - loss: 1.5983e-04 - mean_absolute_error: 0.0077\n",
      "\n",
      "Epoch 04436: loss did not improve\n",
      "Epoch 4437/10000\n",
      "2700/2700 [==============================] - 0s 28us/step - loss: 1.6009e-04 - mean_absolute_error: 0.0076\n",
      "\n",
      "Epoch 04437: loss did not improve\n",
      "Epoch 4438/10000\n",
      "2700/2700 [==============================] - 0s 30us/step - loss: 1.6067e-04 - mean_absolute_error: 0.0077\n",
      "\n",
      "Epoch 04438: loss did not improve\n",
      "Epoch 4439/10000\n",
      "2700/2700 [==============================] - 0s 28us/step - loss: 1.5668e-04 - mean_absolute_error: 0.0074\n",
      "\n",
      "Epoch 04439: loss did not improve\n",
      "Epoch 4440/10000\n",
      "2700/2700 [==============================] - 0s 27us/step - loss: 1.5714e-04 - mean_absolute_error: 0.0075\n",
      "\n",
      "Epoch 04440: loss did not improve\n",
      "Epoch 4441/10000\n",
      "2700/2700 [==============================] - 0s 27us/step - loss: 1.5775e-04 - mean_absolute_error: 0.0076\n",
      "\n",
      "Epoch 04441: loss did not improve\n",
      "Epoch 4442/10000\n",
      "2700/2700 [==============================] - 0s 28us/step - loss: 1.5556e-04 - mean_absolute_error: 0.0074\n",
      "\n",
      "Epoch 04442: loss did not improve\n",
      "Epoch 4443/10000\n",
      "2700/2700 [==============================] - 0s 27us/step - loss: 1.5677e-04 - mean_absolute_error: 0.0074\n",
      "\n",
      "Epoch 04443: loss did not improve\n",
      "Epoch 4444/10000\n",
      "2700/2700 [==============================] - 0s 30us/step - loss: 1.5700e-04 - mean_absolute_error: 0.0075\n",
      "\n",
      "Epoch 04444: loss did not improve\n",
      "Epoch 4445/10000\n",
      "2700/2700 [==============================] - 0s 28us/step - loss: 1.5747e-04 - mean_absolute_error: 0.0075\n",
      "\n",
      "Epoch 04445: loss did not improve\n",
      "Epoch 4446/10000\n",
      "2700/2700 [==============================] - 0s 29us/step - loss: 1.5445e-04 - mean_absolute_error: 0.0073\n",
      "\n",
      "Epoch 04446: loss did not improve\n",
      "Epoch 4447/10000\n",
      "2700/2700 [==============================] - 0s 25us/step - loss: 1.5449e-04 - mean_absolute_error: 0.0074\n",
      "\n",
      "Epoch 04447: loss did not improve\n",
      "Epoch 4448/10000\n",
      "2700/2700 [==============================] - 0s 27us/step - loss: 1.5829e-04 - mean_absolute_error: 0.0076\n",
      "\n",
      "Epoch 04448: loss did not improve\n",
      "Epoch 4449/10000\n",
      "2700/2700 [==============================] - 0s 28us/step - loss: 1.5490e-04 - mean_absolute_error: 0.0074\n",
      "\n",
      "Epoch 04449: loss did not improve\n",
      "Epoch 4450/10000\n",
      "2700/2700 [==============================] - 0s 28us/step - loss: 1.5829e-04 - mean_absolute_error: 0.0077\n",
      "\n",
      "Epoch 04450: loss did not improve\n",
      "Epoch 4451/10000\n",
      "2700/2700 [==============================] - 0s 29us/step - loss: 1.5752e-04 - mean_absolute_error: 0.0076\n",
      "\n",
      "Epoch 04451: loss did not improve\n",
      "Epoch 4452/10000\n",
      "2700/2700 [==============================] - 0s 26us/step - loss: 1.5384e-04 - mean_absolute_error: 0.0072\n",
      "\n",
      "Epoch 04452: loss improved from 0.00015 to 0.00015, saving model to keras/best_baseline_weights.hdf5\n",
      "Epoch 4453/10000\n",
      "2700/2700 [==============================] - 0s 29us/step - loss: 1.6120e-04 - mean_absolute_error: 0.0077\n",
      "\n",
      "Epoch 04453: loss did not improve\n",
      "Epoch 4454/10000\n",
      "2700/2700 [==============================] - 0s 30us/step - loss: 1.5443e-04 - mean_absolute_error: 0.0074\n",
      "\n",
      "Epoch 04454: loss did not improve\n",
      "Epoch 4455/10000\n",
      "2700/2700 [==============================] - 0s 28us/step - loss: 1.5608e-04 - mean_absolute_error: 0.0074\n",
      "\n",
      "Epoch 04455: loss did not improve\n",
      "Epoch 4456/10000\n",
      "2700/2700 [==============================] - 0s 26us/step - loss: 1.5511e-04 - mean_absolute_error: 0.0073\n",
      "\n",
      "Epoch 04456: loss did not improve\n",
      "Epoch 4457/10000\n",
      "2700/2700 [==============================] - 0s 28us/step - loss: 1.5341e-04 - mean_absolute_error: 0.0072\n",
      "\n",
      "Epoch 04457: loss improved from 0.00015 to 0.00015, saving model to keras/best_baseline_weights.hdf5\n",
      "Epoch 4458/10000\n",
      "2700/2700 [==============================] - 0s 29us/step - loss: 1.5742e-04 - mean_absolute_error: 0.0076\n",
      "\n",
      "Epoch 04458: loss did not improve\n",
      "Epoch 4459/10000\n",
      "2700/2700 [==============================] - 0s 28us/step - loss: 1.5701e-04 - mean_absolute_error: 0.0075\n",
      "\n",
      "Epoch 04459: loss did not improve\n",
      "Epoch 4460/10000\n",
      "2700/2700 [==============================] - 0s 28us/step - loss: 1.5589e-04 - mean_absolute_error: 0.0074\n",
      "\n",
      "Epoch 04460: loss did not improve\n",
      "Epoch 4461/10000\n",
      "2700/2700 [==============================] - 0s 26us/step - loss: 1.5744e-04 - mean_absolute_error: 0.0075\n",
      "\n",
      "Epoch 04461: loss did not improve\n",
      "Epoch 4462/10000\n",
      "2700/2700 [==============================] - 0s 30us/step - loss: 1.5462e-04 - mean_absolute_error: 0.0073\n",
      "\n",
      "Epoch 04462: loss did not improve\n",
      "Epoch 4463/10000\n",
      "2700/2700 [==============================] - 0s 29us/step - loss: 1.5628e-04 - mean_absolute_error: 0.0075\n",
      "\n",
      "Epoch 04463: loss did not improve\n",
      "Epoch 4464/10000\n",
      "2700/2700 [==============================] - 0s 29us/step - loss: 1.5681e-04 - mean_absolute_error: 0.0075\n",
      "\n",
      "Epoch 04464: loss did not improve\n",
      "Epoch 4465/10000\n",
      "2700/2700 [==============================] - 0s 27us/step - loss: 1.5407e-04 - mean_absolute_error: 0.0073\n",
      "\n",
      "Epoch 04465: loss did not improve\n",
      "Epoch 4466/10000\n",
      "2700/2700 [==============================] - 0s 29us/step - loss: 1.5343e-04 - mean_absolute_error: 0.0072\n",
      "\n",
      "Epoch 04466: loss did not improve\n",
      "Epoch 4467/10000\n",
      "2700/2700 [==============================] - 0s 28us/step - loss: 1.5607e-04 - mean_absolute_error: 0.0075\n",
      "\n",
      "Epoch 04467: loss did not improve\n",
      "Epoch 4468/10000\n",
      "2700/2700 [==============================] - 0s 29us/step - loss: 1.5832e-04 - mean_absolute_error: 0.0075\n",
      "\n",
      "Epoch 04468: loss did not improve\n",
      "Epoch 4469/10000\n",
      "2700/2700 [==============================] - 0s 27us/step - loss: 1.5484e-04 - mean_absolute_error: 0.0073\n",
      "\n",
      "Epoch 04469: loss did not improve\n",
      "Epoch 4470/10000\n",
      "2700/2700 [==============================] - 0s 27us/step - loss: 1.5653e-04 - mean_absolute_error: 0.0075\n",
      "\n",
      "Epoch 04470: loss did not improve\n",
      "Epoch 4471/10000\n",
      "2700/2700 [==============================] - 0s 29us/step - loss: 1.5864e-04 - mean_absolute_error: 0.0077\n",
      "\n",
      "Epoch 04471: loss did not improve\n",
      "Epoch 4472/10000\n",
      "2700/2700 [==============================] - 0s 30us/step - loss: 1.5923e-04 - mean_absolute_error: 0.0076\n",
      "\n",
      "Epoch 04472: loss did not improve\n",
      "Epoch 4473/10000\n",
      "2700/2700 [==============================] - 0s 28us/step - loss: 1.5639e-04 - mean_absolute_error: 0.0075\n",
      "\n",
      "Epoch 04473: loss did not improve\n",
      "Epoch 4474/10000\n",
      "2700/2700 [==============================] - 0s 27us/step - loss: 1.5574e-04 - mean_absolute_error: 0.0074\n",
      "\n",
      "Epoch 04474: loss did not improve\n",
      "Epoch 4475/10000\n",
      "2700/2700 [==============================] - 0s 30us/step - loss: 1.5447e-04 - mean_absolute_error: 0.0073\n",
      "\n",
      "Epoch 04475: loss did not improve\n",
      "Epoch 4476/10000\n",
      "2700/2700 [==============================] - 0s 28us/step - loss: 1.5677e-04 - mean_absolute_error: 0.0075\n",
      "\n",
      "Epoch 04476: loss did not improve\n",
      "Epoch 4477/10000\n",
      "2700/2700 [==============================] - 0s 28us/step - loss: 1.5304e-04 - mean_absolute_error: 0.0072\n",
      "\n",
      "Epoch 04477: loss improved from 0.00015 to 0.00015, saving model to keras/best_baseline_weights.hdf5\n",
      "Epoch 4478/10000\n",
      "2700/2700 [==============================] - 0s 28us/step - loss: 1.5282e-04 - mean_absolute_error: 0.0072\n",
      "\n",
      "Epoch 04478: loss improved from 0.00015 to 0.00015, saving model to keras/best_baseline_weights.hdf5\n",
      "Epoch 4479/10000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2700/2700 [==============================] - 0s 27us/step - loss: 1.5398e-04 - mean_absolute_error: 0.0073\n",
      "\n",
      "Epoch 04479: loss did not improve\n",
      "Epoch 4480/10000\n",
      "2700/2700 [==============================] - 0s 28us/step - loss: 1.5585e-04 - mean_absolute_error: 0.0075\n",
      "\n",
      "Epoch 04480: loss did not improve\n",
      "Epoch 4481/10000\n",
      "2700/2700 [==============================] - 0s 27us/step - loss: 1.5362e-04 - mean_absolute_error: 0.0073\n",
      "\n",
      "Epoch 04481: loss did not improve\n",
      "Epoch 4482/10000\n",
      "2700/2700 [==============================] - 0s 28us/step - loss: 1.5534e-04 - mean_absolute_error: 0.0075\n",
      "\n",
      "Epoch 04482: loss did not improve\n",
      "Epoch 4483/10000\n",
      "2700/2700 [==============================] - 0s 26us/step - loss: 1.5456e-04 - mean_absolute_error: 0.0073\n",
      "\n",
      "Epoch 04483: loss did not improve\n",
      "Epoch 4484/10000\n",
      "2700/2700 [==============================] - 0s 27us/step - loss: 1.5234e-04 - mean_absolute_error: 0.0073\n",
      "\n",
      "Epoch 04484: loss improved from 0.00015 to 0.00015, saving model to keras/best_baseline_weights.hdf5\n",
      "Epoch 4485/10000\n",
      "2700/2700 [==============================] - 0s 27us/step - loss: 1.5629e-04 - mean_absolute_error: 0.0075\n",
      "\n",
      "Epoch 04485: loss did not improve\n",
      "Epoch 4486/10000\n",
      "2700/2700 [==============================] - 0s 29us/step - loss: 1.5365e-04 - mean_absolute_error: 0.0073\n",
      "\n",
      "Epoch 04486: loss did not improve\n",
      "Epoch 4487/10000\n",
      "2700/2700 [==============================] - 0s 26us/step - loss: 1.5579e-04 - mean_absolute_error: 0.0073\n",
      "\n",
      "Epoch 04487: loss did not improve\n",
      "Epoch 4488/10000\n",
      "2700/2700 [==============================] - 0s 28us/step - loss: 1.5296e-04 - mean_absolute_error: 0.0073\n",
      "\n",
      "Epoch 04488: loss did not improve\n",
      "Epoch 4489/10000\n",
      "2700/2700 [==============================] - 0s 29us/step - loss: 1.5286e-04 - mean_absolute_error: 0.0073\n",
      "\n",
      "Epoch 04489: loss did not improve\n",
      "Epoch 4490/10000\n",
      "2700/2700 [==============================] - 0s 29us/step - loss: 1.5259e-04 - mean_absolute_error: 0.0072\n",
      "\n",
      "Epoch 04490: loss did not improve\n",
      "Epoch 4491/10000\n",
      "2700/2700 [==============================] - 0s 28us/step - loss: 1.5206e-04 - mean_absolute_error: 0.0072\n",
      "\n",
      "Epoch 04491: loss improved from 0.00015 to 0.00015, saving model to keras/best_baseline_weights.hdf5\n",
      "Epoch 4492/10000\n",
      "2700/2700 [==============================] - 0s 26us/step - loss: 1.5484e-04 - mean_absolute_error: 0.0075\n",
      "\n",
      "Epoch 04492: loss did not improve\n",
      "Epoch 4493/10000\n",
      "2700/2700 [==============================] - 0s 29us/step - loss: 1.6189e-04 - mean_absolute_error: 0.0079\n",
      "\n",
      "Epoch 04493: loss did not improve\n",
      "Epoch 4494/10000\n",
      "2700/2700 [==============================] - 0s 27us/step - loss: 1.5464e-04 - mean_absolute_error: 0.0073\n",
      "\n",
      "Epoch 04494: loss did not improve\n",
      "Epoch 4495/10000\n",
      "2700/2700 [==============================] - 0s 31us/step - loss: 1.5400e-04 - mean_absolute_error: 0.0074\n",
      "\n",
      "Epoch 04495: loss did not improve\n",
      "Epoch 4496/10000\n",
      "2700/2700 [==============================] - 0s 25us/step - loss: 1.5248e-04 - mean_absolute_error: 0.0073\n",
      "\n",
      "Epoch 04496: loss did not improve\n",
      "Epoch 4497/10000\n",
      "2700/2700 [==============================] - 0s 27us/step - loss: 1.5309e-04 - mean_absolute_error: 0.0072\n",
      "\n",
      "Epoch 04497: loss did not improve\n",
      "Epoch 4498/10000\n",
      "2700/2700 [==============================] - 0s 29us/step - loss: 1.5415e-04 - mean_absolute_error: 0.0074\n",
      "\n",
      "Epoch 04498: loss did not improve\n",
      "Epoch 4499/10000\n",
      "2700/2700 [==============================] - 0s 31us/step - loss: 1.5392e-04 - mean_absolute_error: 0.0074\n",
      "\n",
      "Epoch 04499: loss did not improve\n",
      "Epoch 4500/10000\n",
      "2700/2700 [==============================] - 0s 26us/step - loss: 1.5732e-04 - mean_absolute_error: 0.0076\n",
      "\n",
      "Epoch 04500: loss did not improve\n",
      "Epoch 4501/10000\n",
      "2700/2700 [==============================] - 0s 29us/step - loss: 1.5546e-04 - mean_absolute_error: 0.0075\n",
      "\n",
      "Epoch 04501: loss did not improve\n",
      "Epoch 4502/10000\n",
      "2700/2700 [==============================] - 0s 28us/step - loss: 1.5198e-04 - mean_absolute_error: 0.0072\n",
      "\n",
      "Epoch 04502: loss improved from 0.00015 to 0.00015, saving model to keras/best_baseline_weights.hdf5\n",
      "Epoch 4503/10000\n",
      "2700/2700 [==============================] - 0s 29us/step - loss: 1.5186e-04 - mean_absolute_error: 0.0072\n",
      "\n",
      "Epoch 04503: loss improved from 0.00015 to 0.00015, saving model to keras/best_baseline_weights.hdf5\n",
      "Epoch 4504/10000\n",
      "2700/2700 [==============================] - 0s 30us/step - loss: 1.5656e-04 - mean_absolute_error: 0.0076\n",
      "\n",
      "Epoch 04504: loss did not improve\n",
      "Epoch 4505/10000\n",
      "2700/2700 [==============================] - 0s 29us/step - loss: 1.5553e-04 - mean_absolute_error: 0.0075\n",
      "\n",
      "Epoch 04505: loss did not improve\n",
      "Epoch 4506/10000\n",
      "2700/2700 [==============================] - 0s 27us/step - loss: 1.5423e-04 - mean_absolute_error: 0.0074\n",
      "\n",
      "Epoch 04506: loss did not improve\n",
      "Epoch 4507/10000\n",
      "2700/2700 [==============================] - 0s 29us/step - loss: 1.5777e-04 - mean_absolute_error: 0.0075\n",
      "\n",
      "Epoch 04507: loss did not improve\n",
      "Epoch 4508/10000\n",
      "2700/2700 [==============================] - 0s 28us/step - loss: 1.5491e-04 - mean_absolute_error: 0.0074\n",
      "\n",
      "Epoch 04508: loss did not improve\n",
      "Epoch 4509/10000\n",
      "2700/2700 [==============================] - 0s 29us/step - loss: 1.5227e-04 - mean_absolute_error: 0.0073\n",
      "\n",
      "Epoch 04509: loss did not improve\n",
      "Epoch 4510/10000\n",
      "2700/2700 [==============================] - 0s 27us/step - loss: 1.5628e-04 - mean_absolute_error: 0.0075\n",
      "\n",
      "Epoch 04510: loss did not improve\n",
      "Epoch 4511/10000\n",
      "2700/2700 [==============================] - 0s 30us/step - loss: 1.5152e-04 - mean_absolute_error: 0.0072\n",
      "\n",
      "Epoch 04511: loss improved from 0.00015 to 0.00015, saving model to keras/best_baseline_weights.hdf5\n",
      "Epoch 4512/10000\n",
      "2700/2700 [==============================] - 0s 29us/step - loss: 1.5134e-04 - mean_absolute_error: 0.0072\n",
      "\n",
      "Epoch 04512: loss improved from 0.00015 to 0.00015, saving model to keras/best_baseline_weights.hdf5\n",
      "Epoch 4513/10000\n",
      "2700/2700 [==============================] - 0s 30us/step - loss: 1.5386e-04 - mean_absolute_error: 0.0074\n",
      "\n",
      "Epoch 04513: loss did not improve\n",
      "Epoch 4514/10000\n",
      "2700/2700 [==============================] - 0s 28us/step - loss: 1.5284e-04 - mean_absolute_error: 0.0074\n",
      "\n",
      "Epoch 04514: loss did not improve\n",
      "Epoch 4515/10000\n",
      "2700/2700 [==============================] - 0s 31us/step - loss: 1.5302e-04 - mean_absolute_error: 0.0073\n",
      "\n",
      "Epoch 04515: loss did not improve\n",
      "Epoch 4516/10000\n",
      "2700/2700 [==============================] - 0s 27us/step - loss: 1.5401e-04 - mean_absolute_error: 0.0074\n",
      "\n",
      "Epoch 04516: loss did not improve\n",
      "Epoch 4517/10000\n",
      "2700/2700 [==============================] - 0s 27us/step - loss: 1.5464e-04 - mean_absolute_error: 0.0075\n",
      "\n",
      "Epoch 04517: loss did not improve\n",
      "Epoch 4518/10000\n",
      "2700/2700 [==============================] - 0s 29us/step - loss: 1.5199e-04 - mean_absolute_error: 0.0073\n",
      "\n",
      "Epoch 04518: loss did not improve\n",
      "Epoch 4519/10000\n",
      "2700/2700 [==============================] - 0s 29us/step - loss: 1.5158e-04 - mean_absolute_error: 0.0072\n",
      "\n",
      "Epoch 04519: loss did not improve\n",
      "Epoch 4520/10000\n",
      "2700/2700 [==============================] - 0s 27us/step - loss: 1.5355e-04 - mean_absolute_error: 0.0073\n",
      "\n",
      "Epoch 04520: loss did not improve\n",
      "Epoch 4521/10000\n",
      "2700/2700 [==============================] - 0s 29us/step - loss: 1.5293e-04 - mean_absolute_error: 0.0074\n",
      "\n",
      "Epoch 04521: loss did not improve\n",
      "Epoch 4522/10000\n",
      "2700/2700 [==============================] - 0s 29us/step - loss: 1.5141e-04 - mean_absolute_error: 0.0073\n",
      "\n",
      "Epoch 04522: loss did not improve\n",
      "Epoch 4523/10000\n",
      "2700/2700 [==============================] - 0s 27us/step - loss: 1.5328e-04 - mean_absolute_error: 0.0073\n",
      "\n",
      "Epoch 04523: loss did not improve\n",
      "Epoch 4524/10000\n",
      "2700/2700 [==============================] - 0s 28us/step - loss: 1.5388e-04 - mean_absolute_error: 0.0073\n",
      "\n",
      "Epoch 04524: loss did not improve\n",
      "Epoch 4525/10000\n",
      "2700/2700 [==============================] - 0s 29us/step - loss: 1.5713e-04 - mean_absolute_error: 0.0076\n",
      "\n",
      "Epoch 04525: loss did not improve\n",
      "Epoch 4526/10000\n",
      "2700/2700 [==============================] - 0s 29us/step - loss: 1.5454e-04 - mean_absolute_error: 0.0074\n",
      "\n",
      "Epoch 04526: loss did not improve\n",
      "Epoch 4527/10000\n",
      "2700/2700 [==============================] - 0s 30us/step - loss: 1.5523e-04 - mean_absolute_error: 0.0076\n",
      "\n",
      "Epoch 04527: loss did not improve\n",
      "Epoch 4528/10000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2700/2700 [==============================] - 0s 26us/step - loss: 1.5091e-04 - mean_absolute_error: 0.0071\n",
      "\n",
      "Epoch 04528: loss improved from 0.00015 to 0.00015, saving model to keras/best_baseline_weights.hdf5\n",
      "Epoch 4529/10000\n",
      "2700/2700 [==============================] - 0s 26us/step - loss: 1.5175e-04 - mean_absolute_error: 0.0072\n",
      "\n",
      "Epoch 04529: loss did not improve\n",
      "Epoch 4530/10000\n",
      "2700/2700 [==============================] - 0s 27us/step - loss: 1.5186e-04 - mean_absolute_error: 0.0072\n",
      "\n",
      "Epoch 04530: loss did not improve\n",
      "Epoch 4531/10000\n",
      "2700/2700 [==============================] - 0s 29us/step - loss: 1.5256e-04 - mean_absolute_error: 0.0072\n",
      "\n",
      "Epoch 04531: loss did not improve\n",
      "Epoch 4532/10000\n",
      "2700/2700 [==============================] - 0s 29us/step - loss: 1.5158e-04 - mean_absolute_error: 0.0073\n",
      "\n",
      "Epoch 04532: loss did not improve\n",
      "Epoch 4533/10000\n",
      "2700/2700 [==============================] - 0s 26us/step - loss: 1.5219e-04 - mean_absolute_error: 0.0073\n",
      "\n",
      "Epoch 04533: loss did not improve\n",
      "Epoch 4534/10000\n",
      "2700/2700 [==============================] - 0s 28us/step - loss: 1.5127e-04 - mean_absolute_error: 0.0073\n",
      "\n",
      "Epoch 04534: loss did not improve\n",
      "Epoch 4535/10000\n",
      "2700/2700 [==============================] - 0s 27us/step - loss: 1.5290e-04 - mean_absolute_error: 0.0073\n",
      "\n",
      "Epoch 04535: loss did not improve\n",
      "Epoch 4536/10000\n",
      "2700/2700 [==============================] - 0s 28us/step - loss: 1.5414e-04 - mean_absolute_error: 0.0075\n",
      "\n",
      "Epoch 04536: loss did not improve\n",
      "Epoch 4537/10000\n",
      "2700/2700 [==============================] - 0s 26us/step - loss: 1.5318e-04 - mean_absolute_error: 0.0074\n",
      "\n",
      "Epoch 04537: loss did not improve\n",
      "Epoch 4538/10000\n",
      "2700/2700 [==============================] - 0s 28us/step - loss: 1.5053e-04 - mean_absolute_error: 0.0072\n",
      "\n",
      "Epoch 04538: loss improved from 0.00015 to 0.00015, saving model to keras/best_baseline_weights.hdf5\n",
      "Epoch 4539/10000\n",
      "2700/2700 [==============================] - 0s 29us/step - loss: 1.5061e-04 - mean_absolute_error: 0.0072\n",
      "\n",
      "Epoch 04539: loss did not improve\n",
      "Epoch 4540/10000\n",
      "2700/2700 [==============================] - 0s 27us/step - loss: 1.5196e-04 - mean_absolute_error: 0.0072\n",
      "\n",
      "Epoch 04540: loss did not improve\n",
      "Epoch 4541/10000\n",
      "2700/2700 [==============================] - 0s 25us/step - loss: 1.5254e-04 - mean_absolute_error: 0.0074\n",
      "\n",
      "Epoch 04541: loss did not improve\n",
      "Epoch 4542/10000\n",
      "2700/2700 [==============================] - 0s 29us/step - loss: 1.5371e-04 - mean_absolute_error: 0.0074\n",
      "\n",
      "Epoch 04542: loss did not improve\n",
      "Epoch 4543/10000\n",
      "2700/2700 [==============================] - 0s 28us/step - loss: 1.5277e-04 - mean_absolute_error: 0.0074\n",
      "\n",
      "Epoch 04543: loss did not improve\n",
      "Epoch 4544/10000\n",
      "2700/2700 [==============================] - 0s 28us/step - loss: 1.5545e-04 - mean_absolute_error: 0.0076\n",
      "\n",
      "Epoch 04544: loss did not improve\n",
      "Epoch 4545/10000\n",
      "2700/2700 [==============================] - 0s 26us/step - loss: 1.5088e-04 - mean_absolute_error: 0.0071\n",
      "\n",
      "Epoch 04545: loss did not improve\n",
      "Epoch 4546/10000\n",
      "2700/2700 [==============================] - 0s 31us/step - loss: 1.5247e-04 - mean_absolute_error: 0.0072\n",
      "\n",
      "Epoch 04546: loss did not improve\n",
      "Epoch 4547/10000\n",
      "2700/2700 [==============================] - 0s 28us/step - loss: 1.5253e-04 - mean_absolute_error: 0.0073\n",
      "\n",
      "Epoch 04547: loss did not improve\n",
      "Epoch 4548/10000\n",
      "2700/2700 [==============================] - 0s 29us/step - loss: 1.5110e-04 - mean_absolute_error: 0.0072\n",
      "\n",
      "Epoch 04548: loss did not improve\n",
      "Epoch 4549/10000\n",
      "2700/2700 [==============================] - 0s 27us/step - loss: 1.5148e-04 - mean_absolute_error: 0.0073\n",
      "\n",
      "Epoch 04549: loss did not improve\n",
      "Epoch 4550/10000\n",
      "2700/2700 [==============================] - 0s 26us/step - loss: 1.5083e-04 - mean_absolute_error: 0.0073\n",
      "\n",
      "Epoch 04550: loss did not improve\n",
      "Epoch 4551/10000\n",
      "2700/2700 [==============================] - 0s 27us/step - loss: 1.5144e-04 - mean_absolute_error: 0.0074\n",
      "\n",
      "Epoch 04551: loss did not improve\n",
      "Epoch 4552/10000\n",
      "2700/2700 [==============================] - 0s 29us/step - loss: 1.5308e-04 - mean_absolute_error: 0.0074\n",
      "\n",
      "Epoch 04552: loss did not improve\n",
      "Epoch 4553/10000\n",
      "2700/2700 [==============================] - 0s 28us/step - loss: 1.5225e-04 - mean_absolute_error: 0.0074\n",
      "\n",
      "Epoch 04553: loss did not improve\n",
      "Epoch 4554/10000\n",
      "2700/2700 [==============================] - 0s 27us/step - loss: 1.4961e-04 - mean_absolute_error: 0.0071\n",
      "\n",
      "Epoch 04554: loss improved from 0.00015 to 0.00015, saving model to keras/best_baseline_weights.hdf5\n",
      "Epoch 4555/10000\n",
      "2700/2700 [==============================] - 0s 44us/step - loss: 1.4910e-04 - mean_absolute_error: 0.0072\n",
      "\n",
      "Epoch 04555: loss improved from 0.00015 to 0.00015, saving model to keras/best_baseline_weights.hdf5\n",
      "Epoch 4556/10000\n",
      "2700/2700 [==============================] - 0s 28us/step - loss: 1.5136e-04 - mean_absolute_error: 0.0073\n",
      "\n",
      "Epoch 04556: loss did not improve\n",
      "Epoch 4557/10000\n",
      "2700/2700 [==============================] - 0s 29us/step - loss: 1.5041e-04 - mean_absolute_error: 0.0071\n",
      "\n",
      "Epoch 04557: loss did not improve\n",
      "Epoch 4558/10000\n",
      "2700/2700 [==============================] - 0s 29us/step - loss: 1.5204e-04 - mean_absolute_error: 0.0073\n",
      "\n",
      "Epoch 04558: loss did not improve\n",
      "Epoch 4559/10000\n",
      "2700/2700 [==============================] - 0s 28us/step - loss: 1.5173e-04 - mean_absolute_error: 0.0072\n",
      "\n",
      "Epoch 04559: loss did not improve\n",
      "Epoch 4560/10000\n",
      "2700/2700 [==============================] - 0s 26us/step - loss: 1.4952e-04 - mean_absolute_error: 0.0072\n",
      "\n",
      "Epoch 04560: loss did not improve\n",
      "Epoch 4561/10000\n",
      "2700/2700 [==============================] - 0s 27us/step - loss: 1.5132e-04 - mean_absolute_error: 0.0073\n",
      "\n",
      "Epoch 04561: loss did not improve\n",
      "Epoch 4562/10000\n",
      "2700/2700 [==============================] - 0s 30us/step - loss: 1.5347e-04 - mean_absolute_error: 0.0075\n",
      "\n",
      "Epoch 04562: loss did not improve\n",
      "Epoch 4563/10000\n",
      "2700/2700 [==============================] - 0s 31us/step - loss: 1.5036e-04 - mean_absolute_error: 0.0072\n",
      "\n",
      "Epoch 04563: loss did not improve\n",
      "Epoch 4564/10000\n",
      "2700/2700 [==============================] - 0s 29us/step - loss: 1.5125e-04 - mean_absolute_error: 0.0072\n",
      "\n",
      "Epoch 04564: loss did not improve\n",
      "Epoch 4565/10000\n",
      "2700/2700 [==============================] - 0s 26us/step - loss: 1.4909e-04 - mean_absolute_error: 0.0071\n",
      "\n",
      "Epoch 04565: loss improved from 0.00015 to 0.00015, saving model to keras/best_baseline_weights.hdf5\n",
      "Epoch 4566/10000\n",
      "2700/2700 [==============================] - 0s 30us/step - loss: 1.5253e-04 - mean_absolute_error: 0.0074\n",
      "\n",
      "Epoch 04566: loss did not improve\n",
      "Epoch 4567/10000\n",
      "2700/2700 [==============================] - 0s 32us/step - loss: 1.5418e-04 - mean_absolute_error: 0.0075\n",
      "\n",
      "Epoch 04567: loss did not improve\n",
      "Epoch 4568/10000\n",
      "2700/2700 [==============================] - 0s 28us/step - loss: 1.4986e-04 - mean_absolute_error: 0.0073\n",
      "\n",
      "Epoch 04568: loss did not improve\n",
      "Epoch 4569/10000\n",
      "2700/2700 [==============================] - 0s 28us/step - loss: 1.5119e-04 - mean_absolute_error: 0.0072\n",
      "\n",
      "Epoch 04569: loss did not improve\n",
      "Epoch 4570/10000\n",
      "2700/2700 [==============================] - 0s 30us/step - loss: 1.4898e-04 - mean_absolute_error: 0.0071\n",
      "\n",
      "Epoch 04570: loss improved from 0.00015 to 0.00015, saving model to keras/best_baseline_weights.hdf5\n",
      "Epoch 4571/10000\n",
      "2700/2700 [==============================] - 0s 26us/step - loss: 1.5269e-04 - mean_absolute_error: 0.0074\n",
      "\n",
      "Epoch 04571: loss did not improve\n",
      "Epoch 4572/10000\n",
      "2700/2700 [==============================] - 0s 29us/step - loss: 1.5000e-04 - mean_absolute_error: 0.0072\n",
      "\n",
      "Epoch 04572: loss did not improve\n",
      "Epoch 4573/10000\n",
      "2700/2700 [==============================] - 0s 33us/step - loss: 1.4984e-04 - mean_absolute_error: 0.0072\n",
      "\n",
      "Epoch 04573: loss did not improve\n",
      "Epoch 4574/10000\n",
      "2700/2700 [==============================] - 0s 28us/step - loss: 1.5267e-04 - mean_absolute_error: 0.0073\n",
      "\n",
      "Epoch 04574: loss did not improve\n",
      "Epoch 4575/10000\n",
      "2700/2700 [==============================] - 0s 31us/step - loss: 1.4867e-04 - mean_absolute_error: 0.0071\n",
      "\n",
      "Epoch 04575: loss improved from 0.00015 to 0.00015, saving model to keras/best_baseline_weights.hdf5\n",
      "Epoch 4576/10000\n",
      "2700/2700 [==============================] - 0s 27us/step - loss: 1.5191e-04 - mean_absolute_error: 0.0074\n",
      "\n",
      "Epoch 04576: loss did not improve\n",
      "Epoch 4577/10000\n",
      "2700/2700 [==============================] - 0s 34us/step - loss: 1.5140e-04 - mean_absolute_error: 0.0073\n",
      "\n",
      "Epoch 04577: loss did not improve\n",
      "Epoch 4578/10000\n",
      "2700/2700 [==============================] - 0s 36us/step - loss: 1.4987e-04 - mean_absolute_error: 0.0072\n",
      "\n",
      "Epoch 04578: loss did not improve\n",
      "Epoch 4579/10000\n",
      "2700/2700 [==============================] - 0s 34us/step - loss: 1.5221e-04 - mean_absolute_error: 0.0075\n",
      "\n",
      "Epoch 04579: loss did not improve\n",
      "Epoch 4580/10000\n",
      "2700/2700 [==============================] - 0s 28us/step - loss: 1.5002e-04 - mean_absolute_error: 0.0072\n",
      "\n",
      "Epoch 04580: loss did not improve\n",
      "Epoch 4581/10000\n",
      "2700/2700 [==============================] - 0s 30us/step - loss: 1.5399e-04 - mean_absolute_error: 0.0074\n",
      "\n",
      "Epoch 04581: loss did not improve\n",
      "Epoch 4582/10000\n",
      "2700/2700 [==============================] - 0s 30us/step - loss: 1.4888e-04 - mean_absolute_error: 0.0071\n",
      "\n",
      "Epoch 04582: loss did not improve\n",
      "Epoch 4583/10000\n",
      "2700/2700 [==============================] - 0s 26us/step - loss: 1.4946e-04 - mean_absolute_error: 0.0072\n",
      "\n",
      "Epoch 04583: loss did not improve\n",
      "Epoch 4584/10000\n",
      "2700/2700 [==============================] - 0s 27us/step - loss: 1.5111e-04 - mean_absolute_error: 0.0072\n",
      "\n",
      "Epoch 04584: loss did not improve\n",
      "Epoch 4585/10000\n",
      "2700/2700 [==============================] - 0s 29us/step - loss: 1.5015e-04 - mean_absolute_error: 0.0072\n",
      "\n",
      "Epoch 04585: loss did not improve\n",
      "Epoch 4586/10000\n",
      "2700/2700 [==============================] - 0s 31us/step - loss: 1.5200e-04 - mean_absolute_error: 0.0074\n",
      "\n",
      "Epoch 04586: loss did not improve\n",
      "Epoch 4587/10000\n",
      "2700/2700 [==============================] - 0s 35us/step - loss: 1.5039e-04 - mean_absolute_error: 0.0074\n",
      "\n",
      "Epoch 04587: loss did not improve\n",
      "Epoch 4588/10000\n",
      "2700/2700 [==============================] - 0s 36us/step - loss: 1.5002e-04 - mean_absolute_error: 0.0073\n",
      "\n",
      "Epoch 04588: loss did not improve\n",
      "Epoch 4589/10000\n",
      "2700/2700 [==============================] - 0s 30us/step - loss: 1.5386e-04 - mean_absolute_error: 0.0074\n",
      "\n",
      "Epoch 04589: loss did not improve\n",
      "Epoch 4590/10000\n",
      "2700/2700 [==============================] - 0s 28us/step - loss: 1.4824e-04 - mean_absolute_error: 0.0072\n",
      "\n",
      "Epoch 04590: loss improved from 0.00015 to 0.00015, saving model to keras/best_baseline_weights.hdf5\n",
      "Epoch 4591/10000\n",
      "2700/2700 [==============================] - 0s 29us/step - loss: 1.5195e-04 - mean_absolute_error: 0.0073\n",
      "\n",
      "Epoch 04591: loss did not improve\n",
      "Epoch 4592/10000\n",
      "2700/2700 [==============================] - 0s 29us/step - loss: 1.5056e-04 - mean_absolute_error: 0.0074\n",
      "\n",
      "Epoch 04592: loss did not improve\n",
      "Epoch 4593/10000\n",
      "2700/2700 [==============================] - 0s 27us/step - loss: 1.5031e-04 - mean_absolute_error: 0.0073\n",
      "\n",
      "Epoch 04593: loss did not improve\n",
      "Epoch 4594/10000\n",
      "2700/2700 [==============================] - 0s 28us/step - loss: 1.4920e-04 - mean_absolute_error: 0.0072\n",
      "\n",
      "Epoch 04594: loss did not improve\n",
      "Epoch 4595/10000\n",
      "2700/2700 [==============================] - 0s 29us/step - loss: 1.5208e-04 - mean_absolute_error: 0.0074\n",
      "\n",
      "Epoch 04595: loss did not improve\n",
      "Epoch 4596/10000\n",
      "2700/2700 [==============================] - 0s 29us/step - loss: 1.5320e-04 - mean_absolute_error: 0.0074\n",
      "\n",
      "Epoch 04596: loss did not improve\n",
      "Epoch 4597/10000\n",
      "2700/2700 [==============================] - 0s 29us/step - loss: 1.5066e-04 - mean_absolute_error: 0.0073\n",
      "\n",
      "Epoch 04597: loss did not improve\n",
      "Epoch 4598/10000\n",
      "2700/2700 [==============================] - 0s 30us/step - loss: 1.5091e-04 - mean_absolute_error: 0.0073\n",
      "\n",
      "Epoch 04598: loss did not improve\n",
      "Epoch 4599/10000\n",
      "2700/2700 [==============================] - 0s 28us/step - loss: 1.4954e-04 - mean_absolute_error: 0.0072\n",
      "\n",
      "Epoch 04599: loss did not improve\n",
      "Epoch 4600/10000\n",
      "2700/2700 [==============================] - 0s 28us/step - loss: 1.5059e-04 - mean_absolute_error: 0.0073\n",
      "\n",
      "Epoch 04600: loss did not improve\n",
      "Epoch 4601/10000\n",
      "2700/2700 [==============================] - 0s 29us/step - loss: 1.4853e-04 - mean_absolute_error: 0.0071\n",
      "\n",
      "Epoch 04601: loss did not improve\n",
      "Epoch 4602/10000\n",
      "2700/2700 [==============================] - 0s 29us/step - loss: 1.4814e-04 - mean_absolute_error: 0.0070\n",
      "\n",
      "Epoch 04602: loss improved from 0.00015 to 0.00015, saving model to keras/best_baseline_weights.hdf5\n",
      "Epoch 4603/10000\n",
      "2700/2700 [==============================] - 0s 28us/step - loss: 1.4909e-04 - mean_absolute_error: 0.0072\n",
      "\n",
      "Epoch 04603: loss did not improve\n",
      "Epoch 4604/10000\n",
      "2700/2700 [==============================] - 0s 26us/step - loss: 1.5003e-04 - mean_absolute_error: 0.0073\n",
      "\n",
      "Epoch 04604: loss did not improve\n",
      "Epoch 4605/10000\n",
      "2700/2700 [==============================] - 0s 30us/step - loss: 1.4940e-04 - mean_absolute_error: 0.0072\n",
      "\n",
      "Epoch 04605: loss did not improve\n",
      "Epoch 4606/10000\n",
      "2700/2700 [==============================] - 0s 29us/step - loss: 1.5307e-04 - mean_absolute_error: 0.0075\n",
      "\n",
      "Epoch 04606: loss did not improve\n",
      "Epoch 4607/10000\n",
      "2700/2700 [==============================] - 0s 26us/step - loss: 1.5294e-04 - mean_absolute_error: 0.0076\n",
      "\n",
      "Epoch 04607: loss did not improve\n",
      "Epoch 4608/10000\n",
      "2700/2700 [==============================] - 0s 26us/step - loss: 1.4958e-04 - mean_absolute_error: 0.0072\n",
      "\n",
      "Epoch 04608: loss did not improve\n",
      "Epoch 4609/10000\n",
      "2700/2700 [==============================] - 0s 29us/step - loss: 1.4851e-04 - mean_absolute_error: 0.0071\n",
      "\n",
      "Epoch 04609: loss did not improve\n",
      "Epoch 4610/10000\n",
      "2700/2700 [==============================] - 0s 27us/step - loss: 1.4966e-04 - mean_absolute_error: 0.0072\n",
      "\n",
      "Epoch 04610: loss did not improve\n",
      "Epoch 4611/10000\n",
      "2700/2700 [==============================] - 0s 28us/step - loss: 1.5057e-04 - mean_absolute_error: 0.0073\n",
      "\n",
      "Epoch 04611: loss did not improve\n",
      "Epoch 4612/10000\n",
      "2700/2700 [==============================] - 0s 26us/step - loss: 1.4805e-04 - mean_absolute_error: 0.0071\n",
      "\n",
      "Epoch 04612: loss improved from 0.00015 to 0.00015, saving model to keras/best_baseline_weights.hdf5\n",
      "Epoch 4613/10000\n",
      "2700/2700 [==============================] - 0s 30us/step - loss: 1.5166e-04 - mean_absolute_error: 0.0074\n",
      "\n",
      "Epoch 04613: loss did not improve\n",
      "Epoch 4614/10000\n",
      "2700/2700 [==============================] - 0s 30us/step - loss: 1.4932e-04 - mean_absolute_error: 0.0072\n",
      "\n",
      "Epoch 04614: loss did not improve\n",
      "Epoch 4615/10000\n",
      "2700/2700 [==============================] - 0s 28us/step - loss: 1.5036e-04 - mean_absolute_error: 0.0073\n",
      "\n",
      "Epoch 04615: loss did not improve\n",
      "Epoch 4616/10000\n",
      "2700/2700 [==============================] - 0s 27us/step - loss: 1.5135e-04 - mean_absolute_error: 0.0074\n",
      "\n",
      "Epoch 04616: loss did not improve\n",
      "Epoch 4617/10000\n",
      "2700/2700 [==============================] - 0s 28us/step - loss: 1.5094e-04 - mean_absolute_error: 0.0073\n",
      "\n",
      "Epoch 04617: loss did not improve\n",
      "Epoch 4618/10000\n",
      "2700/2700 [==============================] - 0s 29us/step - loss: 1.4848e-04 - mean_absolute_error: 0.0072\n",
      "\n",
      "Epoch 04618: loss did not improve\n",
      "Epoch 4619/10000\n",
      "2700/2700 [==============================] - 0s 28us/step - loss: 1.5008e-04 - mean_absolute_error: 0.0073\n",
      "\n",
      "Epoch 04619: loss did not improve\n",
      "Epoch 4620/10000\n",
      "2700/2700 [==============================] - 0s 30us/step - loss: 1.4881e-04 - mean_absolute_error: 0.0071\n",
      "\n",
      "Epoch 04620: loss did not improve\n",
      "Epoch 4621/10000\n",
      "2700/2700 [==============================] - 0s 28us/step - loss: 1.5198e-04 - mean_absolute_error: 0.0074\n",
      "\n",
      "Epoch 04621: loss did not improve\n",
      "Epoch 4622/10000\n",
      "2700/2700 [==============================] - 0s 29us/step - loss: 1.4890e-04 - mean_absolute_error: 0.0072\n",
      "\n",
      "Epoch 04622: loss did not improve\n",
      "Epoch 4623/10000\n",
      "2700/2700 [==============================] - 0s 29us/step - loss: 1.4965e-04 - mean_absolute_error: 0.0072\n",
      "\n",
      "Epoch 04623: loss did not improve\n",
      "Epoch 4624/10000\n",
      "2700/2700 [==============================] - 0s 28us/step - loss: 1.4850e-04 - mean_absolute_error: 0.0071\n",
      "\n",
      "Epoch 04624: loss did not improve\n",
      "Epoch 4625/10000\n",
      "2700/2700 [==============================] - 0s 27us/step - loss: 1.5151e-04 - mean_absolute_error: 0.0074\n",
      "\n",
      "Epoch 04625: loss did not improve\n",
      "Epoch 4626/10000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2700/2700 [==============================] - 0s 28us/step - loss: 1.4917e-04 - mean_absolute_error: 0.0072\n",
      "\n",
      "Epoch 04626: loss did not improve\n",
      "Epoch 4627/10000\n",
      "2700/2700 [==============================] - 0s 29us/step - loss: 1.5023e-04 - mean_absolute_error: 0.0073\n",
      "\n",
      "Epoch 04627: loss did not improve\n",
      "Epoch 4628/10000\n",
      "2700/2700 [==============================] - 0s 28us/step - loss: 1.5413e-04 - mean_absolute_error: 0.0075\n",
      "\n",
      "Epoch 04628: loss did not improve\n",
      "Epoch 4629/10000\n",
      "2700/2700 [==============================] - 0s 28us/step - loss: 1.4802e-04 - mean_absolute_error: 0.0072\n",
      "\n",
      "Epoch 04629: loss improved from 0.00015 to 0.00015, saving model to keras/best_baseline_weights.hdf5\n",
      "Epoch 4630/10000\n",
      "2700/2700 [==============================] - 0s 28us/step - loss: 1.4823e-04 - mean_absolute_error: 0.0072\n",
      "\n",
      "Epoch 04630: loss did not improve\n",
      "Epoch 4631/10000\n",
      "2700/2700 [==============================] - 0s 30us/step - loss: 1.4937e-04 - mean_absolute_error: 0.0071\n",
      "\n",
      "Epoch 04631: loss did not improve\n",
      "Epoch 4632/10000\n",
      "2700/2700 [==============================] - 0s 28us/step - loss: 1.4756e-04 - mean_absolute_error: 0.0071\n",
      "\n",
      "Epoch 04632: loss improved from 0.00015 to 0.00015, saving model to keras/best_baseline_weights.hdf5\n",
      "Epoch 4633/10000\n",
      "2700/2700 [==============================] - 0s 30us/step - loss: 1.4814e-04 - mean_absolute_error: 0.0072\n",
      "\n",
      "Epoch 04633: loss did not improve\n",
      "Epoch 4634/10000\n",
      "2700/2700 [==============================] - 0s 31us/step - loss: 1.4767e-04 - mean_absolute_error: 0.0071\n",
      "\n",
      "Epoch 04634: loss did not improve\n",
      "Epoch 4635/10000\n",
      "2700/2700 [==============================] - 0s 28us/step - loss: 1.4714e-04 - mean_absolute_error: 0.0071\n",
      "\n",
      "Epoch 04635: loss improved from 0.00015 to 0.00015, saving model to keras/best_baseline_weights.hdf5\n",
      "Epoch 4636/10000\n",
      "2700/2700 [==============================] - 0s 27us/step - loss: 1.4847e-04 - mean_absolute_error: 0.0072\n",
      "\n",
      "Epoch 04636: loss did not improve\n",
      "Epoch 4637/10000\n",
      "2700/2700 [==============================] - 0s 28us/step - loss: 1.5079e-04 - mean_absolute_error: 0.0073\n",
      "\n",
      "Epoch 04637: loss did not improve\n",
      "Epoch 4638/10000\n",
      "2700/2700 [==============================] - 0s 29us/step - loss: 1.4989e-04 - mean_absolute_error: 0.0073\n",
      "\n",
      "Epoch 04638: loss did not improve\n",
      "Epoch 4639/10000\n",
      "2700/2700 [==============================] - 0s 29us/step - loss: 1.5073e-04 - mean_absolute_error: 0.0074\n",
      "\n",
      "Epoch 04639: loss did not improve\n",
      "Epoch 4640/10000\n",
      "2700/2700 [==============================] - 0s 26us/step - loss: 1.4856e-04 - mean_absolute_error: 0.0072\n",
      "\n",
      "Epoch 04640: loss did not improve\n",
      "Epoch 4641/10000\n",
      "2700/2700 [==============================] - 0s 28us/step - loss: 1.4638e-04 - mean_absolute_error: 0.0070\n",
      "\n",
      "Epoch 04641: loss improved from 0.00015 to 0.00015, saving model to keras/best_baseline_weights.hdf5\n",
      "Epoch 4642/10000\n",
      "2700/2700 [==============================] - 0s 27us/step - loss: 1.4829e-04 - mean_absolute_error: 0.0072\n",
      "\n",
      "Epoch 04642: loss did not improve\n",
      "Epoch 4643/10000\n",
      "2700/2700 [==============================] - 0s 29us/step - loss: 1.4947e-04 - mean_absolute_error: 0.0073\n",
      "\n",
      "Epoch 04643: loss did not improve\n",
      "Epoch 4644/10000\n",
      "2700/2700 [==============================] - 0s 25us/step - loss: 1.4853e-04 - mean_absolute_error: 0.0072\n",
      "\n",
      "Epoch 04644: loss did not improve\n",
      "Epoch 4645/10000\n",
      "2700/2700 [==============================] - 0s 28us/step - loss: 1.5142e-04 - mean_absolute_error: 0.0075\n",
      "\n",
      "Epoch 04645: loss did not improve\n",
      "Epoch 4646/10000\n",
      "2700/2700 [==============================] - 0s 29us/step - loss: 1.5530e-04 - mean_absolute_error: 0.0076\n",
      "\n",
      "Epoch 04646: loss did not improve\n",
      "Epoch 4647/10000\n",
      "2700/2700 [==============================] - 0s 29us/step - loss: 1.4873e-04 - mean_absolute_error: 0.0072\n",
      "\n",
      "Epoch 04647: loss did not improve\n",
      "Epoch 4648/10000\n",
      "2700/2700 [==============================] - 0s 28us/step - loss: 1.4840e-04 - mean_absolute_error: 0.0072\n",
      "\n",
      "Epoch 04648: loss did not improve\n",
      "Epoch 4649/10000\n",
      "2700/2700 [==============================] - 0s 28us/step - loss: 1.4890e-04 - mean_absolute_error: 0.0072\n",
      "\n",
      "Epoch 04649: loss did not improve\n",
      "Epoch 4650/10000\n",
      "2700/2700 [==============================] - 0s 33us/step - loss: 1.4930e-04 - mean_absolute_error: 0.0073\n",
      "\n",
      "Epoch 04650: loss did not improve\n",
      "Epoch 4651/10000\n",
      "2700/2700 [==============================] - 0s 31us/step - loss: 1.5088e-04 - mean_absolute_error: 0.0074\n",
      "\n",
      "Epoch 04651: loss did not improve\n",
      "Epoch 4652/10000\n",
      "2700/2700 [==============================] - 0s 29us/step - loss: 1.4811e-04 - mean_absolute_error: 0.0073\n",
      "\n",
      "Epoch 04652: loss did not improve\n",
      "Epoch 4653/10000\n",
      "2700/2700 [==============================] - 0s 27us/step - loss: 1.4921e-04 - mean_absolute_error: 0.0072\n",
      "\n",
      "Epoch 04653: loss did not improve\n",
      "Epoch 4654/10000\n",
      "2700/2700 [==============================] - 0s 30us/step - loss: 1.4929e-04 - mean_absolute_error: 0.0073\n",
      "\n",
      "Epoch 04654: loss did not improve\n",
      "Epoch 4655/10000\n",
      "2700/2700 [==============================] - 0s 29us/step - loss: 1.4831e-04 - mean_absolute_error: 0.0072\n",
      "\n",
      "Epoch 04655: loss did not improve\n",
      "Epoch 4656/10000\n",
      "2700/2700 [==============================] - 0s 27us/step - loss: 1.5072e-04 - mean_absolute_error: 0.0074\n",
      "\n",
      "Epoch 04656: loss did not improve\n",
      "Epoch 4657/10000\n",
      "2700/2700 [==============================] - 0s 28us/step - loss: 1.4763e-04 - mean_absolute_error: 0.0071\n",
      "\n",
      "Epoch 04657: loss did not improve\n",
      "Epoch 4658/10000\n",
      "2700/2700 [==============================] - 0s 26us/step - loss: 1.4767e-04 - mean_absolute_error: 0.0071\n",
      "\n",
      "Epoch 04658: loss did not improve\n",
      "Epoch 4659/10000\n",
      "2700/2700 [==============================] - 0s 28us/step - loss: 1.5310e-04 - mean_absolute_error: 0.0076\n",
      "\n",
      "Epoch 04659: loss did not improve\n",
      "Epoch 4660/10000\n",
      "2700/2700 [==============================] - 0s 29us/step - loss: 1.4941e-04 - mean_absolute_error: 0.0073\n",
      "\n",
      "Epoch 04660: loss did not improve\n",
      "Epoch 4661/10000\n",
      "2700/2700 [==============================] - 0s 27us/step - loss: 1.4778e-04 - mean_absolute_error: 0.0072\n",
      "\n",
      "Epoch 04661: loss did not improve\n",
      "Epoch 4662/10000\n",
      "2700/2700 [==============================] - 0s 28us/step - loss: 1.4751e-04 - mean_absolute_error: 0.0072\n",
      "\n",
      "Epoch 04662: loss did not improve\n",
      "Epoch 4663/10000\n",
      "2700/2700 [==============================] - 0s 28us/step - loss: 1.4926e-04 - mean_absolute_error: 0.0072\n",
      "\n",
      "Epoch 04663: loss did not improve\n",
      "Epoch 4664/10000\n",
      "2700/2700 [==============================] - 0s 28us/step - loss: 1.4719e-04 - mean_absolute_error: 0.0071\n",
      "\n",
      "Epoch 04664: loss did not improve\n",
      "Epoch 4665/10000\n",
      "2700/2700 [==============================] - 0s 27us/step - loss: 1.4767e-04 - mean_absolute_error: 0.0073\n",
      "\n",
      "Epoch 04665: loss did not improve\n",
      "Epoch 4666/10000\n",
      "2700/2700 [==============================] - 0s 28us/step - loss: 1.4724e-04 - mean_absolute_error: 0.0071\n",
      "\n",
      "Epoch 04666: loss did not improve\n",
      "Epoch 4667/10000\n",
      "2700/2700 [==============================] - 0s 28us/step - loss: 1.4666e-04 - mean_absolute_error: 0.0070\n",
      "\n",
      "Epoch 04667: loss did not improve\n",
      "Epoch 4668/10000\n",
      "2700/2700 [==============================] - 0s 30us/step - loss: 1.4764e-04 - mean_absolute_error: 0.0071\n",
      "\n",
      "Epoch 04668: loss did not improve\n",
      "Epoch 4669/10000\n",
      "2700/2700 [==============================] - 0s 31us/step - loss: 1.4734e-04 - mean_absolute_error: 0.0072\n",
      "\n",
      "Epoch 04669: loss did not improve\n",
      "Epoch 4670/10000\n",
      "2700/2700 [==============================] - 0s 29us/step - loss: 1.5118e-04 - mean_absolute_error: 0.0074\n",
      "\n",
      "Epoch 04670: loss did not improve\n",
      "Epoch 4671/10000\n",
      "2700/2700 [==============================] - 0s 28us/step - loss: 1.5518e-04 - mean_absolute_error: 0.0077\n",
      "\n",
      "Epoch 04671: loss did not improve\n",
      "Epoch 4672/10000\n",
      "2700/2700 [==============================] - 0s 29us/step - loss: 1.4844e-04 - mean_absolute_error: 0.0073\n",
      "\n",
      "Epoch 04672: loss did not improve\n",
      "Epoch 4673/10000\n",
      "2700/2700 [==============================] - 0s 30us/step - loss: 1.4521e-04 - mean_absolute_error: 0.0069\n",
      "\n",
      "Epoch 04673: loss improved from 0.00015 to 0.00015, saving model to keras/best_baseline_weights.hdf5\n",
      "Epoch 4674/10000\n",
      "2700/2700 [==============================] - 0s 30us/step - loss: 1.4598e-04 - mean_absolute_error: 0.0070\n",
      "\n",
      "Epoch 04674: loss did not improve\n",
      "Epoch 4675/10000\n",
      "2700/2700 [==============================] - 0s 29us/step - loss: 1.4892e-04 - mean_absolute_error: 0.0073\n",
      "\n",
      "Epoch 04675: loss did not improve\n",
      "Epoch 4676/10000\n",
      "2700/2700 [==============================] - 0s 26us/step - loss: 1.5106e-04 - mean_absolute_error: 0.0074\n",
      "\n",
      "Epoch 04676: loss did not improve\n",
      "Epoch 4677/10000\n",
      "2700/2700 [==============================] - 0s 29us/step - loss: 1.4741e-04 - mean_absolute_error: 0.0071\n",
      "\n",
      "Epoch 04677: loss did not improve\n",
      "Epoch 4678/10000\n",
      "2700/2700 [==============================] - 0s 27us/step - loss: 1.4966e-04 - mean_absolute_error: 0.0073\n",
      "\n",
      "Epoch 04678: loss did not improve\n",
      "Epoch 4679/10000\n",
      "2700/2700 [==============================] - 0s 30us/step - loss: 1.5073e-04 - mean_absolute_error: 0.0075\n",
      "\n",
      "Epoch 04679: loss did not improve\n",
      "Epoch 4680/10000\n",
      "2700/2700 [==============================] - 0s 26us/step - loss: 1.4592e-04 - mean_absolute_error: 0.0070\n",
      "\n",
      "Epoch 04680: loss did not improve\n",
      "Epoch 4681/10000\n",
      "2700/2700 [==============================] - 0s 34us/step - loss: 1.5086e-04 - mean_absolute_error: 0.0074\n",
      "\n",
      "Epoch 04681: loss did not improve\n",
      "Epoch 4682/10000\n",
      "2700/2700 [==============================] - 0s 38us/step - loss: 1.4758e-04 - mean_absolute_error: 0.0072\n",
      "\n",
      "Epoch 04682: loss did not improve\n",
      "Epoch 4683/10000\n",
      "2700/2700 [==============================] - 0s 40us/step - loss: 1.4791e-04 - mean_absolute_error: 0.0072\n",
      "\n",
      "Epoch 04683: loss did not improve\n",
      "Epoch 4684/10000\n",
      "2700/2700 [==============================] - 0s 36us/step - loss: 1.4533e-04 - mean_absolute_error: 0.0071\n",
      "\n",
      "Epoch 04684: loss did not improve\n",
      "Epoch 4685/10000\n",
      "2700/2700 [==============================] - 0s 35us/step - loss: 1.4711e-04 - mean_absolute_error: 0.0071\n",
      "\n",
      "Epoch 04685: loss did not improve\n",
      "Epoch 4686/10000\n",
      "2700/2700 [==============================] - 0s 36us/step - loss: 1.4678e-04 - mean_absolute_error: 0.0072\n",
      "\n",
      "Epoch 04686: loss did not improve\n",
      "Epoch 4687/10000\n",
      "2700/2700 [==============================] - 0s 39us/step - loss: 1.4676e-04 - mean_absolute_error: 0.0071\n",
      "\n",
      "Epoch 04687: loss did not improve\n",
      "Epoch 4688/10000\n",
      "2700/2700 [==============================] - 0s 39us/step - loss: 1.4925e-04 - mean_absolute_error: 0.0073\n",
      "\n",
      "Epoch 04688: loss did not improve\n",
      "Epoch 4689/10000\n",
      "2700/2700 [==============================] - 0s 34us/step - loss: 1.4694e-04 - mean_absolute_error: 0.0072\n",
      "\n",
      "Epoch 04689: loss did not improve\n",
      "Epoch 4690/10000\n",
      "2700/2700 [==============================] - 0s 35us/step - loss: 1.4891e-04 - mean_absolute_error: 0.0072\n",
      "\n",
      "Epoch 04690: loss did not improve\n",
      "Epoch 4691/10000\n",
      "2700/2700 [==============================] - 0s 31us/step - loss: 1.4891e-04 - mean_absolute_error: 0.0072\n",
      "\n",
      "Epoch 04691: loss did not improve\n",
      "Epoch 4692/10000\n",
      "2700/2700 [==============================] - 0s 28us/step - loss: 1.4829e-04 - mean_absolute_error: 0.0072\n",
      "\n",
      "Epoch 04692: loss did not improve\n",
      "Epoch 4693/10000\n",
      "2700/2700 [==============================] - 0s 28us/step - loss: 1.4726e-04 - mean_absolute_error: 0.0072\n",
      "\n",
      "Epoch 04693: loss did not improve\n",
      "Epoch 4694/10000\n",
      "2700/2700 [==============================] - 0s 26us/step - loss: 1.4491e-04 - mean_absolute_error: 0.0070\n",
      "\n",
      "Epoch 04694: loss improved from 0.00015 to 0.00014, saving model to keras/best_baseline_weights.hdf5\n",
      "Epoch 4695/10000\n",
      "2700/2700 [==============================] - 0s 28us/step - loss: 1.4562e-04 - mean_absolute_error: 0.0070\n",
      "\n",
      "Epoch 04695: loss did not improve\n",
      "Epoch 4696/10000\n",
      "2700/2700 [==============================] - 0s 29us/step - loss: 1.4772e-04 - mean_absolute_error: 0.0073\n",
      "\n",
      "Epoch 04696: loss did not improve\n",
      "Epoch 4697/10000\n",
      "2700/2700 [==============================] - 0s 29us/step - loss: 1.4958e-04 - mean_absolute_error: 0.0074\n",
      "\n",
      "Epoch 04697: loss did not improve\n",
      "Epoch 4698/10000\n",
      "2700/2700 [==============================] - 0s 26us/step - loss: 1.4664e-04 - mean_absolute_error: 0.0070\n",
      "\n",
      "Epoch 04698: loss did not improve\n",
      "Epoch 4699/10000\n",
      "2700/2700 [==============================] - 0s 44us/step - loss: 1.4809e-04 - mean_absolute_error: 0.0073\n",
      "\n",
      "Epoch 04699: loss did not improve\n",
      "Epoch 4700/10000\n",
      "2700/2700 [==============================] - 0s 36us/step - loss: 1.4917e-04 - mean_absolute_error: 0.0074\n",
      "\n",
      "Epoch 04700: loss did not improve\n",
      "Epoch 4701/10000\n",
      "2700/2700 [==============================] - 0s 36us/step - loss: 1.4901e-04 - mean_absolute_error: 0.0073\n",
      "\n",
      "Epoch 04701: loss did not improve\n",
      "Epoch 4702/10000\n",
      "2700/2700 [==============================] - 0s 36us/step - loss: 1.4834e-04 - mean_absolute_error: 0.0072\n",
      "\n",
      "Epoch 04702: loss did not improve\n",
      "Epoch 4703/10000\n",
      "2700/2700 [==============================] - 0s 29us/step - loss: 1.4729e-04 - mean_absolute_error: 0.0071\n",
      "\n",
      "Epoch 04703: loss did not improve\n",
      "Epoch 4704/10000\n",
      "2700/2700 [==============================] - 0s 29us/step - loss: 1.4733e-04 - mean_absolute_error: 0.0071\n",
      "\n",
      "Epoch 04704: loss did not improve\n",
      "Epoch 4705/10000\n",
      "2700/2700 [==============================] - 0s 28us/step - loss: 1.4879e-04 - mean_absolute_error: 0.0073\n",
      "\n",
      "Epoch 04705: loss did not improve\n",
      "Epoch 4706/10000\n",
      "2700/2700 [==============================] - 0s 29us/step - loss: 1.4699e-04 - mean_absolute_error: 0.0071\n",
      "\n",
      "Epoch 04706: loss did not improve\n",
      "Epoch 4707/10000\n",
      "2700/2700 [==============================] - 0s 27us/step - loss: 1.4827e-04 - mean_absolute_error: 0.0073\n",
      "\n",
      "Epoch 04707: loss did not improve\n",
      "Epoch 4708/10000\n",
      "2700/2700 [==============================] - 0s 29us/step - loss: 1.4538e-04 - mean_absolute_error: 0.0070\n",
      "\n",
      "Epoch 04708: loss did not improve\n",
      "Epoch 4709/10000\n",
      "2700/2700 [==============================] - 0s 33us/step - loss: 1.4714e-04 - mean_absolute_error: 0.0073\n",
      "\n",
      "Epoch 04709: loss did not improve\n",
      "Epoch 4710/10000\n",
      "2700/2700 [==============================] - 0s 37us/step - loss: 1.5517e-04 - mean_absolute_error: 0.0077\n",
      "\n",
      "Epoch 04710: loss did not improve\n",
      "Epoch 4711/10000\n",
      "2700/2700 [==============================] - 0s 37us/step - loss: 1.4737e-04 - mean_absolute_error: 0.0073\n",
      "\n",
      "Epoch 04711: loss did not improve\n",
      "Epoch 4712/10000\n",
      "2700/2700 [==============================] - 0s 29us/step - loss: 1.5052e-04 - mean_absolute_error: 0.0074\n",
      "\n",
      "Epoch 04712: loss did not improve\n",
      "Epoch 4713/10000\n",
      "2700/2700 [==============================] - 0s 31us/step - loss: 1.4812e-04 - mean_absolute_error: 0.0073\n",
      "\n",
      "Epoch 04713: loss did not improve\n",
      "Epoch 4714/10000\n",
      "2700/2700 [==============================] - 0s 27us/step - loss: 1.4557e-04 - mean_absolute_error: 0.0070\n",
      "\n",
      "Epoch 04714: loss did not improve\n",
      "Epoch 4715/10000\n",
      "2700/2700 [==============================] - 0s 29us/step - loss: 1.4715e-04 - mean_absolute_error: 0.0072\n",
      "\n",
      "Epoch 04715: loss did not improve\n",
      "Epoch 4716/10000\n",
      "2700/2700 [==============================] - 0s 28us/step - loss: 1.4622e-04 - mean_absolute_error: 0.0071\n",
      "\n",
      "Epoch 04716: loss did not improve\n",
      "Epoch 4717/10000\n",
      "2700/2700 [==============================] - 0s 28us/step - loss: 1.4612e-04 - mean_absolute_error: 0.0072\n",
      "\n",
      "Epoch 04717: loss did not improve\n",
      "Epoch 4718/10000\n",
      "2700/2700 [==============================] - 0s 31us/step - loss: 1.4581e-04 - mean_absolute_error: 0.0070\n",
      "\n",
      "Epoch 04718: loss did not improve\n",
      "Epoch 4719/10000\n",
      "2700/2700 [==============================] - 0s 27us/step - loss: 1.4658e-04 - mean_absolute_error: 0.0070\n",
      "\n",
      "Epoch 04719: loss did not improve\n",
      "Epoch 4720/10000\n",
      "2700/2700 [==============================] - 0s 31us/step - loss: 1.4983e-04 - mean_absolute_error: 0.0075\n",
      "\n",
      "Epoch 04720: loss did not improve\n",
      "Epoch 4721/10000\n",
      "2700/2700 [==============================] - 0s 30us/step - loss: 1.5263e-04 - mean_absolute_error: 0.0076\n",
      "\n",
      "Epoch 04721: loss did not improve\n",
      "Epoch 4722/10000\n",
      "2700/2700 [==============================] - 0s 30us/step - loss: 1.4606e-04 - mean_absolute_error: 0.0072\n",
      "\n",
      "Epoch 04722: loss did not improve\n",
      "Epoch 4723/10000\n",
      "2700/2700 [==============================] - 0s 31us/step - loss: 1.4546e-04 - mean_absolute_error: 0.0071\n",
      "\n",
      "Epoch 04723: loss did not improve\n",
      "Epoch 4724/10000\n",
      "2700/2700 [==============================] - 0s 26us/step - loss: 1.4476e-04 - mean_absolute_error: 0.0070\n",
      "\n",
      "Epoch 04724: loss improved from 0.00014 to 0.00014, saving model to keras/best_baseline_weights.hdf5\n",
      "Epoch 4725/10000\n",
      "2700/2700 [==============================] - 0s 32us/step - loss: 1.4839e-04 - mean_absolute_error: 0.0072\n",
      "\n",
      "Epoch 04725: loss did not improve\n",
      "Epoch 4726/10000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2700/2700 [==============================] - 0s 30us/step - loss: 1.4678e-04 - mean_absolute_error: 0.0072\n",
      "\n",
      "Epoch 04726: loss did not improve\n",
      "Epoch 4727/10000\n",
      "2700/2700 [==============================] - 0s 29us/step - loss: 1.4652e-04 - mean_absolute_error: 0.0072\n",
      "\n",
      "Epoch 04727: loss did not improve\n",
      "Epoch 4728/10000\n",
      "2700/2700 [==============================] - 0s 27us/step - loss: 1.5351e-04 - mean_absolute_error: 0.0075\n",
      "\n",
      "Epoch 04728: loss did not improve\n",
      "Epoch 4729/10000\n",
      "2700/2700 [==============================] - 0s 27us/step - loss: 1.4679e-04 - mean_absolute_error: 0.0072\n",
      "\n",
      "Epoch 04729: loss did not improve\n",
      "Epoch 4730/10000\n",
      "2700/2700 [==============================] - 0s 29us/step - loss: 1.4930e-04 - mean_absolute_error: 0.0073\n",
      "\n",
      "Epoch 04730: loss did not improve\n",
      "Epoch 4731/10000\n",
      "2700/2700 [==============================] - 0s 31us/step - loss: 1.4650e-04 - mean_absolute_error: 0.0072\n",
      "\n",
      "Epoch 04731: loss did not improve\n",
      "Epoch 4732/10000\n",
      "2700/2700 [==============================] - 0s 30us/step - loss: 1.4700e-04 - mean_absolute_error: 0.0072\n",
      "\n",
      "Epoch 04732: loss did not improve\n",
      "Epoch 4733/10000\n",
      "2700/2700 [==============================] - 0s 30us/step - loss: 1.5168e-04 - mean_absolute_error: 0.0075\n",
      "\n",
      "Epoch 04733: loss did not improve\n",
      "Epoch 4734/10000\n",
      "2700/2700 [==============================] - 0s 27us/step - loss: 1.4608e-04 - mean_absolute_error: 0.0071\n",
      "\n",
      "Epoch 04734: loss did not improve\n",
      "Epoch 4735/10000\n",
      "2700/2700 [==============================] - 0s 27us/step - loss: 1.4788e-04 - mean_absolute_error: 0.0072\n",
      "\n",
      "Epoch 04735: loss did not improve\n",
      "Epoch 4736/10000\n",
      "2700/2700 [==============================] - 0s 28us/step - loss: 1.4562e-04 - mean_absolute_error: 0.0070\n",
      "\n",
      "Epoch 04736: loss did not improve\n",
      "Epoch 4737/10000\n",
      "2700/2700 [==============================] - 0s 30us/step - loss: 1.4660e-04 - mean_absolute_error: 0.0071\n",
      "\n",
      "Epoch 04737: loss did not improve\n",
      "Epoch 4738/10000\n",
      "2700/2700 [==============================] - 0s 29us/step - loss: 1.4571e-04 - mean_absolute_error: 0.0071\n",
      "\n",
      "Epoch 04738: loss did not improve\n",
      "Epoch 4739/10000\n",
      "2700/2700 [==============================] - 0s 30us/step - loss: 1.4478e-04 - mean_absolute_error: 0.0070\n",
      "\n",
      "Epoch 04739: loss did not improve\n",
      "Epoch 4740/10000\n",
      "2700/2700 [==============================] - 0s 28us/step - loss: 1.4689e-04 - mean_absolute_error: 0.0071\n",
      "\n",
      "Epoch 04740: loss did not improve\n",
      "Epoch 4741/10000\n",
      "2700/2700 [==============================] - 0s 30us/step - loss: 1.4607e-04 - mean_absolute_error: 0.0071\n",
      "\n",
      "Epoch 04741: loss did not improve\n",
      "Epoch 4742/10000\n",
      "2700/2700 [==============================] - 0s 31us/step - loss: 1.4686e-04 - mean_absolute_error: 0.0072\n",
      "\n",
      "Epoch 04742: loss did not improve\n",
      "Epoch 4743/10000\n",
      "2700/2700 [==============================] - 0s 29us/step - loss: 1.4517e-04 - mean_absolute_error: 0.0071\n",
      "\n",
      "Epoch 04743: loss did not improve\n",
      "Epoch 4744/10000\n",
      "2700/2700 [==============================] - 0s 31us/step - loss: 1.4452e-04 - mean_absolute_error: 0.0071\n",
      "\n",
      "Epoch 04744: loss improved from 0.00014 to 0.00014, saving model to keras/best_baseline_weights.hdf5\n",
      "Epoch 4745/10000\n",
      "2700/2700 [==============================] - 0s 42us/step - loss: 1.5132e-04 - mean_absolute_error: 0.0075\n",
      "\n",
      "Epoch 04745: loss did not improve\n",
      "Epoch 4746/10000\n",
      "2700/2700 [==============================] - 0s 27us/step - loss: 1.4531e-04 - mean_absolute_error: 0.0071\n",
      "\n",
      "Epoch 04746: loss did not improve\n",
      "Epoch 4747/10000\n",
      "2700/2700 [==============================] - 0s 31us/step - loss: 1.4709e-04 - mean_absolute_error: 0.0071\n",
      "\n",
      "Epoch 04747: loss did not improve\n",
      "Epoch 4748/10000\n",
      "2700/2700 [==============================] - 0s 29us/step - loss: 1.4655e-04 - mean_absolute_error: 0.0072\n",
      "\n",
      "Epoch 04748: loss did not improve\n",
      "Epoch 4749/10000\n",
      "2700/2700 [==============================] - 0s 31us/step - loss: 1.4563e-04 - mean_absolute_error: 0.0070\n",
      "\n",
      "Epoch 04749: loss did not improve\n",
      "Epoch 4750/10000\n",
      "2700/2700 [==============================] - 0s 31us/step - loss: 1.4835e-04 - mean_absolute_error: 0.0073\n",
      "\n",
      "Epoch 04750: loss did not improve\n",
      "Epoch 4751/10000\n",
      "2700/2700 [==============================] - 0s 28us/step - loss: 1.4785e-04 - mean_absolute_error: 0.0071\n",
      "\n",
      "Epoch 04751: loss did not improve\n",
      "Epoch 4752/10000\n",
      "2700/2700 [==============================] - 0s 30us/step - loss: 1.4557e-04 - mean_absolute_error: 0.0071\n",
      "\n",
      "Epoch 04752: loss did not improve\n",
      "Epoch 4753/10000\n",
      "2700/2700 [==============================] - 0s 28us/step - loss: 1.4596e-04 - mean_absolute_error: 0.0072\n",
      "\n",
      "Epoch 04753: loss did not improve\n",
      "Epoch 4754/10000\n",
      "2700/2700 [==============================] - 0s 28us/step - loss: 1.4592e-04 - mean_absolute_error: 0.0071\n",
      "\n",
      "Epoch 04754: loss did not improve\n",
      "Epoch 4755/10000\n",
      "2700/2700 [==============================] - 0s 30us/step - loss: 1.4661e-04 - mean_absolute_error: 0.0071\n",
      "\n",
      "Epoch 04755: loss did not improve\n",
      "Epoch 4756/10000\n",
      "2700/2700 [==============================] - 0s 27us/step - loss: 1.4686e-04 - mean_absolute_error: 0.0072\n",
      "\n",
      "Epoch 04756: loss did not improve\n",
      "Epoch 4757/10000\n",
      "2700/2700 [==============================] - 0s 28us/step - loss: 1.4775e-04 - mean_absolute_error: 0.0073\n",
      "\n",
      "Epoch 04757: loss did not improve\n",
      "Epoch 4758/10000\n",
      "2700/2700 [==============================] - 0s 28us/step - loss: 1.4494e-04 - mean_absolute_error: 0.0073\n",
      "\n",
      "Epoch 04758: loss did not improve\n",
      "Epoch 4759/10000\n",
      "2700/2700 [==============================] - 0s 29us/step - loss: 1.4366e-04 - mean_absolute_error: 0.0070\n",
      "\n",
      "Epoch 04759: loss improved from 0.00014 to 0.00014, saving model to keras/best_baseline_weights.hdf5\n",
      "Epoch 4760/10000\n",
      "2700/2700 [==============================] - 0s 29us/step - loss: 1.4696e-04 - mean_absolute_error: 0.0072\n",
      "\n",
      "Epoch 04760: loss did not improve\n",
      "Epoch 4761/10000\n",
      "2700/2700 [==============================] - 0s 28us/step - loss: 1.4454e-04 - mean_absolute_error: 0.0071\n",
      "\n",
      "Epoch 04761: loss did not improve\n",
      "Epoch 4762/10000\n",
      "2700/2700 [==============================] - 0s 29us/step - loss: 1.4427e-04 - mean_absolute_error: 0.0070\n",
      "\n",
      "Epoch 04762: loss did not improve\n",
      "Epoch 4763/10000\n",
      "2700/2700 [==============================] - 0s 27us/step - loss: 1.4499e-04 - mean_absolute_error: 0.0072\n",
      "\n",
      "Epoch 04763: loss did not improve\n",
      "Epoch 4764/10000\n",
      "2700/2700 [==============================] - 0s 27us/step - loss: 1.5064e-04 - mean_absolute_error: 0.0074\n",
      "\n",
      "Epoch 04764: loss did not improve\n",
      "Epoch 4765/10000\n",
      "2700/2700 [==============================] - 0s 31us/step - loss: 1.4652e-04 - mean_absolute_error: 0.0072\n",
      "\n",
      "Epoch 04765: loss did not improve\n",
      "Epoch 4766/10000\n",
      "2700/2700 [==============================] - 0s 30us/step - loss: 1.4798e-04 - mean_absolute_error: 0.0073\n",
      "\n",
      "Epoch 04766: loss did not improve\n",
      "Epoch 4767/10000\n",
      "2700/2700 [==============================] - 0s 29us/step - loss: 1.4728e-04 - mean_absolute_error: 0.0072\n",
      "\n",
      "Epoch 04767: loss did not improve\n",
      "Epoch 4768/10000\n",
      "2700/2700 [==============================] - 0s 28us/step - loss: 1.4702e-04 - mean_absolute_error: 0.0072\n",
      "\n",
      "Epoch 04768: loss did not improve\n",
      "Epoch 4769/10000\n",
      "2700/2700 [==============================] - 0s 27us/step - loss: 1.4662e-04 - mean_absolute_error: 0.0072\n",
      "\n",
      "Epoch 04769: loss did not improve\n",
      "Epoch 4770/10000\n",
      "2700/2700 [==============================] - 0s 31us/step - loss: 1.4282e-04 - mean_absolute_error: 0.0070\n",
      "\n",
      "Epoch 04770: loss improved from 0.00014 to 0.00014, saving model to keras/best_baseline_weights.hdf5\n",
      "Epoch 4771/10000\n",
      "2700/2700 [==============================] - 0s 32us/step - loss: 1.4617e-04 - mean_absolute_error: 0.0071\n",
      "\n",
      "Epoch 04771: loss did not improve\n",
      "Epoch 4772/10000\n",
      "2700/2700 [==============================] - 0s 30us/step - loss: 1.4264e-04 - mean_absolute_error: 0.0069\n",
      "\n",
      "Epoch 04772: loss improved from 0.00014 to 0.00014, saving model to keras/best_baseline_weights.hdf5\n",
      "Epoch 4773/10000\n",
      "2700/2700 [==============================] - 0s 27us/step - loss: 1.4706e-04 - mean_absolute_error: 0.0073\n",
      "\n",
      "Epoch 04773: loss did not improve\n",
      "Epoch 4774/10000\n",
      "2700/2700 [==============================] - 0s 29us/step - loss: 1.4491e-04 - mean_absolute_error: 0.0072\n",
      "\n",
      "Epoch 04774: loss did not improve\n",
      "Epoch 4775/10000\n",
      "2700/2700 [==============================] - 0s 28us/step - loss: 1.4420e-04 - mean_absolute_error: 0.0071\n",
      "\n",
      "Epoch 04775: loss did not improve\n",
      "Epoch 4776/10000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2700/2700 [==============================] - 0s 34us/step - loss: 1.4621e-04 - mean_absolute_error: 0.0071\n",
      "\n",
      "Epoch 04776: loss did not improve\n",
      "Epoch 4777/10000\n",
      "2700/2700 [==============================] - 0s 36us/step - loss: 1.4576e-04 - mean_absolute_error: 0.0072\n",
      "\n",
      "Epoch 04777: loss did not improve\n",
      "Epoch 4778/10000\n",
      "2700/2700 [==============================] - 0s 39us/step - loss: 1.4487e-04 - mean_absolute_error: 0.0071\n",
      "\n",
      "Epoch 04778: loss did not improve\n",
      "Epoch 4779/10000\n",
      "2700/2700 [==============================] - 0s 36us/step - loss: 1.4747e-04 - mean_absolute_error: 0.0072\n",
      "\n",
      "Epoch 04779: loss did not improve\n",
      "Epoch 4780/10000\n",
      "2700/2700 [==============================] - 0s 30us/step - loss: 1.4897e-04 - mean_absolute_error: 0.0072\n",
      "\n",
      "Epoch 04780: loss did not improve\n",
      "Epoch 4781/10000\n",
      "2700/2700 [==============================] - 0s 29us/step - loss: 1.4523e-04 - mean_absolute_error: 0.0071\n",
      "\n",
      "Epoch 04781: loss did not improve\n",
      "Epoch 4782/10000\n",
      "2700/2700 [==============================] - 0s 46us/step - loss: 1.4514e-04 - mean_absolute_error: 0.0071\n",
      "\n",
      "Epoch 04782: loss did not improve\n",
      "Epoch 4783/10000\n",
      "2700/2700 [==============================] - 0s 43us/step - loss: 1.4623e-04 - mean_absolute_error: 0.0071\n",
      "\n",
      "Epoch 04783: loss did not improve\n",
      "Epoch 4784/10000\n",
      "2700/2700 [==============================] - 0s 45us/step - loss: 1.4426e-04 - mean_absolute_error: 0.0070\n",
      "\n",
      "Epoch 04784: loss did not improve\n",
      "Epoch 4785/10000\n",
      "2700/2700 [==============================] - 0s 38us/step - loss: 1.4397e-04 - mean_absolute_error: 0.0070\n",
      "\n",
      "Epoch 04785: loss did not improve\n",
      "Epoch 4786/10000\n",
      "2700/2700 [==============================] - 0s 41us/step - loss: 1.4314e-04 - mean_absolute_error: 0.0070\n",
      "\n",
      "Epoch 04786: loss did not improve\n",
      "Epoch 4787/10000\n",
      "2700/2700 [==============================] - 0s 40us/step - loss: 1.4577e-04 - mean_absolute_error: 0.0072\n",
      "\n",
      "Epoch 04787: loss did not improve\n",
      "Epoch 4788/10000\n",
      "2700/2700 [==============================] - 0s 30us/step - loss: 1.4617e-04 - mean_absolute_error: 0.0072\n",
      "\n",
      "Epoch 04788: loss did not improve\n",
      "Epoch 4789/10000\n",
      "2700/2700 [==============================] - 0s 27us/step - loss: 1.4322e-04 - mean_absolute_error: 0.0070\n",
      "\n",
      "Epoch 04789: loss did not improve\n",
      "Epoch 4790/10000\n",
      "2700/2700 [==============================] - 0s 30us/step - loss: 1.4462e-04 - mean_absolute_error: 0.0070\n",
      "\n",
      "Epoch 04790: loss did not improve\n",
      "Epoch 4791/10000\n",
      "2700/2700 [==============================] - 0s 32us/step - loss: 1.4529e-04 - mean_absolute_error: 0.0072\n",
      "\n",
      "Epoch 04791: loss did not improve\n",
      "Epoch 4792/10000\n",
      "2700/2700 [==============================] - 0s 29us/step - loss: 1.4774e-04 - mean_absolute_error: 0.0073\n",
      "\n",
      "Epoch 04792: loss did not improve\n",
      "Epoch 4793/10000\n",
      "2700/2700 [==============================] - 0s 31us/step - loss: 1.4623e-04 - mean_absolute_error: 0.0073\n",
      "\n",
      "Epoch 04793: loss did not improve\n",
      "Epoch 4794/10000\n",
      "2700/2700 [==============================] - 0s 28us/step - loss: 1.4441e-04 - mean_absolute_error: 0.0070\n",
      "\n",
      "Epoch 04794: loss did not improve\n",
      "Epoch 4795/10000\n",
      "2700/2700 [==============================] - 0s 30us/step - loss: 1.4790e-04 - mean_absolute_error: 0.0073\n",
      "\n",
      "Epoch 04795: loss did not improve\n",
      "Epoch 4796/10000\n",
      "2700/2700 [==============================] - 0s 29us/step - loss: 1.4618e-04 - mean_absolute_error: 0.0072\n",
      "\n",
      "Epoch 04796: loss did not improve\n",
      "Epoch 4797/10000\n",
      "2700/2700 [==============================] - 0s 29us/step - loss: 1.4872e-04 - mean_absolute_error: 0.0074\n",
      "\n",
      "Epoch 04797: loss did not improve\n",
      "Epoch 4798/10000\n",
      "2700/2700 [==============================] - 0s 30us/step - loss: 1.4519e-04 - mean_absolute_error: 0.0071\n",
      "\n",
      "Epoch 04798: loss did not improve\n",
      "Epoch 4799/10000\n",
      "2700/2700 [==============================] - 0s 30us/step - loss: 1.4741e-04 - mean_absolute_error: 0.0073\n",
      "\n",
      "Epoch 04799: loss did not improve\n",
      "Epoch 4800/10000\n",
      "2700/2700 [==============================] - 0s 31us/step - loss: 1.4490e-04 - mean_absolute_error: 0.0072\n",
      "\n",
      "Epoch 04800: loss did not improve\n",
      "Epoch 4801/10000\n",
      "2700/2700 [==============================] - 0s 30us/step - loss: 1.4373e-04 - mean_absolute_error: 0.0070\n",
      "\n",
      "Epoch 04801: loss did not improve\n",
      "Epoch 4802/10000\n",
      "2700/2700 [==============================] - 0s 30us/step - loss: 1.4664e-04 - mean_absolute_error: 0.0072\n",
      "\n",
      "Epoch 04802: loss did not improve\n",
      "Epoch 4803/10000\n",
      "2700/2700 [==============================] - 0s 29us/step - loss: 1.4336e-04 - mean_absolute_error: 0.0070\n",
      "\n",
      "Epoch 04803: loss did not improve\n",
      "Epoch 4804/10000\n",
      "2700/2700 [==============================] - 0s 31us/step - loss: 1.4376e-04 - mean_absolute_error: 0.0070\n",
      "\n",
      "Epoch 04804: loss did not improve\n",
      "Epoch 4805/10000\n",
      "2700/2700 [==============================] - 0s 33us/step - loss: 1.4484e-04 - mean_absolute_error: 0.0071\n",
      "\n",
      "Epoch 04805: loss did not improve\n",
      "Epoch 4806/10000\n",
      "2700/2700 [==============================] - 0s 32us/step - loss: 1.4354e-04 - mean_absolute_error: 0.0071\n",
      "\n",
      "Epoch 04806: loss did not improve\n",
      "Epoch 4807/10000\n",
      "2700/2700 [==============================] - 0s 50us/step - loss: 1.4483e-04 - mean_absolute_error: 0.0071\n",
      "\n",
      "Epoch 04807: loss did not improve\n",
      "Epoch 4808/10000\n",
      "2700/2700 [==============================] - 0s 33us/step - loss: 1.4522e-04 - mean_absolute_error: 0.0071\n",
      "\n",
      "Epoch 04808: loss did not improve\n",
      "Epoch 4809/10000\n",
      "2700/2700 [==============================] - 0s 38us/step - loss: 1.4415e-04 - mean_absolute_error: 0.0071\n",
      "\n",
      "Epoch 04809: loss did not improve\n",
      "Epoch 4810/10000\n",
      "2700/2700 [==============================] - 0s 28us/step - loss: 1.4624e-04 - mean_absolute_error: 0.0072\n",
      "\n",
      "Epoch 04810: loss did not improve\n",
      "Epoch 4811/10000\n",
      "2700/2700 [==============================] - 0s 31us/step - loss: 1.4703e-04 - mean_absolute_error: 0.0073\n",
      "\n",
      "Epoch 04811: loss did not improve\n",
      "Epoch 4812/10000\n",
      "2700/2700 [==============================] - 0s 34us/step - loss: 1.4363e-04 - mean_absolute_error: 0.0071\n",
      "\n",
      "Epoch 04812: loss did not improve\n",
      "Epoch 4813/10000\n",
      "2700/2700 [==============================] - 0s 32us/step - loss: 1.4366e-04 - mean_absolute_error: 0.0071\n",
      "\n",
      "Epoch 04813: loss did not improve\n",
      "Epoch 4814/10000\n",
      "2700/2700 [==============================] - 0s 34us/step - loss: 1.4659e-04 - mean_absolute_error: 0.0072\n",
      "\n",
      "Epoch 04814: loss did not improve\n",
      "Epoch 4815/10000\n",
      "2700/2700 [==============================] - 0s 33us/step - loss: 1.4454e-04 - mean_absolute_error: 0.0071\n",
      "\n",
      "Epoch 04815: loss did not improve\n",
      "Epoch 4816/10000\n",
      "2700/2700 [==============================] - 0s 32us/step - loss: 1.4421e-04 - mean_absolute_error: 0.0072\n",
      "\n",
      "Epoch 04816: loss did not improve\n",
      "Epoch 4817/10000\n",
      "2700/2700 [==============================] - 0s 30us/step - loss: 1.4265e-04 - mean_absolute_error: 0.0069\n",
      "\n",
      "Epoch 04817: loss did not improve\n",
      "Epoch 4818/10000\n",
      "2700/2700 [==============================] - 0s 35us/step - loss: 1.4835e-04 - mean_absolute_error: 0.0073\n",
      "\n",
      "Epoch 04818: loss did not improve\n",
      "Epoch 4819/10000\n",
      "2700/2700 [==============================] - 0s 32us/step - loss: 1.4518e-04 - mean_absolute_error: 0.0071\n",
      "\n",
      "Epoch 04819: loss did not improve\n",
      "Epoch 4820/10000\n",
      "2700/2700 [==============================] - 0s 32us/step - loss: 1.4309e-04 - mean_absolute_error: 0.0071\n",
      "\n",
      "Epoch 04820: loss did not improve\n",
      "Epoch 4821/10000\n",
      "2700/2700 [==============================] - 0s 31us/step - loss: 1.4653e-04 - mean_absolute_error: 0.0073\n",
      "\n",
      "Epoch 04821: loss did not improve\n",
      "Epoch 4822/10000\n",
      "2700/2700 [==============================] - 0s 30us/step - loss: 1.4354e-04 - mean_absolute_error: 0.0071\n",
      "\n",
      "Epoch 04822: loss did not improve\n",
      "Epoch 4823/10000\n",
      "2700/2700 [==============================] - 0s 32us/step - loss: 1.4680e-04 - mean_absolute_error: 0.0073\n",
      "\n",
      "Epoch 04823: loss did not improve\n",
      "Epoch 4824/10000\n",
      "2700/2700 [==============================] - 0s 27us/step - loss: 1.4483e-04 - mean_absolute_error: 0.0072\n",
      "\n",
      "Epoch 04824: loss did not improve\n",
      "Epoch 4825/10000\n",
      "2700/2700 [==============================] - 0s 34us/step - loss: 1.4553e-04 - mean_absolute_error: 0.0071\n",
      "\n",
      "Epoch 04825: loss did not improve\n",
      "Epoch 4826/10000\n",
      "2700/2700 [==============================] - 0s 31us/step - loss: 1.4667e-04 - mean_absolute_error: 0.0073\n",
      "\n",
      "Epoch 04826: loss did not improve\n",
      "Epoch 4827/10000\n",
      "2700/2700 [==============================] - 0s 31us/step - loss: 1.4312e-04 - mean_absolute_error: 0.0070\n",
      "\n",
      "Epoch 04827: loss did not improve\n",
      "Epoch 4828/10000\n",
      "2700/2700 [==============================] - 0s 27us/step - loss: 1.4397e-04 - mean_absolute_error: 0.0072\n",
      "\n",
      "Epoch 04828: loss did not improve\n",
      "Epoch 4829/10000\n",
      "2700/2700 [==============================] - 0s 26us/step - loss: 1.4357e-04 - mean_absolute_error: 0.0070\n",
      "\n",
      "Epoch 04829: loss did not improve\n",
      "Epoch 4830/10000\n",
      "2700/2700 [==============================] - 0s 30us/step - loss: 1.4263e-04 - mean_absolute_error: 0.0070\n",
      "\n",
      "Epoch 04830: loss improved from 0.00014 to 0.00014, saving model to keras/best_baseline_weights.hdf5\n",
      "Epoch 4831/10000\n",
      "2700/2700 [==============================] - 0s 30us/step - loss: 1.4357e-04 - mean_absolute_error: 0.0070\n",
      "\n",
      "Epoch 04831: loss did not improve\n",
      "Epoch 4832/10000\n",
      "2700/2700 [==============================] - 0s 30us/step - loss: 1.4610e-04 - mean_absolute_error: 0.0072\n",
      "\n",
      "Epoch 04832: loss did not improve\n",
      "Epoch 4833/10000\n",
      "2700/2700 [==============================] - 0s 27us/step - loss: 1.4496e-04 - mean_absolute_error: 0.0072\n",
      "\n",
      "Epoch 04833: loss did not improve\n",
      "Epoch 4834/10000\n",
      "2700/2700 [==============================] - 0s 27us/step - loss: 1.4452e-04 - mean_absolute_error: 0.0072\n",
      "\n",
      "Epoch 04834: loss did not improve\n",
      "Epoch 4835/10000\n",
      "2700/2700 [==============================] - 0s 28us/step - loss: 1.4789e-04 - mean_absolute_error: 0.0074\n",
      "\n",
      "Epoch 04835: loss did not improve\n",
      "Epoch 4836/10000\n",
      "2700/2700 [==============================] - 0s 31us/step - loss: 1.4600e-04 - mean_absolute_error: 0.0072\n",
      "\n",
      "Epoch 04836: loss did not improve\n",
      "Epoch 4837/10000\n",
      "2700/2700 [==============================] - 0s 40us/step - loss: 1.4301e-04 - mean_absolute_error: 0.0070\n",
      "\n",
      "Epoch 04837: loss did not improve\n",
      "Epoch 4838/10000\n",
      "2700/2700 [==============================] - 0s 41us/step - loss: 1.4458e-04 - mean_absolute_error: 0.0072\n",
      "\n",
      "Epoch 04838: loss did not improve\n",
      "Epoch 4839/10000\n",
      "2700/2700 [==============================] - 0s 43us/step - loss: 1.4529e-04 - mean_absolute_error: 0.0072\n",
      "\n",
      "Epoch 04839: loss did not improve\n",
      "Epoch 4840/10000\n",
      "2700/2700 [==============================] - 0s 44us/step - loss: 1.4298e-04 - mean_absolute_error: 0.0070\n",
      "\n",
      "Epoch 04840: loss did not improve\n",
      "Epoch 4841/10000\n",
      "2700/2700 [==============================] - 0s 37us/step - loss: 1.4315e-04 - mean_absolute_error: 0.0071\n",
      "\n",
      "Epoch 04841: loss did not improve\n",
      "Epoch 4842/10000\n",
      "2700/2700 [==============================] - 0s 34us/step - loss: 1.4446e-04 - mean_absolute_error: 0.0071\n",
      "\n",
      "Epoch 04842: loss did not improve\n",
      "Epoch 4843/10000\n",
      "2700/2700 [==============================] - 0s 28us/step - loss: 1.4368e-04 - mean_absolute_error: 0.0071\n",
      "\n",
      "Epoch 04843: loss did not improve\n",
      "Epoch 4844/10000\n",
      "2700/2700 [==============================] - 0s 28us/step - loss: 1.4467e-04 - mean_absolute_error: 0.0071\n",
      "\n",
      "Epoch 04844: loss did not improve\n",
      "Epoch 4845/10000\n",
      "2700/2700 [==============================] - 0s 43us/step - loss: 1.4588e-04 - mean_absolute_error: 0.0072\n",
      "\n",
      "Epoch 04845: loss did not improve\n",
      "Epoch 4846/10000\n",
      "2700/2700 [==============================] - ETA: 0s - loss: 1.4596e-04 - mean_absolute_error: 0.007 - 0s 50us/step - loss: 1.4353e-04 - mean_absolute_error: 0.0071\n",
      "\n",
      "Epoch 04846: loss did not improve\n",
      "Epoch 4847/10000\n",
      "2700/2700 [==============================] - 0s 46us/step - loss: 1.5035e-04 - mean_absolute_error: 0.0077\n",
      "\n",
      "Epoch 04847: loss did not improve\n",
      "Epoch 4848/10000\n",
      "2700/2700 [==============================] - 0s 41us/step - loss: 1.4807e-04 - mean_absolute_error: 0.0074\n",
      "\n",
      "Epoch 04848: loss did not improve\n",
      "Epoch 4849/10000\n",
      "2700/2700 [==============================] - 0s 39us/step - loss: 1.4362e-04 - mean_absolute_error: 0.0072\n",
      "\n",
      "Epoch 04849: loss did not improve\n",
      "Epoch 4850/10000\n",
      "2700/2700 [==============================] - 0s 36us/step - loss: 1.4963e-04 - mean_absolute_error: 0.0076\n",
      "\n",
      "Epoch 04850: loss did not improve\n",
      "Epoch 4851/10000\n",
      "2700/2700 [==============================] - 0s 41us/step - loss: 1.4251e-04 - mean_absolute_error: 0.0070\n",
      "\n",
      "Epoch 04851: loss improved from 0.00014 to 0.00014, saving model to keras/best_baseline_weights.hdf5\n",
      "Epoch 4852/10000\n",
      "2700/2700 [==============================] - 0s 36us/step - loss: 1.4397e-04 - mean_absolute_error: 0.0070\n",
      "\n",
      "Epoch 04852: loss did not improve\n",
      "Epoch 4853/10000\n",
      "2700/2700 [==============================] - 0s 35us/step - loss: 1.4494e-04 - mean_absolute_error: 0.0071\n",
      "\n",
      "Epoch 04853: loss did not improve\n",
      "Epoch 4854/10000\n",
      "2700/2700 [==============================] - 0s 37us/step - loss: 1.4416e-04 - mean_absolute_error: 0.0070\n",
      "\n",
      "Epoch 04854: loss did not improve\n",
      "Epoch 4855/10000\n",
      "2700/2700 [==============================] - 0s 35us/step - loss: 1.4380e-04 - mean_absolute_error: 0.0071\n",
      "\n",
      "Epoch 04855: loss did not improve\n",
      "Epoch 4856/10000\n",
      "2700/2700 [==============================] - 0s 35us/step - loss: 1.4397e-04 - mean_absolute_error: 0.0071\n",
      "\n",
      "Epoch 04856: loss did not improve\n",
      "Epoch 4857/10000\n",
      "2700/2700 [==============================] - 0s 40us/step - loss: 1.4637e-04 - mean_absolute_error: 0.0073: 0s - loss: 1.4720e-04 - mean_absolute_error: 0.007\n",
      "\n",
      "Epoch 04857: loss did not improve\n",
      "Epoch 4858/10000\n",
      "2700/2700 [==============================] - 0s 38us/step - loss: 1.4612e-04 - mean_absolute_error: 0.0073\n",
      "\n",
      "Epoch 04858: loss did not improve\n",
      "Epoch 4859/10000\n",
      "2700/2700 [==============================] - 0s 35us/step - loss: 1.4473e-04 - mean_absolute_error: 0.0072\n",
      "\n",
      "Epoch 04859: loss did not improve\n",
      "Epoch 4860/10000\n",
      "2700/2700 [==============================] - 0s 38us/step - loss: 1.4430e-04 - mean_absolute_error: 0.0071\n",
      "\n",
      "Epoch 04860: loss did not improve\n",
      "Epoch 4861/10000\n",
      "2700/2700 [==============================] - 0s 35us/step - loss: 1.4320e-04 - mean_absolute_error: 0.0071\n",
      "\n",
      "Epoch 04861: loss did not improve\n",
      "Epoch 4862/10000\n",
      "2700/2700 [==============================] - 0s 37us/step - loss: 1.4656e-04 - mean_absolute_error: 0.0073\n",
      "\n",
      "Epoch 04862: loss did not improve\n",
      "Epoch 4863/10000\n",
      "2700/2700 [==============================] - 0s 42us/step - loss: 1.4358e-04 - mean_absolute_error: 0.0070\n",
      "\n",
      "Epoch 04863: loss did not improve\n",
      "Epoch 4864/10000\n",
      "2700/2700 [==============================] - 0s 32us/step - loss: 1.4266e-04 - mean_absolute_error: 0.0070\n",
      "\n",
      "Epoch 04864: loss did not improve\n",
      "Epoch 4865/10000\n",
      "2700/2700 [==============================] - 0s 32us/step - loss: 1.4327e-04 - mean_absolute_error: 0.0072\n",
      "\n",
      "Epoch 04865: loss did not improve\n",
      "Epoch 4866/10000\n",
      "2700/2700 [==============================] - 0s 32us/step - loss: 1.4718e-04 - mean_absolute_error: 0.0072\n",
      "\n",
      "Epoch 04866: loss did not improve\n",
      "Epoch 4867/10000\n",
      "2700/2700 [==============================] - 0s 33us/step - loss: 1.4609e-04 - mean_absolute_error: 0.0073\n",
      "\n",
      "Epoch 04867: loss did not improve\n",
      "Epoch 4868/10000\n",
      "2700/2700 [==============================] - 0s 35us/step - loss: 1.4259e-04 - mean_absolute_error: 0.0071\n",
      "\n",
      "Epoch 04868: loss did not improve\n",
      "Epoch 4869/10000\n",
      "2700/2700 [==============================] - 0s 34us/step - loss: 1.4331e-04 - mean_absolute_error: 0.0071\n",
      "\n",
      "Epoch 04869: loss did not improve\n",
      "Epoch 4870/10000\n",
      "2700/2700 [==============================] - 0s 29us/step - loss: 1.4388e-04 - mean_absolute_error: 0.0071\n",
      "\n",
      "Epoch 04870: loss did not improve\n",
      "Epoch 4871/10000\n",
      "2700/2700 [==============================] - 0s 40us/step - loss: 1.4266e-04 - mean_absolute_error: 0.0070\n",
      "\n",
      "Epoch 04871: loss did not improve\n",
      "Epoch 4872/10000\n",
      "2700/2700 [==============================] - 0s 32us/step - loss: 1.4568e-04 - mean_absolute_error: 0.0071\n",
      "\n",
      "Epoch 04872: loss did not improve\n",
      "Epoch 4873/10000\n",
      "2700/2700 [==============================] - 0s 35us/step - loss: 1.4307e-04 - mean_absolute_error: 0.0070\n",
      "\n",
      "Epoch 04873: loss did not improve\n",
      "Epoch 4874/10000\n",
      "2700/2700 [==============================] - 0s 31us/step - loss: 1.4292e-04 - mean_absolute_error: 0.0070\n",
      "\n",
      "Epoch 04874: loss did not improve\n",
      "Epoch 4875/10000\n",
      "2700/2700 [==============================] - 0s 30us/step - loss: 1.4300e-04 - mean_absolute_error: 0.0071\n",
      "\n",
      "Epoch 04875: loss did not improve\n",
      "Epoch 4876/10000\n",
      "2700/2700 [==============================] - 0s 35us/step - loss: 1.4586e-04 - mean_absolute_error: 0.0073\n",
      "\n",
      "Epoch 04876: loss did not improve\n",
      "Epoch 4877/10000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2700/2700 [==============================] - 0s 51us/step - loss: 1.4485e-04 - mean_absolute_error: 0.0072\n",
      "\n",
      "Epoch 04877: loss did not improve\n",
      "Epoch 4878/10000\n",
      "2700/2700 [==============================] - 0s 37us/step - loss: 1.4676e-04 - mean_absolute_error: 0.0074\n",
      "\n",
      "Epoch 04878: loss did not improve\n",
      "Epoch 4879/10000\n",
      "2700/2700 [==============================] - 0s 37us/step - loss: 1.4279e-04 - mean_absolute_error: 0.0070\n",
      "\n",
      "Epoch 04879: loss did not improve\n",
      "Epoch 4880/10000\n",
      "2700/2700 [==============================] - 0s 36us/step - loss: 1.4308e-04 - mean_absolute_error: 0.0069\n",
      "\n",
      "Epoch 04880: loss did not improve\n",
      "Epoch 4881/10000\n",
      "2700/2700 [==============================] - 0s 37us/step - loss: 1.4304e-04 - mean_absolute_error: 0.0070\n",
      "\n",
      "Epoch 04881: loss did not improve\n",
      "Epoch 4882/10000\n",
      "2700/2700 [==============================] - 0s 35us/step - loss: 1.4630e-04 - mean_absolute_error: 0.0072\n",
      "\n",
      "Epoch 04882: loss did not improve\n",
      "Epoch 4883/10000\n",
      "2700/2700 [==============================] - 0s 37us/step - loss: 1.4552e-04 - mean_absolute_error: 0.0072\n",
      "\n",
      "Epoch 04883: loss did not improve\n",
      "Epoch 4884/10000\n",
      "2700/2700 [==============================] - 0s 37us/step - loss: 1.4170e-04 - mean_absolute_error: 0.0069\n",
      "\n",
      "Epoch 04884: loss improved from 0.00014 to 0.00014, saving model to keras/best_baseline_weights.hdf5\n",
      "Epoch 4885/10000\n",
      "2700/2700 [==============================] - 0s 35us/step - loss: 1.4418e-04 - mean_absolute_error: 0.0071\n",
      "\n",
      "Epoch 04885: loss did not improve\n",
      "Epoch 4886/10000\n",
      "2700/2700 [==============================] - 0s 36us/step - loss: 1.4134e-04 - mean_absolute_error: 0.0068\n",
      "\n",
      "Epoch 04886: loss improved from 0.00014 to 0.00014, saving model to keras/best_baseline_weights.hdf5\n",
      "Epoch 4887/10000\n",
      "2700/2700 [==============================] - 0s 35us/step - loss: 1.4210e-04 - mean_absolute_error: 0.0070\n",
      "\n",
      "Epoch 04887: loss did not improve\n",
      "Epoch 4888/10000\n",
      "2700/2700 [==============================] - 0s 30us/step - loss: 1.4230e-04 - mean_absolute_error: 0.0070\n",
      "\n",
      "Epoch 04888: loss did not improve\n",
      "Epoch 4889/10000\n",
      "2700/2700 [==============================] - 0s 35us/step - loss: 1.4277e-04 - mean_absolute_error: 0.0070\n",
      "\n",
      "Epoch 04889: loss did not improve\n",
      "Epoch 4890/10000\n",
      "2700/2700 [==============================] - 0s 35us/step - loss: 1.4456e-04 - mean_absolute_error: 0.0073\n",
      "\n",
      "Epoch 04890: loss did not improve\n",
      "Epoch 4891/10000\n",
      "2700/2700 [==============================] - 0s 36us/step - loss: 1.4297e-04 - mean_absolute_error: 0.0071\n",
      "\n",
      "Epoch 04891: loss did not improve\n",
      "Epoch 4892/10000\n",
      "2700/2700 [==============================] - 0s 38us/step - loss: 1.4395e-04 - mean_absolute_error: 0.0071\n",
      "\n",
      "Epoch 04892: loss did not improve\n",
      "Epoch 4893/10000\n",
      "2700/2700 [==============================] - 0s 35us/step - loss: 1.4608e-04 - mean_absolute_error: 0.0073\n",
      "\n",
      "Epoch 04893: loss did not improve\n",
      "Epoch 4894/10000\n",
      "2700/2700 [==============================] - 0s 32us/step - loss: 1.4290e-04 - mean_absolute_error: 0.0071\n",
      "\n",
      "Epoch 04894: loss did not improve\n",
      "Epoch 4895/10000\n",
      "2700/2700 [==============================] - 0s 30us/step - loss: 1.4267e-04 - mean_absolute_error: 0.0071\n",
      "\n",
      "Epoch 04895: loss did not improve\n",
      "Epoch 4896/10000\n",
      "2700/2700 [==============================] - 0s 38us/step - loss: 1.4255e-04 - mean_absolute_error: 0.0070\n",
      "\n",
      "Epoch 04896: loss did not improve\n",
      "Epoch 4897/10000\n",
      "2700/2700 [==============================] - 0s 26us/step - loss: 1.4479e-04 - mean_absolute_error: 0.0073\n",
      "\n",
      "Epoch 04897: loss did not improve\n",
      "Epoch 4898/10000\n",
      "2700/2700 [==============================] - 0s 35us/step - loss: 1.4346e-04 - mean_absolute_error: 0.0071\n",
      "\n",
      "Epoch 04898: loss did not improve\n",
      "Epoch 4899/10000\n",
      "2700/2700 [==============================] - 0s 30us/step - loss: 1.4744e-04 - mean_absolute_error: 0.0075\n",
      "\n",
      "Epoch 04899: loss did not improve\n",
      "Epoch 4900/10000\n",
      "2700/2700 [==============================] - 0s 31us/step - loss: 1.4535e-04 - mean_absolute_error: 0.0072\n",
      "\n",
      "Epoch 04900: loss did not improve\n",
      "Epoch 4901/10000\n",
      "2700/2700 [==============================] - 0s 32us/step - loss: 1.4325e-04 - mean_absolute_error: 0.0071\n",
      "\n",
      "Epoch 04901: loss did not improve\n",
      "Epoch 4902/10000\n",
      "2700/2700 [==============================] - 0s 31us/step - loss: 1.4229e-04 - mean_absolute_error: 0.0070\n",
      "\n",
      "Epoch 04902: loss did not improve\n",
      "Epoch 4903/10000\n",
      "2700/2700 [==============================] - 0s 32us/step - loss: 1.4089e-04 - mean_absolute_error: 0.0069\n",
      "\n",
      "Epoch 04903: loss improved from 0.00014 to 0.00014, saving model to keras/best_baseline_weights.hdf5\n",
      "Epoch 4904/10000\n",
      "2700/2700 [==============================] - 0s 31us/step - loss: 1.4489e-04 - mean_absolute_error: 0.0071\n",
      "\n",
      "Epoch 04904: loss did not improve\n",
      "Epoch 4905/10000\n",
      "2700/2700 [==============================] - 0s 35us/step - loss: 1.4277e-04 - mean_absolute_error: 0.0070\n",
      "\n",
      "Epoch 04905: loss did not improve\n",
      "Epoch 4906/10000\n",
      "2700/2700 [==============================] - 0s 29us/step - loss: 1.4400e-04 - mean_absolute_error: 0.0071\n",
      "\n",
      "Epoch 04906: loss did not improve\n",
      "Epoch 4907/10000\n",
      "2700/2700 [==============================] - 0s 31us/step - loss: 1.4217e-04 - mean_absolute_error: 0.0070\n",
      "\n",
      "Epoch 04907: loss did not improve\n",
      "Epoch 4908/10000\n",
      "2700/2700 [==============================] - 0s 32us/step - loss: 1.4740e-04 - mean_absolute_error: 0.0073\n",
      "\n",
      "Epoch 04908: loss did not improve\n",
      "Epoch 4909/10000\n",
      "2700/2700 [==============================] - 0s 30us/step - loss: 1.4144e-04 - mean_absolute_error: 0.0069\n",
      "\n",
      "Epoch 04909: loss did not improve\n",
      "Epoch 4910/10000\n",
      "2700/2700 [==============================] - 0s 32us/step - loss: 1.4215e-04 - mean_absolute_error: 0.0070\n",
      "\n",
      "Epoch 04910: loss did not improve\n",
      "Epoch 4911/10000\n",
      "2700/2700 [==============================] - 0s 31us/step - loss: 1.4331e-04 - mean_absolute_error: 0.0071\n",
      "\n",
      "Epoch 04911: loss did not improve\n",
      "Epoch 4912/10000\n",
      "2700/2700 [==============================] - 0s 33us/step - loss: 1.4040e-04 - mean_absolute_error: 0.0069\n",
      "\n",
      "Epoch 04912: loss improved from 0.00014 to 0.00014, saving model to keras/best_baseline_weights.hdf5\n",
      "Epoch 4913/10000\n",
      "2700/2700 [==============================] - 0s 33us/step - loss: 1.4291e-04 - mean_absolute_error: 0.0070\n",
      "\n",
      "Epoch 04913: loss did not improve\n",
      "Epoch 4914/10000\n",
      "2700/2700 [==============================] - 0s 32us/step - loss: 1.4088e-04 - mean_absolute_error: 0.0070\n",
      "\n",
      "Epoch 04914: loss did not improve\n",
      "Epoch 4915/10000\n",
      "2700/2700 [==============================] - 0s 32us/step - loss: 1.4319e-04 - mean_absolute_error: 0.0072\n",
      "\n",
      "Epoch 04915: loss did not improve\n",
      "Epoch 4916/10000\n",
      "2700/2700 [==============================] - 0s 32us/step - loss: 1.4421e-04 - mean_absolute_error: 0.0072\n",
      "\n",
      "Epoch 04916: loss did not improve\n",
      "Epoch 4917/10000\n",
      "2700/2700 [==============================] - 0s 32us/step - loss: 1.4577e-04 - mean_absolute_error: 0.0073\n",
      "\n",
      "Epoch 04917: loss did not improve\n",
      "Epoch 4918/10000\n",
      "2700/2700 [==============================] - 0s 35us/step - loss: 1.4249e-04 - mean_absolute_error: 0.0071\n",
      "\n",
      "Epoch 04918: loss did not improve\n",
      "Epoch 4919/10000\n",
      "2700/2700 [==============================] - 0s 31us/step - loss: 1.4307e-04 - mean_absolute_error: 0.0071\n",
      "\n",
      "Epoch 04919: loss did not improve\n",
      "Epoch 4920/10000\n",
      "2700/2700 [==============================] - 0s 30us/step - loss: 1.4364e-04 - mean_absolute_error: 0.0072\n",
      "\n",
      "Epoch 04920: loss did not improve\n",
      "Epoch 4921/10000\n",
      "2700/2700 [==============================] - 0s 29us/step - loss: 1.4150e-04 - mean_absolute_error: 0.0070\n",
      "\n",
      "Epoch 04921: loss did not improve\n",
      "Epoch 4922/10000\n",
      "2700/2700 [==============================] - 0s 31us/step - loss: 1.4224e-04 - mean_absolute_error: 0.0070\n",
      "\n",
      "Epoch 04922: loss did not improve\n",
      "Epoch 4923/10000\n",
      "2700/2700 [==============================] - 0s 33us/step - loss: 1.4053e-04 - mean_absolute_error: 0.0069\n",
      "\n",
      "Epoch 04923: loss did not improve\n",
      "Epoch 4924/10000\n",
      "2700/2700 [==============================] - 0s 34us/step - loss: 1.4513e-04 - mean_absolute_error: 0.0073\n",
      "\n",
      "Epoch 04924: loss did not improve\n",
      "Epoch 4925/10000\n",
      "2700/2700 [==============================] - 0s 32us/step - loss: 1.4234e-04 - mean_absolute_error: 0.0070\n",
      "\n",
      "Epoch 04925: loss did not improve\n",
      "Epoch 4926/10000\n",
      "2700/2700 [==============================] - 0s 28us/step - loss: 1.4165e-04 - mean_absolute_error: 0.0069\n",
      "\n",
      "Epoch 04926: loss did not improve\n",
      "Epoch 4927/10000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2700/2700 [==============================] - 0s 34us/step - loss: 1.4138e-04 - mean_absolute_error: 0.0070\n",
      "\n",
      "Epoch 04927: loss did not improve\n",
      "Epoch 4928/10000\n",
      "2700/2700 [==============================] - 0s 28us/step - loss: 1.4106e-04 - mean_absolute_error: 0.0070\n",
      "\n",
      "Epoch 04928: loss did not improve\n",
      "Epoch 4929/10000\n",
      "2700/2700 [==============================] - 0s 29us/step - loss: 1.4158e-04 - mean_absolute_error: 0.0069\n",
      "\n",
      "Epoch 04929: loss did not improve\n",
      "Epoch 4930/10000\n",
      "2700/2700 [==============================] - 0s 28us/step - loss: 1.4236e-04 - mean_absolute_error: 0.0071\n",
      "\n",
      "Epoch 04930: loss did not improve\n",
      "Epoch 4931/10000\n",
      "2700/2700 [==============================] - 0s 28us/step - loss: 1.4272e-04 - mean_absolute_error: 0.0070\n",
      "\n",
      "Epoch 04931: loss did not improve\n",
      "Epoch 4932/10000\n",
      "2700/2700 [==============================] - 0s 27us/step - loss: 1.4146e-04 - mean_absolute_error: 0.0070\n",
      "\n",
      "Epoch 04932: loss did not improve\n",
      "Epoch 4933/10000\n",
      "2700/2700 [==============================] - 0s 28us/step - loss: 1.4209e-04 - mean_absolute_error: 0.0069\n",
      "\n",
      "Epoch 04933: loss did not improve\n",
      "Epoch 4934/10000\n",
      "2700/2700 [==============================] - 0s 28us/step - loss: 1.4094e-04 - mean_absolute_error: 0.0069\n",
      "\n",
      "Epoch 04934: loss did not improve\n",
      "Epoch 4935/10000\n",
      "2700/2700 [==============================] - 0s 29us/step - loss: 1.4146e-04 - mean_absolute_error: 0.0070\n",
      "\n",
      "Epoch 04935: loss did not improve\n",
      "Epoch 4936/10000\n",
      "2700/2700 [==============================] - 0s 27us/step - loss: 1.4289e-04 - mean_absolute_error: 0.0071\n",
      "\n",
      "Epoch 04936: loss did not improve\n",
      "Epoch 4937/10000\n",
      "2700/2700 [==============================] - 0s 30us/step - loss: 1.4383e-04 - mean_absolute_error: 0.0072\n",
      "\n",
      "Epoch 04937: loss did not improve\n",
      "Epoch 4938/10000\n",
      "2700/2700 [==============================] - 0s 30us/step - loss: 1.4192e-04 - mean_absolute_error: 0.0070\n",
      "\n",
      "Epoch 04938: loss did not improve\n",
      "Epoch 4939/10000\n",
      "2700/2700 [==============================] - 0s 29us/step - loss: 1.4183e-04 - mean_absolute_error: 0.0069\n",
      "\n",
      "Epoch 04939: loss did not improve\n",
      "Epoch 4940/10000\n",
      "2700/2700 [==============================] - 0s 28us/step - loss: 1.5181e-04 - mean_absolute_error: 0.0077\n",
      "\n",
      "Epoch 04940: loss did not improve\n",
      "Epoch 4941/10000\n",
      "2700/2700 [==============================] - 0s 27us/step - loss: 1.4464e-04 - mean_absolute_error: 0.0072\n",
      "\n",
      "Epoch 04941: loss did not improve\n",
      "Epoch 4942/10000\n",
      "2700/2700 [==============================] - 0s 29us/step - loss: 1.4343e-04 - mean_absolute_error: 0.0072\n",
      "\n",
      "Epoch 04942: loss did not improve\n",
      "Epoch 4943/10000\n",
      "2700/2700 [==============================] - 0s 28us/step - loss: 1.4276e-04 - mean_absolute_error: 0.0071\n",
      "\n",
      "Epoch 04943: loss did not improve\n",
      "Epoch 4944/10000\n",
      "2700/2700 [==============================] - 0s 30us/step - loss: 1.4463e-04 - mean_absolute_error: 0.0071\n",
      "\n",
      "Epoch 04944: loss did not improve\n",
      "Epoch 4945/10000\n",
      "2700/2700 [==============================] - 0s 25us/step - loss: 1.4848e-04 - mean_absolute_error: 0.0075\n",
      "\n",
      "Epoch 04945: loss did not improve\n",
      "Epoch 4946/10000\n",
      "2700/2700 [==============================] - 0s 31us/step - loss: 1.4004e-04 - mean_absolute_error: 0.0069\n",
      "\n",
      "Epoch 04946: loss improved from 0.00014 to 0.00014, saving model to keras/best_baseline_weights.hdf5\n",
      "Epoch 4947/10000\n",
      "2700/2700 [==============================] - 0s 29us/step - loss: 1.4207e-04 - mean_absolute_error: 0.0069\n",
      "\n",
      "Epoch 04947: loss did not improve\n",
      "Epoch 4948/10000\n",
      "2700/2700 [==============================] - 0s 28us/step - loss: 1.4401e-04 - mean_absolute_error: 0.0072\n",
      "\n",
      "Epoch 04948: loss did not improve\n",
      "Epoch 4949/10000\n",
      "2700/2700 [==============================] - 0s 27us/step - loss: 1.4251e-04 - mean_absolute_error: 0.0070\n",
      "\n",
      "Epoch 04949: loss did not improve\n",
      "Epoch 4950/10000\n",
      "2700/2700 [==============================] - 0s 27us/step - loss: 1.4700e-04 - mean_absolute_error: 0.0074\n",
      "\n",
      "Epoch 04950: loss did not improve\n",
      "Epoch 4951/10000\n",
      "2700/2700 [==============================] - 0s 29us/step - loss: 1.4041e-04 - mean_absolute_error: 0.0069\n",
      "\n",
      "Epoch 04951: loss did not improve\n",
      "Epoch 4952/10000\n",
      "2700/2700 [==============================] - 0s 30us/step - loss: 1.4073e-04 - mean_absolute_error: 0.0069\n",
      "\n",
      "Epoch 04952: loss did not improve\n",
      "Epoch 4953/10000\n",
      "2700/2700 [==============================] - 0s 33us/step - loss: 1.4457e-04 - mean_absolute_error: 0.0073\n",
      "\n",
      "Epoch 04953: loss did not improve\n",
      "Epoch 4954/10000\n",
      "2700/2700 [==============================] - 0s 42us/step - loss: 1.4332e-04 - mean_absolute_error: 0.0072\n",
      "\n",
      "Epoch 04954: loss did not improve\n",
      "Epoch 4955/10000\n",
      "2700/2700 [==============================] - 0s 43us/step - loss: 1.4045e-04 - mean_absolute_error: 0.0069\n",
      "\n",
      "Epoch 04955: loss did not improve\n",
      "Epoch 4956/10000\n",
      "2700/2700 [==============================] - 0s 43us/step - loss: 1.4461e-04 - mean_absolute_error: 0.0072\n",
      "\n",
      "Epoch 04956: loss did not improve\n",
      "Epoch 4957/10000\n",
      "2700/2700 [==============================] - 0s 47us/step - loss: 1.4225e-04 - mean_absolute_error: 0.0070\n",
      "\n",
      "Epoch 04957: loss did not improve\n",
      "Epoch 4958/10000\n",
      "2700/2700 [==============================] - 0s 44us/step - loss: 1.4143e-04 - mean_absolute_error: 0.0069\n",
      "\n",
      "Epoch 04958: loss did not improve\n",
      "Epoch 4959/10000\n",
      "2700/2700 [==============================] - 0s 42us/step - loss: 1.4023e-04 - mean_absolute_error: 0.0069\n",
      "\n",
      "Epoch 04959: loss did not improve\n",
      "Epoch 4960/10000\n",
      "2700/2700 [==============================] - 0s 44us/step - loss: 1.4138e-04 - mean_absolute_error: 0.0069\n",
      "\n",
      "Epoch 04960: loss did not improve\n",
      "Epoch 4961/10000\n",
      "2700/2700 [==============================] - 0s 44us/step - loss: 1.4157e-04 - mean_absolute_error: 0.0071\n",
      "\n",
      "Epoch 04961: loss did not improve\n",
      "Epoch 4962/10000\n",
      "2700/2700 [==============================] - 0s 42us/step - loss: 1.5011e-04 - mean_absolute_error: 0.0076\n",
      "\n",
      "Epoch 04962: loss did not improve\n",
      "Epoch 4963/10000\n",
      "2700/2700 [==============================] - 0s 35us/step - loss: 1.4170e-04 - mean_absolute_error: 0.0070\n",
      "\n",
      "Epoch 04963: loss did not improve\n",
      "Epoch 4964/10000\n",
      "2700/2700 [==============================] - 0s 29us/step - loss: 1.4068e-04 - mean_absolute_error: 0.0069\n",
      "\n",
      "Epoch 04964: loss did not improve\n",
      "Epoch 4965/10000\n",
      "2700/2700 [==============================] - 0s 29us/step - loss: 1.4063e-04 - mean_absolute_error: 0.0069\n",
      "\n",
      "Epoch 04965: loss did not improve\n",
      "Epoch 4966/10000\n",
      "2700/2700 [==============================] - 0s 32us/step - loss: 1.4346e-04 - mean_absolute_error: 0.0072\n",
      "\n",
      "Epoch 04966: loss did not improve\n",
      "Epoch 4967/10000\n",
      "2700/2700 [==============================] - 0s 31us/step - loss: 1.4016e-04 - mean_absolute_error: 0.0069\n",
      "\n",
      "Epoch 04967: loss did not improve\n",
      "Epoch 4968/10000\n",
      "2700/2700 [==============================] - 0s 28us/step - loss: 1.4304e-04 - mean_absolute_error: 0.0071\n",
      "\n",
      "Epoch 04968: loss did not improve\n",
      "Epoch 4969/10000\n",
      "2700/2700 [==============================] - 0s 31us/step - loss: 1.4423e-04 - mean_absolute_error: 0.0072\n",
      "\n",
      "Epoch 04969: loss did not improve\n",
      "Epoch 4970/10000\n",
      "2700/2700 [==============================] - 0s 32us/step - loss: 1.4627e-04 - mean_absolute_error: 0.0073\n",
      "\n",
      "Epoch 04970: loss did not improve\n",
      "Epoch 4971/10000\n",
      "2700/2700 [==============================] - 0s 30us/step - loss: 1.4665e-04 - mean_absolute_error: 0.0075\n",
      "\n",
      "Epoch 04971: loss did not improve\n",
      "Epoch 4972/10000\n",
      "2700/2700 [==============================] - 0s 29us/step - loss: 1.4262e-04 - mean_absolute_error: 0.0070\n",
      "\n",
      "Epoch 04972: loss did not improve\n",
      "Epoch 4973/10000\n",
      "2700/2700 [==============================] - 0s 31us/step - loss: 1.4230e-04 - mean_absolute_error: 0.0070\n",
      "\n",
      "Epoch 04973: loss did not improve\n",
      "Epoch 4974/10000\n",
      "2700/2700 [==============================] - 0s 30us/step - loss: 1.4303e-04 - mean_absolute_error: 0.0070\n",
      "\n",
      "Epoch 04974: loss did not improve\n",
      "Epoch 4975/10000\n",
      "2700/2700 [==============================] - 0s 32us/step - loss: 1.4412e-04 - mean_absolute_error: 0.0073\n",
      "\n",
      "Epoch 04975: loss did not improve\n",
      "Epoch 4976/10000\n",
      "2700/2700 [==============================] - 0s 31us/step - loss: 1.4127e-04 - mean_absolute_error: 0.0070\n",
      "\n",
      "Epoch 04976: loss did not improve\n",
      "Epoch 4977/10000\n",
      "2700/2700 [==============================] - 0s 36us/step - loss: 1.4099e-04 - mean_absolute_error: 0.0069\n",
      "\n",
      "Epoch 04977: loss did not improve\n",
      "Epoch 4978/10000\n",
      "2700/2700 [==============================] - 0s 45us/step - loss: 1.4182e-04 - mean_absolute_error: 0.0069: 0s - loss: 1.3614e-04 - mean_absolute_error: 0.006\n",
      "\n",
      "Epoch 04978: loss did not improve\n",
      "Epoch 4979/10000\n",
      "2700/2700 [==============================] - 0s 40us/step - loss: 1.4087e-04 - mean_absolute_error: 0.0069\n",
      "\n",
      "Epoch 04979: loss did not improve\n",
      "Epoch 4980/10000\n",
      "2700/2700 [==============================] - 0s 43us/step - loss: 1.4007e-04 - mean_absolute_error: 0.0069\n",
      "\n",
      "Epoch 04980: loss did not improve\n",
      "Epoch 4981/10000\n",
      "2700/2700 [==============================] - 0s 42us/step - loss: 1.4164e-04 - mean_absolute_error: 0.0069\n",
      "\n",
      "Epoch 04981: loss did not improve\n",
      "Epoch 4982/10000\n",
      "2700/2700 [==============================] - 0s 37us/step - loss: 1.4148e-04 - mean_absolute_error: 0.0070\n",
      "\n",
      "Epoch 04982: loss did not improve\n",
      "Epoch 4983/10000\n",
      "2700/2700 [==============================] - 0s 42us/step - loss: 1.3915e-04 - mean_absolute_error: 0.0069\n",
      "\n",
      "Epoch 04983: loss improved from 0.00014 to 0.00014, saving model to keras/best_baseline_weights.hdf5\n",
      "Epoch 4984/10000\n",
      "2700/2700 [==============================] - 0s 36us/step - loss: 1.3864e-04 - mean_absolute_error: 0.0068\n",
      "\n",
      "Epoch 04984: loss improved from 0.00014 to 0.00014, saving model to keras/best_baseline_weights.hdf5\n",
      "Epoch 4985/10000\n",
      "2700/2700 [==============================] - 0s 30us/step - loss: 1.4537e-04 - mean_absolute_error: 0.0073\n",
      "\n",
      "Epoch 04985: loss did not improve\n",
      "Epoch 4986/10000\n",
      "2700/2700 [==============================] - 0s 27us/step - loss: 1.4347e-04 - mean_absolute_error: 0.0072\n",
      "\n",
      "Epoch 04986: loss did not improve\n",
      "Epoch 4987/10000\n",
      "2700/2700 [==============================] - 0s 28us/step - loss: 1.4141e-04 - mean_absolute_error: 0.0071\n",
      "\n",
      "Epoch 04987: loss did not improve\n",
      "Epoch 4988/10000\n",
      "2700/2700 [==============================] - 0s 28us/step - loss: 1.4239e-04 - mean_absolute_error: 0.0070\n",
      "\n",
      "Epoch 04988: loss did not improve\n",
      "Epoch 4989/10000\n",
      "2700/2700 [==============================] - 0s 30us/step - loss: 1.4323e-04 - mean_absolute_error: 0.0071\n",
      "\n",
      "Epoch 04989: loss did not improve\n",
      "Epoch 4990/10000\n",
      "2700/2700 [==============================] - 0s 41us/step - loss: 1.4185e-04 - mean_absolute_error: 0.0072\n",
      "\n",
      "Epoch 04990: loss did not improve\n",
      "Epoch 4991/10000\n",
      "2700/2700 [==============================] - 0s 55us/step - loss: 1.4377e-04 - mean_absolute_error: 0.0072\n",
      "\n",
      "Epoch 04991: loss did not improve\n",
      "Epoch 4992/10000\n",
      "2700/2700 [==============================] - 0s 43us/step - loss: 1.4136e-04 - mean_absolute_error: 0.0070\n",
      "\n",
      "Epoch 04992: loss did not improve\n",
      "Epoch 4993/10000\n",
      "2700/2700 [==============================] - 0s 43us/step - loss: 1.4349e-04 - mean_absolute_error: 0.0072\n",
      "\n",
      "Epoch 04993: loss did not improve\n",
      "Epoch 4994/10000\n",
      "2700/2700 [==============================] - 0s 49us/step - loss: 1.4387e-04 - mean_absolute_error: 0.0071\n",
      "\n",
      "Epoch 04994: loss did not improve\n",
      "Epoch 4995/10000\n",
      "2700/2700 [==============================] - 0s 43us/step - loss: 1.4039e-04 - mean_absolute_error: 0.0069\n",
      "\n",
      "Epoch 04995: loss did not improve\n",
      "Epoch 4996/10000\n",
      "2700/2700 [==============================] - 0s 44us/step - loss: 1.4452e-04 - mean_absolute_error: 0.0073\n",
      "\n",
      "Epoch 04996: loss did not improve\n",
      "Epoch 4997/10000\n",
      "2700/2700 [==============================] - 0s 45us/step - loss: 1.4254e-04 - mean_absolute_error: 0.0070\n",
      "\n",
      "Epoch 04997: loss did not improve\n",
      "Epoch 4998/10000\n",
      "2700/2700 [==============================] - 0s 41us/step - loss: 1.4327e-04 - mean_absolute_error: 0.0071\n",
      "\n",
      "Epoch 04998: loss did not improve\n",
      "Epoch 4999/10000\n",
      "2700/2700 [==============================] - 0s 38us/step - loss: 1.4005e-04 - mean_absolute_error: 0.0069\n",
      "\n",
      "Epoch 04999: loss did not improve\n",
      "Epoch 5000/10000\n",
      "2700/2700 [==============================] - 0s 42us/step - loss: 1.4057e-04 - mean_absolute_error: 0.0069\n",
      "\n",
      "Epoch 05000: loss did not improve\n",
      "Epoch 5001/10000\n",
      "2700/2700 [==============================] - 0s 39us/step - loss: 1.4036e-04 - mean_absolute_error: 0.0070\n",
      "\n",
      "Epoch 05001: loss did not improve\n",
      "Epoch 5002/10000\n",
      "2700/2700 [==============================] - 0s 41us/step - loss: 1.4112e-04 - mean_absolute_error: 0.0070\n",
      "\n",
      "Epoch 05002: loss did not improve\n",
      "Epoch 5003/10000\n",
      "2700/2700 [==============================] - 0s 43us/step - loss: 1.3882e-04 - mean_absolute_error: 0.0068\n",
      "\n",
      "Epoch 05003: loss did not improve\n",
      "Epoch 5004/10000\n",
      "2700/2700 [==============================] - 0s 38us/step - loss: 1.4153e-04 - mean_absolute_error: 0.0072\n",
      "\n",
      "Epoch 05004: loss did not improve\n",
      "Epoch 5005/10000\n",
      "2700/2700 [==============================] - 0s 36us/step - loss: 1.4148e-04 - mean_absolute_error: 0.0070\n",
      "\n",
      "Epoch 05005: loss did not improve\n",
      "Epoch 5006/10000\n",
      "2700/2700 [==============================] - 0s 40us/step - loss: 1.4179e-04 - mean_absolute_error: 0.0070\n",
      "\n",
      "Epoch 05006: loss did not improve\n",
      "Epoch 5007/10000\n",
      "2700/2700 [==============================] - 0s 40us/step - loss: 1.4011e-04 - mean_absolute_error: 0.0070\n",
      "\n",
      "Epoch 05007: loss did not improve\n",
      "Epoch 5008/10000\n",
      "2700/2700 [==============================] - 0s 38us/step - loss: 1.4225e-04 - mean_absolute_error: 0.0071\n",
      "\n",
      "Epoch 05008: loss did not improve\n",
      "Epoch 5009/10000\n",
      "2700/2700 [==============================] - 0s 36us/step - loss: 1.4249e-04 - mean_absolute_error: 0.0071\n",
      "\n",
      "Epoch 05009: loss did not improve\n",
      "Epoch 5010/10000\n",
      "2700/2700 [==============================] - 0s 34us/step - loss: 1.3837e-04 - mean_absolute_error: 0.0069\n",
      "\n",
      "Epoch 05010: loss improved from 0.00014 to 0.00014, saving model to keras/best_baseline_weights.hdf5\n",
      "Epoch 5011/10000\n",
      "2700/2700 [==============================] - 0s 40us/step - loss: 1.4035e-04 - mean_absolute_error: 0.0069\n",
      "\n",
      "Epoch 05011: loss did not improve\n",
      "Epoch 5012/10000\n",
      "2700/2700 [==============================] - 0s 34us/step - loss: 1.4220e-04 - mean_absolute_error: 0.0070\n",
      "\n",
      "Epoch 05012: loss did not improve\n",
      "Epoch 5013/10000\n",
      "2700/2700 [==============================] - 0s 37us/step - loss: 1.4099e-04 - mean_absolute_error: 0.0071\n",
      "\n",
      "Epoch 05013: loss did not improve\n",
      "Epoch 5014/10000\n",
      "2700/2700 [==============================] - 0s 39us/step - loss: 1.4372e-04 - mean_absolute_error: 0.0072\n",
      "\n",
      "Epoch 05014: loss did not improve\n",
      "Epoch 5015/10000\n",
      "2700/2700 [==============================] - 0s 35us/step - loss: 1.3944e-04 - mean_absolute_error: 0.0069\n",
      "\n",
      "Epoch 05015: loss did not improve\n",
      "Epoch 5016/10000\n",
      "2700/2700 [==============================] - 0s 36us/step - loss: 1.4041e-04 - mean_absolute_error: 0.0069\n",
      "\n",
      "Epoch 05016: loss did not improve\n",
      "Epoch 5017/10000\n",
      "2700/2700 [==============================] - 0s 36us/step - loss: 1.4107e-04 - mean_absolute_error: 0.0069\n",
      "\n",
      "Epoch 05017: loss did not improve\n",
      "Epoch 5018/10000\n",
      "2700/2700 [==============================] - 0s 37us/step - loss: 1.4212e-04 - mean_absolute_error: 0.0071\n",
      "\n",
      "Epoch 05018: loss did not improve\n",
      "Epoch 5019/10000\n",
      "2700/2700 [==============================] - 0s 37us/step - loss: 1.3807e-04 - mean_absolute_error: 0.0069\n",
      "\n",
      "Epoch 05019: loss improved from 0.00014 to 0.00014, saving model to keras/best_baseline_weights.hdf5\n",
      "Epoch 5020/10000\n",
      "2700/2700 [==============================] - 0s 37us/step - loss: 1.4393e-04 - mean_absolute_error: 0.0071\n",
      "\n",
      "Epoch 05020: loss did not improve\n",
      "Epoch 5021/10000\n",
      "2700/2700 [==============================] - 0s 41us/step - loss: 1.4158e-04 - mean_absolute_error: 0.0070\n",
      "\n",
      "Epoch 05021: loss did not improve\n",
      "Epoch 5022/10000\n",
      "2700/2700 [==============================] - 0s 36us/step - loss: 1.4018e-04 - mean_absolute_error: 0.0070\n",
      "\n",
      "Epoch 05022: loss did not improve\n",
      "Epoch 5023/10000\n",
      "2700/2700 [==============================] - 0s 40us/step - loss: 1.4216e-04 - mean_absolute_error: 0.0071\n",
      "\n",
      "Epoch 05023: loss did not improve\n",
      "Epoch 5024/10000\n",
      "2700/2700 [==============================] - 0s 36us/step - loss: 1.3994e-04 - mean_absolute_error: 0.0069\n",
      "\n",
      "Epoch 05024: loss did not improve\n",
      "Epoch 5025/10000\n",
      "2700/2700 [==============================] - 0s 39us/step - loss: 1.4136e-04 - mean_absolute_error: 0.0069\n",
      "\n",
      "Epoch 05025: loss did not improve\n",
      "Epoch 5026/10000\n",
      "2700/2700 [==============================] - 0s 46us/step - loss: 1.3911e-04 - mean_absolute_error: 0.0070\n",
      "\n",
      "Epoch 05026: loss did not improve\n",
      "Epoch 5027/10000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2700/2700 [==============================] - 0s 44us/step - loss: 1.4052e-04 - mean_absolute_error: 0.0070\n",
      "\n",
      "Epoch 05027: loss did not improve\n",
      "Epoch 5028/10000\n",
      "2700/2700 [==============================] - 0s 37us/step - loss: 1.4054e-04 - mean_absolute_error: 0.0069\n",
      "\n",
      "Epoch 05028: loss did not improve\n",
      "Epoch 5029/10000\n",
      "2700/2700 [==============================] - 0s 37us/step - loss: 1.4166e-04 - mean_absolute_error: 0.0071\n",
      "\n",
      "Epoch 05029: loss did not improve\n",
      "Epoch 5030/10000\n",
      "2700/2700 [==============================] - 0s 38us/step - loss: 1.4368e-04 - mean_absolute_error: 0.0074\n",
      "\n",
      "Epoch 05030: loss did not improve\n",
      "Epoch 5031/10000\n",
      "2700/2700 [==============================] - 0s 37us/step - loss: 1.4007e-04 - mean_absolute_error: 0.0070\n",
      "\n",
      "Epoch 05031: loss did not improve\n",
      "Epoch 5032/10000\n",
      "2700/2700 [==============================] - 0s 29us/step - loss: 1.3871e-04 - mean_absolute_error: 0.0068\n",
      "\n",
      "Epoch 05032: loss did not improve\n",
      "Epoch 5033/10000\n",
      "2700/2700 [==============================] - 0s 29us/step - loss: 1.4063e-04 - mean_absolute_error: 0.0070\n",
      "\n",
      "Epoch 05033: loss did not improve\n",
      "Epoch 5034/10000\n",
      "2700/2700 [==============================] - 0s 29us/step - loss: 1.4148e-04 - mean_absolute_error: 0.0069\n",
      "\n",
      "Epoch 05034: loss did not improve\n",
      "Epoch 5035/10000\n",
      "2700/2700 [==============================] - 0s 30us/step - loss: 1.4029e-04 - mean_absolute_error: 0.0069\n",
      "\n",
      "Epoch 05035: loss did not improve\n",
      "Epoch 5036/10000\n",
      "2700/2700 [==============================] - 0s 31us/step - loss: 1.4044e-04 - mean_absolute_error: 0.0071\n",
      "\n",
      "Epoch 05036: loss did not improve\n",
      "Epoch 5037/10000\n",
      "2700/2700 [==============================] - 0s 40us/step - loss: 1.4307e-04 - mean_absolute_error: 0.0071\n",
      "\n",
      "Epoch 05037: loss did not improve\n",
      "Epoch 5038/10000\n",
      "2700/2700 [==============================] - 0s 54us/step - loss: 1.4044e-04 - mean_absolute_error: 0.0069\n",
      "\n",
      "Epoch 05038: loss did not improve\n",
      "Epoch 5039/10000\n",
      "2700/2700 [==============================] - 0s 41us/step - loss: 1.4366e-04 - mean_absolute_error: 0.0072\n",
      "\n",
      "Epoch 05039: loss did not improve\n",
      "Epoch 5040/10000\n",
      "2700/2700 [==============================] - 0s 36us/step - loss: 1.4661e-04 - mean_absolute_error: 0.0074\n",
      "\n",
      "Epoch 05040: loss did not improve\n",
      "Epoch 5041/10000\n",
      "2700/2700 [==============================] - 0s 36us/step - loss: 1.4268e-04 - mean_absolute_error: 0.0072\n",
      "\n",
      "Epoch 05041: loss did not improve\n",
      "Epoch 5042/10000\n",
      "2700/2700 [==============================] - 0s 32us/step - loss: 1.3971e-04 - mean_absolute_error: 0.0069\n",
      "\n",
      "Epoch 05042: loss did not improve\n",
      "Epoch 5043/10000\n",
      "2700/2700 [==============================] - 0s 31us/step - loss: 1.3829e-04 - mean_absolute_error: 0.0068\n",
      "\n",
      "Epoch 05043: loss did not improve\n",
      "Epoch 5044/10000\n",
      "2700/2700 [==============================] - 0s 29us/step - loss: 1.4059e-04 - mean_absolute_error: 0.0069\n",
      "\n",
      "Epoch 05044: loss did not improve\n",
      "Epoch 5045/10000\n",
      "2700/2700 [==============================] - 0s 31us/step - loss: 1.3892e-04 - mean_absolute_error: 0.0069\n",
      "\n",
      "Epoch 05045: loss did not improve\n",
      "Epoch 5046/10000\n",
      "2700/2700 [==============================] - 0s 28us/step - loss: 1.3934e-04 - mean_absolute_error: 0.0069\n",
      "\n",
      "Epoch 05046: loss did not improve\n",
      "Epoch 5047/10000\n",
      "2700/2700 [==============================] - 0s 29us/step - loss: 1.4231e-04 - mean_absolute_error: 0.0070\n",
      "\n",
      "Epoch 05047: loss did not improve\n",
      "Epoch 5048/10000\n",
      "2700/2700 [==============================] - 0s 29us/step - loss: 1.4079e-04 - mean_absolute_error: 0.0071\n",
      "\n",
      "Epoch 05048: loss did not improve\n",
      "Epoch 5049/10000\n",
      "2700/2700 [==============================] - 0s 28us/step - loss: 1.4167e-04 - mean_absolute_error: 0.0070\n",
      "\n",
      "Epoch 05049: loss did not improve\n",
      "Epoch 5050/10000\n",
      "2700/2700 [==============================] - 0s 30us/step - loss: 1.4032e-04 - mean_absolute_error: 0.0071\n",
      "\n",
      "Epoch 05050: loss did not improve\n",
      "Epoch 5051/10000\n",
      "2700/2700 [==============================] - 0s 28us/step - loss: 1.4010e-04 - mean_absolute_error: 0.0068\n",
      "\n",
      "Epoch 05051: loss did not improve\n",
      "Epoch 5052/10000\n",
      "2700/2700 [==============================] - 0s 29us/step - loss: 1.3944e-04 - mean_absolute_error: 0.0068\n",
      "\n",
      "Epoch 05052: loss did not improve\n",
      "Epoch 5053/10000\n",
      "2700/2700 [==============================] - 0s 29us/step - loss: 1.4012e-04 - mean_absolute_error: 0.0070\n",
      "\n",
      "Epoch 05053: loss did not improve\n",
      "Epoch 5054/10000\n",
      "2700/2700 [==============================] - 0s 31us/step - loss: 1.4171e-04 - mean_absolute_error: 0.0071\n",
      "\n",
      "Epoch 05054: loss did not improve\n",
      "Epoch 5055/10000\n",
      "2700/2700 [==============================] - 0s 44us/step - loss: 1.4299e-04 - mean_absolute_error: 0.0071\n",
      "\n",
      "Epoch 05055: loss did not improve\n",
      "Epoch 5056/10000\n",
      "2700/2700 [==============================] - 0s 35us/step - loss: 1.4012e-04 - mean_absolute_error: 0.0070\n",
      "\n",
      "Epoch 05056: loss did not improve\n",
      "Epoch 5057/10000\n",
      "2700/2700 [==============================] - 0s 34us/step - loss: 1.4083e-04 - mean_absolute_error: 0.0070\n",
      "\n",
      "Epoch 05057: loss did not improve\n",
      "Epoch 5058/10000\n",
      "2700/2700 [==============================] - 0s 31us/step - loss: 1.3970e-04 - mean_absolute_error: 0.0069\n",
      "\n",
      "Epoch 05058: loss did not improve\n",
      "Epoch 5059/10000\n",
      "2700/2700 [==============================] - 0s 29us/step - loss: 1.4060e-04 - mean_absolute_error: 0.0071\n",
      "\n",
      "Epoch 05059: loss did not improve\n",
      "Epoch 5060/10000\n",
      "2700/2700 [==============================] - 0s 31us/step - loss: 1.4387e-04 - mean_absolute_error: 0.0072\n",
      "\n",
      "Epoch 05060: loss did not improve\n",
      "Epoch 5061/10000\n",
      "2700/2700 [==============================] - 0s 31us/step - loss: 1.4198e-04 - mean_absolute_error: 0.0071\n",
      "\n",
      "Epoch 05061: loss did not improve\n",
      "Epoch 5062/10000\n",
      "2700/2700 [==============================] - 0s 33us/step - loss: 1.3945e-04 - mean_absolute_error: 0.0069\n",
      "\n",
      "Epoch 05062: loss did not improve\n",
      "Epoch 5063/10000\n",
      "2700/2700 [==============================] - 0s 31us/step - loss: 1.3871e-04 - mean_absolute_error: 0.0068\n",
      "\n",
      "Epoch 05063: loss did not improve\n",
      "Epoch 5064/10000\n",
      "2700/2700 [==============================] - 0s 32us/step - loss: 1.4155e-04 - mean_absolute_error: 0.0072\n",
      "\n",
      "Epoch 05064: loss did not improve\n",
      "Epoch 5065/10000\n",
      "2700/2700 [==============================] - 0s 34us/step - loss: 1.3970e-04 - mean_absolute_error: 0.0069\n",
      "\n",
      "Epoch 05065: loss did not improve\n",
      "Epoch 5066/10000\n",
      "2700/2700 [==============================] - 0s 31us/step - loss: 1.3932e-04 - mean_absolute_error: 0.0069\n",
      "\n",
      "Epoch 05066: loss did not improve\n",
      "Epoch 5067/10000\n",
      "2700/2700 [==============================] - 0s 30us/step - loss: 1.4518e-04 - mean_absolute_error: 0.0073\n",
      "\n",
      "Epoch 05067: loss did not improve\n",
      "Epoch 5068/10000\n",
      "2700/2700 [==============================] - 0s 32us/step - loss: 1.4237e-04 - mean_absolute_error: 0.0071\n",
      "\n",
      "Epoch 05068: loss did not improve\n",
      "Epoch 5069/10000\n",
      "2700/2700 [==============================] - 0s 28us/step - loss: 1.3938e-04 - mean_absolute_error: 0.0069\n",
      "\n",
      "Epoch 05069: loss did not improve\n",
      "Epoch 5070/10000\n",
      "2700/2700 [==============================] - 0s 32us/step - loss: 1.3936e-04 - mean_absolute_error: 0.0069\n",
      "\n",
      "Epoch 05070: loss did not improve\n",
      "Epoch 5071/10000\n",
      "2700/2700 [==============================] - 0s 33us/step - loss: 1.3959e-04 - mean_absolute_error: 0.0069\n",
      "\n",
      "Epoch 05071: loss did not improve\n",
      "Epoch 5072/10000\n",
      "2700/2700 [==============================] - 0s 33us/step - loss: 1.4016e-04 - mean_absolute_error: 0.0070\n",
      "\n",
      "Epoch 05072: loss did not improve\n",
      "Epoch 5073/10000\n",
      "2700/2700 [==============================] - 0s 33us/step - loss: 1.3864e-04 - mean_absolute_error: 0.0069\n",
      "\n",
      "Epoch 05073: loss did not improve\n",
      "Epoch 5074/10000\n",
      "2700/2700 [==============================] - 0s 33us/step - loss: 1.3813e-04 - mean_absolute_error: 0.0068\n",
      "\n",
      "Epoch 05074: loss did not improve\n",
      "Epoch 5075/10000\n",
      "2700/2700 [==============================] - 0s 31us/step - loss: 1.3997e-04 - mean_absolute_error: 0.0069\n",
      "\n",
      "Epoch 05075: loss did not improve\n",
      "Epoch 5076/10000\n",
      "2700/2700 [==============================] - 0s 33us/step - loss: 1.4035e-04 - mean_absolute_error: 0.0069\n",
      "\n",
      "Epoch 05076: loss did not improve\n",
      "Epoch 5077/10000\n",
      "2700/2700 [==============================] - 0s 33us/step - loss: 1.4136e-04 - mean_absolute_error: 0.0071\n",
      "\n",
      "Epoch 05077: loss did not improve\n",
      "Epoch 5078/10000\n",
      "2700/2700 [==============================] - 0s 31us/step - loss: 1.3967e-04 - mean_absolute_error: 0.0070\n",
      "\n",
      "Epoch 05078: loss did not improve\n",
      "Epoch 5079/10000\n",
      "2700/2700 [==============================] - 0s 40us/step - loss: 1.4170e-04 - mean_absolute_error: 0.0071\n",
      "\n",
      "Epoch 05079: loss did not improve\n",
      "Epoch 5080/10000\n",
      "2700/2700 [==============================] - 0s 43us/step - loss: 1.4435e-04 - mean_absolute_error: 0.0072\n",
      "\n",
      "Epoch 05080: loss did not improve\n",
      "Epoch 5081/10000\n",
      "2700/2700 [==============================] - 0s 37us/step - loss: 1.3832e-04 - mean_absolute_error: 0.0068\n",
      "\n",
      "Epoch 05081: loss did not improve\n",
      "Epoch 5082/10000\n",
      "2700/2700 [==============================] - 0s 43us/step - loss: 1.3895e-04 - mean_absolute_error: 0.0068\n",
      "\n",
      "Epoch 05082: loss did not improve\n",
      "Epoch 5083/10000\n",
      "2700/2700 [==============================] - 0s 39us/step - loss: 1.4051e-04 - mean_absolute_error: 0.0070\n",
      "\n",
      "Epoch 05083: loss did not improve\n",
      "Epoch 5084/10000\n",
      "2700/2700 [==============================] - 0s 37us/step - loss: 1.3943e-04 - mean_absolute_error: 0.0070\n",
      "\n",
      "Epoch 05084: loss did not improve\n",
      "Epoch 5085/10000\n",
      "2700/2700 [==============================] - 0s 37us/step - loss: 1.3878e-04 - mean_absolute_error: 0.0069\n",
      "\n",
      "Epoch 05085: loss did not improve\n",
      "Epoch 5086/10000\n",
      "2700/2700 [==============================] - 0s 28us/step - loss: 1.4046e-04 - mean_absolute_error: 0.0070\n",
      "\n",
      "Epoch 05086: loss did not improve\n",
      "Epoch 5087/10000\n",
      "2700/2700 [==============================] - 0s 27us/step - loss: 1.4060e-04 - mean_absolute_error: 0.0069\n",
      "\n",
      "Epoch 05087: loss did not improve\n",
      "Epoch 5088/10000\n",
      "2700/2700 [==============================] - 0s 28us/step - loss: 1.3955e-04 - mean_absolute_error: 0.0069\n",
      "\n",
      "Epoch 05088: loss did not improve\n",
      "Epoch 5089/10000\n",
      "2700/2700 [==============================] - 0s 29us/step - loss: 1.4317e-04 - mean_absolute_error: 0.0071\n",
      "\n",
      "Epoch 05089: loss did not improve\n",
      "Epoch 5090/10000\n",
      "2700/2700 [==============================] - 0s 29us/step - loss: 1.3900e-04 - mean_absolute_error: 0.0069\n",
      "\n",
      "Epoch 05090: loss did not improve\n",
      "Epoch 5091/10000\n",
      "2700/2700 [==============================] - 0s 30us/step - loss: 1.3870e-04 - mean_absolute_error: 0.0068\n",
      "\n",
      "Epoch 05091: loss did not improve\n",
      "Epoch 5092/10000\n",
      "2700/2700 [==============================] - 0s 53us/step - loss: 1.3868e-04 - mean_absolute_error: 0.0068\n",
      "\n",
      "Epoch 05092: loss did not improve\n",
      "Epoch 5093/10000\n",
      "2700/2700 [==============================] - 0s 42us/step - loss: 1.3962e-04 - mean_absolute_error: 0.0070\n",
      "\n",
      "Epoch 05093: loss did not improve\n",
      "Epoch 5094/10000\n",
      "2700/2700 [==============================] - 0s 36us/step - loss: 1.3895e-04 - mean_absolute_error: 0.0069\n",
      "\n",
      "Epoch 05094: loss did not improve\n",
      "Epoch 5095/10000\n",
      "2700/2700 [==============================] - 0s 40us/step - loss: 1.3979e-04 - mean_absolute_error: 0.0070\n",
      "\n",
      "Epoch 05095: loss did not improve\n",
      "Epoch 5096/10000\n",
      "2700/2700 [==============================] - 0s 38us/step - loss: 1.3818e-04 - mean_absolute_error: 0.0068\n",
      "\n",
      "Epoch 05096: loss did not improve\n",
      "Epoch 5097/10000\n",
      "2700/2700 [==============================] - 0s 34us/step - loss: 1.4111e-04 - mean_absolute_error: 0.0071\n",
      "\n",
      "Epoch 05097: loss did not improve\n",
      "Epoch 5098/10000\n",
      "2700/2700 [==============================] - 0s 37us/step - loss: 1.3829e-04 - mean_absolute_error: 0.0069\n",
      "\n",
      "Epoch 05098: loss did not improve\n",
      "Epoch 5099/10000\n",
      "2700/2700 [==============================] - 0s 39us/step - loss: 1.3945e-04 - mean_absolute_error: 0.0069\n",
      "\n",
      "Epoch 05099: loss did not improve\n",
      "Epoch 5100/10000\n",
      "2700/2700 [==============================] - 0s 34us/step - loss: 1.3808e-04 - mean_absolute_error: 0.0068\n",
      "\n",
      "Epoch 05100: loss did not improve\n",
      "Epoch 5101/10000\n",
      "2700/2700 [==============================] - 0s 35us/step - loss: 1.4084e-04 - mean_absolute_error: 0.0071\n",
      "\n",
      "Epoch 05101: loss did not improve\n",
      "Epoch 5102/10000\n",
      "2700/2700 [==============================] - 0s 35us/step - loss: 1.4099e-04 - mean_absolute_error: 0.0071\n",
      "\n",
      "Epoch 05102: loss did not improve\n",
      "Epoch 5103/10000\n",
      "2700/2700 [==============================] - 0s 36us/step - loss: 1.4149e-04 - mean_absolute_error: 0.0071\n",
      "\n",
      "Epoch 05103: loss did not improve\n",
      "Epoch 5104/10000\n",
      "2700/2700 [==============================] - 0s 31us/step - loss: 1.4120e-04 - mean_absolute_error: 0.0070\n",
      "\n",
      "Epoch 05104: loss did not improve\n",
      "Epoch 5105/10000\n",
      "2700/2700 [==============================] - 0s 33us/step - loss: 1.4518e-04 - mean_absolute_error: 0.0073\n",
      "\n",
      "Epoch 05105: loss did not improve\n",
      "Epoch 5106/10000\n",
      "2700/2700 [==============================] - 0s 35us/step - loss: 1.3986e-04 - mean_absolute_error: 0.0070\n",
      "\n",
      "Epoch 05106: loss did not improve\n",
      "Epoch 5107/10000\n",
      "2700/2700 [==============================] - 0s 34us/step - loss: 1.4100e-04 - mean_absolute_error: 0.0071\n",
      "\n",
      "Epoch 05107: loss did not improve\n",
      "Epoch 5108/10000\n",
      "2700/2700 [==============================] - 0s 32us/step - loss: 1.4116e-04 - mean_absolute_error: 0.0070\n",
      "\n",
      "Epoch 05108: loss did not improve\n",
      "Epoch 5109/10000\n",
      "2700/2700 [==============================] - 0s 34us/step - loss: 1.3914e-04 - mean_absolute_error: 0.0069\n",
      "\n",
      "Epoch 05109: loss did not improve\n",
      "Epoch 5110/10000\n",
      "2700/2700 [==============================] - 0s 26us/step - loss: 1.3836e-04 - mean_absolute_error: 0.0068\n",
      "\n",
      "Epoch 05110: loss did not improve\n",
      "Epoch 5111/10000\n",
      "2700/2700 [==============================] - 0s 33us/step - loss: 1.3915e-04 - mean_absolute_error: 0.0069\n",
      "\n",
      "Epoch 05111: loss did not improve\n",
      "Epoch 5112/10000\n",
      "2700/2700 [==============================] - 0s 34us/step - loss: 1.3875e-04 - mean_absolute_error: 0.0069\n",
      "\n",
      "Epoch 05112: loss did not improve\n",
      "Epoch 5113/10000\n",
      "2700/2700 [==============================] - 0s 34us/step - loss: 1.3824e-04 - mean_absolute_error: 0.0068\n",
      "\n",
      "Epoch 05113: loss did not improve\n",
      "Epoch 5114/10000\n",
      "2700/2700 [==============================] - 0s 31us/step - loss: 1.4309e-04 - mean_absolute_error: 0.0071\n",
      "\n",
      "Epoch 05114: loss did not improve\n",
      "Epoch 5115/10000\n",
      "2700/2700 [==============================] - 0s 33us/step - loss: 1.3882e-04 - mean_absolute_error: 0.0068\n",
      "\n",
      "Epoch 05115: loss did not improve\n",
      "Epoch 5116/10000\n",
      "2700/2700 [==============================] - 0s 31us/step - loss: 1.3892e-04 - mean_absolute_error: 0.0068\n",
      "\n",
      "Epoch 05116: loss did not improve\n",
      "Epoch 5117/10000\n",
      "2700/2700 [==============================] - 0s 32us/step - loss: 1.4370e-04 - mean_absolute_error: 0.0072\n",
      "\n",
      "Epoch 05117: loss did not improve\n",
      "Epoch 5118/10000\n",
      "2700/2700 [==============================] - 0s 33us/step - loss: 1.3900e-04 - mean_absolute_error: 0.0069\n",
      "\n",
      "Epoch 05118: loss did not improve\n",
      "Epoch 5119/10000\n",
      "2700/2700 [==============================] - 0s 31us/step - loss: 1.3902e-04 - mean_absolute_error: 0.0070\n",
      "\n",
      "Epoch 05119: loss did not improve\n",
      "Epoch 5120/10000\n",
      "2700/2700 [==============================] - 0s 32us/step - loss: 1.4002e-04 - mean_absolute_error: 0.0069\n",
      "\n",
      "Epoch 05120: loss did not improve\n",
      "Epoch 5121/10000\n",
      "2700/2700 [==============================] - 0s 32us/step - loss: 1.3952e-04 - mean_absolute_error: 0.0069\n",
      "\n",
      "Epoch 05121: loss did not improve\n",
      "Epoch 5122/10000\n",
      "2700/2700 [==============================] - 0s 36us/step - loss: 1.3909e-04 - mean_absolute_error: 0.0069\n",
      "\n",
      "Epoch 05122: loss did not improve\n",
      "Epoch 5123/10000\n",
      "2700/2700 [==============================] - 0s 33us/step - loss: 1.3927e-04 - mean_absolute_error: 0.0070\n",
      "\n",
      "Epoch 05123: loss did not improve\n",
      "Epoch 5124/10000\n",
      "2700/2700 [==============================] - 0s 33us/step - loss: 1.3800e-04 - mean_absolute_error: 0.0068\n",
      "\n",
      "Epoch 05124: loss improved from 0.00014 to 0.00014, saving model to keras/best_baseline_weights.hdf5\n",
      "Epoch 5125/10000\n",
      "2700/2700 [==============================] - 0s 35us/step - loss: 1.4088e-04 - mean_absolute_error: 0.0070\n",
      "\n",
      "Epoch 05125: loss did not improve\n",
      "Epoch 5126/10000\n",
      "2700/2700 [==============================] - 0s 36us/step - loss: 1.4009e-04 - mean_absolute_error: 0.0071\n",
      "\n",
      "Epoch 05126: loss did not improve\n",
      "Epoch 5127/10000\n",
      "2700/2700 [==============================] - 0s 40us/step - loss: 1.3989e-04 - mean_absolute_error: 0.0070\n",
      "\n",
      "Epoch 05127: loss did not improve\n",
      "Epoch 5128/10000\n",
      "2700/2700 [==============================] - 0s 36us/step - loss: 1.3836e-04 - mean_absolute_error: 0.0069\n",
      "\n",
      "Epoch 05128: loss did not improve\n",
      "Epoch 5129/10000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2700/2700 [==============================] - 0s 30us/step - loss: 1.3684e-04 - mean_absolute_error: 0.0067\n",
      "\n",
      "Epoch 05129: loss improved from 0.00014 to 0.00014, saving model to keras/best_baseline_weights.hdf5\n",
      "Epoch 5130/10000\n",
      "2700/2700 [==============================] - 0s 29us/step - loss: 1.3917e-04 - mean_absolute_error: 0.0069\n",
      "\n",
      "Epoch 05130: loss did not improve\n",
      "Epoch 5131/10000\n",
      "2700/2700 [==============================] - 0s 27us/step - loss: 1.4142e-04 - mean_absolute_error: 0.0072\n",
      "\n",
      "Epoch 05131: loss did not improve\n",
      "Epoch 5132/10000\n",
      "2700/2700 [==============================] - 0s 30us/step - loss: 1.4003e-04 - mean_absolute_error: 0.0071\n",
      "\n",
      "Epoch 05132: loss did not improve\n",
      "Epoch 5133/10000\n",
      "2700/2700 [==============================] - 0s 31us/step - loss: 1.3750e-04 - mean_absolute_error: 0.0068\n",
      "\n",
      "Epoch 05133: loss did not improve\n",
      "Epoch 5134/10000\n",
      "2700/2700 [==============================] - 0s 32us/step - loss: 1.4336e-04 - mean_absolute_error: 0.0073\n",
      "\n",
      "Epoch 05134: loss did not improve\n",
      "Epoch 5135/10000\n",
      "2700/2700 [==============================] - 0s 27us/step - loss: 1.3793e-04 - mean_absolute_error: 0.0068\n",
      "\n",
      "Epoch 05135: loss did not improve\n",
      "Epoch 5136/10000\n",
      "2700/2700 [==============================] - 0s 31us/step - loss: 1.3966e-04 - mean_absolute_error: 0.0070\n",
      "\n",
      "Epoch 05136: loss did not improve\n",
      "Epoch 5137/10000\n",
      "2700/2700 [==============================] - 0s 29us/step - loss: 1.3737e-04 - mean_absolute_error: 0.0068\n",
      "\n",
      "Epoch 05137: loss did not improve\n",
      "Epoch 5138/10000\n",
      "2700/2700 [==============================] - 0s 28us/step - loss: 1.4569e-04 - mean_absolute_error: 0.0074\n",
      "\n",
      "Epoch 05138: loss did not improve\n",
      "Epoch 5139/10000\n",
      "2700/2700 [==============================] - 0s 28us/step - loss: 1.3854e-04 - mean_absolute_error: 0.0069\n",
      "\n",
      "Epoch 05139: loss did not improve\n",
      "Epoch 5140/10000\n",
      "2700/2700 [==============================] - 0s 27us/step - loss: 1.3818e-04 - mean_absolute_error: 0.0068\n",
      "\n",
      "Epoch 05140: loss did not improve\n",
      "Epoch 5141/10000\n",
      "2700/2700 [==============================] - 0s 30us/step - loss: 1.4011e-04 - mean_absolute_error: 0.0070\n",
      "\n",
      "Epoch 05141: loss did not improve\n",
      "Epoch 5142/10000\n",
      "2700/2700 [==============================] - 0s 29us/step - loss: 1.4202e-04 - mean_absolute_error: 0.0072\n",
      "\n",
      "Epoch 05142: loss did not improve\n",
      "Epoch 5143/10000\n",
      "2700/2700 [==============================] - 0s 29us/step - loss: 1.3895e-04 - mean_absolute_error: 0.0069\n",
      "\n",
      "Epoch 05143: loss did not improve\n",
      "Epoch 5144/10000\n",
      "2700/2700 [==============================] - 0s 38us/step - loss: 1.4108e-04 - mean_absolute_error: 0.0071\n",
      "\n",
      "Epoch 05144: loss did not improve\n",
      "Epoch 5145/10000\n",
      "2700/2700 [==============================] - 0s 34us/step - loss: 1.4326e-04 - mean_absolute_error: 0.0071\n",
      "\n",
      "Epoch 05145: loss did not improve\n",
      "Epoch 5146/10000\n",
      "2700/2700 [==============================] - 0s 38us/step - loss: 1.3829e-04 - mean_absolute_error: 0.0069\n",
      "\n",
      "Epoch 05146: loss did not improve\n",
      "Epoch 5147/10000\n",
      "2700/2700 [==============================] - 0s 28us/step - loss: 1.3655e-04 - mean_absolute_error: 0.0068\n",
      "\n",
      "Epoch 05147: loss improved from 0.00014 to 0.00014, saving model to keras/best_baseline_weights.hdf5\n",
      "Epoch 5148/10000\n",
      "2700/2700 [==============================] - 0s 27us/step - loss: 1.3936e-04 - mean_absolute_error: 0.0070\n",
      "\n",
      "Epoch 05148: loss did not improve\n",
      "Epoch 5149/10000\n",
      "2700/2700 [==============================] - 0s 31us/step - loss: 1.3782e-04 - mean_absolute_error: 0.0069\n",
      "\n",
      "Epoch 05149: loss did not improve\n",
      "Epoch 5150/10000\n",
      "2700/2700 [==============================] - 0s 46us/step - loss: 1.3810e-04 - mean_absolute_error: 0.0069\n",
      "\n",
      "Epoch 05150: loss did not improve\n",
      "Epoch 5151/10000\n",
      "2700/2700 [==============================] - 0s 32us/step - loss: 1.3898e-04 - mean_absolute_error: 0.0069\n",
      "\n",
      "Epoch 05151: loss did not improve\n",
      "Epoch 5152/10000\n",
      "2700/2700 [==============================] - 0s 30us/step - loss: 1.4087e-04 - mean_absolute_error: 0.0071\n",
      "\n",
      "Epoch 05152: loss did not improve\n",
      "Epoch 5153/10000\n",
      "2700/2700 [==============================] - 0s 29us/step - loss: 1.3938e-04 - mean_absolute_error: 0.0070\n",
      "\n",
      "Epoch 05153: loss did not improve\n",
      "Epoch 5154/10000\n",
      "2700/2700 [==============================] - 0s 31us/step - loss: 1.3762e-04 - mean_absolute_error: 0.0069\n",
      "\n",
      "Epoch 05154: loss did not improve\n",
      "Epoch 5155/10000\n",
      "2700/2700 [==============================] - 0s 26us/step - loss: 1.3923e-04 - mean_absolute_error: 0.0070\n",
      "\n",
      "Epoch 05155: loss did not improve\n",
      "Epoch 5156/10000\n",
      "2700/2700 [==============================] - 0s 29us/step - loss: 1.4022e-04 - mean_absolute_error: 0.0070\n",
      "\n",
      "Epoch 05156: loss did not improve\n",
      "Epoch 5157/10000\n",
      "2700/2700 [==============================] - 0s 30us/step - loss: 1.3856e-04 - mean_absolute_error: 0.0068\n",
      "\n",
      "Epoch 05157: loss did not improve\n",
      "Epoch 5158/10000\n",
      "2700/2700 [==============================] - 0s 29us/step - loss: 1.4045e-04 - mean_absolute_error: 0.0071\n",
      "\n",
      "Epoch 05158: loss did not improve\n",
      "Epoch 5159/10000\n",
      "2700/2700 [==============================] - 0s 29us/step - loss: 1.3988e-04 - mean_absolute_error: 0.0070\n",
      "\n",
      "Epoch 05159: loss did not improve\n",
      "Epoch 5160/10000\n",
      "2700/2700 [==============================] - 0s 28us/step - loss: 1.3812e-04 - mean_absolute_error: 0.0069\n",
      "\n",
      "Epoch 05160: loss did not improve\n",
      "Epoch 5161/10000\n",
      "2700/2700 [==============================] - 0s 31us/step - loss: 1.3879e-04 - mean_absolute_error: 0.0069\n",
      "\n",
      "Epoch 05161: loss did not improve\n",
      "Epoch 5162/10000\n",
      "2700/2700 [==============================] - 0s 29us/step - loss: 1.3933e-04 - mean_absolute_error: 0.0069\n",
      "\n",
      "Epoch 05162: loss did not improve\n",
      "Epoch 5163/10000\n",
      "2700/2700 [==============================] - 0s 28us/step - loss: 1.4003e-04 - mean_absolute_error: 0.0069\n",
      "\n",
      "Epoch 05163: loss did not improve\n",
      "Epoch 5164/10000\n",
      "2700/2700 [==============================] - 0s 29us/step - loss: 1.3767e-04 - mean_absolute_error: 0.0068\n",
      "\n",
      "Epoch 05164: loss did not improve\n",
      "Epoch 5165/10000\n",
      "2700/2700 [==============================] - 0s 30us/step - loss: 1.4156e-04 - mean_absolute_error: 0.0070\n",
      "\n",
      "Epoch 05165: loss did not improve\n",
      "Epoch 5166/10000\n",
      "2700/2700 [==============================] - 0s 31us/step - loss: 1.3981e-04 - mean_absolute_error: 0.0071\n",
      "\n",
      "Epoch 05166: loss did not improve\n",
      "Epoch 5167/10000\n",
      "2700/2700 [==============================] - 0s 29us/step - loss: 1.3941e-04 - mean_absolute_error: 0.0070\n",
      "\n",
      "Epoch 05167: loss did not improve\n",
      "Epoch 5168/10000\n",
      "2700/2700 [==============================] - 0s 30us/step - loss: 1.3709e-04 - mean_absolute_error: 0.0068\n",
      "\n",
      "Epoch 05168: loss did not improve\n",
      "Epoch 5169/10000\n",
      "2700/2700 [==============================] - 0s 26us/step - loss: 1.4230e-04 - mean_absolute_error: 0.0072\n",
      "\n",
      "Epoch 05169: loss did not improve\n",
      "Epoch 5170/10000\n",
      "2700/2700 [==============================] - 0s 29us/step - loss: 1.3836e-04 - mean_absolute_error: 0.0069\n",
      "\n",
      "Epoch 05170: loss did not improve\n",
      "Epoch 5171/10000\n",
      "2700/2700 [==============================] - 0s 28us/step - loss: 1.3770e-04 - mean_absolute_error: 0.0068\n",
      "\n",
      "Epoch 05171: loss did not improve\n",
      "Epoch 5172/10000\n",
      "2700/2700 [==============================] - 0s 31us/step - loss: 1.4113e-04 - mean_absolute_error: 0.0070\n",
      "\n",
      "Epoch 05172: loss did not improve\n",
      "Epoch 5173/10000\n",
      "2700/2700 [==============================] - 0s 31us/step - loss: 1.3712e-04 - mean_absolute_error: 0.0068\n",
      "\n",
      "Epoch 05173: loss did not improve\n",
      "Epoch 5174/10000\n",
      "2700/2700 [==============================] - 0s 30us/step - loss: 1.4036e-04 - mean_absolute_error: 0.0071\n",
      "\n",
      "Epoch 05174: loss did not improve\n",
      "Epoch 5175/10000\n",
      "2700/2700 [==============================] - 0s 31us/step - loss: 1.4218e-04 - mean_absolute_error: 0.0071\n",
      "\n",
      "Epoch 05175: loss did not improve\n",
      "Epoch 5176/10000\n",
      "2700/2700 [==============================] - 0s 31us/step - loss: 1.3719e-04 - mean_absolute_error: 0.0068\n",
      "\n",
      "Epoch 05176: loss did not improve\n",
      "Epoch 5177/10000\n",
      "2700/2700 [==============================] - 0s 37us/step - loss: 1.3821e-04 - mean_absolute_error: 0.0068\n",
      "\n",
      "Epoch 05177: loss did not improve\n",
      "Epoch 5178/10000\n",
      "2700/2700 [==============================] - 0s 34us/step - loss: 1.3683e-04 - mean_absolute_error: 0.0068\n",
      "\n",
      "Epoch 05178: loss did not improve\n",
      "Epoch 5179/10000\n",
      "2700/2700 [==============================] - 0s 30us/step - loss: 1.4567e-04 - mean_absolute_error: 0.0075\n",
      "\n",
      "Epoch 05179: loss did not improve\n",
      "Epoch 5180/10000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2700/2700 [==============================] - 0s 31us/step - loss: 1.4040e-04 - mean_absolute_error: 0.0070\n",
      "\n",
      "Epoch 05180: loss did not improve\n",
      "Epoch 5181/10000\n",
      "2700/2700 [==============================] - 0s 27us/step - loss: 1.3840e-04 - mean_absolute_error: 0.0068\n",
      "\n",
      "Epoch 05181: loss did not improve\n",
      "Epoch 5182/10000\n",
      "2700/2700 [==============================] - 0s 29us/step - loss: 1.3804e-04 - mean_absolute_error: 0.0069\n",
      "\n",
      "Epoch 05182: loss did not improve\n",
      "Epoch 5183/10000\n",
      "2700/2700 [==============================] - 0s 30us/step - loss: 1.3900e-04 - mean_absolute_error: 0.0070\n",
      "\n",
      "Epoch 05183: loss did not improve\n",
      "Epoch 5184/10000\n",
      "2700/2700 [==============================] - 0s 31us/step - loss: 1.4475e-04 - mean_absolute_error: 0.0074\n",
      "\n",
      "Epoch 05184: loss did not improve\n",
      "Epoch 5185/10000\n",
      "2700/2700 [==============================] - 0s 28us/step - loss: 1.3981e-04 - mean_absolute_error: 0.0071\n",
      "\n",
      "Epoch 05185: loss did not improve\n",
      "Epoch 5186/10000\n",
      "2700/2700 [==============================] - 0s 30us/step - loss: 1.3772e-04 - mean_absolute_error: 0.0068\n",
      "\n",
      "Epoch 05186: loss did not improve\n",
      "Epoch 5187/10000\n",
      "2700/2700 [==============================] - 0s 38us/step - loss: 1.3756e-04 - mean_absolute_error: 0.0069\n",
      "\n",
      "Epoch 05187: loss did not improve\n",
      "Epoch 5188/10000\n",
      "2700/2700 [==============================] - 0s 28us/step - loss: 1.3794e-04 - mean_absolute_error: 0.0069\n",
      "\n",
      "Epoch 05188: loss did not improve\n",
      "Epoch 5189/10000\n",
      "2700/2700 [==============================] - 0s 32us/step - loss: 1.4016e-04 - mean_absolute_error: 0.0070\n",
      "\n",
      "Epoch 05189: loss did not improve\n",
      "Epoch 5190/10000\n",
      "2700/2700 [==============================] - 0s 30us/step - loss: 1.3793e-04 - mean_absolute_error: 0.0070\n",
      "\n",
      "Epoch 05190: loss did not improve\n",
      "Epoch 5191/10000\n",
      "2700/2700 [==============================] - 0s 28us/step - loss: 1.4140e-04 - mean_absolute_error: 0.0071\n",
      "\n",
      "Epoch 05191: loss did not improve\n",
      "Epoch 5192/10000\n",
      "2700/2700 [==============================] - 0s 27us/step - loss: 1.3956e-04 - mean_absolute_error: 0.0070\n",
      "\n",
      "Epoch 05192: loss did not improve\n",
      "Epoch 5193/10000\n",
      "2700/2700 [==============================] - 0s 38us/step - loss: 1.3723e-04 - mean_absolute_error: 0.0068\n",
      "\n",
      "Epoch 05193: loss did not improve\n",
      "Epoch 5194/10000\n",
      "2700/2700 [==============================] - 0s 32us/step - loss: 1.3684e-04 - mean_absolute_error: 0.0068\n",
      "\n",
      "Epoch 05194: loss did not improve\n",
      "Epoch 5195/10000\n",
      "2700/2700 [==============================] - 0s 28us/step - loss: 1.3816e-04 - mean_absolute_error: 0.0070\n",
      "\n",
      "Epoch 05195: loss did not improve\n",
      "Epoch 5196/10000\n",
      "2700/2700 [==============================] - 0s 34us/step - loss: 1.3851e-04 - mean_absolute_error: 0.0069\n",
      "\n",
      "Epoch 05196: loss did not improve\n",
      "Epoch 5197/10000\n",
      "2700/2700 [==============================] - 0s 44us/step - loss: 1.3699e-04 - mean_absolute_error: 0.0067\n",
      "\n",
      "Epoch 05197: loss did not improve\n",
      "Epoch 5198/10000\n",
      "2700/2700 [==============================] - 0s 28us/step - loss: 1.4092e-04 - mean_absolute_error: 0.0071\n",
      "\n",
      "Epoch 05198: loss did not improve\n",
      "Epoch 5199/10000\n",
      "2700/2700 [==============================] - 0s 29us/step - loss: 1.4031e-04 - mean_absolute_error: 0.0070\n",
      "\n",
      "Epoch 05199: loss did not improve\n",
      "Epoch 5200/10000\n",
      "2700/2700 [==============================] - 0s 30us/step - loss: 1.3995e-04 - mean_absolute_error: 0.0071\n",
      "\n",
      "Epoch 05200: loss did not improve\n",
      "Epoch 5201/10000\n",
      "2700/2700 [==============================] - 0s 35us/step - loss: 1.3795e-04 - mean_absolute_error: 0.0069\n",
      "\n",
      "Epoch 05201: loss did not improve\n",
      "Epoch 5202/10000\n",
      "2700/2700 [==============================] - 0s 27us/step - loss: 1.3611e-04 - mean_absolute_error: 0.0067\n",
      "\n",
      "Epoch 05202: loss improved from 0.00014 to 0.00014, saving model to keras/best_baseline_weights.hdf5\n",
      "Epoch 5203/10000\n",
      "2700/2700 [==============================] - 0s 29us/step - loss: 1.3714e-04 - mean_absolute_error: 0.0067\n",
      "\n",
      "Epoch 05203: loss did not improve\n",
      "Epoch 5204/10000\n",
      "2700/2700 [==============================] - 0s 30us/step - loss: 1.3785e-04 - mean_absolute_error: 0.0069\n",
      "\n",
      "Epoch 05204: loss did not improve\n",
      "Epoch 5205/10000\n",
      "2700/2700 [==============================] - 0s 33us/step - loss: 1.4127e-04 - mean_absolute_error: 0.0070\n",
      "\n",
      "Epoch 05205: loss did not improve\n",
      "Epoch 5206/10000\n",
      "2700/2700 [==============================] - 0s 28us/step - loss: 1.3536e-04 - mean_absolute_error: 0.0068\n",
      "\n",
      "Epoch 05206: loss improved from 0.00014 to 0.00014, saving model to keras/best_baseline_weights.hdf5\n",
      "Epoch 5207/10000\n",
      "2700/2700 [==============================] - 0s 29us/step - loss: 1.3974e-04 - mean_absolute_error: 0.0071\n",
      "\n",
      "Epoch 05207: loss did not improve\n",
      "Epoch 5208/10000\n",
      "2700/2700 [==============================] - 0s 28us/step - loss: 1.4078e-04 - mean_absolute_error: 0.0072\n",
      "\n",
      "Epoch 05208: loss did not improve\n",
      "Epoch 5209/10000\n",
      "2700/2700 [==============================] - 0s 31us/step - loss: 1.3972e-04 - mean_absolute_error: 0.0071\n",
      "\n",
      "Epoch 05209: loss did not improve\n",
      "Epoch 5210/10000\n",
      "2700/2700 [==============================] - 0s 30us/step - loss: 1.3654e-04 - mean_absolute_error: 0.0068\n",
      "\n",
      "Epoch 05210: loss did not improve\n",
      "Epoch 5211/10000\n",
      "2700/2700 [==============================] - 0s 29us/step - loss: 1.3726e-04 - mean_absolute_error: 0.0068\n",
      "\n",
      "Epoch 05211: loss did not improve\n",
      "Epoch 5212/10000\n",
      "2700/2700 [==============================] - 0s 28us/step - loss: 1.3865e-04 - mean_absolute_error: 0.0070\n",
      "\n",
      "Epoch 05212: loss did not improve\n",
      "Epoch 5213/10000\n",
      "2700/2700 [==============================] - 0s 33us/step - loss: 1.3898e-04 - mean_absolute_error: 0.0071\n",
      "\n",
      "Epoch 05213: loss did not improve\n",
      "Epoch 5214/10000\n",
      "2700/2700 [==============================] - 0s 29us/step - loss: 1.3775e-04 - mean_absolute_error: 0.0069\n",
      "\n",
      "Epoch 05214: loss did not improve\n",
      "Epoch 5215/10000\n",
      "2700/2700 [==============================] - 0s 30us/step - loss: 1.3858e-04 - mean_absolute_error: 0.0069\n",
      "\n",
      "Epoch 05215: loss did not improve\n",
      "Epoch 5216/10000\n",
      "2700/2700 [==============================] - 0s 28us/step - loss: 1.4258e-04 - mean_absolute_error: 0.0073\n",
      "\n",
      "Epoch 05216: loss did not improve\n",
      "Epoch 5217/10000\n",
      "2700/2700 [==============================] - 0s 29us/step - loss: 1.4154e-04 - mean_absolute_error: 0.0072\n",
      "\n",
      "Epoch 05217: loss did not improve\n",
      "Epoch 5218/10000\n",
      "2700/2700 [==============================] - 0s 26us/step - loss: 1.3713e-04 - mean_absolute_error: 0.0069\n",
      "\n",
      "Epoch 05218: loss did not improve\n",
      "Epoch 5219/10000\n",
      "2700/2700 [==============================] - 0s 30us/step - loss: 1.4172e-04 - mean_absolute_error: 0.0073\n",
      "\n",
      "Epoch 05219: loss did not improve\n",
      "Epoch 5220/10000\n",
      "2700/2700 [==============================] - 0s 29us/step - loss: 1.3794e-04 - mean_absolute_error: 0.0068\n",
      "\n",
      "Epoch 05220: loss did not improve\n",
      "Epoch 5221/10000\n",
      "2700/2700 [==============================] - 0s 33us/step - loss: 1.3681e-04 - mean_absolute_error: 0.0068\n",
      "\n",
      "Epoch 05221: loss did not improve\n",
      "Epoch 5222/10000\n",
      "2700/2700 [==============================] - 0s 33us/step - loss: 1.3937e-04 - mean_absolute_error: 0.0071\n",
      "\n",
      "Epoch 05222: loss did not improve\n",
      "Epoch 5223/10000\n",
      "2700/2700 [==============================] - 0s 28us/step - loss: 1.3807e-04 - mean_absolute_error: 0.0069\n",
      "\n",
      "Epoch 05223: loss did not improve\n",
      "Epoch 5224/10000\n",
      "2700/2700 [==============================] - 0s 50us/step - loss: 1.3660e-04 - mean_absolute_error: 0.0067\n",
      "\n",
      "Epoch 05224: loss did not improve\n",
      "Epoch 5225/10000\n",
      "2700/2700 [==============================] - 0s 35us/step - loss: 1.3658e-04 - mean_absolute_error: 0.0067\n",
      "\n",
      "Epoch 05225: loss did not improve\n",
      "Epoch 5226/10000\n",
      "2700/2700 [==============================] - 0s 31us/step - loss: 1.4101e-04 - mean_absolute_error: 0.0072\n",
      "\n",
      "Epoch 05226: loss did not improve\n",
      "Epoch 5227/10000\n",
      "2700/2700 [==============================] - 0s 38us/step - loss: 1.3647e-04 - mean_absolute_error: 0.0068\n",
      "\n",
      "Epoch 05227: loss did not improve\n",
      "Epoch 5228/10000\n",
      "2700/2700 [==============================] - 0s 34us/step - loss: 1.3716e-04 - mean_absolute_error: 0.0070\n",
      "\n",
      "Epoch 05228: loss did not improve\n",
      "Epoch 5229/10000\n",
      "2700/2700 [==============================] - 0s 28us/step - loss: 1.3575e-04 - mean_absolute_error: 0.0067\n",
      "\n",
      "Epoch 05229: loss did not improve\n",
      "Epoch 5230/10000\n",
      "2700/2700 [==============================] - 0s 29us/step - loss: 1.3615e-04 - mean_absolute_error: 0.0068\n",
      "\n",
      "Epoch 05230: loss did not improve\n",
      "Epoch 5231/10000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2700/2700 [==============================] - 0s 30us/step - loss: 1.3707e-04 - mean_absolute_error: 0.0068\n",
      "\n",
      "Epoch 05231: loss did not improve\n",
      "Epoch 5232/10000\n",
      "2700/2700 [==============================] - 0s 28us/step - loss: 1.3796e-04 - mean_absolute_error: 0.0068\n",
      "\n",
      "Epoch 05232: loss did not improve\n",
      "Epoch 5233/10000\n",
      "2700/2700 [==============================] - 0s 28us/step - loss: 1.3813e-04 - mean_absolute_error: 0.0070\n",
      "\n",
      "Epoch 05233: loss did not improve\n",
      "Epoch 5234/10000\n",
      "2700/2700 [==============================] - 0s 31us/step - loss: 1.4100e-04 - mean_absolute_error: 0.0072\n",
      "\n",
      "Epoch 05234: loss did not improve\n",
      "Epoch 5235/10000\n",
      "2700/2700 [==============================] - 0s 29us/step - loss: 1.3964e-04 - mean_absolute_error: 0.0070\n",
      "\n",
      "Epoch 05235: loss did not improve\n",
      "Epoch 5236/10000\n",
      "2700/2700 [==============================] - 0s 29us/step - loss: 1.4003e-04 - mean_absolute_error: 0.0070\n",
      "\n",
      "Epoch 05236: loss did not improve\n",
      "Epoch 5237/10000\n",
      "2700/2700 [==============================] - 0s 28us/step - loss: 1.4003e-04 - mean_absolute_error: 0.0070\n",
      "\n",
      "Epoch 05237: loss did not improve\n",
      "Epoch 5238/10000\n",
      "2700/2700 [==============================] - 0s 27us/step - loss: 1.3634e-04 - mean_absolute_error: 0.0068\n",
      "\n",
      "Epoch 05238: loss did not improve\n",
      "Epoch 5239/10000\n",
      "2700/2700 [==============================] - 0s 29us/step - loss: 1.3546e-04 - mean_absolute_error: 0.0067\n",
      "\n",
      "Epoch 05239: loss did not improve\n",
      "Epoch 5240/10000\n",
      "2700/2700 [==============================] - 0s 28us/step - loss: 1.3832e-04 - mean_absolute_error: 0.0069\n",
      "\n",
      "Epoch 05240: loss did not improve\n",
      "Epoch 5241/10000\n",
      "2700/2700 [==============================] - 0s 27us/step - loss: 1.3814e-04 - mean_absolute_error: 0.0069\n",
      "\n",
      "Epoch 05241: loss did not improve\n",
      "Epoch 5242/10000\n",
      "2700/2700 [==============================] - 0s 26us/step - loss: 1.3833e-04 - mean_absolute_error: 0.0069\n",
      "\n",
      "Epoch 05242: loss did not improve\n",
      "Epoch 5243/10000\n",
      "2700/2700 [==============================] - 0s 28us/step - loss: 1.3830e-04 - mean_absolute_error: 0.0070: 0s - loss: 1.4438e-04 - mean_absolute_error: 0.007\n",
      "\n",
      "Epoch 05243: loss did not improve\n",
      "Epoch 5244/10000\n",
      "2700/2700 [==============================] - 0s 30us/step - loss: 1.4184e-04 - mean_absolute_error: 0.0071\n",
      "\n",
      "Epoch 05244: loss did not improve\n",
      "Epoch 5245/10000\n",
      "2700/2700 [==============================] - 0s 33us/step - loss: 1.4037e-04 - mean_absolute_error: 0.0071\n",
      "\n",
      "Epoch 05245: loss did not improve\n",
      "Epoch 5246/10000\n",
      "2700/2700 [==============================] - 0s 29us/step - loss: 1.3600e-04 - mean_absolute_error: 0.0068\n",
      "\n",
      "Epoch 05246: loss did not improve\n",
      "Epoch 5247/10000\n",
      "2700/2700 [==============================] - 0s 26us/step - loss: 1.3544e-04 - mean_absolute_error: 0.0067\n",
      "\n",
      "Epoch 05247: loss did not improve\n",
      "Epoch 5248/10000\n",
      "2700/2700 [==============================] - 0s 29us/step - loss: 1.3614e-04 - mean_absolute_error: 0.0067\n",
      "\n",
      "Epoch 05248: loss did not improve\n",
      "Epoch 5249/10000\n",
      "2700/2700 [==============================] - 0s 28us/step - loss: 1.3704e-04 - mean_absolute_error: 0.0068\n",
      "\n",
      "Epoch 05249: loss did not improve\n",
      "Epoch 5250/10000\n",
      "2700/2700 [==============================] - 0s 29us/step - loss: 1.3906e-04 - mean_absolute_error: 0.0069\n",
      "\n",
      "Epoch 05250: loss did not improve\n",
      "Epoch 5251/10000\n",
      "2700/2700 [==============================] - 0s 25us/step - loss: 1.3687e-04 - mean_absolute_error: 0.0069\n",
      "\n",
      "Epoch 05251: loss did not improve\n",
      "Epoch 5252/10000\n",
      "2700/2700 [==============================] - 0s 30us/step - loss: 1.3654e-04 - mean_absolute_error: 0.0068\n",
      "\n",
      "Epoch 05252: loss did not improve\n",
      "Epoch 5253/10000\n",
      "2700/2700 [==============================] - 0s 30us/step - loss: 1.4158e-04 - mean_absolute_error: 0.0073\n",
      "\n",
      "Epoch 05253: loss did not improve\n",
      "Epoch 5254/10000\n",
      "2700/2700 [==============================] - 0s 27us/step - loss: 1.3896e-04 - mean_absolute_error: 0.0069\n",
      "\n",
      "Epoch 05254: loss did not improve\n",
      "Epoch 5255/10000\n",
      "2700/2700 [==============================] - 0s 29us/step - loss: 1.3729e-04 - mean_absolute_error: 0.0069\n",
      "\n",
      "Epoch 05255: loss did not improve\n",
      "Epoch 5256/10000\n",
      "2700/2700 [==============================] - 0s 29us/step - loss: 1.4453e-04 - mean_absolute_error: 0.0072\n",
      "\n",
      "Epoch 05256: loss did not improve\n",
      "Epoch 5257/10000\n",
      "2700/2700 [==============================] - 0s 28us/step - loss: 1.3624e-04 - mean_absolute_error: 0.0069\n",
      "\n",
      "Epoch 05257: loss did not improve\n",
      "Epoch 5258/10000\n",
      "2700/2700 [==============================] - 0s 29us/step - loss: 1.3907e-04 - mean_absolute_error: 0.0070\n",
      "\n",
      "Epoch 05258: loss did not improve\n",
      "Epoch 5259/10000\n",
      "2700/2700 [==============================] - 0s 31us/step - loss: 1.3750e-04 - mean_absolute_error: 0.0068\n",
      "\n",
      "Epoch 05259: loss did not improve\n",
      "Epoch 5260/10000\n",
      "2700/2700 [==============================] - 0s 28us/step - loss: 1.3825e-04 - mean_absolute_error: 0.0069\n",
      "\n",
      "Epoch 05260: loss did not improve\n",
      "Epoch 5261/10000\n",
      "2700/2700 [==============================] - 0s 29us/step - loss: 1.3862e-04 - mean_absolute_error: 0.0070\n",
      "\n",
      "Epoch 05261: loss did not improve\n",
      "Epoch 5262/10000\n",
      "2700/2700 [==============================] - 0s 28us/step - loss: 1.3732e-04 - mean_absolute_error: 0.0068\n",
      "\n",
      "Epoch 05262: loss did not improve\n",
      "Epoch 5263/10000\n",
      "2700/2700 [==============================] - 0s 30us/step - loss: 1.3752e-04 - mean_absolute_error: 0.0068\n",
      "\n",
      "Epoch 05263: loss did not improve\n",
      "Epoch 5264/10000\n",
      "2700/2700 [==============================] - 0s 31us/step - loss: 1.3626e-04 - mean_absolute_error: 0.0068\n",
      "\n",
      "Epoch 05264: loss did not improve\n",
      "Epoch 5265/10000\n",
      "2700/2700 [==============================] - 0s 29us/step - loss: 1.3620e-04 - mean_absolute_error: 0.0068\n",
      "\n",
      "Epoch 05265: loss did not improve\n",
      "Epoch 5266/10000\n",
      "2700/2700 [==============================] - 0s 28us/step - loss: 1.3670e-04 - mean_absolute_error: 0.0069\n",
      "\n",
      "Epoch 05266: loss did not improve\n",
      "Epoch 5267/10000\n",
      "2700/2700 [==============================] - 0s 28us/step - loss: 1.3685e-04 - mean_absolute_error: 0.0068\n",
      "\n",
      "Epoch 05267: loss did not improve\n",
      "Epoch 5268/10000\n",
      "2700/2700 [==============================] - 0s 29us/step - loss: 1.3607e-04 - mean_absolute_error: 0.0067\n",
      "\n",
      "Epoch 05268: loss did not improve\n",
      "Epoch 5269/10000\n",
      "2700/2700 [==============================] - 0s 27us/step - loss: 1.3838e-04 - mean_absolute_error: 0.0070\n",
      "\n",
      "Epoch 05269: loss did not improve\n",
      "Epoch 5270/10000\n",
      "2700/2700 [==============================] - 0s 30us/step - loss: 1.3790e-04 - mean_absolute_error: 0.0071\n",
      "\n",
      "Epoch 05270: loss did not improve\n",
      "Epoch 5271/10000\n",
      "2700/2700 [==============================] - 0s 29us/step - loss: 1.4070e-04 - mean_absolute_error: 0.0071\n",
      "\n",
      "Epoch 05271: loss did not improve\n",
      "Epoch 5272/10000\n",
      "2700/2700 [==============================] - 0s 29us/step - loss: 1.4179e-04 - mean_absolute_error: 0.0072\n",
      "\n",
      "Epoch 05272: loss did not improve\n",
      "Epoch 5273/10000\n",
      "2700/2700 [==============================] - 0s 27us/step - loss: 1.3657e-04 - mean_absolute_error: 0.0068\n",
      "\n",
      "Epoch 05273: loss did not improve\n",
      "Epoch 5274/10000\n",
      "2700/2700 [==============================] - 0s 28us/step - loss: 1.3827e-04 - mean_absolute_error: 0.0070\n",
      "\n",
      "Epoch 05274: loss did not improve\n",
      "Epoch 5275/10000\n",
      "2700/2700 [==============================] - 0s 28us/step - loss: 1.3501e-04 - mean_absolute_error: 0.0066\n",
      "\n",
      "Epoch 05275: loss improved from 0.00014 to 0.00014, saving model to keras/best_baseline_weights.hdf5\n",
      "Epoch 5276/10000\n",
      "2700/2700 [==============================] - 0s 30us/step - loss: 1.3717e-04 - mean_absolute_error: 0.0069\n",
      "\n",
      "Epoch 05276: loss did not improve\n",
      "Epoch 5277/10000\n",
      "2700/2700 [==============================] - 0s 27us/step - loss: 1.3965e-04 - mean_absolute_error: 0.0070\n",
      "\n",
      "Epoch 05277: loss did not improve\n",
      "Epoch 5278/10000\n",
      "2700/2700 [==============================] - 0s 30us/step - loss: 1.3595e-04 - mean_absolute_error: 0.0067\n",
      "\n",
      "Epoch 05278: loss did not improve\n",
      "Epoch 5279/10000\n",
      "2700/2700 [==============================] - 0s 31us/step - loss: 1.3860e-04 - mean_absolute_error: 0.0071\n",
      "\n",
      "Epoch 05279: loss did not improve\n",
      "Epoch 5280/10000\n",
      "2700/2700 [==============================] - 0s 26us/step - loss: 1.3892e-04 - mean_absolute_error: 0.0070\n",
      "\n",
      "Epoch 05280: loss did not improve\n",
      "Epoch 5281/10000\n",
      "2700/2700 [==============================] - 0s 29us/step - loss: 1.3534e-04 - mean_absolute_error: 0.0068\n",
      "\n",
      "Epoch 05281: loss did not improve\n",
      "Epoch 5282/10000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2700/2700 [==============================] - 0s 30us/step - loss: 1.3701e-04 - mean_absolute_error: 0.0068\n",
      "\n",
      "Epoch 05282: loss did not improve\n",
      "Epoch 5283/10000\n",
      "2700/2700 [==============================] - 0s 25us/step - loss: 1.3791e-04 - mean_absolute_error: 0.0071\n",
      "\n",
      "Epoch 05283: loss did not improve\n",
      "Epoch 5284/10000\n",
      "2700/2700 [==============================] - 0s 28us/step - loss: 1.3636e-04 - mean_absolute_error: 0.0068\n",
      "\n",
      "Epoch 05284: loss did not improve\n",
      "Epoch 5285/10000\n",
      "2700/2700 [==============================] - 0s 29us/step - loss: 1.3808e-04 - mean_absolute_error: 0.0069\n",
      "\n",
      "Epoch 05285: loss did not improve\n",
      "Epoch 5286/10000\n",
      "2700/2700 [==============================] - 0s 28us/step - loss: 1.3736e-04 - mean_absolute_error: 0.0069\n",
      "\n",
      "Epoch 05286: loss did not improve\n",
      "Epoch 5287/10000\n",
      "2700/2700 [==============================] - 0s 27us/step - loss: 1.4099e-04 - mean_absolute_error: 0.0071\n",
      "\n",
      "Epoch 05287: loss did not improve\n",
      "Epoch 5288/10000\n",
      "2700/2700 [==============================] - 0s 30us/step - loss: 1.3761e-04 - mean_absolute_error: 0.0068\n",
      "\n",
      "Epoch 05288: loss did not improve\n",
      "Epoch 5289/10000\n",
      "2700/2700 [==============================] - 0s 28us/step - loss: 1.3659e-04 - mean_absolute_error: 0.0068\n",
      "\n",
      "Epoch 05289: loss did not improve\n",
      "Epoch 5290/10000\n",
      "2700/2700 [==============================] - 0s 30us/step - loss: 1.3644e-04 - mean_absolute_error: 0.0068\n",
      "\n",
      "Epoch 05290: loss did not improve\n",
      "Epoch 5291/10000\n",
      "2700/2700 [==============================] - 0s 30us/step - loss: 1.3755e-04 - mean_absolute_error: 0.0069\n",
      "\n",
      "Epoch 05291: loss did not improve\n",
      "Epoch 5292/10000\n",
      "2700/2700 [==============================] - 0s 27us/step - loss: 1.4105e-04 - mean_absolute_error: 0.0072\n",
      "\n",
      "Epoch 05292: loss did not improve\n",
      "Epoch 5293/10000\n",
      "2700/2700 [==============================] - 0s 28us/step - loss: 1.3709e-04 - mean_absolute_error: 0.0068\n",
      "\n",
      "Epoch 05293: loss did not improve\n",
      "Epoch 5294/10000\n",
      "2700/2700 [==============================] - 0s 27us/step - loss: 1.4054e-04 - mean_absolute_error: 0.0070\n",
      "\n",
      "Epoch 05294: loss did not improve\n",
      "Epoch 5295/10000\n",
      "2700/2700 [==============================] - 0s 27us/step - loss: 1.3590e-04 - mean_absolute_error: 0.0068\n",
      "\n",
      "Epoch 05295: loss did not improve\n",
      "Epoch 5296/10000\n",
      "2700/2700 [==============================] - 0s 31us/step - loss: 1.3841e-04 - mean_absolute_error: 0.0069\n",
      "\n",
      "Epoch 05296: loss did not improve\n",
      "Epoch 5297/10000\n",
      "2700/2700 [==============================] - 0s 29us/step - loss: 1.3675e-04 - mean_absolute_error: 0.0068\n",
      "\n",
      "Epoch 05297: loss did not improve\n",
      "Epoch 5298/10000\n",
      "2700/2700 [==============================] - 0s 28us/step - loss: 1.3601e-04 - mean_absolute_error: 0.0068\n",
      "\n",
      "Epoch 05298: loss did not improve\n",
      "Epoch 5299/10000\n",
      "2700/2700 [==============================] - 0s 31us/step - loss: 1.3962e-04 - mean_absolute_error: 0.0071\n",
      "\n",
      "Epoch 05299: loss did not improve\n",
      "Epoch 5300/10000\n",
      "2700/2700 [==============================] - 0s 25us/step - loss: 1.3573e-04 - mean_absolute_error: 0.0068\n",
      "\n",
      "Epoch 05300: loss did not improve\n",
      "Epoch 5301/10000\n",
      "2700/2700 [==============================] - 0s 30us/step - loss: 1.3530e-04 - mean_absolute_error: 0.0068\n",
      "\n",
      "Epoch 05301: loss did not improve\n",
      "Epoch 5302/10000\n",
      "2700/2700 [==============================] - 0s 29us/step - loss: 1.3659e-04 - mean_absolute_error: 0.0070\n",
      "\n",
      "Epoch 05302: loss did not improve\n",
      "Epoch 5303/10000\n",
      "2700/2700 [==============================] - 0s 30us/step - loss: 1.3509e-04 - mean_absolute_error: 0.0067\n",
      "\n",
      "Epoch 05303: loss did not improve\n",
      "Epoch 5304/10000\n",
      "2700/2700 [==============================] - 0s 26us/step - loss: 1.3712e-04 - mean_absolute_error: 0.0068\n",
      "\n",
      "Epoch 05304: loss did not improve\n",
      "Epoch 5305/10000\n",
      "2700/2700 [==============================] - 0s 28us/step - loss: 1.3674e-04 - mean_absolute_error: 0.0069\n",
      "\n",
      "Epoch 05305: loss did not improve\n",
      "Epoch 5306/10000\n",
      "2700/2700 [==============================] - 0s 30us/step - loss: 1.3488e-04 - mean_absolute_error: 0.0067\n",
      "\n",
      "Epoch 05306: loss improved from 0.00014 to 0.00013, saving model to keras/best_baseline_weights.hdf5\n",
      "Epoch 5307/10000\n",
      "2700/2700 [==============================] - 0s 31us/step - loss: 1.3621e-04 - mean_absolute_error: 0.0069\n",
      "\n",
      "Epoch 05307: loss did not improve\n",
      "Epoch 5308/10000\n",
      "2700/2700 [==============================] - 0s 28us/step - loss: 1.3801e-04 - mean_absolute_error: 0.0070\n",
      "\n",
      "Epoch 05308: loss did not improve\n",
      "Epoch 5309/10000\n",
      "2700/2700 [==============================] - 0s 28us/step - loss: 1.3470e-04 - mean_absolute_error: 0.0068\n",
      "\n",
      "Epoch 05309: loss improved from 0.00013 to 0.00013, saving model to keras/best_baseline_weights.hdf5\n",
      "Epoch 5310/10000\n",
      "2700/2700 [==============================] - 0s 27us/step - loss: 1.3812e-04 - mean_absolute_error: 0.0070\n",
      "\n",
      "Epoch 05310: loss did not improve\n",
      "Epoch 5311/10000\n",
      "2700/2700 [==============================] - 0s 30us/step - loss: 1.3908e-04 - mean_absolute_error: 0.0070\n",
      "\n",
      "Epoch 05311: loss did not improve\n",
      "Epoch 5312/10000\n",
      "2700/2700 [==============================] - 0s 31us/step - loss: 1.3552e-04 - mean_absolute_error: 0.0069\n",
      "\n",
      "Epoch 05312: loss did not improve\n",
      "Epoch 5313/10000\n",
      "2700/2700 [==============================] - 0s 31us/step - loss: 1.3723e-04 - mean_absolute_error: 0.0069\n",
      "\n",
      "Epoch 05313: loss did not improve\n",
      "Epoch 5314/10000\n",
      "2700/2700 [==============================] - 0s 30us/step - loss: 1.3616e-04 - mean_absolute_error: 0.0068\n",
      "\n",
      "Epoch 05314: loss did not improve\n",
      "Epoch 5315/10000\n",
      "2700/2700 [==============================] - 0s 26us/step - loss: 1.3957e-04 - mean_absolute_error: 0.0071\n",
      "\n",
      "Epoch 05315: loss did not improve\n",
      "Epoch 5316/10000\n",
      "2700/2700 [==============================] - 0s 30us/step - loss: 1.3747e-04 - mean_absolute_error: 0.0069\n",
      "\n",
      "Epoch 05316: loss did not improve\n",
      "Epoch 5317/10000\n",
      "2700/2700 [==============================] - 0s 29us/step - loss: 1.3543e-04 - mean_absolute_error: 0.0068\n",
      "\n",
      "Epoch 05317: loss did not improve\n",
      "Epoch 5318/10000\n",
      "2700/2700 [==============================] - 0s 31us/step - loss: 1.3539e-04 - mean_absolute_error: 0.0068\n",
      "\n",
      "Epoch 05318: loss did not improve\n",
      "Epoch 5319/10000\n",
      "2700/2700 [==============================] - 0s 30us/step - loss: 1.3532e-04 - mean_absolute_error: 0.0067\n",
      "\n",
      "Epoch 05319: loss did not improve\n",
      "Epoch 5320/10000\n",
      "2700/2700 [==============================] - 0s 30us/step - loss: 1.4183e-04 - mean_absolute_error: 0.0072\n",
      "\n",
      "Epoch 05320: loss did not improve\n",
      "Epoch 5321/10000\n",
      "2700/2700 [==============================] - 0s 30us/step - loss: 1.3879e-04 - mean_absolute_error: 0.0070\n",
      "\n",
      "Epoch 05321: loss did not improve\n",
      "Epoch 5322/10000\n",
      "2700/2700 [==============================] - 0s 30us/step - loss: 1.3645e-04 - mean_absolute_error: 0.0069\n",
      "\n",
      "Epoch 05322: loss did not improve\n",
      "Epoch 5323/10000\n",
      "2700/2700 [==============================] - 0s 30us/step - loss: 1.3683e-04 - mean_absolute_error: 0.0069\n",
      "\n",
      "Epoch 05323: loss did not improve\n",
      "Epoch 5324/10000\n",
      "2700/2700 [==============================] - 0s 31us/step - loss: 1.4024e-04 - mean_absolute_error: 0.0071\n",
      "\n",
      "Epoch 05324: loss did not improve\n",
      "Epoch 5325/10000\n",
      "2700/2700 [==============================] - 0s 32us/step - loss: 1.3432e-04 - mean_absolute_error: 0.0067\n",
      "\n",
      "Epoch 05325: loss improved from 0.00013 to 0.00013, saving model to keras/best_baseline_weights.hdf5\n",
      "Epoch 5326/10000\n",
      "2700/2700 [==============================] - 0s 32us/step - loss: 1.3603e-04 - mean_absolute_error: 0.0068\n",
      "\n",
      "Epoch 05326: loss did not improve\n",
      "Epoch 5327/10000\n",
      "2700/2700 [==============================] - 0s 28us/step - loss: 1.3618e-04 - mean_absolute_error: 0.0068\n",
      "\n",
      "Epoch 05327: loss did not improve\n",
      "Epoch 5328/10000\n",
      "2700/2700 [==============================] - 0s 30us/step - loss: 1.3984e-04 - mean_absolute_error: 0.0071\n",
      "\n",
      "Epoch 05328: loss did not improve\n",
      "Epoch 5329/10000\n",
      "2700/2700 [==============================] - 0s 30us/step - loss: 1.3537e-04 - mean_absolute_error: 0.0068\n",
      "\n",
      "Epoch 05329: loss did not improve\n",
      "Epoch 5330/10000\n",
      "2700/2700 [==============================] - 0s 29us/step - loss: 1.3668e-04 - mean_absolute_error: 0.0069\n",
      "\n",
      "Epoch 05330: loss did not improve\n",
      "Epoch 5331/10000\n",
      "2700/2700 [==============================] - 0s 29us/step - loss: 1.3533e-04 - mean_absolute_error: 0.0068\n",
      "\n",
      "Epoch 05331: loss did not improve\n",
      "Epoch 5332/10000\n",
      "2700/2700 [==============================] - 0s 28us/step - loss: 1.3440e-04 - mean_absolute_error: 0.0067\n",
      "\n",
      "Epoch 05332: loss did not improve\n",
      "Epoch 5333/10000\n",
      "2700/2700 [==============================] - 0s 29us/step - loss: 1.3640e-04 - mean_absolute_error: 0.0068\n",
      "\n",
      "Epoch 05333: loss did not improve\n",
      "Epoch 5334/10000\n",
      "2700/2700 [==============================] - 0s 32us/step - loss: 1.3928e-04 - mean_absolute_error: 0.0071\n",
      "\n",
      "Epoch 05334: loss did not improve\n",
      "Epoch 5335/10000\n",
      "2700/2700 [==============================] - 0s 31us/step - loss: 1.3597e-04 - mean_absolute_error: 0.0068\n",
      "\n",
      "Epoch 05335: loss did not improve\n",
      "Epoch 5336/10000\n",
      "2700/2700 [==============================] - 0s 28us/step - loss: 1.3572e-04 - mean_absolute_error: 0.0068\n",
      "\n",
      "Epoch 05336: loss did not improve\n",
      "Epoch 5337/10000\n",
      "2700/2700 [==============================] - 0s 27us/step - loss: 1.3599e-04 - mean_absolute_error: 0.0067\n",
      "\n",
      "Epoch 05337: loss did not improve\n",
      "Epoch 5338/10000\n",
      "2700/2700 [==============================] - 0s 29us/step - loss: 1.3704e-04 - mean_absolute_error: 0.0069\n",
      "\n",
      "Epoch 05338: loss did not improve\n",
      "Epoch 5339/10000\n",
      "2700/2700 [==============================] - 0s 30us/step - loss: 1.3511e-04 - mean_absolute_error: 0.0067\n",
      "\n",
      "Epoch 05339: loss did not improve\n",
      "Epoch 5340/10000\n",
      "2700/2700 [==============================] - 0s 27us/step - loss: 1.3483e-04 - mean_absolute_error: 0.0067\n",
      "\n",
      "Epoch 05340: loss did not improve\n",
      "Epoch 5341/10000\n",
      "2700/2700 [==============================] - 0s 28us/step - loss: 1.3454e-04 - mean_absolute_error: 0.0068\n",
      "\n",
      "Epoch 05341: loss did not improve\n",
      "Epoch 5342/10000\n",
      "2700/2700 [==============================] - 0s 30us/step - loss: 1.3622e-04 - mean_absolute_error: 0.0068\n",
      "\n",
      "Epoch 05342: loss did not improve\n",
      "Epoch 5343/10000\n",
      "2700/2700 [==============================] - 0s 28us/step - loss: 1.3657e-04 - mean_absolute_error: 0.0068\n",
      "\n",
      "Epoch 05343: loss did not improve\n",
      "Epoch 5344/10000\n",
      "2700/2700 [==============================] - 0s 28us/step - loss: 1.3402e-04 - mean_absolute_error: 0.0067\n",
      "\n",
      "Epoch 05344: loss improved from 0.00013 to 0.00013, saving model to keras/best_baseline_weights.hdf5\n",
      "Epoch 5345/10000\n",
      "2700/2700 [==============================] - 0s 29us/step - loss: 1.3755e-04 - mean_absolute_error: 0.0069\n",
      "\n",
      "Epoch 05345: loss did not improve\n",
      "Epoch 5346/10000\n",
      "2700/2700 [==============================] - 0s 25us/step - loss: 1.4086e-04 - mean_absolute_error: 0.0072\n",
      "\n",
      "Epoch 05346: loss did not improve\n",
      "Epoch 5347/10000\n",
      "2700/2700 [==============================] - 0s 28us/step - loss: 1.3572e-04 - mean_absolute_error: 0.0068\n",
      "\n",
      "Epoch 05347: loss did not improve\n",
      "Epoch 5348/10000\n",
      "2700/2700 [==============================] - 0s 30us/step - loss: 1.3574e-04 - mean_absolute_error: 0.0068\n",
      "\n",
      "Epoch 05348: loss did not improve\n",
      "Epoch 5349/10000\n",
      "2700/2700 [==============================] - 0s 30us/step - loss: 1.3553e-04 - mean_absolute_error: 0.0069\n",
      "\n",
      "Epoch 05349: loss did not improve\n",
      "Epoch 5350/10000\n",
      "2700/2700 [==============================] - 0s 26us/step - loss: 1.3921e-04 - mean_absolute_error: 0.0071\n",
      "\n",
      "Epoch 05350: loss did not improve\n",
      "Epoch 5351/10000\n",
      "2700/2700 [==============================] - 0s 29us/step - loss: 1.3816e-04 - mean_absolute_error: 0.0071\n",
      "\n",
      "Epoch 05351: loss did not improve\n",
      "Epoch 5352/10000\n",
      "2700/2700 [==============================] - 0s 27us/step - loss: 1.3483e-04 - mean_absolute_error: 0.0067\n",
      "\n",
      "Epoch 05352: loss did not improve\n",
      "Epoch 5353/10000\n",
      "2700/2700 [==============================] - 0s 28us/step - loss: 1.3446e-04 - mean_absolute_error: 0.0066\n",
      "\n",
      "Epoch 05353: loss did not improve\n",
      "Epoch 5354/10000\n",
      "2700/2700 [==============================] - 0s 29us/step - loss: 1.3548e-04 - mean_absolute_error: 0.0068\n",
      "\n",
      "Epoch 05354: loss did not improve\n",
      "Epoch 5355/10000\n",
      "2700/2700 [==============================] - 0s 27us/step - loss: 1.3676e-04 - mean_absolute_error: 0.0069\n",
      "\n",
      "Epoch 05355: loss did not improve\n",
      "Epoch 5356/10000\n",
      "2700/2700 [==============================] - 0s 28us/step - loss: 1.3563e-04 - mean_absolute_error: 0.0068\n",
      "\n",
      "Epoch 05356: loss did not improve\n",
      "Epoch 5357/10000\n",
      "2700/2700 [==============================] - 0s 28us/step - loss: 1.3330e-04 - mean_absolute_error: 0.0066\n",
      "\n",
      "Epoch 05357: loss improved from 0.00013 to 0.00013, saving model to keras/best_baseline_weights.hdf5\n",
      "Epoch 5358/10000\n",
      "2700/2700 [==============================] - 0s 30us/step - loss: 1.3599e-04 - mean_absolute_error: 0.0069\n",
      "\n",
      "Epoch 05358: loss did not improve\n",
      "Epoch 5359/10000\n",
      "2700/2700 [==============================] - 0s 28us/step - loss: 1.3557e-04 - mean_absolute_error: 0.0068\n",
      "\n",
      "Epoch 05359: loss did not improve\n",
      "Epoch 5360/10000\n",
      "2700/2700 [==============================] - 0s 27us/step - loss: 1.3438e-04 - mean_absolute_error: 0.0067\n",
      "\n",
      "Epoch 05360: loss did not improve\n",
      "Epoch 5361/10000\n",
      "2700/2700 [==============================] - 0s 29us/step - loss: 1.3506e-04 - mean_absolute_error: 0.0067\n",
      "\n",
      "Epoch 05361: loss did not improve\n",
      "Epoch 5362/10000\n",
      "2700/2700 [==============================] - 0s 30us/step - loss: 1.3850e-04 - mean_absolute_error: 0.0070\n",
      "\n",
      "Epoch 05362: loss did not improve\n",
      "Epoch 5363/10000\n",
      "2700/2700 [==============================] - 0s 28us/step - loss: 1.3712e-04 - mean_absolute_error: 0.0069\n",
      "\n",
      "Epoch 05363: loss did not improve\n",
      "Epoch 5364/10000\n",
      "2700/2700 [==============================] - 0s 27us/step - loss: 1.3756e-04 - mean_absolute_error: 0.0070\n",
      "\n",
      "Epoch 05364: loss did not improve\n",
      "Epoch 5365/10000\n",
      "2700/2700 [==============================] - 0s 31us/step - loss: 1.3565e-04 - mean_absolute_error: 0.0068\n",
      "\n",
      "Epoch 05365: loss did not improve\n",
      "Epoch 5366/10000\n",
      "2700/2700 [==============================] - 0s 32us/step - loss: 1.3541e-04 - mean_absolute_error: 0.0069\n",
      "\n",
      "Epoch 05366: loss did not improve\n",
      "Epoch 5367/10000\n",
      "2700/2700 [==============================] - 0s 31us/step - loss: 1.3726e-04 - mean_absolute_error: 0.0069\n",
      "\n",
      "Epoch 05367: loss did not improve\n",
      "Epoch 5368/10000\n",
      "2700/2700 [==============================] - 0s 31us/step - loss: 1.3406e-04 - mean_absolute_error: 0.0067\n",
      "\n",
      "Epoch 05368: loss did not improve\n",
      "Epoch 5369/10000\n",
      "2700/2700 [==============================] - 0s 31us/step - loss: 1.4097e-04 - mean_absolute_error: 0.0071\n",
      "\n",
      "Epoch 05369: loss did not improve\n",
      "Epoch 5370/10000\n",
      "2700/2700 [==============================] - 0s 37us/step - loss: 1.3520e-04 - mean_absolute_error: 0.0068\n",
      "\n",
      "Epoch 05370: loss did not improve\n",
      "Epoch 5371/10000\n",
      "2700/2700 [==============================] - 0s 36us/step - loss: 1.3410e-04 - mean_absolute_error: 0.0068\n",
      "\n",
      "Epoch 05371: loss did not improve\n",
      "Epoch 5372/10000\n",
      "2700/2700 [==============================] - 0s 30us/step - loss: 1.3537e-04 - mean_absolute_error: 0.0068\n",
      "\n",
      "Epoch 05372: loss did not improve\n",
      "Epoch 5373/10000\n",
      "2700/2700 [==============================] - 0s 31us/step - loss: 1.3720e-04 - mean_absolute_error: 0.0069\n",
      "\n",
      "Epoch 05373: loss did not improve\n",
      "Epoch 5374/10000\n",
      "2700/2700 [==============================] - 0s 32us/step - loss: 1.3835e-04 - mean_absolute_error: 0.0070\n",
      "\n",
      "Epoch 05374: loss did not improve\n",
      "Epoch 5375/10000\n",
      "2700/2700 [==============================] - 0s 30us/step - loss: 1.3272e-04 - mean_absolute_error: 0.0066\n",
      "\n",
      "Epoch 05375: loss improved from 0.00013 to 0.00013, saving model to keras/best_baseline_weights.hdf5\n",
      "Epoch 5376/10000\n",
      "2700/2700 [==============================] - 0s 35us/step - loss: 1.3730e-04 - mean_absolute_error: 0.0070\n",
      "\n",
      "Epoch 05376: loss did not improve\n",
      "Epoch 5377/10000\n",
      "2700/2700 [==============================] - 0s 29us/step - loss: 1.3737e-04 - mean_absolute_error: 0.0070\n",
      "\n",
      "Epoch 05377: loss did not improve\n",
      "Epoch 5378/10000\n",
      "2700/2700 [==============================] - 0s 29us/step - loss: 1.3325e-04 - mean_absolute_error: 0.0066\n",
      "\n",
      "Epoch 05378: loss did not improve\n",
      "Epoch 5379/10000\n",
      "2700/2700 [==============================] - 0s 28us/step - loss: 1.3412e-04 - mean_absolute_error: 0.0067\n",
      "\n",
      "Epoch 05379: loss did not improve\n",
      "Epoch 5380/10000\n",
      "2700/2700 [==============================] - 0s 31us/step - loss: 1.3481e-04 - mean_absolute_error: 0.0066\n",
      "\n",
      "Epoch 05380: loss did not improve\n",
      "Epoch 5381/10000\n",
      "2700/2700 [==============================] - 0s 30us/step - loss: 1.3489e-04 - mean_absolute_error: 0.0069\n",
      "\n",
      "Epoch 05381: loss did not improve\n",
      "Epoch 5382/10000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2700/2700 [==============================] - 0s 32us/step - loss: 1.3407e-04 - mean_absolute_error: 0.0066\n",
      "\n",
      "Epoch 05382: loss did not improve\n",
      "Epoch 5383/10000\n",
      "2700/2700 [==============================] - 0s 32us/step - loss: 1.3571e-04 - mean_absolute_error: 0.0069\n",
      "\n",
      "Epoch 05383: loss did not improve\n",
      "Epoch 5384/10000\n",
      "2700/2700 [==============================] - 0s 31us/step - loss: 1.3291e-04 - mean_absolute_error: 0.0066\n",
      "\n",
      "Epoch 05384: loss did not improve\n",
      "Epoch 5385/10000\n",
      "2700/2700 [==============================] - 0s 28us/step - loss: 1.3871e-04 - mean_absolute_error: 0.0070\n",
      "\n",
      "Epoch 05385: loss did not improve\n",
      "Epoch 5386/10000\n",
      "2700/2700 [==============================] - 0s 31us/step - loss: 1.3681e-04 - mean_absolute_error: 0.0070\n",
      "\n",
      "Epoch 05386: loss did not improve\n",
      "Epoch 5387/10000\n",
      "2700/2700 [==============================] - 0s 30us/step - loss: 1.3492e-04 - mean_absolute_error: 0.0068\n",
      "\n",
      "Epoch 05387: loss did not improve\n",
      "Epoch 5388/10000\n",
      "2700/2700 [==============================] - 0s 30us/step - loss: 1.3569e-04 - mean_absolute_error: 0.0068\n",
      "\n",
      "Epoch 05388: loss did not improve\n",
      "Epoch 5389/10000\n",
      "2700/2700 [==============================] - 0s 29us/step - loss: 1.3682e-04 - mean_absolute_error: 0.0069\n",
      "\n",
      "Epoch 05389: loss did not improve\n",
      "Epoch 5390/10000\n",
      "2700/2700 [==============================] - 0s 30us/step - loss: 1.3705e-04 - mean_absolute_error: 0.0070\n",
      "\n",
      "Epoch 05390: loss did not improve\n",
      "Epoch 5391/10000\n",
      "2700/2700 [==============================] - 0s 31us/step - loss: 1.3477e-04 - mean_absolute_error: 0.0067\n",
      "\n",
      "Epoch 05391: loss did not improve\n",
      "Epoch 5392/10000\n",
      "2700/2700 [==============================] - 0s 29us/step - loss: 1.3623e-04 - mean_absolute_error: 0.0068\n",
      "\n",
      "Epoch 05392: loss did not improve\n",
      "Epoch 5393/10000\n",
      "2700/2700 [==============================] - 0s 29us/step - loss: 1.3584e-04 - mean_absolute_error: 0.0068\n",
      "\n",
      "Epoch 05393: loss did not improve\n",
      "Epoch 5394/10000\n",
      "2700/2700 [==============================] - 0s 29us/step - loss: 1.3503e-04 - mean_absolute_error: 0.0067\n",
      "\n",
      "Epoch 05394: loss did not improve\n",
      "Epoch 5395/10000\n",
      "2700/2700 [==============================] - 0s 29us/step - loss: 1.3628e-04 - mean_absolute_error: 0.0068\n",
      "\n",
      "Epoch 05395: loss did not improve\n",
      "Epoch 5396/10000\n",
      "2700/2700 [==============================] - 0s 30us/step - loss: 1.3310e-04 - mean_absolute_error: 0.0066\n",
      "\n",
      "Epoch 05396: loss did not improve\n",
      "Epoch 5397/10000\n",
      "2700/2700 [==============================] - 0s 29us/step - loss: 1.3401e-04 - mean_absolute_error: 0.0066\n",
      "\n",
      "Epoch 05397: loss did not improve\n",
      "Epoch 5398/10000\n",
      "2700/2700 [==============================] - 0s 28us/step - loss: 1.3314e-04 - mean_absolute_error: 0.0066\n",
      "\n",
      "Epoch 05398: loss did not improve\n",
      "Epoch 5399/10000\n",
      "2700/2700 [==============================] - 0s 31us/step - loss: 1.3538e-04 - mean_absolute_error: 0.0068\n",
      "\n",
      "Epoch 05399: loss did not improve\n",
      "Epoch 5400/10000\n",
      "2700/2700 [==============================] - 0s 47us/step - loss: 1.3384e-04 - mean_absolute_error: 0.0066\n",
      "\n",
      "Epoch 05400: loss did not improve\n",
      "Epoch 5401/10000\n",
      "2700/2700 [==============================] - 0s 32us/step - loss: 1.3599e-04 - mean_absolute_error: 0.0068\n",
      "\n",
      "Epoch 05401: loss did not improve\n",
      "Epoch 5402/10000\n",
      "2700/2700 [==============================] - 0s 27us/step - loss: 1.3569e-04 - mean_absolute_error: 0.0068\n",
      "\n",
      "Epoch 05402: loss did not improve\n",
      "Epoch 5403/10000\n",
      "2700/2700 [==============================] - 0s 28us/step - loss: 1.3725e-04 - mean_absolute_error: 0.0069\n",
      "\n",
      "Epoch 05403: loss did not improve\n",
      "Epoch 5404/10000\n",
      "2700/2700 [==============================] - 0s 31us/step - loss: 1.3387e-04 - mean_absolute_error: 0.0067\n",
      "\n",
      "Epoch 05404: loss did not improve\n",
      "Epoch 5405/10000\n",
      "2700/2700 [==============================] - 0s 31us/step - loss: 1.3542e-04 - mean_absolute_error: 0.0068\n",
      "\n",
      "Epoch 05405: loss did not improve\n",
      "Epoch 5406/10000\n",
      "2700/2700 [==============================] - 0s 32us/step - loss: 1.3389e-04 - mean_absolute_error: 0.0067\n",
      "\n",
      "Epoch 05406: loss did not improve\n",
      "Epoch 5407/10000\n",
      "2700/2700 [==============================] - 0s 33us/step - loss: 1.3558e-04 - mean_absolute_error: 0.0068\n",
      "\n",
      "Epoch 05407: loss did not improve\n",
      "Epoch 5408/10000\n",
      "2700/2700 [==============================] - 0s 30us/step - loss: 1.3430e-04 - mean_absolute_error: 0.0067\n",
      "\n",
      "Epoch 05408: loss did not improve\n",
      "Epoch 5409/10000\n",
      "2700/2700 [==============================] - 0s 28us/step - loss: 1.3716e-04 - mean_absolute_error: 0.0069\n",
      "\n",
      "Epoch 05409: loss did not improve\n",
      "Epoch 5410/10000\n",
      "2700/2700 [==============================] - 0s 32us/step - loss: 1.3403e-04 - mean_absolute_error: 0.0066\n",
      "\n",
      "Epoch 05410: loss did not improve\n",
      "Epoch 5411/10000\n",
      "2700/2700 [==============================] - 0s 30us/step - loss: 1.3429e-04 - mean_absolute_error: 0.0067\n",
      "\n",
      "Epoch 05411: loss did not improve\n",
      "Epoch 5412/10000\n",
      "2700/2700 [==============================] - 0s 30us/step - loss: 1.3409e-04 - mean_absolute_error: 0.0067\n",
      "\n",
      "Epoch 05412: loss did not improve\n",
      "Epoch 5413/10000\n",
      "2700/2700 [==============================] - 0s 30us/step - loss: 1.3671e-04 - mean_absolute_error: 0.0068\n",
      "\n",
      "Epoch 05413: loss did not improve\n",
      "Epoch 5414/10000\n",
      "2700/2700 [==============================] - 0s 30us/step - loss: 1.3408e-04 - mean_absolute_error: 0.0067\n",
      "\n",
      "Epoch 05414: loss did not improve\n",
      "Epoch 5415/10000\n",
      "2700/2700 [==============================] - 0s 30us/step - loss: 1.3500e-04 - mean_absolute_error: 0.0069\n",
      "\n",
      "Epoch 05415: loss did not improve\n",
      "Epoch 5416/10000\n",
      "2700/2700 [==============================] - 0s 29us/step - loss: 1.3236e-04 - mean_absolute_error: 0.0066\n",
      "\n",
      "Epoch 05416: loss improved from 0.00013 to 0.00013, saving model to keras/best_baseline_weights.hdf5\n",
      "Epoch 5417/10000\n",
      "2700/2700 [==============================] - 0s 32us/step - loss: 1.3411e-04 - mean_absolute_error: 0.0067\n",
      "\n",
      "Epoch 05417: loss did not improve\n",
      "Epoch 5418/10000\n",
      "2700/2700 [==============================] - 0s 31us/step - loss: 1.3550e-04 - mean_absolute_error: 0.0068\n",
      "\n",
      "Epoch 05418: loss did not improve\n",
      "Epoch 5419/10000\n",
      "2700/2700 [==============================] - 0s 30us/step - loss: 1.3321e-04 - mean_absolute_error: 0.0067\n",
      "\n",
      "Epoch 05419: loss did not improve\n",
      "Epoch 5420/10000\n",
      "2700/2700 [==============================] - 0s 26us/step - loss: 1.3727e-04 - mean_absolute_error: 0.0069\n",
      "\n",
      "Epoch 05420: loss did not improve\n",
      "Epoch 5421/10000\n",
      "2700/2700 [==============================] - 0s 28us/step - loss: 1.3523e-04 - mean_absolute_error: 0.0068\n",
      "\n",
      "Epoch 05421: loss did not improve\n",
      "Epoch 5422/10000\n",
      "2700/2700 [==============================] - 0s 29us/step - loss: 1.3544e-04 - mean_absolute_error: 0.0068\n",
      "\n",
      "Epoch 05422: loss did not improve\n",
      "Epoch 5423/10000\n",
      "2700/2700 [==============================] - 0s 31us/step - loss: 1.4141e-04 - mean_absolute_error: 0.0073\n",
      "\n",
      "Epoch 05423: loss did not improve\n",
      "Epoch 5424/10000\n",
      "2700/2700 [==============================] - 0s 30us/step - loss: 1.3651e-04 - mean_absolute_error: 0.0070\n",
      "\n",
      "Epoch 05424: loss did not improve\n",
      "Epoch 5425/10000\n",
      "2700/2700 [==============================] - 0s 30us/step - loss: 1.3522e-04 - mean_absolute_error: 0.0068\n",
      "\n",
      "Epoch 05425: loss did not improve\n",
      "Epoch 5426/10000\n",
      "2700/2700 [==============================] - 0s 31us/step - loss: 1.3409e-04 - mean_absolute_error: 0.0067\n",
      "\n",
      "Epoch 05426: loss did not improve\n",
      "Epoch 5427/10000\n",
      "2700/2700 [==============================] - 0s 28us/step - loss: 1.3548e-04 - mean_absolute_error: 0.0068\n",
      "\n",
      "Epoch 05427: loss did not improve\n",
      "Epoch 5428/10000\n",
      "2700/2700 [==============================] - 0s 29us/step - loss: 1.3716e-04 - mean_absolute_error: 0.0070\n",
      "\n",
      "Epoch 05428: loss did not improve\n",
      "Epoch 5429/10000\n",
      "2700/2700 [==============================] - 0s 26us/step - loss: 1.3305e-04 - mean_absolute_error: 0.0065\n",
      "\n",
      "Epoch 05429: loss did not improve\n",
      "Epoch 5430/10000\n",
      "2700/2700 [==============================] - 0s 30us/step - loss: 1.3302e-04 - mean_absolute_error: 0.0066\n",
      "\n",
      "Epoch 05430: loss did not improve\n",
      "Epoch 5431/10000\n",
      "2700/2700 [==============================] - 0s 29us/step - loss: 1.3475e-04 - mean_absolute_error: 0.0067\n",
      "\n",
      "Epoch 05431: loss did not improve\n",
      "Epoch 5432/10000\n",
      "2700/2700 [==============================] - 0s 33us/step - loss: 1.3562e-04 - mean_absolute_error: 0.0068\n",
      "\n",
      "Epoch 05432: loss did not improve\n",
      "Epoch 5433/10000\n",
      "2700/2700 [==============================] - 0s 46us/step - loss: 1.3559e-04 - mean_absolute_error: 0.0069\n",
      "\n",
      "Epoch 05433: loss did not improve\n",
      "Epoch 5434/10000\n",
      "2700/2700 [==============================] - 0s 42us/step - loss: 1.3247e-04 - mean_absolute_error: 0.0067\n",
      "\n",
      "Epoch 05434: loss did not improve\n",
      "Epoch 5435/10000\n",
      "2700/2700 [==============================] - 0s 39us/step - loss: 1.3417e-04 - mean_absolute_error: 0.0067\n",
      "\n",
      "Epoch 05435: loss did not improve\n",
      "Epoch 5436/10000\n",
      "2700/2700 [==============================] - 0s 30us/step - loss: 1.3310e-04 - mean_absolute_error: 0.0066\n",
      "\n",
      "Epoch 05436: loss did not improve\n",
      "Epoch 5437/10000\n",
      "2700/2700 [==============================] - 0s 30us/step - loss: 1.3369e-04 - mean_absolute_error: 0.0067\n",
      "\n",
      "Epoch 05437: loss did not improve\n",
      "Epoch 5438/10000\n",
      "2700/2700 [==============================] - 0s 30us/step - loss: 1.3398e-04 - mean_absolute_error: 0.0067\n",
      "\n",
      "Epoch 05438: loss did not improve\n",
      "Epoch 5439/10000\n",
      "2700/2700 [==============================] - 0s 29us/step - loss: 1.3315e-04 - mean_absolute_error: 0.0068\n",
      "\n",
      "Epoch 05439: loss did not improve\n",
      "Epoch 5440/10000\n",
      "2700/2700 [==============================] - 0s 27us/step - loss: 1.3260e-04 - mean_absolute_error: 0.0066\n",
      "\n",
      "Epoch 05440: loss did not improve\n",
      "Epoch 5441/10000\n",
      "2700/2700 [==============================] - 0s 28us/step - loss: 1.3350e-04 - mean_absolute_error: 0.0067\n",
      "\n",
      "Epoch 05441: loss did not improve\n",
      "Epoch 5442/10000\n",
      "2700/2700 [==============================] - 0s 35us/step - loss: 1.3442e-04 - mean_absolute_error: 0.0069\n",
      "\n",
      "Epoch 05442: loss did not improve\n",
      "Epoch 5443/10000\n",
      "2700/2700 [==============================] - 0s 29us/step - loss: 1.3502e-04 - mean_absolute_error: 0.0068\n",
      "\n",
      "Epoch 05443: loss did not improve\n",
      "Epoch 5444/10000\n",
      "2700/2700 [==============================] - 0s 31us/step - loss: 1.3290e-04 - mean_absolute_error: 0.0066\n",
      "\n",
      "Epoch 05444: loss did not improve\n",
      "Epoch 5445/10000\n",
      "2700/2700 [==============================] - 0s 29us/step - loss: 1.3395e-04 - mean_absolute_error: 0.0067\n",
      "\n",
      "Epoch 05445: loss did not improve\n",
      "Epoch 5446/10000\n",
      "2700/2700 [==============================] - 0s 30us/step - loss: 1.3645e-04 - mean_absolute_error: 0.0069\n",
      "\n",
      "Epoch 05446: loss did not improve\n",
      "Epoch 5447/10000\n",
      "2700/2700 [==============================] - 0s 31us/step - loss: 1.3458e-04 - mean_absolute_error: 0.0069\n",
      "\n",
      "Epoch 05447: loss did not improve\n",
      "Epoch 5448/10000\n",
      "2700/2700 [==============================] - 0s 30us/step - loss: 1.3765e-04 - mean_absolute_error: 0.0071\n",
      "\n",
      "Epoch 05448: loss did not improve\n",
      "Epoch 5449/10000\n",
      "2700/2700 [==============================] - 0s 33us/step - loss: 1.3600e-04 - mean_absolute_error: 0.0069\n",
      "\n",
      "Epoch 05449: loss did not improve\n",
      "Epoch 5450/10000\n",
      "2700/2700 [==============================] - 0s 30us/step - loss: 1.3583e-04 - mean_absolute_error: 0.0068\n",
      "\n",
      "Epoch 05450: loss did not improve\n",
      "Epoch 5451/10000\n",
      "2700/2700 [==============================] - 0s 30us/step - loss: 1.3549e-04 - mean_absolute_error: 0.0069\n",
      "\n",
      "Epoch 05451: loss did not improve\n",
      "Epoch 5452/10000\n",
      "2700/2700 [==============================] - 0s 29us/step - loss: 1.3446e-04 - mean_absolute_error: 0.0068\n",
      "\n",
      "Epoch 05452: loss did not improve\n",
      "Epoch 5453/10000\n",
      "2700/2700 [==============================] - 0s 33us/step - loss: 1.3347e-04 - mean_absolute_error: 0.0068\n",
      "\n",
      "Epoch 05453: loss did not improve\n",
      "Epoch 5454/10000\n",
      "2700/2700 [==============================] - 0s 34us/step - loss: 1.3349e-04 - mean_absolute_error: 0.0067\n",
      "\n",
      "Epoch 05454: loss did not improve\n",
      "Epoch 5455/10000\n",
      "2700/2700 [==============================] - 0s 31us/step - loss: 1.3355e-04 - mean_absolute_error: 0.0068\n",
      "\n",
      "Epoch 05455: loss did not improve\n",
      "Epoch 5456/10000\n",
      "2700/2700 [==============================] - 0s 30us/step - loss: 1.3852e-04 - mean_absolute_error: 0.0071\n",
      "\n",
      "Epoch 05456: loss did not improve\n",
      "Epoch 5457/10000\n",
      "2700/2700 [==============================] - 0s 32us/step - loss: 1.3359e-04 - mean_absolute_error: 0.0068\n",
      "\n",
      "Epoch 05457: loss did not improve\n",
      "Epoch 5458/10000\n",
      "2700/2700 [==============================] - 0s 29us/step - loss: 1.3275e-04 - mean_absolute_error: 0.0066\n",
      "\n",
      "Epoch 05458: loss did not improve\n",
      "Epoch 5459/10000\n",
      "2700/2700 [==============================] - 0s 36us/step - loss: 1.3660e-04 - mean_absolute_error: 0.0071\n",
      "\n",
      "Epoch 05459: loss did not improve\n",
      "Epoch 5460/10000\n",
      "2700/2700 [==============================] - 0s 46us/step - loss: 1.3287e-04 - mean_absolute_error: 0.0067\n",
      "\n",
      "Epoch 05460: loss did not improve\n",
      "Epoch 5461/10000\n",
      "2700/2700 [==============================] - 0s 43us/step - loss: 1.3341e-04 - mean_absolute_error: 0.0067\n",
      "\n",
      "Epoch 05461: loss did not improve\n",
      "Epoch 5462/10000\n",
      "2700/2700 [==============================] - 0s 30us/step - loss: 1.3433e-04 - mean_absolute_error: 0.0067\n",
      "\n",
      "Epoch 05462: loss did not improve\n",
      "Epoch 5463/10000\n",
      "2700/2700 [==============================] - 0s 32us/step - loss: 1.3533e-04 - mean_absolute_error: 0.0068\n",
      "\n",
      "Epoch 05463: loss did not improve\n",
      "Epoch 5464/10000\n",
      "2700/2700 [==============================] - 0s 31us/step - loss: 1.3636e-04 - mean_absolute_error: 0.0068\n",
      "\n",
      "Epoch 05464: loss did not improve\n",
      "Epoch 5465/10000\n",
      "2700/2700 [==============================] - 0s 34us/step - loss: 1.3264e-04 - mean_absolute_error: 0.0067\n",
      "\n",
      "Epoch 05465: loss did not improve\n",
      "Epoch 5466/10000\n",
      "2700/2700 [==============================] - 0s 31us/step - loss: 1.3433e-04 - mean_absolute_error: 0.0067\n",
      "\n",
      "Epoch 05466: loss did not improve\n",
      "Epoch 5467/10000\n",
      "2700/2700 [==============================] - 0s 31us/step - loss: 1.3133e-04 - mean_absolute_error: 0.0065\n",
      "\n",
      "Epoch 05467: loss improved from 0.00013 to 0.00013, saving model to keras/best_baseline_weights.hdf5\n",
      "Epoch 5468/10000\n",
      "2700/2700 [==============================] - 0s 29us/step - loss: 1.3699e-04 - mean_absolute_error: 0.0070\n",
      "\n",
      "Epoch 05468: loss did not improve\n",
      "Epoch 5469/10000\n",
      "2700/2700 [==============================] - 0s 30us/step - loss: 1.3575e-04 - mean_absolute_error: 0.0068\n",
      "\n",
      "Epoch 05469: loss did not improve\n",
      "Epoch 5470/10000\n",
      "2700/2700 [==============================] - 0s 29us/step - loss: 1.3190e-04 - mean_absolute_error: 0.0066\n",
      "\n",
      "Epoch 05470: loss did not improve\n",
      "Epoch 5471/10000\n",
      "2700/2700 [==============================] - 0s 30us/step - loss: 1.3359e-04 - mean_absolute_error: 0.0068\n",
      "\n",
      "Epoch 05471: loss did not improve\n",
      "Epoch 5472/10000\n",
      "2700/2700 [==============================] - 0s 28us/step - loss: 1.3234e-04 - mean_absolute_error: 0.0066\n",
      "\n",
      "Epoch 05472: loss did not improve\n",
      "Epoch 5473/10000\n",
      "2700/2700 [==============================] - 0s 28us/step - loss: 1.3350e-04 - mean_absolute_error: 0.0066\n",
      "\n",
      "Epoch 05473: loss did not improve\n",
      "Epoch 5474/10000\n",
      "2700/2700 [==============================] - 0s 31us/step - loss: 1.3286e-04 - mean_absolute_error: 0.0066\n",
      "\n",
      "Epoch 05474: loss did not improve\n",
      "Epoch 5475/10000\n",
      "2700/2700 [==============================] - 0s 35us/step - loss: 1.3699e-04 - mean_absolute_error: 0.0070\n",
      "\n",
      "Epoch 05475: loss did not improve\n",
      "Epoch 5476/10000\n",
      "2700/2700 [==============================] - 0s 30us/step - loss: 1.3383e-04 - mean_absolute_error: 0.0066\n",
      "\n",
      "Epoch 05476: loss did not improve\n",
      "Epoch 5477/10000\n",
      "2700/2700 [==============================] - 0s 29us/step - loss: 1.3517e-04 - mean_absolute_error: 0.0067\n",
      "\n",
      "Epoch 05477: loss did not improve\n",
      "Epoch 5478/10000\n",
      "2700/2700 [==============================] - 0s 29us/step - loss: 1.3569e-04 - mean_absolute_error: 0.0070\n",
      "\n",
      "Epoch 05478: loss did not improve\n",
      "Epoch 5479/10000\n",
      "2700/2700 [==============================] - 0s 32us/step - loss: 1.3356e-04 - mean_absolute_error: 0.0066\n",
      "\n",
      "Epoch 05479: loss did not improve\n",
      "Epoch 5480/10000\n",
      "2700/2700 [==============================] - 0s 31us/step - loss: 1.3326e-04 - mean_absolute_error: 0.0067\n",
      "\n",
      "Epoch 05480: loss did not improve\n",
      "Epoch 5481/10000\n",
      "2700/2700 [==============================] - 0s 28us/step - loss: 1.3432e-04 - mean_absolute_error: 0.0069\n",
      "\n",
      "Epoch 05481: loss did not improve\n",
      "Epoch 5482/10000\n",
      "2700/2700 [==============================] - 0s 29us/step - loss: 1.3227e-04 - mean_absolute_error: 0.0066\n",
      "\n",
      "Epoch 05482: loss did not improve\n",
      "Epoch 5483/10000\n",
      "2700/2700 [==============================] - 0s 36us/step - loss: 1.3360e-04 - mean_absolute_error: 0.0067\n",
      "\n",
      "Epoch 05483: loss did not improve\n",
      "Epoch 5484/10000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2700/2700 [==============================] - 0s 33us/step - loss: 1.3901e-04 - mean_absolute_error: 0.0072\n",
      "\n",
      "Epoch 05484: loss did not improve\n",
      "Epoch 5485/10000\n",
      "2700/2700 [==============================] - 0s 31us/step - loss: 1.3510e-04 - mean_absolute_error: 0.0068\n",
      "\n",
      "Epoch 05485: loss did not improve\n",
      "Epoch 5486/10000\n",
      "2700/2700 [==============================] - 0s 28us/step - loss: 1.3406e-04 - mean_absolute_error: 0.0067\n",
      "\n",
      "Epoch 05486: loss did not improve\n",
      "Epoch 5487/10000\n",
      "2700/2700 [==============================] - 0s 32us/step - loss: 1.3366e-04 - mean_absolute_error: 0.0068\n",
      "\n",
      "Epoch 05487: loss did not improve\n",
      "Epoch 5488/10000\n",
      "2700/2700 [==============================] - 0s 30us/step - loss: 1.3281e-04 - mean_absolute_error: 0.0066\n",
      "\n",
      "Epoch 05488: loss did not improve\n",
      "Epoch 5489/10000\n",
      "2700/2700 [==============================] - 0s 32us/step - loss: 1.3416e-04 - mean_absolute_error: 0.0067\n",
      "\n",
      "Epoch 05489: loss did not improve\n",
      "Epoch 5490/10000\n",
      "2700/2700 [==============================] - 0s 30us/step - loss: 1.3429e-04 - mean_absolute_error: 0.0067\n",
      "\n",
      "Epoch 05490: loss did not improve\n",
      "Epoch 5491/10000\n",
      "2700/2700 [==============================] - 0s 32us/step - loss: 1.3901e-04 - mean_absolute_error: 0.0071\n",
      "\n",
      "Epoch 05491: loss did not improve\n",
      "Epoch 5492/10000\n",
      "2700/2700 [==============================] - 0s 31us/step - loss: 1.3475e-04 - mean_absolute_error: 0.0069\n",
      "\n",
      "Epoch 05492: loss did not improve\n",
      "Epoch 5493/10000\n",
      "2700/2700 [==============================] - 0s 29us/step - loss: 1.3304e-04 - mean_absolute_error: 0.0067\n",
      "\n",
      "Epoch 05493: loss did not improve\n",
      "Epoch 5494/10000\n",
      "2700/2700 [==============================] - 0s 27us/step - loss: 1.3477e-04 - mean_absolute_error: 0.0067\n",
      "\n",
      "Epoch 05494: loss did not improve\n",
      "Epoch 5495/10000\n",
      "2700/2700 [==============================] - 0s 32us/step - loss: 1.3722e-04 - mean_absolute_error: 0.0069\n",
      "\n",
      "Epoch 05495: loss did not improve\n",
      "Epoch 5496/10000\n",
      "2700/2700 [==============================] - 0s 29us/step - loss: 1.3375e-04 - mean_absolute_error: 0.0067\n",
      "\n",
      "Epoch 05496: loss did not improve\n",
      "Epoch 5497/10000\n",
      "2700/2700 [==============================] - 0s 34us/step - loss: 1.3281e-04 - mean_absolute_error: 0.0066\n",
      "\n",
      "Epoch 05497: loss did not improve\n",
      "Epoch 5498/10000\n",
      "2700/2700 [==============================] - 0s 29us/step - loss: 1.3590e-04 - mean_absolute_error: 0.0068\n",
      "\n",
      "Epoch 05498: loss did not improve\n",
      "Epoch 5499/10000\n",
      "2700/2700 [==============================] - 0s 27us/step - loss: 1.3285e-04 - mean_absolute_error: 0.0067\n",
      "\n",
      "Epoch 05499: loss did not improve\n",
      "Epoch 5500/10000\n",
      "2700/2700 [==============================] - 0s 36us/step - loss: 1.4175e-04 - mean_absolute_error: 0.0073\n",
      "\n",
      "Epoch 05500: loss did not improve\n",
      "Epoch 5501/10000\n",
      "2700/2700 [==============================] - 0s 33us/step - loss: 1.3924e-04 - mean_absolute_error: 0.0070\n",
      "\n",
      "Epoch 05501: loss did not improve\n",
      "Epoch 5502/10000\n",
      "2700/2700 [==============================] - 0s 32us/step - loss: 1.3364e-04 - mean_absolute_error: 0.0067\n",
      "\n",
      "Epoch 05502: loss did not improve\n",
      "Epoch 5503/10000\n",
      "2700/2700 [==============================] - 0s 31us/step - loss: 1.3254e-04 - mean_absolute_error: 0.0067\n",
      "\n",
      "Epoch 05503: loss did not improve\n",
      "Epoch 5504/10000\n",
      "2700/2700 [==============================] - 0s 31us/step - loss: 1.3437e-04 - mean_absolute_error: 0.0069\n",
      "\n",
      "Epoch 05504: loss did not improve\n",
      "Epoch 5505/10000\n",
      "2700/2700 [==============================] - 0s 30us/step - loss: 1.3759e-04 - mean_absolute_error: 0.0071\n",
      "\n",
      "Epoch 05505: loss did not improve\n",
      "Epoch 5506/10000\n",
      "2700/2700 [==============================] - 0s 31us/step - loss: 1.3512e-04 - mean_absolute_error: 0.0068\n",
      "\n",
      "Epoch 05506: loss did not improve\n",
      "Epoch 5507/10000\n",
      "2700/2700 [==============================] - 0s 33us/step - loss: 1.3355e-04 - mean_absolute_error: 0.0067\n",
      "\n",
      "Epoch 05507: loss did not improve\n",
      "Epoch 5508/10000\n",
      "2700/2700 [==============================] - 0s 34us/step - loss: 1.3300e-04 - mean_absolute_error: 0.0066\n",
      "\n",
      "Epoch 05508: loss did not improve\n",
      "Epoch 5509/10000\n",
      "2700/2700 [==============================] - 0s 31us/step - loss: 1.3364e-04 - mean_absolute_error: 0.0067\n",
      "\n",
      "Epoch 05509: loss did not improve\n",
      "Epoch 5510/10000\n",
      "2700/2700 [==============================] - 0s 31us/step - loss: 1.3335e-04 - mean_absolute_error: 0.0067\n",
      "\n",
      "Epoch 05510: loss did not improve\n",
      "Epoch 5511/10000\n",
      "2700/2700 [==============================] - 0s 31us/step - loss: 1.3273e-04 - mean_absolute_error: 0.0067\n",
      "\n",
      "Epoch 05511: loss did not improve\n",
      "Epoch 5512/10000\n",
      "2700/2700 [==============================] - 0s 30us/step - loss: 1.3488e-04 - mean_absolute_error: 0.0069\n",
      "\n",
      "Epoch 05512: loss did not improve\n",
      "Epoch 5513/10000\n",
      "2700/2700 [==============================] - 0s 35us/step - loss: 1.3328e-04 - mean_absolute_error: 0.0066\n",
      "\n",
      "Epoch 05513: loss did not improve\n",
      "Epoch 5514/10000\n",
      "2700/2700 [==============================] - 0s 29us/step - loss: 1.3265e-04 - mean_absolute_error: 0.0067\n",
      "\n",
      "Epoch 05514: loss did not improve\n",
      "Epoch 5515/10000\n",
      "2700/2700 [==============================] - 0s 32us/step - loss: 1.3467e-04 - mean_absolute_error: 0.0068\n",
      "\n",
      "Epoch 05515: loss did not improve\n",
      "Epoch 5516/10000\n",
      "2700/2700 [==============================] - 0s 32us/step - loss: 1.3352e-04 - mean_absolute_error: 0.0067\n",
      "\n",
      "Epoch 05516: loss did not improve\n",
      "Epoch 5517/10000\n",
      "2700/2700 [==============================] - 0s 36us/step - loss: 1.3344e-04 - mean_absolute_error: 0.0066\n",
      "\n",
      "Epoch 05517: loss did not improve\n",
      "Epoch 5518/10000\n",
      "2700/2700 [==============================] - 0s 35us/step - loss: 1.3287e-04 - mean_absolute_error: 0.0067\n",
      "\n",
      "Epoch 05518: loss did not improve\n",
      "Epoch 5519/10000\n",
      "2700/2700 [==============================] - 0s 32us/step - loss: 1.3366e-04 - mean_absolute_error: 0.0069\n",
      "\n",
      "Epoch 05519: loss did not improve\n",
      "Epoch 5520/10000\n",
      "2700/2700 [==============================] - 0s 30us/step - loss: 1.3357e-04 - mean_absolute_error: 0.0067\n",
      "\n",
      "Epoch 05520: loss did not improve\n",
      "Epoch 5521/10000\n",
      "2700/2700 [==============================] - 0s 27us/step - loss: 1.3249e-04 - mean_absolute_error: 0.0066\n",
      "\n",
      "Epoch 05521: loss did not improve\n",
      "Epoch 5522/10000\n",
      "2700/2700 [==============================] - 0s 30us/step - loss: 1.3448e-04 - mean_absolute_error: 0.0069\n",
      "\n",
      "Epoch 05522: loss did not improve\n",
      "Epoch 5523/10000\n",
      "2700/2700 [==============================] - 0s 30us/step - loss: 1.3494e-04 - mean_absolute_error: 0.0068\n",
      "\n",
      "Epoch 05523: loss did not improve\n",
      "Epoch 5524/10000\n",
      "2700/2700 [==============================] - 0s 31us/step - loss: 1.3382e-04 - mean_absolute_error: 0.0068\n",
      "\n",
      "Epoch 05524: loss did not improve\n",
      "Epoch 5525/10000\n",
      "2700/2700 [==============================] - 0s 30us/step - loss: 1.3508e-04 - mean_absolute_error: 0.0069\n",
      "\n",
      "Epoch 05525: loss did not improve\n",
      "Epoch 5526/10000\n",
      "2700/2700 [==============================] - 0s 29us/step - loss: 1.3279e-04 - mean_absolute_error: 0.0066\n",
      "\n",
      "Epoch 05526: loss did not improve\n",
      "Epoch 5527/10000\n",
      "2700/2700 [==============================] - 0s 31us/step - loss: 1.3482e-04 - mean_absolute_error: 0.0068\n",
      "\n",
      "Epoch 05527: loss did not improve\n",
      "Epoch 5528/10000\n",
      "2700/2700 [==============================] - 0s 31us/step - loss: 1.3458e-04 - mean_absolute_error: 0.0068\n",
      "\n",
      "Epoch 05528: loss did not improve\n",
      "Epoch 5529/10000\n",
      "2700/2700 [==============================] - 0s 29us/step - loss: 1.3519e-04 - mean_absolute_error: 0.0068\n",
      "\n",
      "Epoch 05529: loss did not improve\n",
      "Epoch 5530/10000\n",
      "2700/2700 [==============================] - 0s 29us/step - loss: 1.3690e-04 - mean_absolute_error: 0.0070\n",
      "\n",
      "Epoch 05530: loss did not improve\n",
      "Epoch 5531/10000\n",
      "2700/2700 [==============================] - 0s 32us/step - loss: 1.3370e-04 - mean_absolute_error: 0.0068\n",
      "\n",
      "Epoch 05531: loss did not improve\n",
      "Epoch 5532/10000\n",
      "2700/2700 [==============================] - 0s 28us/step - loss: 1.3082e-04 - mean_absolute_error: 0.0064\n",
      "\n",
      "Epoch 05532: loss improved from 0.00013 to 0.00013, saving model to keras/best_baseline_weights.hdf5\n",
      "Epoch 5533/10000\n",
      "2700/2700 [==============================] - 0s 31us/step - loss: 1.3212e-04 - mean_absolute_error: 0.0066\n",
      "\n",
      "Epoch 05533: loss did not improve\n",
      "Epoch 5534/10000\n",
      "2700/2700 [==============================] - 0s 30us/step - loss: 1.3182e-04 - mean_absolute_error: 0.0067\n",
      "\n",
      "Epoch 05534: loss did not improve\n",
      "Epoch 5535/10000\n",
      "2700/2700 [==============================] - 0s 42us/step - loss: 1.3304e-04 - mean_absolute_error: 0.0067\n",
      "\n",
      "Epoch 05535: loss did not improve\n",
      "Epoch 5536/10000\n",
      "2700/2700 [==============================] - 0s 49us/step - loss: 1.3736e-04 - mean_absolute_error: 0.0070\n",
      "\n",
      "Epoch 05536: loss did not improve\n",
      "Epoch 5537/10000\n",
      "2700/2700 [==============================] - 0s 37us/step - loss: 1.3471e-04 - mean_absolute_error: 0.0069\n",
      "\n",
      "Epoch 05537: loss did not improve\n",
      "Epoch 5538/10000\n",
      "2700/2700 [==============================] - 0s 35us/step - loss: 1.3406e-04 - mean_absolute_error: 0.0069\n",
      "\n",
      "Epoch 05538: loss did not improve\n",
      "Epoch 5539/10000\n",
      "2700/2700 [==============================] - 0s 25us/step - loss: 1.3229e-04 - mean_absolute_error: 0.0067\n",
      "\n",
      "Epoch 05539: loss did not improve\n",
      "Epoch 5540/10000\n",
      "2700/2700 [==============================] - 0s 29us/step - loss: 1.3520e-04 - mean_absolute_error: 0.0068\n",
      "\n",
      "Epoch 05540: loss did not improve\n",
      "Epoch 5541/10000\n",
      "2700/2700 [==============================] - 0s 32us/step - loss: 1.3223e-04 - mean_absolute_error: 0.0067\n",
      "\n",
      "Epoch 05541: loss did not improve\n",
      "Epoch 5542/10000\n",
      "2700/2700 [==============================] - 0s 33us/step - loss: 1.3266e-04 - mean_absolute_error: 0.0067\n",
      "\n",
      "Epoch 05542: loss did not improve\n",
      "Epoch 5543/10000\n",
      "2700/2700 [==============================] - 0s 42us/step - loss: 1.3152e-04 - mean_absolute_error: 0.0065\n",
      "\n",
      "Epoch 05543: loss did not improve\n",
      "Epoch 5544/10000\n",
      "2700/2700 [==============================] - 0s 41us/step - loss: 1.3529e-04 - mean_absolute_error: 0.0068\n",
      "\n",
      "Epoch 05544: loss did not improve\n",
      "Epoch 5545/10000\n",
      "2700/2700 [==============================] - 0s 34us/step - loss: 1.3371e-04 - mean_absolute_error: 0.0068\n",
      "\n",
      "Epoch 05545: loss did not improve\n",
      "Epoch 5546/10000\n",
      "2700/2700 [==============================] - 0s 42us/step - loss: 1.3164e-04 - mean_absolute_error: 0.0065\n",
      "\n",
      "Epoch 05546: loss did not improve\n",
      "Epoch 5547/10000\n",
      "2700/2700 [==============================] - 0s 73us/step - loss: 1.3173e-04 - mean_absolute_error: 0.0066\n",
      "\n",
      "Epoch 05547: loss did not improve\n",
      "Epoch 5548/10000\n",
      "2700/2700 [==============================] - 0s 56us/step - loss: 1.3295e-04 - mean_absolute_error: 0.0068: 0s - loss: 1.6399e-04 - mean_absolute_error: 0.00\n",
      "\n",
      "Epoch 05548: loss did not improve\n",
      "Epoch 5549/10000\n",
      "2700/2700 [==============================] - 0s 41us/step - loss: 1.3176e-04 - mean_absolute_error: 0.0066\n",
      "\n",
      "Epoch 05549: loss did not improve\n",
      "Epoch 5550/10000\n",
      "2700/2700 [==============================] - 0s 35us/step - loss: 1.3268e-04 - mean_absolute_error: 0.0065\n",
      "\n",
      "Epoch 05550: loss did not improve\n",
      "Epoch 5551/10000\n",
      "2700/2700 [==============================] - 0s 36us/step - loss: 1.3221e-04 - mean_absolute_error: 0.0066\n",
      "\n",
      "Epoch 05551: loss did not improve\n",
      "Epoch 5552/10000\n",
      "2700/2700 [==============================] - 0s 29us/step - loss: 1.3348e-04 - mean_absolute_error: 0.0069\n",
      "\n",
      "Epoch 05552: loss did not improve\n",
      "Epoch 5553/10000\n",
      "2700/2700 [==============================] - 0s 32us/step - loss: 1.3453e-04 - mean_absolute_error: 0.0069\n",
      "\n",
      "Epoch 05553: loss did not improve\n",
      "Epoch 5554/10000\n",
      "2700/2700 [==============================] - 0s 30us/step - loss: 1.3534e-04 - mean_absolute_error: 0.0069\n",
      "\n",
      "Epoch 05554: loss did not improve\n",
      "Epoch 5555/10000\n",
      "2700/2700 [==============================] - 0s 36us/step - loss: 1.3480e-04 - mean_absolute_error: 0.0069\n",
      "\n",
      "Epoch 05555: loss did not improve\n",
      "Epoch 5556/10000\n",
      "2700/2700 [==============================] - 0s 48us/step - loss: 1.3273e-04 - mean_absolute_error: 0.0067\n",
      "\n",
      "Epoch 05556: loss did not improve\n",
      "Epoch 5557/10000\n",
      "2700/2700 [==============================] - 0s 39us/step - loss: 1.3168e-04 - mean_absolute_error: 0.0066\n",
      "\n",
      "Epoch 05557: loss did not improve\n",
      "Epoch 5558/10000\n",
      "2700/2700 [==============================] - 0s 50us/step - loss: 1.3495e-04 - mean_absolute_error: 0.0069\n",
      "\n",
      "Epoch 05558: loss did not improve\n",
      "Epoch 5559/10000\n",
      "2700/2700 [==============================] - 0s 29us/step - loss: 1.3656e-04 - mean_absolute_error: 0.0070\n",
      "\n",
      "Epoch 05559: loss did not improve\n",
      "Epoch 5560/10000\n",
      "2700/2700 [==============================] - 0s 30us/step - loss: 1.3248e-04 - mean_absolute_error: 0.0066\n",
      "\n",
      "Epoch 05560: loss did not improve\n",
      "Epoch 5561/10000\n",
      "2700/2700 [==============================] - 0s 28us/step - loss: 1.3252e-04 - mean_absolute_error: 0.0066\n",
      "\n",
      "Epoch 05561: loss did not improve\n",
      "Epoch 5562/10000\n",
      "2700/2700 [==============================] - 0s 28us/step - loss: 1.3201e-04 - mean_absolute_error: 0.0066\n",
      "\n",
      "Epoch 05562: loss did not improve\n",
      "Epoch 5563/10000\n",
      "2700/2700 [==============================] - 0s 38us/step - loss: 1.3177e-04 - mean_absolute_error: 0.0066\n",
      "\n",
      "Epoch 05563: loss did not improve\n",
      "Epoch 5564/10000\n",
      "2700/2700 [==============================] - 0s 34us/step - loss: 1.3159e-04 - mean_absolute_error: 0.0065\n",
      "\n",
      "Epoch 05564: loss did not improve\n",
      "Epoch 5565/10000\n",
      "2700/2700 [==============================] - 0s 30us/step - loss: 1.3166e-04 - mean_absolute_error: 0.0066\n",
      "\n",
      "Epoch 05565: loss did not improve\n",
      "Epoch 5566/10000\n",
      "2700/2700 [==============================] - 0s 32us/step - loss: 1.3176e-04 - mean_absolute_error: 0.0067\n",
      "\n",
      "Epoch 05566: loss did not improve\n",
      "Epoch 5567/10000\n",
      "2700/2700 [==============================] - 0s 27us/step - loss: 1.3347e-04 - mean_absolute_error: 0.0067\n",
      "\n",
      "Epoch 05567: loss did not improve\n",
      "Epoch 5568/10000\n",
      "2700/2700 [==============================] - 0s 29us/step - loss: 1.3314e-04 - mean_absolute_error: 0.0067\n",
      "\n",
      "Epoch 05568: loss did not improve\n",
      "Epoch 5569/10000\n",
      "2700/2700 [==============================] - 0s 31us/step - loss: 1.3136e-04 - mean_absolute_error: 0.0065\n",
      "\n",
      "Epoch 05569: loss did not improve\n",
      "Epoch 5570/10000\n",
      "2700/2700 [==============================] - 0s 29us/step - loss: 1.3112e-04 - mean_absolute_error: 0.0066\n",
      "\n",
      "Epoch 05570: loss did not improve\n",
      "Epoch 5571/10000\n",
      "2700/2700 [==============================] - 0s 25us/step - loss: 1.3402e-04 - mean_absolute_error: 0.0067\n",
      "\n",
      "Epoch 05571: loss did not improve\n",
      "Epoch 5572/10000\n",
      "2700/2700 [==============================] - 0s 31us/step - loss: 1.3388e-04 - mean_absolute_error: 0.0067\n",
      "\n",
      "Epoch 05572: loss did not improve\n",
      "Epoch 5573/10000\n",
      "2700/2700 [==============================] - 0s 29us/step - loss: 1.3618e-04 - mean_absolute_error: 0.0068\n",
      "\n",
      "Epoch 05573: loss did not improve\n",
      "Epoch 5574/10000\n",
      "2700/2700 [==============================] - 0s 28us/step - loss: 1.3147e-04 - mean_absolute_error: 0.0065\n",
      "\n",
      "Epoch 05574: loss did not improve\n",
      "Epoch 5575/10000\n",
      "2700/2700 [==============================] - 0s 34us/step - loss: 1.3203e-04 - mean_absolute_error: 0.0066\n",
      "\n",
      "Epoch 05575: loss did not improve\n",
      "Epoch 5576/10000\n",
      "2700/2700 [==============================] - 0s 32us/step - loss: 1.3260e-04 - mean_absolute_error: 0.0067\n",
      "\n",
      "Epoch 05576: loss did not improve\n",
      "Epoch 5577/10000\n",
      "2700/2700 [==============================] - 0s 33us/step - loss: 1.3503e-04 - mean_absolute_error: 0.0069\n",
      "\n",
      "Epoch 05577: loss did not improve\n",
      "Epoch 5578/10000\n",
      "2700/2700 [==============================] - 0s 30us/step - loss: 1.3684e-04 - mean_absolute_error: 0.0070\n",
      "\n",
      "Epoch 05578: loss did not improve\n",
      "Epoch 5579/10000\n",
      "2700/2700 [==============================] - 0s 35us/step - loss: 1.3252e-04 - mean_absolute_error: 0.0067\n",
      "\n",
      "Epoch 05579: loss did not improve\n",
      "Epoch 5580/10000\n",
      "2700/2700 [==============================] - 0s 30us/step - loss: 1.3474e-04 - mean_absolute_error: 0.0069\n",
      "\n",
      "Epoch 05580: loss did not improve\n",
      "Epoch 5581/10000\n",
      "2700/2700 [==============================] - 0s 29us/step - loss: 1.3332e-04 - mean_absolute_error: 0.0067\n",
      "\n",
      "Epoch 05581: loss did not improve\n",
      "Epoch 5582/10000\n",
      "2700/2700 [==============================] - 0s 29us/step - loss: 1.3104e-04 - mean_absolute_error: 0.0065\n",
      "\n",
      "Epoch 05582: loss did not improve\n",
      "Epoch 5583/10000\n",
      "2700/2700 [==============================] - 0s 27us/step - loss: 1.3172e-04 - mean_absolute_error: 0.0066\n",
      "\n",
      "Epoch 05583: loss did not improve\n",
      "Epoch 5584/10000\n",
      "2700/2700 [==============================] - 0s 37us/step - loss: 1.3583e-04 - mean_absolute_error: 0.0070\n",
      "\n",
      "Epoch 05584: loss did not improve\n",
      "Epoch 5585/10000\n",
      "2700/2700 [==============================] - 0s 31us/step - loss: 1.3235e-04 - mean_absolute_error: 0.0066\n",
      "\n",
      "Epoch 05585: loss did not improve\n",
      "Epoch 5586/10000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2700/2700 [==============================] - 0s 33us/step - loss: 1.3308e-04 - mean_absolute_error: 0.0068\n",
      "\n",
      "Epoch 05586: loss did not improve\n",
      "Epoch 5587/10000\n",
      "2700/2700 [==============================] - 0s 29us/step - loss: 1.3197e-04 - mean_absolute_error: 0.0066\n",
      "\n",
      "Epoch 05587: loss did not improve\n",
      "Epoch 5588/10000\n",
      "2700/2700 [==============================] - 0s 29us/step - loss: 1.3098e-04 - mean_absolute_error: 0.0067\n",
      "\n",
      "Epoch 05588: loss did not improve\n",
      "Epoch 5589/10000\n",
      "2700/2700 [==============================] - 0s 28us/step - loss: 1.4101e-04 - mean_absolute_error: 0.0072\n",
      "\n",
      "Epoch 05589: loss did not improve\n",
      "Epoch 5590/10000\n",
      "2700/2700 [==============================] - 0s 30us/step - loss: 1.3178e-04 - mean_absolute_error: 0.0067\n",
      "\n",
      "Epoch 05590: loss did not improve\n",
      "Epoch 5591/10000\n",
      "2700/2700 [==============================] - 0s 29us/step - loss: 1.3074e-04 - mean_absolute_error: 0.0065\n",
      "\n",
      "Epoch 05591: loss improved from 0.00013 to 0.00013, saving model to keras/best_baseline_weights.hdf5\n",
      "Epoch 5592/10000\n",
      "2700/2700 [==============================] - 0s 30us/step - loss: 1.3184e-04 - mean_absolute_error: 0.0067\n",
      "\n",
      "Epoch 05592: loss did not improve\n",
      "Epoch 5593/10000\n",
      "2700/2700 [==============================] - 0s 28us/step - loss: 1.3418e-04 - mean_absolute_error: 0.0068\n",
      "\n",
      "Epoch 05593: loss did not improve\n",
      "Epoch 5594/10000\n",
      "2700/2700 [==============================] - 0s 29us/step - loss: 1.3184e-04 - mean_absolute_error: 0.0066\n",
      "\n",
      "Epoch 05594: loss did not improve\n",
      "Epoch 5595/10000\n",
      "2700/2700 [==============================] - 0s 26us/step - loss: 1.3489e-04 - mean_absolute_error: 0.0068\n",
      "\n",
      "Epoch 05595: loss did not improve\n",
      "Epoch 5596/10000\n",
      "2700/2700 [==============================] - 0s 28us/step - loss: 1.3140e-04 - mean_absolute_error: 0.0066\n",
      "\n",
      "Epoch 05596: loss did not improve\n",
      "Epoch 5597/10000\n",
      "2700/2700 [==============================] - 0s 29us/step - loss: 1.3169e-04 - mean_absolute_error: 0.0066\n",
      "\n",
      "Epoch 05597: loss did not improve\n",
      "Epoch 5598/10000\n",
      "2700/2700 [==============================] - 0s 31us/step - loss: 1.3086e-04 - mean_absolute_error: 0.0066\n",
      "\n",
      "Epoch 05598: loss did not improve\n",
      "Epoch 5599/10000\n",
      "2700/2700 [==============================] - 0s 28us/step - loss: 1.3667e-04 - mean_absolute_error: 0.0070\n",
      "\n",
      "Epoch 05599: loss did not improve\n",
      "Epoch 5600/10000\n",
      "2700/2700 [==============================] - 0s 29us/step - loss: 1.3396e-04 - mean_absolute_error: 0.0068\n",
      "\n",
      "Epoch 05600: loss did not improve\n",
      "Epoch 5601/10000\n",
      "2700/2700 [==============================] - 0s 28us/step - loss: 1.3309e-04 - mean_absolute_error: 0.0066\n",
      "\n",
      "Epoch 05601: loss did not improve\n",
      "Epoch 5602/10000\n",
      "2700/2700 [==============================] - 0s 28us/step - loss: 1.3139e-04 - mean_absolute_error: 0.0067\n",
      "\n",
      "Epoch 05602: loss did not improve\n",
      "Epoch 5603/10000\n",
      "2700/2700 [==============================] - 0s 26us/step - loss: 1.3237e-04 - mean_absolute_error: 0.0068\n",
      "\n",
      "Epoch 05603: loss did not improve\n",
      "Epoch 5604/10000\n",
      "2700/2700 [==============================] - 0s 29us/step - loss: 1.3191e-04 - mean_absolute_error: 0.0067\n",
      "\n",
      "Epoch 05604: loss did not improve\n",
      "Epoch 5605/10000\n",
      "2700/2700 [==============================] - 0s 27us/step - loss: 1.3195e-04 - mean_absolute_error: 0.0066\n",
      "\n",
      "Epoch 05605: loss did not improve\n",
      "Epoch 5606/10000\n",
      "2700/2700 [==============================] - 0s 29us/step - loss: 1.3100e-04 - mean_absolute_error: 0.0065\n",
      "\n",
      "Epoch 05606: loss did not improve\n",
      "Epoch 5607/10000\n",
      "2700/2700 [==============================] - 0s 29us/step - loss: 1.3198e-04 - mean_absolute_error: 0.0066\n",
      "\n",
      "Epoch 05607: loss did not improve\n",
      "Epoch 5608/10000\n",
      "2700/2700 [==============================] - 0s 29us/step - loss: 1.3457e-04 - mean_absolute_error: 0.0069\n",
      "\n",
      "Epoch 05608: loss did not improve\n",
      "Epoch 5609/10000\n",
      "2700/2700 [==============================] - 0s 30us/step - loss: 1.3804e-04 - mean_absolute_error: 0.0072\n",
      "\n",
      "Epoch 05609: loss did not improve\n",
      "Epoch 5610/10000\n",
      "2700/2700 [==============================] - 0s 28us/step - loss: 1.3270e-04 - mean_absolute_error: 0.0067\n",
      "\n",
      "Epoch 05610: loss did not improve\n",
      "Epoch 5611/10000\n",
      "2700/2700 [==============================] - 0s 31us/step - loss: 1.3092e-04 - mean_absolute_error: 0.0066\n",
      "\n",
      "Epoch 05611: loss did not improve\n",
      "Epoch 5612/10000\n",
      "2700/2700 [==============================] - 0s 30us/step - loss: 1.3200e-04 - mean_absolute_error: 0.0068\n",
      "\n",
      "Epoch 05612: loss did not improve\n",
      "Epoch 5613/10000\n",
      "2700/2700 [==============================] - 0s 32us/step - loss: 1.3164e-04 - mean_absolute_error: 0.0066\n",
      "\n",
      "Epoch 05613: loss did not improve\n",
      "Epoch 5614/10000\n",
      "2700/2700 [==============================] - 0s 31us/step - loss: 1.3484e-04 - mean_absolute_error: 0.0071\n",
      "\n",
      "Epoch 05614: loss did not improve\n",
      "Epoch 5615/10000\n",
      "2700/2700 [==============================] - 0s 29us/step - loss: 1.3306e-04 - mean_absolute_error: 0.0068\n",
      "\n",
      "Epoch 05615: loss did not improve\n",
      "Epoch 5616/10000\n",
      "2700/2700 [==============================] - 0s 30us/step - loss: 1.3301e-04 - mean_absolute_error: 0.0068\n",
      "\n",
      "Epoch 05616: loss did not improve\n",
      "Epoch 5617/10000\n",
      "2700/2700 [==============================] - 0s 35us/step - loss: 1.3198e-04 - mean_absolute_error: 0.0066\n",
      "\n",
      "Epoch 05617: loss did not improve\n",
      "Epoch 5618/10000\n",
      "2700/2700 [==============================] - 0s 33us/step - loss: 1.3086e-04 - mean_absolute_error: 0.0065\n",
      "\n",
      "Epoch 05618: loss did not improve\n",
      "Epoch 5619/10000\n",
      "2700/2700 [==============================] - 0s 26us/step - loss: 1.3486e-04 - mean_absolute_error: 0.0071\n",
      "\n",
      "Epoch 05619: loss did not improve\n",
      "Epoch 5620/10000\n",
      "2700/2700 [==============================] - 0s 30us/step - loss: 1.3134e-04 - mean_absolute_error: 0.0065\n",
      "\n",
      "Epoch 05620: loss did not improve\n",
      "Epoch 5621/10000\n",
      "2700/2700 [==============================] - 0s 30us/step - loss: 1.3124e-04 - mean_absolute_error: 0.0066\n",
      "\n",
      "Epoch 05621: loss did not improve\n",
      "Epoch 5622/10000\n",
      "2700/2700 [==============================] - 0s 34us/step - loss: 1.3143e-04 - mean_absolute_error: 0.0065\n",
      "\n",
      "Epoch 05622: loss did not improve\n",
      "Epoch 5623/10000\n",
      "2700/2700 [==============================] - 0s 30us/step - loss: 1.3443e-04 - mean_absolute_error: 0.0069\n",
      "\n",
      "Epoch 05623: loss did not improve\n",
      "Epoch 5624/10000\n",
      "2700/2700 [==============================] - 0s 27us/step - loss: 1.3695e-04 - mean_absolute_error: 0.0071\n",
      "\n",
      "Epoch 05624: loss did not improve\n",
      "Epoch 5625/10000\n",
      "2700/2700 [==============================] - 0s 31us/step - loss: 1.3394e-04 - mean_absolute_error: 0.0070\n",
      "\n",
      "Epoch 05625: loss did not improve\n",
      "Epoch 5626/10000\n",
      "2700/2700 [==============================] - 0s 35us/step - loss: 1.3168e-04 - mean_absolute_error: 0.0066\n",
      "\n",
      "Epoch 05626: loss did not improve\n",
      "Epoch 5627/10000\n",
      "2700/2700 [==============================] - 0s 30us/step - loss: 1.3451e-04 - mean_absolute_error: 0.0069\n",
      "\n",
      "Epoch 05627: loss did not improve\n",
      "Epoch 5628/10000\n",
      "2700/2700 [==============================] - 0s 29us/step - loss: 1.3226e-04 - mean_absolute_error: 0.0067\n",
      "\n",
      "Epoch 05628: loss did not improve\n",
      "Epoch 5629/10000\n",
      "2700/2700 [==============================] - 0s 28us/step - loss: 1.3380e-04 - mean_absolute_error: 0.0068\n",
      "\n",
      "Epoch 05629: loss did not improve\n",
      "Epoch 5630/10000\n",
      "2700/2700 [==============================] - 0s 29us/step - loss: 1.3366e-04 - mean_absolute_error: 0.0068\n",
      "\n",
      "Epoch 05630: loss did not improve\n",
      "Epoch 5631/10000\n",
      "2700/2700 [==============================] - 0s 31us/step - loss: 1.3107e-04 - mean_absolute_error: 0.0066\n",
      "\n",
      "Epoch 05631: loss did not improve\n",
      "Epoch 5632/10000\n",
      "2700/2700 [==============================] - 0s 30us/step - loss: 1.3135e-04 - mean_absolute_error: 0.0065\n",
      "\n",
      "Epoch 05632: loss did not improve\n",
      "Epoch 5633/10000\n",
      "2700/2700 [==============================] - 0s 31us/step - loss: 1.3284e-04 - mean_absolute_error: 0.0066\n",
      "\n",
      "Epoch 05633: loss did not improve\n",
      "Epoch 5634/10000\n",
      "2700/2700 [==============================] - 0s 28us/step - loss: 1.3399e-04 - mean_absolute_error: 0.0068\n",
      "\n",
      "Epoch 05634: loss did not improve\n",
      "Epoch 5635/10000\n",
      "2700/2700 [==============================] - 0s 27us/step - loss: 1.3049e-04 - mean_absolute_error: 0.0065\n",
      "\n",
      "Epoch 05635: loss improved from 0.00013 to 0.00013, saving model to keras/best_baseline_weights.hdf5\n",
      "Epoch 5636/10000\n",
      "2700/2700 [==============================] - 0s 31us/step - loss: 1.3192e-04 - mean_absolute_error: 0.0066\n",
      "\n",
      "Epoch 05636: loss did not improve\n",
      "Epoch 5637/10000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2700/2700 [==============================] - 0s 34us/step - loss: 1.3107e-04 - mean_absolute_error: 0.0066\n",
      "\n",
      "Epoch 05637: loss did not improve\n",
      "Epoch 5638/10000\n",
      "2700/2700 [==============================] - 0s 31us/step - loss: 1.3351e-04 - mean_absolute_error: 0.0068\n",
      "\n",
      "Epoch 05638: loss did not improve\n",
      "Epoch 5639/10000\n",
      "2700/2700 [==============================] - 0s 32us/step - loss: 1.3476e-04 - mean_absolute_error: 0.0069\n",
      "\n",
      "Epoch 05639: loss did not improve\n",
      "Epoch 5640/10000\n",
      "2700/2700 [==============================] - 0s 30us/step - loss: 1.3222e-04 - mean_absolute_error: 0.0066\n",
      "\n",
      "Epoch 05640: loss did not improve\n",
      "Epoch 5641/10000\n",
      "2700/2700 [==============================] - 0s 29us/step - loss: 1.3099e-04 - mean_absolute_error: 0.0066\n",
      "\n",
      "Epoch 05641: loss did not improve\n",
      "Epoch 5642/10000\n",
      "2700/2700 [==============================] - 0s 27us/step - loss: 1.3130e-04 - mean_absolute_error: 0.0066\n",
      "\n",
      "Epoch 05642: loss did not improve\n",
      "Epoch 5643/10000\n",
      "2700/2700 [==============================] - 0s 31us/step - loss: 1.3184e-04 - mean_absolute_error: 0.0067\n",
      "\n",
      "Epoch 05643: loss did not improve\n",
      "Epoch 5644/10000\n",
      "2700/2700 [==============================] - 0s 28us/step - loss: 1.3235e-04 - mean_absolute_error: 0.0068\n",
      "\n",
      "Epoch 05644: loss did not improve\n",
      "Epoch 5645/10000\n",
      "2700/2700 [==============================] - 0s 30us/step - loss: 1.3119e-04 - mean_absolute_error: 0.0066\n",
      "\n",
      "Epoch 05645: loss did not improve\n",
      "Epoch 5646/10000\n",
      "2700/2700 [==============================] - 0s 30us/step - loss: 1.3125e-04 - mean_absolute_error: 0.0066\n",
      "\n",
      "Epoch 05646: loss did not improve\n",
      "Epoch 5647/10000\n",
      "2700/2700 [==============================] - 0s 30us/step - loss: 1.3124e-04 - mean_absolute_error: 0.0066\n",
      "\n",
      "Epoch 05647: loss did not improve\n",
      "Epoch 5648/10000\n",
      "2700/2700 [==============================] - 0s 28us/step - loss: 1.3274e-04 - mean_absolute_error: 0.0067\n",
      "\n",
      "Epoch 05648: loss did not improve\n",
      "Epoch 5649/10000\n",
      "2700/2700 [==============================] - 0s 29us/step - loss: 1.3235e-04 - mean_absolute_error: 0.0067\n",
      "\n",
      "Epoch 05649: loss did not improve\n",
      "Epoch 5650/10000\n",
      "2700/2700 [==============================] - 0s 31us/step - loss: 1.3515e-04 - mean_absolute_error: 0.0069\n",
      "\n",
      "Epoch 05650: loss did not improve\n",
      "Epoch 5651/10000\n",
      "2700/2700 [==============================] - 0s 32us/step - loss: 1.3536e-04 - mean_absolute_error: 0.0070\n",
      "\n",
      "Epoch 05651: loss did not improve\n",
      "Epoch 5652/10000\n",
      "2700/2700 [==============================] - 0s 42us/step - loss: 1.3255e-04 - mean_absolute_error: 0.0067\n",
      "\n",
      "Epoch 05652: loss did not improve\n",
      "Epoch 5653/10000\n",
      "2700/2700 [==============================] - 0s 31us/step - loss: 1.3605e-04 - mean_absolute_error: 0.0070\n",
      "\n",
      "Epoch 05653: loss did not improve\n",
      "Epoch 5654/10000\n",
      "2700/2700 [==============================] - 0s 29us/step - loss: 1.3168e-04 - mean_absolute_error: 0.0066\n",
      "\n",
      "Epoch 05654: loss did not improve\n",
      "Epoch 5655/10000\n",
      "2700/2700 [==============================] - 0s 30us/step - loss: 1.3209e-04 - mean_absolute_error: 0.0067\n",
      "\n",
      "Epoch 05655: loss did not improve\n",
      "Epoch 5656/10000\n",
      "2700/2700 [==============================] - 0s 33us/step - loss: 1.3735e-04 - mean_absolute_error: 0.0070\n",
      "\n",
      "Epoch 05656: loss did not improve\n",
      "Epoch 5657/10000\n",
      "2700/2700 [==============================] - 0s 29us/step - loss: 1.3129e-04 - mean_absolute_error: 0.0066\n",
      "\n",
      "Epoch 05657: loss did not improve\n",
      "Epoch 5658/10000\n",
      "2700/2700 [==============================] - 0s 30us/step - loss: 1.3560e-04 - mean_absolute_error: 0.0069\n",
      "\n",
      "Epoch 05658: loss did not improve\n",
      "Epoch 5659/10000\n",
      "2700/2700 [==============================] - 0s 36us/step - loss: 1.3208e-04 - mean_absolute_error: 0.0066\n",
      "\n",
      "Epoch 05659: loss did not improve\n",
      "Epoch 5660/10000\n",
      "2700/2700 [==============================] - 0s 31us/step - loss: 1.3337e-04 - mean_absolute_error: 0.0067\n",
      "\n",
      "Epoch 05660: loss did not improve\n",
      "Epoch 5661/10000\n",
      "2700/2700 [==============================] - 0s 28us/step - loss: 1.3313e-04 - mean_absolute_error: 0.0067\n",
      "\n",
      "Epoch 05661: loss did not improve\n",
      "Epoch 5662/10000\n",
      "2700/2700 [==============================] - 0s 30us/step - loss: 1.3395e-04 - mean_absolute_error: 0.0069\n",
      "\n",
      "Epoch 05662: loss did not improve\n",
      "Epoch 5663/10000\n",
      "2700/2700 [==============================] - 0s 31us/step - loss: 1.3130e-04 - mean_absolute_error: 0.0067\n",
      "\n",
      "Epoch 05663: loss did not improve\n",
      "Epoch 5664/10000\n",
      "2700/2700 [==============================] - 0s 34us/step - loss: 1.3169e-04 - mean_absolute_error: 0.0066\n",
      "\n",
      "Epoch 05664: loss did not improve\n",
      "Epoch 5665/10000\n",
      "2700/2700 [==============================] - 0s 31us/step - loss: 1.3395e-04 - mean_absolute_error: 0.0068\n",
      "\n",
      "Epoch 05665: loss did not improve\n",
      "Epoch 5666/10000\n",
      "2700/2700 [==============================] - 0s 30us/step - loss: 1.3064e-04 - mean_absolute_error: 0.0065\n",
      "\n",
      "Epoch 05666: loss did not improve\n",
      "Epoch 5667/10000\n",
      "2700/2700 [==============================] - 0s 27us/step - loss: 1.3355e-04 - mean_absolute_error: 0.0069\n",
      "\n",
      "Epoch 05667: loss did not improve\n",
      "Epoch 5668/10000\n",
      "2700/2700 [==============================] - 0s 36us/step - loss: 1.3396e-04 - mean_absolute_error: 0.0069\n",
      "\n",
      "Epoch 05668: loss did not improve\n",
      "Epoch 5669/10000\n",
      "2700/2700 [==============================] - 0s 32us/step - loss: 1.3136e-04 - mean_absolute_error: 0.0066\n",
      "\n",
      "Epoch 05669: loss did not improve\n",
      "Epoch 5670/10000\n",
      "2700/2700 [==============================] - 0s 35us/step - loss: 1.3199e-04 - mean_absolute_error: 0.0067\n",
      "\n",
      "Epoch 05670: loss did not improve\n",
      "Epoch 5671/10000\n",
      "2700/2700 [==============================] - 0s 31us/step - loss: 1.3076e-04 - mean_absolute_error: 0.0065\n",
      "\n",
      "Epoch 05671: loss did not improve\n",
      "Epoch 5672/10000\n",
      "2700/2700 [==============================] - 0s 30us/step - loss: 1.2983e-04 - mean_absolute_error: 0.0064\n",
      "\n",
      "Epoch 05672: loss improved from 0.00013 to 0.00013, saving model to keras/best_baseline_weights.hdf5\n",
      "Epoch 5673/10000\n",
      "2700/2700 [==============================] - 0s 30us/step - loss: 1.2993e-04 - mean_absolute_error: 0.0065\n",
      "\n",
      "Epoch 05673: loss did not improve\n",
      "Epoch 5674/10000\n",
      "2700/2700 [==============================] - 0s 35us/step - loss: 1.3178e-04 - mean_absolute_error: 0.0066\n",
      "\n",
      "Epoch 05674: loss did not improve\n",
      "Epoch 5675/10000\n",
      "2700/2700 [==============================] - 0s 29us/step - loss: 1.3008e-04 - mean_absolute_error: 0.0065\n",
      "\n",
      "Epoch 05675: loss did not improve\n",
      "Epoch 5676/10000\n",
      "2700/2700 [==============================] - 0s 31us/step - loss: 1.3358e-04 - mean_absolute_error: 0.0067\n",
      "\n",
      "Epoch 05676: loss did not improve\n",
      "Epoch 5677/10000\n",
      "2700/2700 [==============================] - 0s 29us/step - loss: 1.3178e-04 - mean_absolute_error: 0.0066\n",
      "\n",
      "Epoch 05677: loss did not improve\n",
      "Epoch 5678/10000\n",
      "2700/2700 [==============================] - 0s 30us/step - loss: 1.3074e-04 - mean_absolute_error: 0.0067\n",
      "\n",
      "Epoch 05678: loss did not improve\n",
      "Epoch 5679/10000\n",
      "2700/2700 [==============================] - 0s 33us/step - loss: 1.3219e-04 - mean_absolute_error: 0.0067\n",
      "\n",
      "Epoch 05679: loss did not improve\n",
      "Epoch 5680/10000\n",
      "2700/2700 [==============================] - 0s 30us/step - loss: 1.3062e-04 - mean_absolute_error: 0.0066\n",
      "\n",
      "Epoch 05680: loss did not improve\n",
      "Epoch 5681/10000\n",
      "2700/2700 [==============================] - 0s 28us/step - loss: 1.3331e-04 - mean_absolute_error: 0.0068\n",
      "\n",
      "Epoch 05681: loss did not improve\n",
      "Epoch 5682/10000\n",
      "2700/2700 [==============================] - 0s 28us/step - loss: 1.3326e-04 - mean_absolute_error: 0.0067\n",
      "\n",
      "Epoch 05682: loss did not improve\n",
      "Epoch 5683/10000\n",
      "2700/2700 [==============================] - 0s 27us/step - loss: 1.3118e-04 - mean_absolute_error: 0.0067\n",
      "\n",
      "Epoch 05683: loss did not improve\n",
      "Epoch 5684/10000\n",
      "2700/2700 [==============================] - 0s 29us/step - loss: 1.3431e-04 - mean_absolute_error: 0.0070\n",
      "\n",
      "Epoch 05684: loss did not improve\n",
      "Epoch 5685/10000\n",
      "2700/2700 [==============================] - 0s 33us/step - loss: 1.3217e-04 - mean_absolute_error: 0.0066\n",
      "\n",
      "Epoch 05685: loss did not improve\n",
      "Epoch 5686/10000\n",
      "2700/2700 [==============================] - 0s 27us/step - loss: 1.3426e-04 - mean_absolute_error: 0.0069\n",
      "\n",
      "Epoch 05686: loss did not improve\n",
      "Epoch 5687/10000\n",
      "2700/2700 [==============================] - 0s 37us/step - loss: 1.3026e-04 - mean_absolute_error: 0.0065\n",
      "\n",
      "Epoch 05687: loss did not improve\n",
      "Epoch 5688/10000\n",
      "2700/2700 [==============================] - 0s 30us/step - loss: 1.3035e-04 - mean_absolute_error: 0.0065\n",
      "\n",
      "Epoch 05688: loss did not improve\n",
      "Epoch 5689/10000\n",
      "2700/2700 [==============================] - 0s 29us/step - loss: 1.2920e-04 - mean_absolute_error: 0.0065\n",
      "\n",
      "Epoch 05689: loss improved from 0.00013 to 0.00013, saving model to keras/best_baseline_weights.hdf5\n",
      "Epoch 5690/10000\n",
      "2700/2700 [==============================] - 0s 28us/step - loss: 1.3324e-04 - mean_absolute_error: 0.0068\n",
      "\n",
      "Epoch 05690: loss did not improve\n",
      "Epoch 5691/10000\n",
      "2700/2700 [==============================] - 0s 35us/step - loss: 1.3197e-04 - mean_absolute_error: 0.0066\n",
      "\n",
      "Epoch 05691: loss did not improve\n",
      "Epoch 5692/10000\n",
      "2700/2700 [==============================] - 0s 30us/step - loss: 1.3063e-04 - mean_absolute_error: 0.0066\n",
      "\n",
      "Epoch 05692: loss did not improve\n",
      "Epoch 5693/10000\n",
      "2700/2700 [==============================] - 0s 29us/step - loss: 1.3160e-04 - mean_absolute_error: 0.0066\n",
      "\n",
      "Epoch 05693: loss did not improve\n",
      "Epoch 5694/10000\n",
      "2700/2700 [==============================] - 0s 29us/step - loss: 1.2961e-04 - mean_absolute_error: 0.0065\n",
      "\n",
      "Epoch 05694: loss did not improve\n",
      "Epoch 5695/10000\n",
      "2700/2700 [==============================] - 0s 38us/step - loss: 1.3312e-04 - mean_absolute_error: 0.0068\n",
      "\n",
      "Epoch 05695: loss did not improve\n",
      "Epoch 5696/10000\n",
      "2700/2700 [==============================] - 0s 31us/step - loss: 1.3146e-04 - mean_absolute_error: 0.0066\n",
      "\n",
      "Epoch 05696: loss did not improve\n",
      "Epoch 5697/10000\n",
      "2700/2700 [==============================] - 0s 30us/step - loss: 1.3069e-04 - mean_absolute_error: 0.0066\n",
      "\n",
      "Epoch 05697: loss did not improve\n",
      "Epoch 5698/10000\n",
      "2700/2700 [==============================] - 0s 30us/step - loss: 1.3332e-04 - mean_absolute_error: 0.0068\n",
      "\n",
      "Epoch 05698: loss did not improve\n",
      "Epoch 5699/10000\n",
      "2700/2700 [==============================] - 0s 25us/step - loss: 1.3276e-04 - mean_absolute_error: 0.0067\n",
      "\n",
      "Epoch 05699: loss did not improve\n",
      "Epoch 5700/10000\n",
      "2700/2700 [==============================] - 0s 36us/step - loss: 1.3097e-04 - mean_absolute_error: 0.0066\n",
      "\n",
      "Epoch 05700: loss did not improve\n",
      "Epoch 5701/10000\n",
      "2700/2700 [==============================] - 0s 32us/step - loss: 1.3100e-04 - mean_absolute_error: 0.0065\n",
      "\n",
      "Epoch 05701: loss did not improve\n",
      "Epoch 5702/10000\n",
      "2700/2700 [==============================] - 0s 29us/step - loss: 1.3271e-04 - mean_absolute_error: 0.0067\n",
      "\n",
      "Epoch 05702: loss did not improve\n",
      "Epoch 5703/10000\n",
      "2700/2700 [==============================] - 0s 32us/step - loss: 1.2980e-04 - mean_absolute_error: 0.0064\n",
      "\n",
      "Epoch 05703: loss did not improve\n",
      "Epoch 5704/10000\n",
      "2700/2700 [==============================] - 0s 30us/step - loss: 1.3066e-04 - mean_absolute_error: 0.0065\n",
      "\n",
      "Epoch 05704: loss did not improve\n",
      "Epoch 5705/10000\n",
      "2700/2700 [==============================] - 0s 32us/step - loss: 1.3118e-04 - mean_absolute_error: 0.0066\n",
      "\n",
      "Epoch 05705: loss did not improve\n",
      "Epoch 5706/10000\n",
      "2700/2700 [==============================] - 0s 36us/step - loss: 1.3448e-04 - mean_absolute_error: 0.0068\n",
      "\n",
      "Epoch 05706: loss did not improve\n",
      "Epoch 5707/10000\n",
      "2700/2700 [==============================] - 0s 35us/step - loss: 1.3329e-04 - mean_absolute_error: 0.0069\n",
      "\n",
      "Epoch 05707: loss did not improve\n",
      "Epoch 5708/10000\n",
      "2700/2700 [==============================] - 0s 38us/step - loss: 1.2965e-04 - mean_absolute_error: 0.0065\n",
      "\n",
      "Epoch 05708: loss did not improve\n",
      "Epoch 5709/10000\n",
      "2700/2700 [==============================] - 0s 33us/step - loss: 1.3411e-04 - mean_absolute_error: 0.0069\n",
      "\n",
      "Epoch 05709: loss did not improve\n",
      "Epoch 5710/10000\n",
      "2700/2700 [==============================] - 0s 32us/step - loss: 1.3092e-04 - mean_absolute_error: 0.0067\n",
      "\n",
      "Epoch 05710: loss did not improve\n",
      "Epoch 5711/10000\n",
      "2700/2700 [==============================] - 0s 33us/step - loss: 1.3116e-04 - mean_absolute_error: 0.0068\n",
      "\n",
      "Epoch 05711: loss did not improve\n",
      "Epoch 5712/10000\n",
      "2700/2700 [==============================] - 0s 28us/step - loss: 1.3080e-04 - mean_absolute_error: 0.0066\n",
      "\n",
      "Epoch 05712: loss did not improve\n",
      "Epoch 5713/10000\n",
      "2700/2700 [==============================] - 0s 29us/step - loss: 1.3007e-04 - mean_absolute_error: 0.0066\n",
      "\n",
      "Epoch 05713: loss did not improve\n",
      "Epoch 5714/10000\n",
      "2700/2700 [==============================] - 0s 26us/step - loss: 1.3474e-04 - mean_absolute_error: 0.0069\n",
      "\n",
      "Epoch 05714: loss did not improve\n",
      "Epoch 5715/10000\n",
      "2700/2700 [==============================] - 0s 28us/step - loss: 1.3210e-04 - mean_absolute_error: 0.0067\n",
      "\n",
      "Epoch 05715: loss did not improve\n",
      "Epoch 5716/10000\n",
      "2700/2700 [==============================] - 0s 32us/step - loss: 1.3150e-04 - mean_absolute_error: 0.0067\n",
      "\n",
      "Epoch 05716: loss did not improve\n",
      "Epoch 5717/10000\n",
      "2700/2700 [==============================] - 0s 31us/step - loss: 1.3143e-04 - mean_absolute_error: 0.0065\n",
      "\n",
      "Epoch 05717: loss did not improve\n",
      "Epoch 5718/10000\n",
      "2700/2700 [==============================] - 0s 28us/step - loss: 1.3439e-04 - mean_absolute_error: 0.0069\n",
      "\n",
      "Epoch 05718: loss did not improve\n",
      "Epoch 5719/10000\n",
      "2700/2700 [==============================] - 0s 31us/step - loss: 1.3162e-04 - mean_absolute_error: 0.0067\n",
      "\n",
      "Epoch 05719: loss did not improve\n",
      "Epoch 5720/10000\n",
      "2700/2700 [==============================] - 0s 27us/step - loss: 1.3214e-04 - mean_absolute_error: 0.0066\n",
      "\n",
      "Epoch 05720: loss did not improve\n",
      "Epoch 5721/10000\n",
      "2700/2700 [==============================] - 0s 28us/step - loss: 1.3216e-04 - mean_absolute_error: 0.0069\n",
      "\n",
      "Epoch 05721: loss did not improve\n",
      "Epoch 5722/10000\n",
      "2700/2700 [==============================] - 0s 29us/step - loss: 1.3142e-04 - mean_absolute_error: 0.0066\n",
      "\n",
      "Epoch 05722: loss did not improve\n",
      "Epoch 5723/10000\n",
      "2700/2700 [==============================] - 0s 28us/step - loss: 1.3204e-04 - mean_absolute_error: 0.0067\n",
      "\n",
      "Epoch 05723: loss did not improve\n",
      "Epoch 5724/10000\n",
      "2700/2700 [==============================] - 0s 28us/step - loss: 1.2981e-04 - mean_absolute_error: 0.0064\n",
      "\n",
      "Epoch 05724: loss did not improve\n",
      "Epoch 5725/10000\n",
      "2700/2700 [==============================] - 0s 27us/step - loss: 1.3182e-04 - mean_absolute_error: 0.0067\n",
      "\n",
      "Epoch 05725: loss did not improve\n",
      "Epoch 5726/10000\n",
      "2700/2700 [==============================] - 0s 28us/step - loss: 1.2803e-04 - mean_absolute_error: 0.0064\n",
      "\n",
      "Epoch 05726: loss improved from 0.00013 to 0.00013, saving model to keras/best_baseline_weights.hdf5\n",
      "Epoch 5727/10000\n",
      "2700/2700 [==============================] - 0s 29us/step - loss: 1.3181e-04 - mean_absolute_error: 0.0067\n",
      "\n",
      "Epoch 05727: loss did not improve\n",
      "Epoch 5728/10000\n",
      "2700/2700 [==============================] - 0s 28us/step - loss: 1.3181e-04 - mean_absolute_error: 0.0066\n",
      "\n",
      "Epoch 05728: loss did not improve\n",
      "Epoch 5729/10000\n",
      "2700/2700 [==============================] - 0s 29us/step - loss: 1.3044e-04 - mean_absolute_error: 0.0065\n",
      "\n",
      "Epoch 05729: loss did not improve\n",
      "Epoch 5730/10000\n",
      "2700/2700 [==============================] - 0s 29us/step - loss: 1.2945e-04 - mean_absolute_error: 0.0065\n",
      "\n",
      "Epoch 05730: loss did not improve\n",
      "Epoch 5731/10000\n",
      "2700/2700 [==============================] - 0s 30us/step - loss: 1.2983e-04 - mean_absolute_error: 0.0065\n",
      "\n",
      "Epoch 05731: loss did not improve\n",
      "Epoch 5732/10000\n",
      "2700/2700 [==============================] - 0s 28us/step - loss: 1.3144e-04 - mean_absolute_error: 0.0066\n",
      "\n",
      "Epoch 05732: loss did not improve\n",
      "Epoch 5733/10000\n",
      "2700/2700 [==============================] - 0s 27us/step - loss: 1.3647e-04 - mean_absolute_error: 0.0070\n",
      "\n",
      "Epoch 05733: loss did not improve\n",
      "Epoch 5734/10000\n",
      "2700/2700 [==============================] - 0s 29us/step - loss: 1.2981e-04 - mean_absolute_error: 0.0065\n",
      "\n",
      "Epoch 05734: loss did not improve\n",
      "Epoch 5735/10000\n",
      "2700/2700 [==============================] - 0s 29us/step - loss: 1.3157e-04 - mean_absolute_error: 0.0068\n",
      "\n",
      "Epoch 05735: loss did not improve\n",
      "Epoch 5736/10000\n",
      "2700/2700 [==============================] - 0s 30us/step - loss: 1.3163e-04 - mean_absolute_error: 0.0067\n",
      "\n",
      "Epoch 05736: loss did not improve\n",
      "Epoch 5737/10000\n",
      "2700/2700 [==============================] - 0s 31us/step - loss: 1.3022e-04 - mean_absolute_error: 0.0065\n",
      "\n",
      "Epoch 05737: loss did not improve\n",
      "Epoch 5738/10000\n",
      "2700/2700 [==============================] - 0s 27us/step - loss: 1.3002e-04 - mean_absolute_error: 0.0066\n",
      "\n",
      "Epoch 05738: loss did not improve\n",
      "Epoch 5739/10000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2700/2700 [==============================] - 0s 29us/step - loss: 1.3297e-04 - mean_absolute_error: 0.0068\n",
      "\n",
      "Epoch 05739: loss did not improve\n",
      "Epoch 5740/10000\n",
      "2700/2700 [==============================] - 0s 29us/step - loss: 1.3093e-04 - mean_absolute_error: 0.0065\n",
      "\n",
      "Epoch 05740: loss did not improve\n",
      "Epoch 5741/10000\n",
      "2700/2700 [==============================] - 0s 30us/step - loss: 1.2841e-04 - mean_absolute_error: 0.0064\n",
      "\n",
      "Epoch 05741: loss did not improve\n",
      "Epoch 5742/10000\n",
      "2700/2700 [==============================] - 0s 27us/step - loss: 1.3090e-04 - mean_absolute_error: 0.0066\n",
      "\n",
      "Epoch 05742: loss did not improve\n",
      "Epoch 5743/10000\n",
      "2700/2700 [==============================] - 0s 27us/step - loss: 1.3073e-04 - mean_absolute_error: 0.0066\n",
      "\n",
      "Epoch 05743: loss did not improve\n",
      "Epoch 5744/10000\n",
      "2700/2700 [==============================] - 0s 40us/step - loss: 1.3085e-04 - mean_absolute_error: 0.0066\n",
      "\n",
      "Epoch 05744: loss did not improve\n",
      "Epoch 5745/10000\n",
      "2700/2700 [==============================] - 0s 29us/step - loss: 1.3178e-04 - mean_absolute_error: 0.0066\n",
      "\n",
      "Epoch 05745: loss did not improve\n",
      "Epoch 5746/10000\n",
      "2700/2700 [==============================] - 0s 29us/step - loss: 1.2942e-04 - mean_absolute_error: 0.0064\n",
      "\n",
      "Epoch 05746: loss did not improve\n",
      "Epoch 5747/10000\n",
      "2700/2700 [==============================] - 0s 30us/step - loss: 1.2899e-04 - mean_absolute_error: 0.0064\n",
      "\n",
      "Epoch 05747: loss did not improve\n",
      "Epoch 5748/10000\n",
      "2700/2700 [==============================] - 0s 30us/step - loss: 1.3218e-04 - mean_absolute_error: 0.0066\n",
      "\n",
      "Epoch 05748: loss did not improve\n",
      "Epoch 5749/10000\n",
      "2700/2700 [==============================] - 0s 26us/step - loss: 1.2956e-04 - mean_absolute_error: 0.0065\n",
      "\n",
      "Epoch 05749: loss did not improve\n",
      "Epoch 5750/10000\n",
      "2700/2700 [==============================] - 0s 28us/step - loss: 1.2942e-04 - mean_absolute_error: 0.0066\n",
      "\n",
      "Epoch 05750: loss did not improve\n",
      "Epoch 5751/10000\n",
      "2700/2700 [==============================] - 0s 30us/step - loss: 1.3033e-04 - mean_absolute_error: 0.0066\n",
      "\n",
      "Epoch 05751: loss did not improve\n",
      "Epoch 5752/10000\n",
      "2700/2700 [==============================] - 0s 28us/step - loss: 1.2852e-04 - mean_absolute_error: 0.0064\n",
      "\n",
      "Epoch 05752: loss did not improve\n",
      "Epoch 5753/10000\n",
      "2700/2700 [==============================] - 0s 27us/step - loss: 1.3150e-04 - mean_absolute_error: 0.0068\n",
      "\n",
      "Epoch 05753: loss did not improve\n",
      "Epoch 5754/10000\n",
      "2700/2700 [==============================] - 0s 28us/step - loss: 1.2982e-04 - mean_absolute_error: 0.0066\n",
      "\n",
      "Epoch 05754: loss did not improve\n",
      "Epoch 5755/10000\n",
      "2700/2700 [==============================] - 0s 29us/step - loss: 1.3249e-04 - mean_absolute_error: 0.0068\n",
      "\n",
      "Epoch 05755: loss did not improve\n",
      "Epoch 5756/10000\n",
      "2700/2700 [==============================] - 0s 28us/step - loss: 1.3502e-04 - mean_absolute_error: 0.0070\n",
      "\n",
      "Epoch 05756: loss did not improve\n",
      "Epoch 5757/10000\n",
      "2700/2700 [==============================] - 0s 28us/step - loss: 1.2973e-04 - mean_absolute_error: 0.0065\n",
      "\n",
      "Epoch 05757: loss did not improve\n",
      "Epoch 5758/10000\n",
      "2700/2700 [==============================] - 0s 30us/step - loss: 1.3548e-04 - mean_absolute_error: 0.0071\n",
      "\n",
      "Epoch 05758: loss did not improve\n",
      "Epoch 5759/10000\n",
      "2700/2700 [==============================] - 0s 28us/step - loss: 1.3263e-04 - mean_absolute_error: 0.0068\n",
      "\n",
      "Epoch 05759: loss did not improve\n",
      "Epoch 5760/10000\n",
      "2700/2700 [==============================] - 0s 27us/step - loss: 1.3155e-04 - mean_absolute_error: 0.0067\n",
      "\n",
      "Epoch 05760: loss did not improve\n",
      "Epoch 5761/10000\n",
      "2700/2700 [==============================] - 0s 26us/step - loss: 1.2923e-04 - mean_absolute_error: 0.0065\n",
      "\n",
      "Epoch 05761: loss did not improve\n",
      "Epoch 5762/10000\n",
      "2700/2700 [==============================] - 0s 31us/step - loss: 1.3162e-04 - mean_absolute_error: 0.0067\n",
      "\n",
      "Epoch 05762: loss did not improve\n",
      "Epoch 5763/10000\n",
      "2700/2700 [==============================] - 0s 29us/step - loss: 1.3061e-04 - mean_absolute_error: 0.0066\n",
      "\n",
      "Epoch 05763: loss did not improve\n",
      "Epoch 5764/10000\n",
      "2700/2700 [==============================] - 0s 28us/step - loss: 1.3110e-04 - mean_absolute_error: 0.0067\n",
      "\n",
      "Epoch 05764: loss did not improve\n",
      "Epoch 5765/10000\n",
      "2700/2700 [==============================] - 0s 26us/step - loss: 1.3007e-04 - mean_absolute_error: 0.0066\n",
      "\n",
      "Epoch 05765: loss did not improve\n",
      "Epoch 5766/10000\n",
      "2700/2700 [==============================] - 0s 27us/step - loss: 1.3660e-04 - mean_absolute_error: 0.0070\n",
      "\n",
      "Epoch 05766: loss did not improve\n",
      "Epoch 5767/10000\n",
      "2700/2700 [==============================] - 0s 31us/step - loss: 1.3023e-04 - mean_absolute_error: 0.0065\n",
      "\n",
      "Epoch 05767: loss did not improve\n",
      "Epoch 5768/10000\n",
      "2700/2700 [==============================] - 0s 30us/step - loss: 1.3247e-04 - mean_absolute_error: 0.0067\n",
      "\n",
      "Epoch 05768: loss did not improve\n",
      "Epoch 5769/10000\n",
      "2700/2700 [==============================] - 0s 30us/step - loss: 1.2981e-04 - mean_absolute_error: 0.0065\n",
      "\n",
      "Epoch 05769: loss did not improve\n",
      "Epoch 5770/10000\n",
      "2700/2700 [==============================] - 0s 27us/step - loss: 1.3312e-04 - mean_absolute_error: 0.0067\n",
      "\n",
      "Epoch 05770: loss did not improve\n",
      "Epoch 5771/10000\n",
      "2700/2700 [==============================] - 0s 28us/step - loss: 1.3342e-04 - mean_absolute_error: 0.0067\n",
      "\n",
      "Epoch 05771: loss did not improve\n",
      "Epoch 5772/10000\n",
      "2700/2700 [==============================] - 0s 31us/step - loss: 1.3170e-04 - mean_absolute_error: 0.0067\n",
      "\n",
      "Epoch 05772: loss did not improve\n",
      "Epoch 5773/10000\n",
      "2700/2700 [==============================] - 0s 29us/step - loss: 1.3453e-04 - mean_absolute_error: 0.0070\n",
      "\n",
      "Epoch 05773: loss did not improve\n",
      "Epoch 5774/10000\n",
      "2700/2700 [==============================] - 0s 29us/step - loss: 1.2853e-04 - mean_absolute_error: 0.0064\n",
      "\n",
      "Epoch 05774: loss did not improve\n",
      "Epoch 5775/10000\n",
      "2700/2700 [==============================] - 0s 26us/step - loss: 1.2791e-04 - mean_absolute_error: 0.0064\n",
      "\n",
      "Epoch 05775: loss improved from 0.00013 to 0.00013, saving model to keras/best_baseline_weights.hdf5\n",
      "Epoch 5776/10000\n",
      "2700/2700 [==============================] - 0s 29us/step - loss: 1.3058e-04 - mean_absolute_error: 0.0067\n",
      "\n",
      "Epoch 05776: loss did not improve\n",
      "Epoch 5777/10000\n",
      "2700/2700 [==============================] - 0s 27us/step - loss: 1.3053e-04 - mean_absolute_error: 0.0065\n",
      "\n",
      "Epoch 05777: loss did not improve\n",
      "Epoch 5778/10000\n",
      "2700/2700 [==============================] - 0s 30us/step - loss: 1.3076e-04 - mean_absolute_error: 0.0067\n",
      "\n",
      "Epoch 05778: loss did not improve\n",
      "Epoch 5779/10000\n",
      "2700/2700 [==============================] - 0s 29us/step - loss: 1.2882e-04 - mean_absolute_error: 0.0065\n",
      "\n",
      "Epoch 05779: loss did not improve\n",
      "Epoch 5780/10000\n",
      "2700/2700 [==============================] - 0s 26us/step - loss: 1.3212e-04 - mean_absolute_error: 0.0068\n",
      "\n",
      "Epoch 05780: loss did not improve\n",
      "Epoch 5781/10000\n",
      "2700/2700 [==============================] - 0s 28us/step - loss: 1.2960e-04 - mean_absolute_error: 0.0066\n",
      "\n",
      "Epoch 05781: loss did not improve\n",
      "Epoch 5782/10000\n",
      "2700/2700 [==============================] - 0s 28us/step - loss: 1.3152e-04 - mean_absolute_error: 0.0067\n",
      "\n",
      "Epoch 05782: loss did not improve\n",
      "Epoch 5783/10000\n",
      "2700/2700 [==============================] - 0s 26us/step - loss: 1.3228e-04 - mean_absolute_error: 0.0068\n",
      "\n",
      "Epoch 05783: loss did not improve\n",
      "Epoch 5784/10000\n",
      "2700/2700 [==============================] - 0s 28us/step - loss: 1.3078e-04 - mean_absolute_error: 0.0066\n",
      "\n",
      "Epoch 05784: loss did not improve\n",
      "Epoch 5785/10000\n",
      "2700/2700 [==============================] - 0s 29us/step - loss: 1.3351e-04 - mean_absolute_error: 0.0068\n",
      "\n",
      "Epoch 05785: loss did not improve\n",
      "Epoch 5786/10000\n",
      "2700/2700 [==============================] - 0s 29us/step - loss: 1.2999e-04 - mean_absolute_error: 0.0066\n",
      "\n",
      "Epoch 05786: loss did not improve\n",
      "Epoch 5787/10000\n",
      "2700/2700 [==============================] - 0s 29us/step - loss: 1.2824e-04 - mean_absolute_error: 0.0064\n",
      "\n",
      "Epoch 05787: loss did not improve\n",
      "Epoch 5788/10000\n",
      "2700/2700 [==============================] - 0s 28us/step - loss: 1.3046e-04 - mean_absolute_error: 0.0065\n",
      "\n",
      "Epoch 05788: loss did not improve\n",
      "Epoch 5789/10000\n",
      "2700/2700 [==============================] - 0s 30us/step - loss: 1.3151e-04 - mean_absolute_error: 0.0067\n",
      "\n",
      "Epoch 05789: loss did not improve\n",
      "Epoch 5790/10000\n",
      "2700/2700 [==============================] - 0s 30us/step - loss: 1.3182e-04 - mean_absolute_error: 0.0066\n",
      "\n",
      "Epoch 05790: loss did not improve\n",
      "Epoch 5791/10000\n",
      "2700/2700 [==============================] - 0s 28us/step - loss: 1.2866e-04 - mean_absolute_error: 0.0065\n",
      "\n",
      "Epoch 05791: loss did not improve\n",
      "Epoch 5792/10000\n",
      "2700/2700 [==============================] - 0s 27us/step - loss: 1.3115e-04 - mean_absolute_error: 0.0068\n",
      "\n",
      "Epoch 05792: loss did not improve\n",
      "Epoch 5793/10000\n",
      "2700/2700 [==============================] - 0s 27us/step - loss: 1.3079e-04 - mean_absolute_error: 0.0065\n",
      "\n",
      "Epoch 05793: loss did not improve\n",
      "Epoch 5794/10000\n",
      "2700/2700 [==============================] - 0s 29us/step - loss: 1.3090e-04 - mean_absolute_error: 0.0066\n",
      "\n",
      "Epoch 05794: loss did not improve\n",
      "Epoch 5795/10000\n",
      "2700/2700 [==============================] - 0s 31us/step - loss: 1.3012e-04 - mean_absolute_error: 0.0065\n",
      "\n",
      "Epoch 05795: loss did not improve\n",
      "Epoch 5796/10000\n",
      "2700/2700 [==============================] - 0s 29us/step - loss: 1.2939e-04 - mean_absolute_error: 0.0065\n",
      "\n",
      "Epoch 05796: loss did not improve\n",
      "Epoch 5797/10000\n",
      "2700/2700 [==============================] - 0s 27us/step - loss: 1.3249e-04 - mean_absolute_error: 0.0067\n",
      "\n",
      "Epoch 05797: loss did not improve\n",
      "Epoch 5798/10000\n",
      "2700/2700 [==============================] - 0s 29us/step - loss: 1.2948e-04 - mean_absolute_error: 0.0065\n",
      "\n",
      "Epoch 05798: loss did not improve\n",
      "Epoch 5799/10000\n",
      "2700/2700 [==============================] - 0s 30us/step - loss: 1.2954e-04 - mean_absolute_error: 0.0064\n",
      "\n",
      "Epoch 05799: loss did not improve\n",
      "Epoch 5800/10000\n",
      "2700/2700 [==============================] - 0s 28us/step - loss: 1.3549e-04 - mean_absolute_error: 0.0069\n",
      "\n",
      "Epoch 05800: loss did not improve\n",
      "Epoch 5801/10000\n",
      "2700/2700 [==============================] - 0s 25us/step - loss: 1.3101e-04 - mean_absolute_error: 0.0067\n",
      "\n",
      "Epoch 05801: loss did not improve\n",
      "Epoch 5802/10000\n",
      "2700/2700 [==============================] - 0s 27us/step - loss: 1.2930e-04 - mean_absolute_error: 0.0065\n",
      "\n",
      "Epoch 05802: loss did not improve\n",
      "Epoch 5803/10000\n",
      "2700/2700 [==============================] - 0s 29us/step - loss: 1.2891e-04 - mean_absolute_error: 0.0064\n",
      "\n",
      "Epoch 05803: loss did not improve\n",
      "Epoch 5804/10000\n",
      "2700/2700 [==============================] - 0s 27us/step - loss: 1.3220e-04 - mean_absolute_error: 0.0066\n",
      "\n",
      "Epoch 05804: loss did not improve\n",
      "Epoch 5805/10000\n",
      "2700/2700 [==============================] - 0s 26us/step - loss: 1.2924e-04 - mean_absolute_error: 0.0065\n",
      "\n",
      "Epoch 05805: loss did not improve\n",
      "Epoch 5806/10000\n",
      "2700/2700 [==============================] - 0s 29us/step - loss: 1.3004e-04 - mean_absolute_error: 0.0066\n",
      "\n",
      "Epoch 05806: loss did not improve\n",
      "Epoch 5807/10000\n",
      "2700/2700 [==============================] - 0s 28us/step - loss: 1.2912e-04 - mean_absolute_error: 0.0065\n",
      "\n",
      "Epoch 05807: loss did not improve\n",
      "Epoch 5808/10000\n",
      "2700/2700 [==============================] - 0s 30us/step - loss: 1.3141e-04 - mean_absolute_error: 0.0066\n",
      "\n",
      "Epoch 05808: loss did not improve\n",
      "Epoch 5809/10000\n",
      "2700/2700 [==============================] - 0s 29us/step - loss: 1.3174e-04 - mean_absolute_error: 0.0067\n",
      "\n",
      "Epoch 05809: loss did not improve\n",
      "Epoch 5810/10000\n",
      "2700/2700 [==============================] - 0s 26us/step - loss: 1.3404e-04 - mean_absolute_error: 0.0069\n",
      "\n",
      "Epoch 05810: loss did not improve\n",
      "Epoch 5811/10000\n",
      "2700/2700 [==============================] - 0s 28us/step - loss: 1.3339e-04 - mean_absolute_error: 0.0068\n",
      "\n",
      "Epoch 05811: loss did not improve\n",
      "Epoch 5812/10000\n",
      "2700/2700 [==============================] - 0s 28us/step - loss: 1.2876e-04 - mean_absolute_error: 0.0065\n",
      "\n",
      "Epoch 05812: loss did not improve\n",
      "Epoch 5813/10000\n",
      "2700/2700 [==============================] - 0s 30us/step - loss: 1.3010e-04 - mean_absolute_error: 0.0066\n",
      "\n",
      "Epoch 05813: loss did not improve\n",
      "Epoch 5814/10000\n",
      "2700/2700 [==============================] - 0s 28us/step - loss: 1.3060e-04 - mean_absolute_error: 0.0066\n",
      "\n",
      "Epoch 05814: loss did not improve\n",
      "Epoch 5815/10000\n",
      "2700/2700 [==============================] - 0s 29us/step - loss: 1.2951e-04 - mean_absolute_error: 0.0067\n",
      "\n",
      "Epoch 05815: loss did not improve\n",
      "Epoch 5816/10000\n",
      "2700/2700 [==============================] - 0s 28us/step - loss: 1.3013e-04 - mean_absolute_error: 0.0065\n",
      "\n",
      "Epoch 05816: loss did not improve\n",
      "Epoch 5817/10000\n",
      "2700/2700 [==============================] - 0s 26us/step - loss: 1.3048e-04 - mean_absolute_error: 0.0066\n",
      "\n",
      "Epoch 05817: loss did not improve\n",
      "Epoch 5818/10000\n",
      "2700/2700 [==============================] - 0s 28us/step - loss: 1.2906e-04 - mean_absolute_error: 0.0065\n",
      "\n",
      "Epoch 05818: loss did not improve\n",
      "Epoch 5819/10000\n",
      "2700/2700 [==============================] - 0s 31us/step - loss: 1.3015e-04 - mean_absolute_error: 0.0065\n",
      "\n",
      "Epoch 05819: loss did not improve\n",
      "Epoch 5820/10000\n",
      "2700/2700 [==============================] - 0s 30us/step - loss: 1.2980e-04 - mean_absolute_error: 0.0066\n",
      "\n",
      "Epoch 05820: loss did not improve\n",
      "Epoch 5821/10000\n",
      "2700/2700 [==============================] - 0s 27us/step - loss: 1.2922e-04 - mean_absolute_error: 0.0065\n",
      "\n",
      "Epoch 05821: loss did not improve\n",
      "Epoch 5822/10000\n",
      "2700/2700 [==============================] - 0s 30us/step - loss: 1.2874e-04 - mean_absolute_error: 0.0064\n",
      "\n",
      "Epoch 05822: loss did not improve\n",
      "Epoch 5823/10000\n",
      "2700/2700 [==============================] - 0s 25us/step - loss: 1.3093e-04 - mean_absolute_error: 0.0067\n",
      "\n",
      "Epoch 05823: loss did not improve\n",
      "Epoch 5824/10000\n",
      "2700/2700 [==============================] - 0s 29us/step - loss: 1.2944e-04 - mean_absolute_error: 0.0065\n",
      "\n",
      "Epoch 05824: loss did not improve\n",
      "Epoch 5825/10000\n",
      "2700/2700 [==============================] - 0s 30us/step - loss: 1.3249e-04 - mean_absolute_error: 0.0068\n",
      "\n",
      "Epoch 05825: loss did not improve\n",
      "Epoch 5826/10000\n",
      "2700/2700 [==============================] - 0s 27us/step - loss: 1.2921e-04 - mean_absolute_error: 0.0064\n",
      "\n",
      "Epoch 05826: loss did not improve\n",
      "Epoch 5827/10000\n",
      "2700/2700 [==============================] - 0s 28us/step - loss: 1.2877e-04 - mean_absolute_error: 0.0065\n",
      "\n",
      "Epoch 05827: loss did not improve\n",
      "Epoch 5828/10000\n",
      "2700/2700 [==============================] - 0s 28us/step - loss: 1.2877e-04 - mean_absolute_error: 0.0065\n",
      "\n",
      "Epoch 05828: loss did not improve\n",
      "Epoch 5829/10000\n",
      "2700/2700 [==============================] - 0s 28us/step - loss: 1.2932e-04 - mean_absolute_error: 0.0065\n",
      "\n",
      "Epoch 05829: loss did not improve\n",
      "Epoch 5830/10000\n",
      "2700/2700 [==============================] - 0s 30us/step - loss: 1.2841e-04 - mean_absolute_error: 0.0065\n",
      "\n",
      "Epoch 05830: loss did not improve\n",
      "Epoch 5831/10000\n",
      "2700/2700 [==============================] - 0s 31us/step - loss: 1.3105e-04 - mean_absolute_error: 0.0067\n",
      "\n",
      "Epoch 05831: loss did not improve\n",
      "Epoch 5832/10000\n",
      "2700/2700 [==============================] - 0s 29us/step - loss: 1.3086e-04 - mean_absolute_error: 0.0066\n",
      "\n",
      "Epoch 05832: loss did not improve\n",
      "Epoch 5833/10000\n",
      "2700/2700 [==============================] - 0s 30us/step - loss: 1.3168e-04 - mean_absolute_error: 0.0067\n",
      "\n",
      "Epoch 05833: loss did not improve\n",
      "Epoch 5834/10000\n",
      "2700/2700 [==============================] - 0s 29us/step - loss: 1.2869e-04 - mean_absolute_error: 0.0065\n",
      "\n",
      "Epoch 05834: loss did not improve\n",
      "Epoch 5835/10000\n",
      "2700/2700 [==============================] - 0s 28us/step - loss: 1.2898e-04 - mean_absolute_error: 0.0065\n",
      "\n",
      "Epoch 05835: loss did not improve\n",
      "Epoch 5836/10000\n",
      "2700/2700 [==============================] - 0s 29us/step - loss: 1.2967e-04 - mean_absolute_error: 0.0066\n",
      "\n",
      "Epoch 05836: loss did not improve\n",
      "Epoch 5837/10000\n",
      "2700/2700 [==============================] - 0s 27us/step - loss: 1.3371e-04 - mean_absolute_error: 0.0070\n",
      "\n",
      "Epoch 05837: loss did not improve\n",
      "Epoch 5838/10000\n",
      "2700/2700 [==============================] - 0s 29us/step - loss: 1.3018e-04 - mean_absolute_error: 0.0067\n",
      "\n",
      "Epoch 05838: loss did not improve\n",
      "Epoch 5839/10000\n",
      "2700/2700 [==============================] - 0s 31us/step - loss: 1.3201e-04 - mean_absolute_error: 0.0067\n",
      "\n",
      "Epoch 05839: loss did not improve\n",
      "Epoch 5840/10000\n",
      "2700/2700 [==============================] - 0s 30us/step - loss: 1.2838e-04 - mean_absolute_error: 0.0064\n",
      "\n",
      "Epoch 05840: loss did not improve\n",
      "Epoch 5841/10000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2700/2700 [==============================] - 0s 28us/step - loss: 1.2975e-04 - mean_absolute_error: 0.0066\n",
      "\n",
      "Epoch 05841: loss did not improve\n",
      "Epoch 5842/10000\n",
      "2700/2700 [==============================] - 0s 29us/step - loss: 1.3027e-04 - mean_absolute_error: 0.0066\n",
      "\n",
      "Epoch 05842: loss did not improve\n",
      "Epoch 5843/10000\n",
      "2700/2700 [==============================] - 0s 28us/step - loss: 1.2929e-04 - mean_absolute_error: 0.0065\n",
      "\n",
      "Epoch 05843: loss did not improve\n",
      "Epoch 5844/10000\n",
      "2700/2700 [==============================] - 0s 27us/step - loss: 1.2789e-04 - mean_absolute_error: 0.0064\n",
      "\n",
      "Epoch 05844: loss improved from 0.00013 to 0.00013, saving model to keras/best_baseline_weights.hdf5\n",
      "Epoch 5845/10000\n",
      "2700/2700 [==============================] - 0s 29us/step - loss: 1.3081e-04 - mean_absolute_error: 0.0066\n",
      "\n",
      "Epoch 05845: loss did not improve\n",
      "Epoch 5846/10000\n",
      "2700/2700 [==============================] - 0s 31us/step - loss: 1.2910e-04 - mean_absolute_error: 0.0066\n",
      "\n",
      "Epoch 05846: loss did not improve\n",
      "Epoch 5847/10000\n",
      "2700/2700 [==============================] - 0s 28us/step - loss: 1.3065e-04 - mean_absolute_error: 0.0066\n",
      "\n",
      "Epoch 05847: loss did not improve\n",
      "Epoch 5848/10000\n",
      "2700/2700 [==============================] - 0s 34us/step - loss: 1.2929e-04 - mean_absolute_error: 0.0066\n",
      "\n",
      "Epoch 05848: loss did not improve\n",
      "Epoch 5849/10000\n",
      "2700/2700 [==============================] - 0s 28us/step - loss: 1.3168e-04 - mean_absolute_error: 0.0068\n",
      "\n",
      "Epoch 05849: loss did not improve\n",
      "Epoch 5850/10000\n",
      "2700/2700 [==============================] - 0s 30us/step - loss: 1.2987e-04 - mean_absolute_error: 0.0066\n",
      "\n",
      "Epoch 05850: loss did not improve\n",
      "Epoch 5851/10000\n",
      "2700/2700 [==============================] - 0s 29us/step - loss: 1.3034e-04 - mean_absolute_error: 0.0067\n",
      "\n",
      "Epoch 05851: loss did not improve\n",
      "Epoch 5852/10000\n",
      "2700/2700 [==============================] - 0s 27us/step - loss: 1.3017e-04 - mean_absolute_error: 0.0067\n",
      "\n",
      "Epoch 05852: loss did not improve\n",
      "Epoch 5853/10000\n",
      "2700/2700 [==============================] - 0s 27us/step - loss: 1.3220e-04 - mean_absolute_error: 0.0068\n",
      "\n",
      "Epoch 05853: loss did not improve\n",
      "Epoch 5854/10000\n",
      "2700/2700 [==============================] - 0s 29us/step - loss: 1.2873e-04 - mean_absolute_error: 0.0064\n",
      "\n",
      "Epoch 05854: loss did not improve\n",
      "Epoch 5855/10000\n",
      "2700/2700 [==============================] - 0s 28us/step - loss: 1.2891e-04 - mean_absolute_error: 0.0065\n",
      "\n",
      "Epoch 05855: loss did not improve\n",
      "Epoch 5856/10000\n",
      "2700/2700 [==============================] - 0s 33us/step - loss: 1.3122e-04 - mean_absolute_error: 0.0067\n",
      "\n",
      "Epoch 05856: loss did not improve\n",
      "Epoch 5857/10000\n",
      "2700/2700 [==============================] - 0s 26us/step - loss: 1.3644e-04 - mean_absolute_error: 0.0073\n",
      "\n",
      "Epoch 05857: loss did not improve\n",
      "Epoch 5858/10000\n",
      "2700/2700 [==============================] - 0s 29us/step - loss: 1.3064e-04 - mean_absolute_error: 0.0066\n",
      "\n",
      "Epoch 05858: loss did not improve\n",
      "Epoch 5859/10000\n",
      "2700/2700 [==============================] - 0s 28us/step - loss: 1.3045e-04 - mean_absolute_error: 0.0066\n",
      "\n",
      "Epoch 05859: loss did not improve\n",
      "Epoch 5860/10000\n",
      "2700/2700 [==============================] - 0s 30us/step - loss: 1.3036e-04 - mean_absolute_error: 0.0066\n",
      "\n",
      "Epoch 05860: loss did not improve\n",
      "Epoch 5861/10000\n",
      "2700/2700 [==============================] - 0s 25us/step - loss: 1.3028e-04 - mean_absolute_error: 0.0066\n",
      "\n",
      "Epoch 05861: loss did not improve\n",
      "Epoch 5862/10000\n",
      "2700/2700 [==============================] - 0s 31us/step - loss: 1.2969e-04 - mean_absolute_error: 0.0066\n",
      "\n",
      "Epoch 05862: loss did not improve\n",
      "Epoch 5863/10000\n",
      "2700/2700 [==============================] - 0s 28us/step - loss: 1.3017e-04 - mean_absolute_error: 0.0067\n",
      "\n",
      "Epoch 05863: loss did not improve\n",
      "Epoch 5864/10000\n",
      "2700/2700 [==============================] - 0s 29us/step - loss: 1.3381e-04 - mean_absolute_error: 0.0068\n",
      "\n",
      "Epoch 05864: loss did not improve\n",
      "Epoch 5865/10000\n",
      "2700/2700 [==============================] - 0s 27us/step - loss: 1.3131e-04 - mean_absolute_error: 0.0067\n",
      "\n",
      "Epoch 05865: loss did not improve\n",
      "Epoch 5866/10000\n",
      "2700/2700 [==============================] - 0s 31us/step - loss: 1.3529e-04 - mean_absolute_error: 0.0070\n",
      "\n",
      "Epoch 05866: loss did not improve\n",
      "Epoch 5867/10000\n",
      "2700/2700 [==============================] - 0s 30us/step - loss: 1.2943e-04 - mean_absolute_error: 0.0066\n",
      "\n",
      "Epoch 05867: loss did not improve\n",
      "Epoch 5868/10000\n",
      "2700/2700 [==============================] - 0s 29us/step - loss: 1.2816e-04 - mean_absolute_error: 0.0065\n",
      "\n",
      "Epoch 05868: loss did not improve\n",
      "Epoch 5869/10000\n",
      "2700/2700 [==============================] - 0s 27us/step - loss: 1.2900e-04 - mean_absolute_error: 0.0065\n",
      "\n",
      "Epoch 05869: loss did not improve\n",
      "Epoch 5870/10000\n",
      "2700/2700 [==============================] - 0s 26us/step - loss: 1.2798e-04 - mean_absolute_error: 0.0065\n",
      "\n",
      "Epoch 05870: loss did not improve\n",
      "Epoch 5871/10000\n",
      "2700/2700 [==============================] - 0s 29us/step - loss: 1.3271e-04 - mean_absolute_error: 0.0068\n",
      "\n",
      "Epoch 05871: loss did not improve\n",
      "Epoch 5872/10000\n",
      "2700/2700 [==============================] - 0s 30us/step - loss: 1.3332e-04 - mean_absolute_error: 0.0068\n",
      "\n",
      "Epoch 05872: loss did not improve\n",
      "Epoch 5873/10000\n",
      "2700/2700 [==============================] - 0s 28us/step - loss: 1.2965e-04 - mean_absolute_error: 0.0066\n",
      "\n",
      "Epoch 05873: loss did not improve\n",
      "Epoch 5874/10000\n",
      "2700/2700 [==============================] - 0s 27us/step - loss: 1.2910e-04 - mean_absolute_error: 0.0065\n",
      "\n",
      "Epoch 05874: loss did not improve\n",
      "Epoch 5875/10000\n",
      "2700/2700 [==============================] - 0s 29us/step - loss: 1.3231e-04 - mean_absolute_error: 0.0067\n",
      "\n",
      "Epoch 05875: loss did not improve\n",
      "Epoch 5876/10000\n",
      "2700/2700 [==============================] - 0s 28us/step - loss: 1.3140e-04 - mean_absolute_error: 0.0067\n",
      "\n",
      "Epoch 05876: loss did not improve\n",
      "Epoch 5877/10000\n",
      "2700/2700 [==============================] - 0s 30us/step - loss: 1.3430e-04 - mean_absolute_error: 0.0069\n",
      "\n",
      "Epoch 05877: loss did not improve\n",
      "Epoch 5878/10000\n",
      "2700/2700 [==============================] - 0s 29us/step - loss: 1.2907e-04 - mean_absolute_error: 0.0065\n",
      "\n",
      "Epoch 05878: loss did not improve\n",
      "Epoch 5879/10000\n",
      "2700/2700 [==============================] - 0s 30us/step - loss: 1.2911e-04 - mean_absolute_error: 0.0065\n",
      "\n",
      "Epoch 05879: loss did not improve\n",
      "Epoch 5880/10000\n",
      "2700/2700 [==============================] - 0s 29us/step - loss: 1.3165e-04 - mean_absolute_error: 0.0068\n",
      "\n",
      "Epoch 05880: loss did not improve\n",
      "Epoch 5881/10000\n",
      "2700/2700 [==============================] - 0s 29us/step - loss: 1.2954e-04 - mean_absolute_error: 0.0066\n",
      "\n",
      "Epoch 05881: loss did not improve\n",
      "Epoch 5882/10000\n",
      "2700/2700 [==============================] - 0s 28us/step - loss: 1.3126e-04 - mean_absolute_error: 0.0067\n",
      "\n",
      "Epoch 05882: loss did not improve\n",
      "Epoch 5883/10000\n",
      "2700/2700 [==============================] - 0s 27us/step - loss: 1.2853e-04 - mean_absolute_error: 0.0066\n",
      "\n",
      "Epoch 05883: loss did not improve\n",
      "Epoch 5884/10000\n",
      "2700/2700 [==============================] - 0s 29us/step - loss: 1.2993e-04 - mean_absolute_error: 0.0066\n",
      "\n",
      "Epoch 05884: loss did not improve\n",
      "Epoch 5885/10000\n",
      "2700/2700 [==============================] - 0s 30us/step - loss: 1.2750e-04 - mean_absolute_error: 0.0064\n",
      "\n",
      "Epoch 05885: loss improved from 0.00013 to 0.00013, saving model to keras/best_baseline_weights.hdf5\n",
      "Epoch 5886/10000\n",
      "2700/2700 [==============================] - 0s 31us/step - loss: 1.3025e-04 - mean_absolute_error: 0.0066\n",
      "\n",
      "Epoch 05886: loss did not improve\n",
      "Epoch 5887/10000\n",
      "2700/2700 [==============================] - 0s 28us/step - loss: 1.3004e-04 - mean_absolute_error: 0.0067\n",
      "\n",
      "Epoch 05887: loss did not improve\n",
      "Epoch 5888/10000\n",
      "2700/2700 [==============================] - 0s 29us/step - loss: 1.2869e-04 - mean_absolute_error: 0.0065\n",
      "\n",
      "Epoch 05888: loss did not improve\n",
      "Epoch 5889/10000\n",
      "2700/2700 [==============================] - 0s 30us/step - loss: 1.3176e-04 - mean_absolute_error: 0.0067\n",
      "\n",
      "Epoch 05889: loss did not improve\n",
      "Epoch 5890/10000\n",
      "2700/2700 [==============================] - 0s 30us/step - loss: 1.3230e-04 - mean_absolute_error: 0.0067\n",
      "\n",
      "Epoch 05890: loss did not improve\n",
      "Epoch 5891/10000\n",
      "2700/2700 [==============================] - 0s 28us/step - loss: 1.2926e-04 - mean_absolute_error: 0.0066\n",
      "\n",
      "Epoch 05891: loss did not improve\n",
      "Epoch 5892/10000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2700/2700 [==============================] - 0s 30us/step - loss: 1.2933e-04 - mean_absolute_error: 0.0065\n",
      "\n",
      "Epoch 05892: loss did not improve\n",
      "Epoch 5893/10000\n",
      "2700/2700 [==============================] - 0s 26us/step - loss: 1.2810e-04 - mean_absolute_error: 0.0064\n",
      "\n",
      "Epoch 05893: loss did not improve\n",
      "Epoch 5894/10000\n",
      "2700/2700 [==============================] - 0s 28us/step - loss: 1.2737e-04 - mean_absolute_error: 0.0064\n",
      "\n",
      "Epoch 05894: loss improved from 0.00013 to 0.00013, saving model to keras/best_baseline_weights.hdf5\n",
      "Epoch 5895/10000\n",
      "2700/2700 [==============================] - 0s 26us/step - loss: 1.2986e-04 - mean_absolute_error: 0.0066\n",
      "\n",
      "Epoch 05895: loss did not improve\n",
      "Epoch 5896/10000\n",
      "2700/2700 [==============================] - 0s 28us/step - loss: 1.2792e-04 - mean_absolute_error: 0.0064\n",
      "\n",
      "Epoch 05896: loss did not improve\n",
      "Epoch 5897/10000\n",
      "2700/2700 [==============================] - 0s 29us/step - loss: 1.2883e-04 - mean_absolute_error: 0.0065\n",
      "\n",
      "Epoch 05897: loss did not improve\n",
      "Epoch 5898/10000\n",
      "2700/2700 [==============================] - 0s 28us/step - loss: 1.2949e-04 - mean_absolute_error: 0.0066\n",
      "\n",
      "Epoch 05898: loss did not improve\n",
      "Epoch 5899/10000\n",
      "2700/2700 [==============================] - 0s 27us/step - loss: 1.3188e-04 - mean_absolute_error: 0.0067\n",
      "\n",
      "Epoch 05899: loss did not improve\n",
      "Epoch 5900/10000\n",
      "2700/2700 [==============================] - 0s 29us/step - loss: 1.3165e-04 - mean_absolute_error: 0.0068\n",
      "\n",
      "Epoch 05900: loss did not improve\n",
      "Epoch 5901/10000\n",
      "2700/2700 [==============================] - 0s 26us/step - loss: 1.3441e-04 - mean_absolute_error: 0.0069\n",
      "\n",
      "Epoch 05901: loss did not improve\n",
      "Epoch 5902/10000\n",
      "2700/2700 [==============================] - 0s 27us/step - loss: 1.2792e-04 - mean_absolute_error: 0.0064\n",
      "\n",
      "Epoch 05902: loss did not improve\n",
      "Epoch 5903/10000\n",
      "2700/2700 [==============================] - 0s 31us/step - loss: 1.2957e-04 - mean_absolute_error: 0.0066\n",
      "\n",
      "Epoch 05903: loss did not improve\n",
      "Epoch 5904/10000\n",
      "2700/2700 [==============================] - 0s 30us/step - loss: 1.2860e-04 - mean_absolute_error: 0.0064\n",
      "\n",
      "Epoch 05904: loss did not improve\n",
      "Epoch 5905/10000\n",
      "2700/2700 [==============================] - 0s 28us/step - loss: 1.2856e-04 - mean_absolute_error: 0.0065\n",
      "\n",
      "Epoch 05905: loss did not improve\n",
      "Epoch 5906/10000\n",
      "2700/2700 [==============================] - 0s 31us/step - loss: 1.2940e-04 - mean_absolute_error: 0.0065\n",
      "\n",
      "Epoch 05906: loss did not improve\n",
      "Epoch 5907/10000\n",
      "2700/2700 [==============================] - 0s 25us/step - loss: 1.2885e-04 - mean_absolute_error: 0.0065\n",
      "\n",
      "Epoch 05907: loss did not improve\n",
      "Epoch 5908/10000\n",
      "2700/2700 [==============================] - 0s 28us/step - loss: 1.2898e-04 - mean_absolute_error: 0.0065\n",
      "\n",
      "Epoch 05908: loss did not improve\n",
      "Epoch 5909/10000\n",
      "2700/2700 [==============================] - 0s 29us/step - loss: 1.2970e-04 - mean_absolute_error: 0.0065\n",
      "\n",
      "Epoch 05909: loss did not improve\n",
      "Epoch 5910/10000\n",
      "2700/2700 [==============================] - 0s 29us/step - loss: 1.3117e-04 - mean_absolute_error: 0.0067\n",
      "\n",
      "Epoch 05910: loss did not improve\n",
      "Epoch 5911/10000\n",
      "2700/2700 [==============================] - 0s 26us/step - loss: 1.3189e-04 - mean_absolute_error: 0.0067\n",
      "\n",
      "Epoch 05911: loss did not improve\n",
      "Epoch 5912/10000\n",
      "2700/2700 [==============================] - 0s 28us/step - loss: 1.3133e-04 - mean_absolute_error: 0.0067\n",
      "\n",
      "Epoch 05912: loss did not improve\n",
      "Epoch 5913/10000\n",
      "2700/2700 [==============================] - 0s 28us/step - loss: 1.3054e-04 - mean_absolute_error: 0.0066\n",
      "\n",
      "Epoch 05913: loss did not improve\n",
      "Epoch 5914/10000\n",
      "2700/2700 [==============================] - 0s 30us/step - loss: 1.2929e-04 - mean_absolute_error: 0.0065\n",
      "\n",
      "Epoch 05914: loss did not improve\n",
      "Epoch 5915/10000\n",
      "2700/2700 [==============================] - 0s 27us/step - loss: 1.2843e-04 - mean_absolute_error: 0.0064\n",
      "\n",
      "Epoch 05915: loss did not improve\n",
      "Epoch 5916/10000\n",
      "2700/2700 [==============================] - 0s 28us/step - loss: 1.3136e-04 - mean_absolute_error: 0.0067\n",
      "\n",
      "Epoch 05916: loss did not improve\n",
      "Epoch 5917/10000\n",
      "2700/2700 [==============================] - 0s 30us/step - loss: 1.3336e-04 - mean_absolute_error: 0.0070\n",
      "\n",
      "Epoch 05917: loss did not improve\n",
      "Epoch 5918/10000\n",
      "2700/2700 [==============================] - 0s 27us/step - loss: 1.2902e-04 - mean_absolute_error: 0.0067\n",
      "\n",
      "Epoch 05918: loss did not improve\n",
      "Epoch 5919/10000\n",
      "2700/2700 [==============================] - 0s 27us/step - loss: 1.2879e-04 - mean_absolute_error: 0.0065\n",
      "\n",
      "Epoch 05919: loss did not improve\n",
      "Epoch 5920/10000\n",
      "2700/2700 [==============================] - 0s 29us/step - loss: 1.2826e-04 - mean_absolute_error: 0.0065\n",
      "\n",
      "Epoch 05920: loss did not improve\n",
      "Epoch 5921/10000\n",
      "2700/2700 [==============================] - 0s 28us/step - loss: 1.2928e-04 - mean_absolute_error: 0.0065\n",
      "\n",
      "Epoch 05921: loss did not improve\n",
      "Epoch 5922/10000\n",
      "2700/2700 [==============================] - 0s 28us/step - loss: 1.3058e-04 - mean_absolute_error: 0.0068\n",
      "\n",
      "Epoch 05922: loss did not improve\n",
      "Epoch 5923/10000\n",
      "2700/2700 [==============================] - 0s 27us/step - loss: 1.2945e-04 - mean_absolute_error: 0.0067\n",
      "\n",
      "Epoch 05923: loss did not improve\n",
      "Epoch 5924/10000\n",
      "2700/2700 [==============================] - 0s 30us/step - loss: 1.2777e-04 - mean_absolute_error: 0.0065\n",
      "\n",
      "Epoch 05924: loss did not improve\n",
      "Epoch 5925/10000\n",
      "2700/2700 [==============================] - 0s 28us/step - loss: 1.2734e-04 - mean_absolute_error: 0.0064\n",
      "\n",
      "Epoch 05925: loss improved from 0.00013 to 0.00013, saving model to keras/best_baseline_weights.hdf5\n",
      "Epoch 5926/10000\n",
      "2700/2700 [==============================] - 0s 30us/step - loss: 1.2852e-04 - mean_absolute_error: 0.0066\n",
      "\n",
      "Epoch 05926: loss did not improve\n",
      "Epoch 5927/10000\n",
      "2700/2700 [==============================] - 0s 33us/step - loss: 1.2909e-04 - mean_absolute_error: 0.0065\n",
      "\n",
      "Epoch 05927: loss did not improve\n",
      "Epoch 5928/10000\n",
      "2700/2700 [==============================] - 0s 28us/step - loss: 1.3375e-04 - mean_absolute_error: 0.0070\n",
      "\n",
      "Epoch 05928: loss did not improve\n",
      "Epoch 5929/10000\n",
      "2700/2700 [==============================] - 0s 26us/step - loss: 1.3060e-04 - mean_absolute_error: 0.0067\n",
      "\n",
      "Epoch 05929: loss did not improve\n",
      "Epoch 5930/10000\n",
      "2700/2700 [==============================] - 0s 29us/step - loss: 1.3174e-04 - mean_absolute_error: 0.0067\n",
      "\n",
      "Epoch 05930: loss did not improve\n",
      "Epoch 5931/10000\n",
      "2700/2700 [==============================] - 0s 29us/step - loss: 1.2950e-04 - mean_absolute_error: 0.0066\n",
      "\n",
      "Epoch 05931: loss did not improve\n",
      "Epoch 5932/10000\n",
      "2700/2700 [==============================] - 0s 30us/step - loss: 1.2857e-04 - mean_absolute_error: 0.0066\n",
      "\n",
      "Epoch 05932: loss did not improve\n",
      "Epoch 5933/10000\n",
      "2700/2700 [==============================] - 0s 31us/step - loss: 1.2936e-04 - mean_absolute_error: 0.0065\n",
      "\n",
      "Epoch 05933: loss did not improve\n",
      "Epoch 5934/10000\n",
      "2700/2700 [==============================] - 0s 29us/step - loss: 1.3134e-04 - mean_absolute_error: 0.0068\n",
      "\n",
      "Epoch 05934: loss did not improve\n",
      "Epoch 5935/10000\n",
      "2700/2700 [==============================] - 0s 30us/step - loss: 1.2997e-04 - mean_absolute_error: 0.0067\n",
      "\n",
      "Epoch 05935: loss did not improve\n",
      "Epoch 5936/10000\n",
      "2700/2700 [==============================] - 0s 28us/step - loss: 1.3114e-04 - mean_absolute_error: 0.0067\n",
      "\n",
      "Epoch 05936: loss did not improve\n",
      "Epoch 5937/10000\n",
      "2700/2700 [==============================] - 0s 28us/step - loss: 1.3104e-04 - mean_absolute_error: 0.0067\n",
      "\n",
      "Epoch 05937: loss did not improve\n",
      "Epoch 5938/10000\n",
      "2700/2700 [==============================] - 0s 29us/step - loss: 1.2846e-04 - mean_absolute_error: 0.0065\n",
      "\n",
      "Epoch 05938: loss did not improve\n",
      "Epoch 5939/10000\n",
      "2700/2700 [==============================] - 0s 27us/step - loss: 1.3121e-04 - mean_absolute_error: 0.0067\n",
      "\n",
      "Epoch 05939: loss did not improve\n",
      "Epoch 5940/10000\n",
      "2700/2700 [==============================] - 0s 29us/step - loss: 1.2721e-04 - mean_absolute_error: 0.0064\n",
      "\n",
      "Epoch 05940: loss improved from 0.00013 to 0.00013, saving model to keras/best_baseline_weights.hdf5\n",
      "Epoch 5941/10000\n",
      "2700/2700 [==============================] - 0s 29us/step - loss: 1.2925e-04 - mean_absolute_error: 0.0066\n",
      "\n",
      "Epoch 05941: loss did not improve\n",
      "Epoch 5942/10000\n",
      "2700/2700 [==============================] - 0s 29us/step - loss: 1.2767e-04 - mean_absolute_error: 0.0064\n",
      "\n",
      "Epoch 05942: loss did not improve\n",
      "Epoch 5943/10000\n",
      "2700/2700 [==============================] - 0s 26us/step - loss: 1.2933e-04 - mean_absolute_error: 0.0066\n",
      "\n",
      "Epoch 05943: loss did not improve\n",
      "Epoch 5944/10000\n",
      "2700/2700 [==============================] - 0s 27us/step - loss: 1.2997e-04 - mean_absolute_error: 0.0067\n",
      "\n",
      "Epoch 05944: loss did not improve\n",
      "Epoch 5945/10000\n",
      "2700/2700 [==============================] - 0s 27us/step - loss: 1.3036e-04 - mean_absolute_error: 0.0066\n",
      "\n",
      "Epoch 05945: loss did not improve\n",
      "Epoch 5946/10000\n",
      "2700/2700 [==============================] - 0s 28us/step - loss: 1.2882e-04 - mean_absolute_error: 0.0065\n",
      "\n",
      "Epoch 05946: loss did not improve\n",
      "Epoch 5947/10000\n",
      "2700/2700 [==============================] - 0s 29us/step - loss: 1.2865e-04 - mean_absolute_error: 0.0064\n",
      "\n",
      "Epoch 05947: loss did not improve\n",
      "Epoch 5948/10000\n",
      "2700/2700 [==============================] - 0s 28us/step - loss: 1.2940e-04 - mean_absolute_error: 0.0066\n",
      "\n",
      "Epoch 05948: loss did not improve\n",
      "Epoch 5949/10000\n",
      "2700/2700 [==============================] - 0s 29us/step - loss: 1.2899e-04 - mean_absolute_error: 0.0066\n",
      "\n",
      "Epoch 05949: loss did not improve\n",
      "Epoch 5950/10000\n",
      "2700/2700 [==============================] - 0s 28us/step - loss: 1.2703e-04 - mean_absolute_error: 0.0064\n",
      "\n",
      "Epoch 05950: loss improved from 0.00013 to 0.00013, saving model to keras/best_baseline_weights.hdf5\n",
      "Epoch 5951/10000\n",
      "2700/2700 [==============================] - 0s 28us/step - loss: 1.2808e-04 - mean_absolute_error: 0.0066\n",
      "\n",
      "Epoch 05951: loss did not improve\n",
      "Epoch 5952/10000\n",
      "2700/2700 [==============================] - 0s 26us/step - loss: 1.3015e-04 - mean_absolute_error: 0.0068\n",
      "\n",
      "Epoch 05952: loss did not improve\n",
      "Epoch 5953/10000\n",
      "2700/2700 [==============================] - 0s 30us/step - loss: 1.2772e-04 - mean_absolute_error: 0.0065\n",
      "\n",
      "Epoch 05953: loss did not improve\n",
      "Epoch 5954/10000\n",
      "2700/2700 [==============================] - 0s 28us/step - loss: 1.2766e-04 - mean_absolute_error: 0.0064\n",
      "\n",
      "Epoch 05954: loss did not improve\n",
      "Epoch 5955/10000\n",
      "2700/2700 [==============================] - 0s 28us/step - loss: 1.2892e-04 - mean_absolute_error: 0.0066\n",
      "\n",
      "Epoch 05955: loss did not improve\n",
      "Epoch 5956/10000\n",
      "2700/2700 [==============================] - 0s 28us/step - loss: 1.3206e-04 - mean_absolute_error: 0.0067\n",
      "\n",
      "Epoch 05956: loss did not improve\n",
      "Epoch 5957/10000\n",
      "2700/2700 [==============================] - 0s 28us/step - loss: 1.2954e-04 - mean_absolute_error: 0.0066\n",
      "\n",
      "Epoch 05957: loss did not improve\n",
      "Epoch 5958/10000\n",
      "2700/2700 [==============================] - 0s 28us/step - loss: 1.2850e-04 - mean_absolute_error: 0.0066\n",
      "\n",
      "Epoch 05958: loss did not improve\n",
      "Epoch 5959/10000\n",
      "2700/2700 [==============================] - 0s 28us/step - loss: 1.3025e-04 - mean_absolute_error: 0.0066\n",
      "\n",
      "Epoch 05959: loss did not improve\n",
      "Epoch 5960/10000\n",
      "2700/2700 [==============================] - 0s 28us/step - loss: 1.2768e-04 - mean_absolute_error: 0.0065\n",
      "\n",
      "Epoch 05960: loss did not improve\n",
      "Epoch 5961/10000\n",
      "2700/2700 [==============================] - 0s 28us/step - loss: 1.2922e-04 - mean_absolute_error: 0.0065\n",
      "\n",
      "Epoch 05961: loss did not improve\n",
      "Epoch 5962/10000\n",
      "2700/2700 [==============================] - 0s 29us/step - loss: 1.2969e-04 - mean_absolute_error: 0.0066\n",
      "\n",
      "Epoch 05962: loss did not improve\n",
      "Epoch 5963/10000\n",
      "2700/2700 [==============================] - 0s 28us/step - loss: 1.3033e-04 - mean_absolute_error: 0.0067\n",
      "\n",
      "Epoch 05963: loss did not improve\n",
      "Epoch 5964/10000\n",
      "2700/2700 [==============================] - 0s 30us/step - loss: 1.3093e-04 - mean_absolute_error: 0.0068\n",
      "\n",
      "Epoch 05964: loss did not improve\n",
      "Epoch 5965/10000\n",
      "2700/2700 [==============================] - 0s 27us/step - loss: 1.2942e-04 - mean_absolute_error: 0.0066\n",
      "\n",
      "Epoch 05965: loss did not improve\n",
      "Epoch 5966/10000\n",
      "2700/2700 [==============================] - 0s 27us/step - loss: 1.2833e-04 - mean_absolute_error: 0.0065\n",
      "\n",
      "Epoch 05966: loss did not improve\n",
      "Epoch 5967/10000\n",
      "2700/2700 [==============================] - 0s 28us/step - loss: 1.2692e-04 - mean_absolute_error: 0.0065\n",
      "\n",
      "Epoch 05967: loss improved from 0.00013 to 0.00013, saving model to keras/best_baseline_weights.hdf5\n",
      "Epoch 5968/10000\n",
      "2700/2700 [==============================] - 0s 28us/step - loss: 1.2699e-04 - mean_absolute_error: 0.0064\n",
      "\n",
      "Epoch 05968: loss did not improve\n",
      "Epoch 5969/10000\n",
      "2700/2700 [==============================] - 0s 29us/step - loss: 1.3147e-04 - mean_absolute_error: 0.0067\n",
      "\n",
      "Epoch 05969: loss did not improve\n",
      "Epoch 5970/10000\n",
      "2700/2700 [==============================] - 0s 28us/step - loss: 1.3079e-04 - mean_absolute_error: 0.0067\n",
      "\n",
      "Epoch 05970: loss did not improve\n",
      "Epoch 5971/10000\n",
      "2700/2700 [==============================] - 0s 28us/step - loss: 1.2783e-04 - mean_absolute_error: 0.0064\n",
      "\n",
      "Epoch 05971: loss did not improve\n",
      "Epoch 5972/10000\n",
      "2700/2700 [==============================] - 0s 28us/step - loss: 1.2868e-04 - mean_absolute_error: 0.0065\n",
      "\n",
      "Epoch 05972: loss did not improve\n",
      "Epoch 5973/10000\n",
      "2700/2700 [==============================] - 0s 28us/step - loss: 1.3026e-04 - mean_absolute_error: 0.0067\n",
      "\n",
      "Epoch 05973: loss did not improve\n",
      "Epoch 5974/10000\n",
      "2700/2700 [==============================] - 0s 26us/step - loss: 1.2787e-04 - mean_absolute_error: 0.0064\n",
      "\n",
      "Epoch 05974: loss did not improve\n",
      "Epoch 5975/10000\n",
      "2700/2700 [==============================] - 0s 27us/step - loss: 1.3122e-04 - mean_absolute_error: 0.0067\n",
      "\n",
      "Epoch 05975: loss did not improve\n",
      "Epoch 5976/10000\n",
      "2700/2700 [==============================] - 0s 27us/step - loss: 1.2983e-04 - mean_absolute_error: 0.0067\n",
      "\n",
      "Epoch 05976: loss did not improve\n",
      "Epoch 5977/10000\n",
      "2700/2700 [==============================] - 0s 29us/step - loss: 1.3036e-04 - mean_absolute_error: 0.0067\n",
      "\n",
      "Epoch 05977: loss did not improve\n",
      "Epoch 5978/10000\n",
      "2700/2700 [==============================] - 0s 30us/step - loss: 1.2708e-04 - mean_absolute_error: 0.0064\n",
      "\n",
      "Epoch 05978: loss did not improve\n",
      "Epoch 5979/10000\n",
      "2700/2700 [==============================] - 0s 29us/step - loss: 1.2766e-04 - mean_absolute_error: 0.0064\n",
      "\n",
      "Epoch 05979: loss did not improve\n",
      "Epoch 5980/10000\n",
      "2700/2700 [==============================] - 0s 29us/step - loss: 1.2831e-04 - mean_absolute_error: 0.0065\n",
      "\n",
      "Epoch 05980: loss did not improve\n",
      "Epoch 5981/10000\n",
      "2700/2700 [==============================] - 0s 29us/step - loss: 1.2921e-04 - mean_absolute_error: 0.0067\n",
      "\n",
      "Epoch 05981: loss did not improve\n",
      "Epoch 5982/10000\n",
      "2700/2700 [==============================] - 0s 30us/step - loss: 1.2769e-04 - mean_absolute_error: 0.0065\n",
      "\n",
      "Epoch 05982: loss did not improve\n",
      "Epoch 5983/10000\n",
      "2700/2700 [==============================] - 0s 26us/step - loss: 1.3208e-04 - mean_absolute_error: 0.0067\n",
      "\n",
      "Epoch 05983: loss did not improve\n",
      "Epoch 5984/10000\n",
      "2700/2700 [==============================] - 0s 29us/step - loss: 1.2789e-04 - mean_absolute_error: 0.0066\n",
      "\n",
      "Epoch 05984: loss did not improve\n",
      "Epoch 5985/10000\n",
      "2700/2700 [==============================] - 0s 28us/step - loss: 1.2909e-04 - mean_absolute_error: 0.0066\n",
      "\n",
      "Epoch 05985: loss did not improve\n",
      "Epoch 5986/10000\n",
      "2700/2700 [==============================] - 0s 31us/step - loss: 1.2967e-04 - mean_absolute_error: 0.0067\n",
      "\n",
      "Epoch 05986: loss did not improve\n",
      "Epoch 5987/10000\n",
      "2700/2700 [==============================] - 0s 29us/step - loss: 1.2909e-04 - mean_absolute_error: 0.0066\n",
      "\n",
      "Epoch 05987: loss did not improve\n",
      "Epoch 5988/10000\n",
      "2700/2700 [==============================] - 0s 27us/step - loss: 1.2935e-04 - mean_absolute_error: 0.0067\n",
      "\n",
      "Epoch 05988: loss did not improve\n",
      "Epoch 5989/10000\n",
      "2700/2700 [==============================] - 0s 28us/step - loss: 1.2790e-04 - mean_absolute_error: 0.0064\n",
      "\n",
      "Epoch 05989: loss did not improve\n",
      "Epoch 5990/10000\n",
      "2700/2700 [==============================] - 0s 27us/step - loss: 1.2907e-04 - mean_absolute_error: 0.0066\n",
      "\n",
      "Epoch 05990: loss did not improve\n",
      "Epoch 5991/10000\n",
      "2700/2700 [==============================] - 0s 29us/step - loss: 1.3019e-04 - mean_absolute_error: 0.0066\n",
      "\n",
      "Epoch 05991: loss did not improve\n",
      "Epoch 5992/10000\n",
      "2700/2700 [==============================] - 0s 27us/step - loss: 1.2888e-04 - mean_absolute_error: 0.0066\n",
      "\n",
      "Epoch 05992: loss did not improve\n",
      "Epoch 5993/10000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2700/2700 [==============================] - 0s 31us/step - loss: 1.2862e-04 - mean_absolute_error: 0.0066\n",
      "\n",
      "Epoch 05993: loss did not improve\n",
      "Epoch 5994/10000\n",
      "2700/2700 [==============================] - 0s 28us/step - loss: 1.2679e-04 - mean_absolute_error: 0.0063\n",
      "\n",
      "Epoch 05994: loss improved from 0.00013 to 0.00013, saving model to keras/best_baseline_weights.hdf5\n",
      "Epoch 5995/10000\n",
      "2700/2700 [==============================] - 0s 30us/step - loss: 1.2994e-04 - mean_absolute_error: 0.0067\n",
      "\n",
      "Epoch 05995: loss did not improve\n",
      "Epoch 5996/10000\n",
      "2700/2700 [==============================] - 0s 28us/step - loss: 1.2779e-04 - mean_absolute_error: 0.0065\n",
      "\n",
      "Epoch 05996: loss did not improve\n",
      "Epoch 5997/10000\n",
      "2700/2700 [==============================] - 0s 25us/step - loss: 1.3077e-04 - mean_absolute_error: 0.0068\n",
      "\n",
      "Epoch 05997: loss did not improve\n",
      "Epoch 5998/10000\n",
      "2700/2700 [==============================] - 0s 27us/step - loss: 1.2876e-04 - mean_absolute_error: 0.0065\n",
      "\n",
      "Epoch 05998: loss did not improve\n",
      "Epoch 5999/10000\n",
      "2700/2700 [==============================] - 0s 29us/step - loss: 1.2779e-04 - mean_absolute_error: 0.0065\n",
      "\n",
      "Epoch 05999: loss did not improve\n",
      "Epoch 6000/10000\n",
      "2700/2700 [==============================] - 0s 28us/step - loss: 1.2938e-04 - mean_absolute_error: 0.0066\n",
      "\n",
      "Epoch 06000: loss did not improve\n",
      "Epoch 6001/10000\n",
      "2700/2700 [==============================] - 0s 25us/step - loss: 1.2952e-04 - mean_absolute_error: 0.0068\n",
      "\n",
      "Epoch 06001: loss did not improve\n",
      "Epoch 6002/10000\n",
      "2700/2700 [==============================] - 0s 31us/step - loss: 1.3108e-04 - mean_absolute_error: 0.0068\n",
      "\n",
      "Epoch 06002: loss did not improve\n",
      "Epoch 6003/10000\n",
      "2700/2700 [==============================] - 0s 29us/step - loss: 1.2931e-04 - mean_absolute_error: 0.0067\n",
      "\n",
      "Epoch 06003: loss did not improve\n",
      "Epoch 6004/10000\n",
      "2700/2700 [==============================] - 0s 29us/step - loss: 1.2830e-04 - mean_absolute_error: 0.0065\n",
      "\n",
      "Epoch 06004: loss did not improve\n",
      "Epoch 6005/10000\n",
      "2700/2700 [==============================] - 0s 31us/step - loss: 1.2881e-04 - mean_absolute_error: 0.0066\n",
      "\n",
      "Epoch 06005: loss did not improve\n",
      "Epoch 6006/10000\n",
      "2700/2700 [==============================] - 0s 26us/step - loss: 1.2746e-04 - mean_absolute_error: 0.0065\n",
      "\n",
      "Epoch 06006: loss did not improve\n",
      "Epoch 6007/10000\n",
      "2700/2700 [==============================] - 0s 30us/step - loss: 1.2822e-04 - mean_absolute_error: 0.0066\n",
      "\n",
      "Epoch 06007: loss did not improve\n",
      "Epoch 6008/10000\n",
      "2700/2700 [==============================] - 0s 29us/step - loss: 1.2874e-04 - mean_absolute_error: 0.0065\n",
      "\n",
      "Epoch 06008: loss did not improve\n",
      "Epoch 6009/10000\n",
      "2700/2700 [==============================] - 0s 28us/step - loss: 1.3017e-04 - mean_absolute_error: 0.0067\n",
      "\n",
      "Epoch 06009: loss did not improve\n",
      "Epoch 6010/10000\n",
      "2700/2700 [==============================] - 0s 27us/step - loss: 1.2824e-04 - mean_absolute_error: 0.0066\n",
      "\n",
      "Epoch 06010: loss did not improve\n",
      "Epoch 6011/10000\n",
      "2700/2700 [==============================] - 0s 29us/step - loss: 1.3226e-04 - mean_absolute_error: 0.0068\n",
      "\n",
      "Epoch 06011: loss did not improve\n",
      "Epoch 6012/10000\n",
      "2700/2700 [==============================] - 0s 30us/step - loss: 1.3011e-04 - mean_absolute_error: 0.0067\n",
      "\n",
      "Epoch 06012: loss did not improve\n",
      "Epoch 6013/10000\n",
      "2700/2700 [==============================] - 0s 28us/step - loss: 1.2952e-04 - mean_absolute_error: 0.0067\n",
      "\n",
      "Epoch 06013: loss did not improve\n",
      "Epoch 6014/10000\n",
      "2700/2700 [==============================] - 0s 27us/step - loss: 1.2807e-04 - mean_absolute_error: 0.0065\n",
      "\n",
      "Epoch 06014: loss did not improve\n",
      "Epoch 6015/10000\n",
      "2700/2700 [==============================] - 0s 29us/step - loss: 1.2961e-04 - mean_absolute_error: 0.0068\n",
      "\n",
      "Epoch 06015: loss did not improve\n",
      "Epoch 6016/10000\n",
      "2700/2700 [==============================] - 0s 31us/step - loss: 1.3471e-04 - mean_absolute_error: 0.0070\n",
      "\n",
      "Epoch 06016: loss did not improve\n",
      "Epoch 6017/10000\n",
      "2700/2700 [==============================] - 0s 29us/step - loss: 1.3137e-04 - mean_absolute_error: 0.0069\n",
      "\n",
      "Epoch 06017: loss did not improve\n",
      "Epoch 6018/10000\n",
      "2700/2700 [==============================] - 0s 30us/step - loss: 1.2884e-04 - mean_absolute_error: 0.0066\n",
      "\n",
      "Epoch 06018: loss did not improve\n",
      "Epoch 6019/10000\n",
      "2700/2700 [==============================] - 0s 29us/step - loss: 1.3194e-04 - mean_absolute_error: 0.0069\n",
      "\n",
      "Epoch 06019: loss did not improve\n",
      "Epoch 6020/10000\n",
      "2700/2700 [==============================] - 0s 28us/step - loss: 1.2850e-04 - mean_absolute_error: 0.0064\n",
      "\n",
      "Epoch 06020: loss did not improve\n",
      "Epoch 6021/10000\n",
      "2700/2700 [==============================] - 0s 29us/step - loss: 1.2906e-04 - mean_absolute_error: 0.0067\n",
      "\n",
      "Epoch 06021: loss did not improve\n",
      "Epoch 6022/10000\n",
      "2700/2700 [==============================] - 0s 30us/step - loss: 1.3054e-04 - mean_absolute_error: 0.0067\n",
      "\n",
      "Epoch 06022: loss did not improve\n",
      "Epoch 6023/10000\n",
      "2700/2700 [==============================] - 0s 30us/step - loss: 1.3057e-04 - mean_absolute_error: 0.0067\n",
      "\n",
      "Epoch 06023: loss did not improve\n",
      "Epoch 6024/10000\n",
      "2700/2700 [==============================] - 0s 26us/step - loss: 1.2694e-04 - mean_absolute_error: 0.0064\n",
      "\n",
      "Epoch 06024: loss did not improve\n",
      "Epoch 6025/10000\n",
      "2700/2700 [==============================] - 0s 30us/step - loss: 1.2873e-04 - mean_absolute_error: 0.0067\n",
      "\n",
      "Epoch 06025: loss did not improve\n",
      "Epoch 6026/10000\n",
      "2700/2700 [==============================] - 0s 29us/step - loss: 1.2828e-04 - mean_absolute_error: 0.0065\n",
      "\n",
      "Epoch 06026: loss did not improve\n",
      "Epoch 6027/10000\n",
      "2700/2700 [==============================] - 0s 30us/step - loss: 1.2751e-04 - mean_absolute_error: 0.0064\n",
      "\n",
      "Epoch 06027: loss did not improve\n",
      "Epoch 6028/10000\n",
      "2700/2700 [==============================] - 0s 27us/step - loss: 1.2756e-04 - mean_absolute_error: 0.0064\n",
      "\n",
      "Epoch 06028: loss did not improve\n",
      "Epoch 6029/10000\n",
      "2700/2700 [==============================] - 0s 30us/step - loss: 1.2684e-04 - mean_absolute_error: 0.0063\n",
      "\n",
      "Epoch 06029: loss did not improve\n",
      "Epoch 6030/10000\n",
      "2700/2700 [==============================] - 0s 28us/step - loss: 1.2910e-04 - mean_absolute_error: 0.0065\n",
      "\n",
      "Epoch 06030: loss did not improve\n",
      "Epoch 6031/10000\n",
      "2700/2700 [==============================] - 0s 28us/step - loss: 1.2703e-04 - mean_absolute_error: 0.0065\n",
      "\n",
      "Epoch 06031: loss did not improve\n",
      "Epoch 6032/10000\n",
      "2700/2700 [==============================] - 0s 30us/step - loss: 1.2800e-04 - mean_absolute_error: 0.0065\n",
      "\n",
      "Epoch 06032: loss did not improve\n",
      "Epoch 6033/10000\n",
      "2700/2700 [==============================] - 0s 25us/step - loss: 1.2777e-04 - mean_absolute_error: 0.0065\n",
      "\n",
      "Epoch 06033: loss did not improve\n",
      "Epoch 6034/10000\n",
      "2700/2700 [==============================] - 0s 30us/step - loss: 1.3222e-04 - mean_absolute_error: 0.0068\n",
      "\n",
      "Epoch 06034: loss did not improve\n",
      "Epoch 6035/10000\n",
      "2700/2700 [==============================] - 0s 28us/step - loss: 1.2919e-04 - mean_absolute_error: 0.0066\n",
      "\n",
      "Epoch 06035: loss did not improve\n",
      "Epoch 6036/10000\n",
      "2700/2700 [==============================] - 0s 29us/step - loss: 1.2837e-04 - mean_absolute_error: 0.0066\n",
      "\n",
      "Epoch 06036: loss did not improve\n",
      "Epoch 6037/10000\n",
      "2700/2700 [==============================] - 0s 26us/step - loss: 1.2799e-04 - mean_absolute_error: 0.0065\n",
      "\n",
      "Epoch 06037: loss did not improve\n",
      "Epoch 6038/10000\n",
      "2700/2700 [==============================] - 0s 28us/step - loss: 1.2815e-04 - mean_absolute_error: 0.0066\n",
      "\n",
      "Epoch 06038: loss did not improve\n",
      "Epoch 6039/10000\n",
      "2700/2700 [==============================] - 0s 29us/step - loss: 1.2833e-04 - mean_absolute_error: 0.0067\n",
      "\n",
      "Epoch 06039: loss did not improve\n",
      "Epoch 6040/10000\n",
      "2700/2700 [==============================] - 0s 29us/step - loss: 1.2930e-04 - mean_absolute_error: 0.0066\n",
      "\n",
      "Epoch 06040: loss did not improve\n",
      "Epoch 6041/10000\n",
      "2700/2700 [==============================] - 0s 27us/step - loss: 1.2651e-04 - mean_absolute_error: 0.0064\n",
      "\n",
      "Epoch 06041: loss improved from 0.00013 to 0.00013, saving model to keras/best_baseline_weights.hdf5\n",
      "Epoch 6042/10000\n",
      "2700/2700 [==============================] - 0s 27us/step - loss: 1.2864e-04 - mean_absolute_error: 0.0066\n",
      "\n",
      "Epoch 06042: loss did not improve\n",
      "Epoch 6043/10000\n",
      "2700/2700 [==============================] - 0s 29us/step - loss: 1.2869e-04 - mean_absolute_error: 0.0066\n",
      "\n",
      "Epoch 06043: loss did not improve\n",
      "Epoch 6044/10000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2700/2700 [==============================] - 0s 29us/step - loss: 1.3058e-04 - mean_absolute_error: 0.0068\n",
      "\n",
      "Epoch 06044: loss did not improve\n",
      "Epoch 6045/10000\n",
      "2700/2700 [==============================] - 0s 29us/step - loss: 1.3265e-04 - mean_absolute_error: 0.0070\n",
      "\n",
      "Epoch 06045: loss did not improve\n",
      "Epoch 6046/10000\n",
      "2700/2700 [==============================] - 0s 25us/step - loss: 1.2930e-04 - mean_absolute_error: 0.0067\n",
      "\n",
      "Epoch 06046: loss did not improve\n",
      "Epoch 6047/10000\n",
      "2700/2700 [==============================] - 0s 28us/step - loss: 1.2887e-04 - mean_absolute_error: 0.0065\n",
      "\n",
      "Epoch 06047: loss did not improve\n",
      "Epoch 6048/10000\n",
      "2700/2700 [==============================] - 0s 28us/step - loss: 1.2797e-04 - mean_absolute_error: 0.0065\n",
      "\n",
      "Epoch 06048: loss did not improve\n",
      "Epoch 6049/10000\n",
      "2700/2700 [==============================] - 0s 28us/step - loss: 1.2832e-04 - mean_absolute_error: 0.0065\n",
      "\n",
      "Epoch 06049: loss did not improve\n",
      "Epoch 6050/10000\n",
      "2700/2700 [==============================] - 0s 27us/step - loss: 1.2935e-04 - mean_absolute_error: 0.0065\n",
      "\n",
      "Epoch 06050: loss did not improve\n",
      "Epoch 6051/10000\n",
      "2700/2700 [==============================] - 0s 28us/step - loss: 1.2670e-04 - mean_absolute_error: 0.0064\n",
      "\n",
      "Epoch 06051: loss did not improve\n",
      "Epoch 6052/10000\n",
      "2700/2700 [==============================] - 0s 28us/step - loss: 1.2707e-04 - mean_absolute_error: 0.0065\n",
      "\n",
      "Epoch 06052: loss did not improve\n",
      "Epoch 6053/10000\n",
      "2700/2700 [==============================] - 0s 29us/step - loss: 1.2830e-04 - mean_absolute_error: 0.0066\n",
      "\n",
      "Epoch 06053: loss did not improve\n",
      "Epoch 6054/10000\n",
      "2700/2700 [==============================] - 0s 27us/step - loss: 1.3661e-04 - mean_absolute_error: 0.0072\n",
      "\n",
      "Epoch 06054: loss did not improve\n",
      "Epoch 6055/10000\n",
      "2700/2700 [==============================] - 0s 31us/step - loss: 1.2797e-04 - mean_absolute_error: 0.0066\n",
      "\n",
      "Epoch 06055: loss did not improve\n",
      "Epoch 6056/10000\n",
      "2700/2700 [==============================] - 0s 28us/step - loss: 1.2914e-04 - mean_absolute_error: 0.0066\n",
      "\n",
      "Epoch 06056: loss did not improve\n",
      "Epoch 6057/10000\n",
      "2700/2700 [==============================] - 0s 28us/step - loss: 1.2972e-04 - mean_absolute_error: 0.0066\n",
      "\n",
      "Epoch 06057: loss did not improve\n",
      "Epoch 6058/10000\n",
      "2700/2700 [==============================] - 0s 32us/step - loss: 1.2845e-04 - mean_absolute_error: 0.0065\n",
      "\n",
      "Epoch 06058: loss did not improve\n",
      "Epoch 6059/10000\n",
      "2700/2700 [==============================] - 0s 26us/step - loss: 1.3161e-04 - mean_absolute_error: 0.0068\n",
      "\n",
      "Epoch 06059: loss did not improve\n",
      "Epoch 6060/10000\n",
      "2700/2700 [==============================] - 0s 27us/step - loss: 1.3170e-04 - mean_absolute_error: 0.0068\n",
      "\n",
      "Epoch 06060: loss did not improve\n",
      "Epoch 6061/10000\n",
      "2700/2700 [==============================] - 0s 29us/step - loss: 1.2748e-04 - mean_absolute_error: 0.0064\n",
      "\n",
      "Epoch 06061: loss did not improve\n",
      "Epoch 6062/10000\n",
      "2700/2700 [==============================] - 0s 28us/step - loss: 1.2858e-04 - mean_absolute_error: 0.0066\n",
      "\n",
      "Epoch 06062: loss did not improve\n",
      "Epoch 6063/10000\n",
      "2700/2700 [==============================] - 0s 26us/step - loss: 1.2707e-04 - mean_absolute_error: 0.0065\n",
      "\n",
      "Epoch 06063: loss did not improve\n",
      "Epoch 6064/10000\n",
      "2700/2700 [==============================] - 0s 29us/step - loss: 1.3182e-04 - mean_absolute_error: 0.0068\n",
      "\n",
      "Epoch 06064: loss did not improve\n",
      "Epoch 6065/10000\n",
      "2700/2700 [==============================] - 0s 29us/step - loss: 1.2681e-04 - mean_absolute_error: 0.0065\n",
      "\n",
      "Epoch 06065: loss did not improve\n",
      "Epoch 6066/10000\n",
      "2700/2700 [==============================] - 0s 28us/step - loss: 1.2899e-04 - mean_absolute_error: 0.0066\n",
      "\n",
      "Epoch 06066: loss did not improve\n",
      "Epoch 6067/10000\n",
      "2700/2700 [==============================] - 0s 26us/step - loss: 1.2681e-04 - mean_absolute_error: 0.0064\n",
      "\n",
      "Epoch 06067: loss did not improve\n",
      "Epoch 6068/10000\n",
      "2700/2700 [==============================] - 0s 29us/step - loss: 1.2908e-04 - mean_absolute_error: 0.0065\n",
      "\n",
      "Epoch 06068: loss did not improve\n",
      "Epoch 6069/10000\n",
      "2700/2700 [==============================] - 0s 29us/step - loss: 1.2819e-04 - mean_absolute_error: 0.0066\n",
      "\n",
      "Epoch 06069: loss did not improve\n",
      "Epoch 6070/10000\n",
      "2700/2700 [==============================] - 0s 29us/step - loss: 1.3062e-04 - mean_absolute_error: 0.0068\n",
      "\n",
      "Epoch 06070: loss did not improve\n",
      "Epoch 6071/10000\n",
      "2700/2700 [==============================] - 0s 27us/step - loss: 1.2858e-04 - mean_absolute_error: 0.0066\n",
      "\n",
      "Epoch 06071: loss did not improve\n",
      "Epoch 6072/10000\n",
      "2700/2700 [==============================] - 0s 27us/step - loss: 1.2807e-04 - mean_absolute_error: 0.0065\n",
      "\n",
      "Epoch 06072: loss did not improve\n",
      "Epoch 6073/10000\n",
      "2700/2700 [==============================] - 0s 28us/step - loss: 1.2740e-04 - mean_absolute_error: 0.0064\n",
      "\n",
      "Epoch 06073: loss did not improve\n",
      "Epoch 6074/10000\n",
      "2700/2700 [==============================] - 0s 30us/step - loss: 1.2639e-04 - mean_absolute_error: 0.0065\n",
      "\n",
      "Epoch 06074: loss improved from 0.00013 to 0.00013, saving model to keras/best_baseline_weights.hdf5\n",
      "Epoch 6075/10000\n",
      "2700/2700 [==============================] - 0s 28us/step - loss: 1.2656e-04 - mean_absolute_error: 0.0064\n",
      "\n",
      "Epoch 06075: loss did not improve\n",
      "Epoch 6076/10000\n",
      "2700/2700 [==============================] - 0s 27us/step - loss: 1.2608e-04 - mean_absolute_error: 0.0064\n",
      "\n",
      "Epoch 06076: loss improved from 0.00013 to 0.00013, saving model to keras/best_baseline_weights.hdf5\n",
      "Epoch 6077/10000\n",
      "2700/2700 [==============================] - 0s 29us/step - loss: 1.3146e-04 - mean_absolute_error: 0.0068\n",
      "\n",
      "Epoch 06077: loss did not improve\n",
      "Epoch 6078/10000\n",
      "2700/2700 [==============================] - 0s 30us/step - loss: 1.3018e-04 - mean_absolute_error: 0.0067\n",
      "\n",
      "Epoch 06078: loss did not improve\n",
      "Epoch 6079/10000\n",
      "2700/2700 [==============================] - 0s 29us/step - loss: 1.2778e-04 - mean_absolute_error: 0.0066\n",
      "\n",
      "Epoch 06079: loss did not improve\n",
      "Epoch 6080/10000\n",
      "2700/2700 [==============================] - 0s 28us/step - loss: 1.2599e-04 - mean_absolute_error: 0.0064\n",
      "\n",
      "Epoch 06080: loss improved from 0.00013 to 0.00013, saving model to keras/best_baseline_weights.hdf5\n",
      "Epoch 6081/10000\n",
      "2700/2700 [==============================] - 0s 29us/step - loss: 1.2839e-04 - mean_absolute_error: 0.0065\n",
      "\n",
      "Epoch 06081: loss did not improve\n",
      "Epoch 6082/10000\n",
      "2700/2700 [==============================] - 0s 27us/step - loss: 1.2851e-04 - mean_absolute_error: 0.0066\n",
      "\n",
      "Epoch 06082: loss did not improve\n",
      "Epoch 6083/10000\n",
      "2700/2700 [==============================] - 0s 30us/step - loss: 1.2943e-04 - mean_absolute_error: 0.0067\n",
      "\n",
      "Epoch 06083: loss did not improve\n",
      "Epoch 6084/10000\n",
      "2700/2700 [==============================] - 0s 30us/step - loss: 1.2644e-04 - mean_absolute_error: 0.0063\n",
      "\n",
      "Epoch 06084: loss did not improve\n",
      "Epoch 6085/10000\n",
      "2700/2700 [==============================] - 0s 30us/step - loss: 1.2734e-04 - mean_absolute_error: 0.0064\n",
      "\n",
      "Epoch 06085: loss did not improve\n",
      "Epoch 6086/10000\n",
      "2700/2700 [==============================] - 0s 29us/step - loss: 1.2591e-04 - mean_absolute_error: 0.0065\n",
      "\n",
      "Epoch 06086: loss improved from 0.00013 to 0.00013, saving model to keras/best_baseline_weights.hdf5\n",
      "Epoch 6087/10000\n",
      "2700/2700 [==============================] - 0s 35us/step - loss: 1.2618e-04 - mean_absolute_error: 0.0064\n",
      "\n",
      "Epoch 06087: loss did not improve\n",
      "Epoch 6088/10000\n",
      "2700/2700 [==============================] - 0s 47us/step - loss: 1.2816e-04 - mean_absolute_error: 0.0066\n",
      "\n",
      "Epoch 06088: loss did not improve\n",
      "Epoch 6089/10000\n",
      "2700/2700 [==============================] - 0s 30us/step - loss: 1.2837e-04 - mean_absolute_error: 0.0066\n",
      "\n",
      "Epoch 06089: loss did not improve\n",
      "Epoch 6090/10000\n",
      "2700/2700 [==============================] - 0s 28us/step - loss: 1.3022e-04 - mean_absolute_error: 0.0068\n",
      "\n",
      "Epoch 06090: loss did not improve\n",
      "Epoch 6091/10000\n",
      "2700/2700 [==============================] - 0s 28us/step - loss: 1.2623e-04 - mean_absolute_error: 0.0065\n",
      "\n",
      "Epoch 06091: loss did not improve\n",
      "Epoch 6092/10000\n",
      "2700/2700 [==============================] - 0s 29us/step - loss: 1.2955e-04 - mean_absolute_error: 0.0067\n",
      "\n",
      "Epoch 06092: loss did not improve\n",
      "Epoch 6093/10000\n",
      "2700/2700 [==============================] - 0s 30us/step - loss: 1.3038e-04 - mean_absolute_error: 0.0067\n",
      "\n",
      "Epoch 06093: loss did not improve\n",
      "Epoch 6094/10000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2700/2700 [==============================] - 0s 28us/step - loss: 1.2666e-04 - mean_absolute_error: 0.0064\n",
      "\n",
      "Epoch 06094: loss did not improve\n",
      "Epoch 6095/10000\n",
      "2700/2700 [==============================] - 0s 28us/step - loss: 1.2866e-04 - mean_absolute_error: 0.0065\n",
      "\n",
      "Epoch 06095: loss did not improve\n",
      "Epoch 6096/10000\n",
      "2700/2700 [==============================] - 0s 29us/step - loss: 1.3492e-04 - mean_absolute_error: 0.0071\n",
      "\n",
      "Epoch 06096: loss did not improve\n",
      "Epoch 6097/10000\n",
      "2700/2700 [==============================] - 0s 28us/step - loss: 1.3028e-04 - mean_absolute_error: 0.0067\n",
      "\n",
      "Epoch 06097: loss did not improve\n",
      "Epoch 6098/10000\n",
      "2700/2700 [==============================] - 0s 30us/step - loss: 1.2988e-04 - mean_absolute_error: 0.0066\n",
      "\n",
      "Epoch 06098: loss did not improve\n",
      "Epoch 6099/10000\n",
      "2700/2700 [==============================] - 0s 27us/step - loss: 1.2633e-04 - mean_absolute_error: 0.0064\n",
      "\n",
      "Epoch 06099: loss did not improve\n",
      "Epoch 6100/10000\n",
      "2700/2700 [==============================] - 0s 27us/step - loss: 1.3287e-04 - mean_absolute_error: 0.0070\n",
      "\n",
      "Epoch 06100: loss did not improve\n",
      "Epoch 6101/10000\n",
      "2700/2700 [==============================] - 0s 30us/step - loss: 1.2909e-04 - mean_absolute_error: 0.0065\n",
      "\n",
      "Epoch 06101: loss did not improve\n",
      "Epoch 6102/10000\n",
      "2700/2700 [==============================] - 0s 29us/step - loss: 1.2853e-04 - mean_absolute_error: 0.0067\n",
      "\n",
      "Epoch 06102: loss did not improve\n",
      "Epoch 6103/10000\n",
      "2700/2700 [==============================] - 0s 28us/step - loss: 1.2695e-04 - mean_absolute_error: 0.0065\n",
      "\n",
      "Epoch 06103: loss did not improve\n",
      "Epoch 6104/10000\n",
      "2700/2700 [==============================] - 0s 30us/step - loss: 1.2748e-04 - mean_absolute_error: 0.0064\n",
      "\n",
      "Epoch 06104: loss did not improve\n",
      "Epoch 6105/10000\n",
      "2700/2700 [==============================] - 0s 29us/step - loss: 1.2892e-04 - mean_absolute_error: 0.0066\n",
      "\n",
      "Epoch 06105: loss did not improve\n",
      "Epoch 6106/10000\n",
      "2700/2700 [==============================] - 0s 28us/step - loss: 1.2783e-04 - mean_absolute_error: 0.0066\n",
      "\n",
      "Epoch 06106: loss did not improve\n",
      "Epoch 6107/10000\n",
      "2700/2700 [==============================] - 0s 28us/step - loss: 1.2762e-04 - mean_absolute_error: 0.0065\n",
      "\n",
      "Epoch 06107: loss did not improve\n",
      "Epoch 6108/10000\n",
      "2700/2700 [==============================] - 0s 29us/step - loss: 1.2583e-04 - mean_absolute_error: 0.0065\n",
      "\n",
      "Epoch 06108: loss improved from 0.00013 to 0.00013, saving model to keras/best_baseline_weights.hdf5\n",
      "Epoch 6109/10000\n",
      "2700/2700 [==============================] - 0s 26us/step - loss: 1.2877e-04 - mean_absolute_error: 0.0067\n",
      "\n",
      "Epoch 06109: loss did not improve\n",
      "Epoch 6110/10000\n",
      "2700/2700 [==============================] - 0s 28us/step - loss: 1.2629e-04 - mean_absolute_error: 0.0065\n",
      "\n",
      "Epoch 06110: loss did not improve\n",
      "Epoch 6111/10000\n",
      "2700/2700 [==============================] - 0s 28us/step - loss: 1.2697e-04 - mean_absolute_error: 0.0064\n",
      "\n",
      "Epoch 06111: loss did not improve\n",
      "Epoch 6112/10000\n",
      "2700/2700 [==============================] - 0s 28us/step - loss: 1.2890e-04 - mean_absolute_error: 0.0067\n",
      "\n",
      "Epoch 06112: loss did not improve\n",
      "Epoch 6113/10000\n",
      "2700/2700 [==============================] - 0s 30us/step - loss: 1.2790e-04 - mean_absolute_error: 0.0066\n",
      "\n",
      "Epoch 06113: loss did not improve\n",
      "Epoch 6114/10000\n",
      "2700/2700 [==============================] - 0s 26us/step - loss: 1.3154e-04 - mean_absolute_error: 0.0068\n",
      "\n",
      "Epoch 06114: loss did not improve\n",
      "Epoch 6115/10000\n",
      "2700/2700 [==============================] - 0s 35us/step - loss: 1.2800e-04 - mean_absolute_error: 0.0065\n",
      "\n",
      "Epoch 06115: loss did not improve\n",
      "Epoch 6116/10000\n",
      "2700/2700 [==============================] - 0s 37us/step - loss: 1.2956e-04 - mean_absolute_error: 0.0066\n",
      "\n",
      "Epoch 06116: loss did not improve\n",
      "Epoch 6117/10000\n",
      "2700/2700 [==============================] - 0s 44us/step - loss: 1.3138e-04 - mean_absolute_error: 0.0068\n",
      "\n",
      "Epoch 06117: loss did not improve\n",
      "Epoch 6118/10000\n",
      "2700/2700 [==============================] - 0s 37us/step - loss: 1.2604e-04 - mean_absolute_error: 0.0064\n",
      "\n",
      "Epoch 06118: loss did not improve\n",
      "Epoch 6119/10000\n",
      "2700/2700 [==============================] - 0s 37us/step - loss: 1.3276e-04 - mean_absolute_error: 0.0070\n",
      "\n",
      "Epoch 06119: loss did not improve\n",
      "Epoch 6120/10000\n",
      "2700/2700 [==============================] - 0s 35us/step - loss: 1.2875e-04 - mean_absolute_error: 0.0066\n",
      "\n",
      "Epoch 06120: loss did not improve\n",
      "Epoch 6121/10000\n",
      "2700/2700 [==============================] - 0s 35us/step - loss: 1.2950e-04 - mean_absolute_error: 0.0067\n",
      "\n",
      "Epoch 06121: loss did not improve\n",
      "Epoch 6122/10000\n",
      "2700/2700 [==============================] - 0s 35us/step - loss: 1.2948e-04 - mean_absolute_error: 0.0066\n",
      "\n",
      "Epoch 06122: loss did not improve\n",
      "Epoch 6123/10000\n",
      "2700/2700 [==============================] - 0s 32us/step - loss: 1.3127e-04 - mean_absolute_error: 0.0068\n",
      "\n",
      "Epoch 06123: loss did not improve\n",
      "Epoch 6124/10000\n",
      "2700/2700 [==============================] - 0s 30us/step - loss: 1.2653e-04 - mean_absolute_error: 0.0065\n",
      "\n",
      "Epoch 06124: loss did not improve\n",
      "Epoch 6125/10000\n",
      "2700/2700 [==============================] - 0s 29us/step - loss: 1.2617e-04 - mean_absolute_error: 0.0064\n",
      "\n",
      "Epoch 06125: loss did not improve\n",
      "Epoch 6126/10000\n",
      "2700/2700 [==============================] - 0s 34us/step - loss: 1.2702e-04 - mean_absolute_error: 0.0065\n",
      "\n",
      "Epoch 06126: loss did not improve\n",
      "Epoch 6127/10000\n",
      "2700/2700 [==============================] - 0s 27us/step - loss: 1.2777e-04 - mean_absolute_error: 0.0065\n",
      "\n",
      "Epoch 06127: loss did not improve\n",
      "Epoch 6128/10000\n",
      "2700/2700 [==============================] - 0s 29us/step - loss: 1.2895e-04 - mean_absolute_error: 0.0066\n",
      "\n",
      "Epoch 06128: loss did not improve\n",
      "Epoch 6129/10000\n",
      "2700/2700 [==============================] - 0s 30us/step - loss: 1.2908e-04 - mean_absolute_error: 0.0067\n",
      "\n",
      "Epoch 06129: loss did not improve\n",
      "Epoch 6130/10000\n",
      "2700/2700 [==============================] - 0s 27us/step - loss: 1.2709e-04 - mean_absolute_error: 0.0063\n",
      "\n",
      "Epoch 06130: loss did not improve\n",
      "Epoch 6131/10000\n",
      "2700/2700 [==============================] - 0s 32us/step - loss: 1.2924e-04 - mean_absolute_error: 0.0067\n",
      "\n",
      "Epoch 06131: loss did not improve\n",
      "Epoch 6132/10000\n",
      "2700/2700 [==============================] - 0s 26us/step - loss: 1.2628e-04 - mean_absolute_error: 0.0064\n",
      "\n",
      "Epoch 06132: loss did not improve\n",
      "Epoch 6133/10000\n",
      "2700/2700 [==============================] - 0s 30us/step - loss: 1.2776e-04 - mean_absolute_error: 0.0066\n",
      "\n",
      "Epoch 06133: loss did not improve\n",
      "Epoch 6134/10000\n",
      "2700/2700 [==============================] - 0s 28us/step - loss: 1.3057e-04 - mean_absolute_error: 0.0067\n",
      "\n",
      "Epoch 06134: loss did not improve\n",
      "Epoch 6135/10000\n",
      "2700/2700 [==============================] - 0s 30us/step - loss: 1.2655e-04 - mean_absolute_error: 0.0065\n",
      "\n",
      "Epoch 06135: loss did not improve\n",
      "Epoch 6136/10000\n",
      "2700/2700 [==============================] - 0s 26us/step - loss: 1.2739e-04 - mean_absolute_error: 0.0065\n",
      "\n",
      "Epoch 06136: loss did not improve\n",
      "Epoch 6137/10000\n",
      "2700/2700 [==============================] - 0s 28us/step - loss: 1.2697e-04 - mean_absolute_error: 0.0065\n",
      "\n",
      "Epoch 06137: loss did not improve\n",
      "Epoch 6138/10000\n",
      "2700/2700 [==============================] - 0s 30us/step - loss: 1.2990e-04 - mean_absolute_error: 0.0067\n",
      "\n",
      "Epoch 06138: loss did not improve\n",
      "Epoch 6139/10000\n",
      "2700/2700 [==============================] - 0s 27us/step - loss: 1.2652e-04 - mean_absolute_error: 0.0064\n",
      "\n",
      "Epoch 06139: loss did not improve\n",
      "Epoch 6140/10000\n",
      "2700/2700 [==============================] - 0s 27us/step - loss: 1.2745e-04 - mean_absolute_error: 0.0065\n",
      "\n",
      "Epoch 06140: loss did not improve\n",
      "Epoch 6141/10000\n",
      "2700/2700 [==============================] - 0s 27us/step - loss: 1.2979e-04 - mean_absolute_error: 0.0068\n",
      "\n",
      "Epoch 06141: loss did not improve\n",
      "Epoch 6142/10000\n",
      "2700/2700 [==============================] - 0s 29us/step - loss: 1.3626e-04 - mean_absolute_error: 0.0073\n",
      "\n",
      "Epoch 06142: loss did not improve\n",
      "Epoch 6143/10000\n",
      "2700/2700 [==============================] - 0s 27us/step - loss: 1.2970e-04 - mean_absolute_error: 0.0067\n",
      "\n",
      "Epoch 06143: loss did not improve\n",
      "Epoch 6144/10000\n",
      "2700/2700 [==============================] - 0s 27us/step - loss: 1.2789e-04 - mean_absolute_error: 0.0066\n",
      "\n",
      "Epoch 06144: loss did not improve\n",
      "Epoch 6145/10000\n",
      "2700/2700 [==============================] - 0s 30us/step - loss: 1.2661e-04 - mean_absolute_error: 0.0064\n",
      "\n",
      "Epoch 06145: loss did not improve\n",
      "Epoch 6146/10000\n",
      "2700/2700 [==============================] - 0s 27us/step - loss: 1.2874e-04 - mean_absolute_error: 0.0067\n",
      "\n",
      "Epoch 06146: loss did not improve\n",
      "Epoch 6147/10000\n",
      "2700/2700 [==============================] - 0s 30us/step - loss: 1.2589e-04 - mean_absolute_error: 0.0064\n",
      "\n",
      "Epoch 06147: loss did not improve\n",
      "Epoch 6148/10000\n",
      "2700/2700 [==============================] - 0s 25us/step - loss: 1.2950e-04 - mean_absolute_error: 0.0067\n",
      "\n",
      "Epoch 06148: loss did not improve\n",
      "Epoch 6149/10000\n",
      "2700/2700 [==============================] - 0s 27us/step - loss: 1.2887e-04 - mean_absolute_error: 0.0067\n",
      "\n",
      "Epoch 06149: loss did not improve\n",
      "Epoch 6150/10000\n",
      "2700/2700 [==============================] - 0s 29us/step - loss: 1.3105e-04 - mean_absolute_error: 0.0068\n",
      "\n",
      "Epoch 06150: loss did not improve\n",
      "Epoch 6151/10000\n",
      "2700/2700 [==============================] - 0s 28us/step - loss: 1.2655e-04 - mean_absolute_error: 0.0065\n",
      "\n",
      "Epoch 06151: loss did not improve\n",
      "Epoch 6152/10000\n",
      "2700/2700 [==============================] - 0s 26us/step - loss: 1.2874e-04 - mean_absolute_error: 0.0066\n",
      "\n",
      "Epoch 06152: loss did not improve\n",
      "Epoch 6153/10000\n",
      "2700/2700 [==============================] - 0s 27us/step - loss: 1.2731e-04 - mean_absolute_error: 0.0065\n",
      "\n",
      "Epoch 06153: loss did not improve\n",
      "Epoch 6154/10000\n",
      "2700/2700 [==============================] - 0s 31us/step - loss: 1.2672e-04 - mean_absolute_error: 0.0064\n",
      "\n",
      "Epoch 06154: loss did not improve\n",
      "Epoch 6155/10000\n",
      "2700/2700 [==============================] - 0s 28us/step - loss: 1.2716e-04 - mean_absolute_error: 0.0064\n",
      "\n",
      "Epoch 06155: loss did not improve\n",
      "Epoch 6156/10000\n",
      "2700/2700 [==============================] - 0s 29us/step - loss: 1.2646e-04 - mean_absolute_error: 0.0065\n",
      "\n",
      "Epoch 06156: loss did not improve\n",
      "Epoch 6157/10000\n",
      "2700/2700 [==============================] - 0s 27us/step - loss: 1.2896e-04 - mean_absolute_error: 0.0067\n",
      "\n",
      "Epoch 06157: loss did not improve\n",
      "Epoch 6158/10000\n",
      "2700/2700 [==============================] - 0s 28us/step - loss: 1.2813e-04 - mean_absolute_error: 0.0065\n",
      "\n",
      "Epoch 06158: loss did not improve\n",
      "Epoch 6159/10000\n",
      "2700/2700 [==============================] - 0s 28us/step - loss: 1.2792e-04 - mean_absolute_error: 0.0066\n",
      "\n",
      "Epoch 06159: loss did not improve\n",
      "Epoch 6160/10000\n",
      "2700/2700 [==============================] - 0s 29us/step - loss: 1.3030e-04 - mean_absolute_error: 0.0067\n",
      "\n",
      "Epoch 06160: loss did not improve\n",
      "Epoch 6161/10000\n",
      "2700/2700 [==============================] - 0s 26us/step - loss: 1.2551e-04 - mean_absolute_error: 0.0064\n",
      "\n",
      "Epoch 06161: loss improved from 0.00013 to 0.00013, saving model to keras/best_baseline_weights.hdf5\n",
      "Epoch 6162/10000\n",
      "2700/2700 [==============================] - 0s 27us/step - loss: 1.2569e-04 - mean_absolute_error: 0.0064\n",
      "\n",
      "Epoch 06162: loss did not improve\n",
      "Epoch 6163/10000\n",
      "2700/2700 [==============================] - 0s 29us/step - loss: 1.2723e-04 - mean_absolute_error: 0.0066\n",
      "\n",
      "Epoch 06163: loss did not improve\n",
      "Epoch 6164/10000\n",
      "2700/2700 [==============================] - 0s 29us/step - loss: 1.2756e-04 - mean_absolute_error: 0.0065\n",
      "\n",
      "Epoch 06164: loss did not improve\n",
      "Epoch 6165/10000\n",
      "2700/2700 [==============================] - 0s 28us/step - loss: 1.2574e-04 - mean_absolute_error: 0.0063\n",
      "\n",
      "Epoch 06165: loss did not improve\n",
      "Epoch 6166/10000\n",
      "2700/2700 [==============================] - 0s 27us/step - loss: 1.2747e-04 - mean_absolute_error: 0.0066\n",
      "\n",
      "Epoch 06166: loss did not improve\n",
      "Epoch 6167/10000\n",
      "2700/2700 [==============================] - 0s 32us/step - loss: 1.2661e-04 - mean_absolute_error: 0.0064\n",
      "\n",
      "Epoch 06167: loss did not improve\n",
      "Epoch 6168/10000\n",
      "2700/2700 [==============================] - 0s 29us/step - loss: 1.2772e-04 - mean_absolute_error: 0.0066\n",
      "\n",
      "Epoch 06168: loss did not improve\n",
      "Epoch 6169/10000\n",
      "2700/2700 [==============================] - 0s 27us/step - loss: 1.2937e-04 - mean_absolute_error: 0.0066\n",
      "\n",
      "Epoch 06169: loss did not improve\n",
      "Epoch 6170/10000\n",
      "2700/2700 [==============================] - 0s 28us/step - loss: 1.2776e-04 - mean_absolute_error: 0.0065\n",
      "\n",
      "Epoch 06170: loss did not improve\n",
      "Epoch 6171/10000\n",
      "2700/2700 [==============================] - 0s 29us/step - loss: 1.2971e-04 - mean_absolute_error: 0.0068\n",
      "\n",
      "Epoch 06171: loss did not improve\n",
      "Epoch 6172/10000\n",
      "2700/2700 [==============================] - 0s 29us/step - loss: 1.2863e-04 - mean_absolute_error: 0.0066\n",
      "\n",
      "Epoch 06172: loss did not improve\n",
      "Epoch 6173/10000\n",
      "2700/2700 [==============================] - 0s 28us/step - loss: 1.2879e-04 - mean_absolute_error: 0.0068\n",
      "\n",
      "Epoch 06173: loss did not improve\n",
      "Epoch 6174/10000\n",
      "2700/2700 [==============================] - 0s 28us/step - loss: 1.2759e-04 - mean_absolute_error: 0.0064\n",
      "\n",
      "Epoch 06174: loss did not improve\n",
      "Epoch 6175/10000\n",
      "2700/2700 [==============================] - 0s 27us/step - loss: 1.2695e-04 - mean_absolute_error: 0.0065\n",
      "\n",
      "Epoch 06175: loss did not improve\n",
      "Epoch 6176/10000\n",
      "2700/2700 [==============================] - 0s 27us/step - loss: 1.2787e-04 - mean_absolute_error: 0.0067\n",
      "\n",
      "Epoch 06176: loss did not improve\n",
      "Epoch 6177/10000\n",
      "2700/2700 [==============================] - 0s 29us/step - loss: 1.2627e-04 - mean_absolute_error: 0.0064\n",
      "\n",
      "Epoch 06177: loss did not improve\n",
      "Epoch 6178/10000\n",
      "2700/2700 [==============================] - 0s 33us/step - loss: 1.2707e-04 - mean_absolute_error: 0.0064\n",
      "\n",
      "Epoch 06178: loss did not improve\n",
      "Epoch 6179/10000\n",
      "2700/2700 [==============================] - 0s 26us/step - loss: 1.3369e-04 - mean_absolute_error: 0.0070\n",
      "\n",
      "Epoch 06179: loss did not improve\n",
      "Epoch 6180/10000\n",
      "2700/2700 [==============================] - 0s 29us/step - loss: 1.2693e-04 - mean_absolute_error: 0.0065\n",
      "\n",
      "Epoch 06180: loss did not improve\n",
      "Epoch 6181/10000\n",
      "2700/2700 [==============================] - 0s 28us/step - loss: 1.2883e-04 - mean_absolute_error: 0.0065\n",
      "\n",
      "Epoch 06181: loss did not improve\n",
      "Epoch 6182/10000\n",
      "2700/2700 [==============================] - 0s 29us/step - loss: 1.2784e-04 - mean_absolute_error: 0.0066\n",
      "\n",
      "Epoch 06182: loss did not improve\n",
      "Epoch 6183/10000\n",
      "2700/2700 [==============================] - 0s 27us/step - loss: 1.2756e-04 - mean_absolute_error: 0.0066\n",
      "\n",
      "Epoch 06183: loss did not improve\n",
      "Epoch 6184/10000\n",
      "2700/2700 [==============================] - 0s 28us/step - loss: 1.2638e-04 - mean_absolute_error: 0.0065\n",
      "\n",
      "Epoch 06184: loss did not improve\n",
      "Epoch 6185/10000\n",
      "2700/2700 [==============================] - 0s 29us/step - loss: 1.2682e-04 - mean_absolute_error: 0.0066: 0s - loss: 1.3070e-04 - mean_absolute_error: 0.006\n",
      "\n",
      "Epoch 06185: loss did not improve\n",
      "Epoch 6186/10000\n",
      "2700/2700 [==============================] - 0s 28us/step - loss: 1.2659e-04 - mean_absolute_error: 0.0066\n",
      "\n",
      "Epoch 06186: loss did not improve\n",
      "Epoch 6187/10000\n",
      "2700/2700 [==============================] - 0s 27us/step - loss: 1.2938e-04 - mean_absolute_error: 0.0066\n",
      "\n",
      "Epoch 06187: loss did not improve\n",
      "Epoch 6188/10000\n",
      "2700/2700 [==============================] - 0s 28us/step - loss: 1.2913e-04 - mean_absolute_error: 0.0067\n",
      "\n",
      "Epoch 06188: loss did not improve\n",
      "Epoch 6189/10000\n",
      "2700/2700 [==============================] - 0s 29us/step - loss: 1.2822e-04 - mean_absolute_error: 0.0066\n",
      "\n",
      "Epoch 06189: loss did not improve\n",
      "Epoch 6190/10000\n",
      "2700/2700 [==============================] - 0s 29us/step - loss: 1.3049e-04 - mean_absolute_error: 0.0068\n",
      "\n",
      "Epoch 06190: loss did not improve\n",
      "Epoch 6191/10000\n",
      "2700/2700 [==============================] - 0s 28us/step - loss: 1.2617e-04 - mean_absolute_error: 0.0065\n",
      "\n",
      "Epoch 06191: loss did not improve\n",
      "Epoch 6192/10000\n",
      "2700/2700 [==============================] - 0s 28us/step - loss: 1.2661e-04 - mean_absolute_error: 0.0065\n",
      "\n",
      "Epoch 06192: loss did not improve\n",
      "Epoch 6193/10000\n",
      "2700/2700 [==============================] - 0s 29us/step - loss: 1.2725e-04 - mean_absolute_error: 0.0065\n",
      "\n",
      "Epoch 06193: loss did not improve\n",
      "Epoch 6194/10000\n",
      "2700/2700 [==============================] - 0s 29us/step - loss: 1.2882e-04 - mean_absolute_error: 0.0066\n",
      "\n",
      "Epoch 06194: loss did not improve\n",
      "Epoch 6195/10000\n",
      "2700/2700 [==============================] - 0s 29us/step - loss: 1.2675e-04 - mean_absolute_error: 0.0063\n",
      "\n",
      "Epoch 06195: loss did not improve\n",
      "Epoch 6196/10000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2700/2700 [==============================] - 0s 32us/step - loss: 1.2683e-04 - mean_absolute_error: 0.0065\n",
      "\n",
      "Epoch 06196: loss did not improve\n",
      "Epoch 6197/10000\n",
      "2700/2700 [==============================] - 0s 26us/step - loss: 1.2810e-04 - mean_absolute_error: 0.0065\n",
      "\n",
      "Epoch 06197: loss did not improve\n",
      "Epoch 6198/10000\n",
      "2700/2700 [==============================] - 0s 29us/step - loss: 1.2731e-04 - mean_absolute_error: 0.0065\n",
      "\n",
      "Epoch 06198: loss did not improve\n",
      "Epoch 6199/10000\n",
      "2700/2700 [==============================] - 0s 27us/step - loss: 1.2865e-04 - mean_absolute_error: 0.0066\n",
      "\n",
      "Epoch 06199: loss did not improve\n",
      "Epoch 6200/10000\n",
      "2700/2700 [==============================] - 0s 29us/step - loss: 1.2778e-04 - mean_absolute_error: 0.0065\n",
      "\n",
      "Epoch 06200: loss did not improve\n",
      "Epoch 6201/10000\n",
      "2700/2700 [==============================] - 0s 26us/step - loss: 1.2932e-04 - mean_absolute_error: 0.0067\n",
      "\n",
      "Epoch 06201: loss did not improve\n",
      "Epoch 6202/10000\n",
      "2700/2700 [==============================] - 0s 27us/step - loss: 1.2527e-04 - mean_absolute_error: 0.0064\n",
      "\n",
      "Epoch 06202: loss improved from 0.00013 to 0.00013, saving model to keras/best_baseline_weights.hdf5\n",
      "Epoch 6203/10000\n",
      "2700/2700 [==============================] - 0s 28us/step - loss: 1.3098e-04 - mean_absolute_error: 0.0068\n",
      "\n",
      "Epoch 06203: loss did not improve\n",
      "Epoch 6204/10000\n",
      "2700/2700 [==============================] - 0s 27us/step - loss: 1.2572e-04 - mean_absolute_error: 0.0063\n",
      "\n",
      "Epoch 06204: loss did not improve\n",
      "Epoch 6205/10000\n",
      "2700/2700 [==============================] - 0s 28us/step - loss: 1.3013e-04 - mean_absolute_error: 0.0069\n",
      "\n",
      "Epoch 06205: loss did not improve\n",
      "Epoch 6206/10000\n",
      "2700/2700 [==============================] - 0s 27us/step - loss: 1.2764e-04 - mean_absolute_error: 0.0066\n",
      "\n",
      "Epoch 06206: loss did not improve\n",
      "Epoch 6207/10000\n",
      "2700/2700 [==============================] - 0s 28us/step - loss: 1.2756e-04 - mean_absolute_error: 0.0066\n",
      "\n",
      "Epoch 06207: loss did not improve\n",
      "Epoch 6208/10000\n",
      "2700/2700 [==============================] - 0s 28us/step - loss: 1.2627e-04 - mean_absolute_error: 0.0065\n",
      "\n",
      "Epoch 06208: loss did not improve\n",
      "Epoch 6209/10000\n",
      "2700/2700 [==============================] - 0s 28us/step - loss: 1.3136e-04 - mean_absolute_error: 0.0069\n",
      "\n",
      "Epoch 06209: loss did not improve\n",
      "Epoch 6210/10000\n",
      "2700/2700 [==============================] - 0s 28us/step - loss: 1.2609e-04 - mean_absolute_error: 0.0064\n",
      "\n",
      "Epoch 06210: loss did not improve\n",
      "Epoch 6211/10000\n",
      "2700/2700 [==============================] - 0s 28us/step - loss: 1.2601e-04 - mean_absolute_error: 0.0064\n",
      "\n",
      "Epoch 06211: loss did not improve\n",
      "Epoch 6212/10000\n",
      "2700/2700 [==============================] - 0s 28us/step - loss: 1.2617e-04 - mean_absolute_error: 0.0064\n",
      "\n",
      "Epoch 06212: loss did not improve\n",
      "Epoch 6213/10000\n",
      "2700/2700 [==============================] - 0s 29us/step - loss: 1.2601e-04 - mean_absolute_error: 0.0063\n",
      "\n",
      "Epoch 06213: loss did not improve\n",
      "Epoch 6214/10000\n",
      "2700/2700 [==============================] - 0s 26us/step - loss: 1.3068e-04 - mean_absolute_error: 0.0068\n",
      "\n",
      "Epoch 06214: loss did not improve\n",
      "Epoch 6215/10000\n",
      "2700/2700 [==============================] - 0s 30us/step - loss: 1.3340e-04 - mean_absolute_error: 0.0070\n",
      "\n",
      "Epoch 06215: loss did not improve\n",
      "Epoch 6216/10000\n",
      "2700/2700 [==============================] - 0s 29us/step - loss: 1.2719e-04 - mean_absolute_error: 0.0065\n",
      "\n",
      "Epoch 06216: loss did not improve\n",
      "Epoch 6217/10000\n",
      "2700/2700 [==============================] - 0s 27us/step - loss: 1.2717e-04 - mean_absolute_error: 0.0065\n",
      "\n",
      "Epoch 06217: loss did not improve\n",
      "Epoch 6218/10000\n",
      "2700/2700 [==============================] - 0s 27us/step - loss: 1.2911e-04 - mean_absolute_error: 0.0069\n",
      "\n",
      "Epoch 06218: loss did not improve\n",
      "Epoch 6219/10000\n",
      "2700/2700 [==============================] - 0s 28us/step - loss: 1.2669e-04 - mean_absolute_error: 0.0065\n",
      "\n",
      "Epoch 06219: loss did not improve\n",
      "Epoch 6220/10000\n",
      "2700/2700 [==============================] - 0s 30us/step - loss: 1.2781e-04 - mean_absolute_error: 0.0065\n",
      "\n",
      "Epoch 06220: loss did not improve\n",
      "Epoch 6221/10000\n",
      "2700/2700 [==============================] - 0s 29us/step - loss: 1.2607e-04 - mean_absolute_error: 0.0064\n",
      "\n",
      "Epoch 06221: loss did not improve\n",
      "Epoch 6222/10000\n",
      "2700/2700 [==============================] - 0s 28us/step - loss: 1.2575e-04 - mean_absolute_error: 0.0065\n",
      "\n",
      "Epoch 06222: loss did not improve\n",
      "Epoch 6223/10000\n",
      "2700/2700 [==============================] - 0s 28us/step - loss: 1.2702e-04 - mean_absolute_error: 0.0065\n",
      "\n",
      "Epoch 06223: loss did not improve\n",
      "Epoch 6224/10000\n",
      "2700/2700 [==============================] - 0s 30us/step - loss: 1.2824e-04 - mean_absolute_error: 0.0066\n",
      "\n",
      "Epoch 06224: loss did not improve\n",
      "Epoch 6225/10000\n",
      "2700/2700 [==============================] - 0s 28us/step - loss: 1.2990e-04 - mean_absolute_error: 0.0067\n",
      "\n",
      "Epoch 06225: loss did not improve\n",
      "Epoch 6226/10000\n",
      "2700/2700 [==============================] - 0s 26us/step - loss: 1.2711e-04 - mean_absolute_error: 0.0065\n",
      "\n",
      "Epoch 06226: loss did not improve\n",
      "Epoch 6227/10000\n",
      "2700/2700 [==============================] - 0s 29us/step - loss: 1.2650e-04 - mean_absolute_error: 0.0065\n",
      "\n",
      "Epoch 06227: loss did not improve\n",
      "Epoch 6228/10000\n",
      "2700/2700 [==============================] - 0s 28us/step - loss: 1.2814e-04 - mean_absolute_error: 0.0066\n",
      "\n",
      "Epoch 06228: loss did not improve\n",
      "Epoch 6229/10000\n",
      "2700/2700 [==============================] - 0s 29us/step - loss: 1.2794e-04 - mean_absolute_error: 0.0066\n",
      "\n",
      "Epoch 06229: loss did not improve\n",
      "Epoch 6230/10000\n",
      "2700/2700 [==============================] - 0s 30us/step - loss: 1.2789e-04 - mean_absolute_error: 0.0065\n",
      "\n",
      "Epoch 06230: loss did not improve\n",
      "Epoch 6231/10000\n",
      "2700/2700 [==============================] - 0s 26us/step - loss: 1.2750e-04 - mean_absolute_error: 0.0065\n",
      "\n",
      "Epoch 06231: loss did not improve\n",
      "Epoch 6232/10000\n",
      "2700/2700 [==============================] - 0s 31us/step - loss: 1.2983e-04 - mean_absolute_error: 0.0067\n",
      "\n",
      "Epoch 06232: loss did not improve\n",
      "Epoch 6233/10000\n",
      "2700/2700 [==============================] - 0s 32us/step - loss: 1.2697e-04 - mean_absolute_error: 0.0066\n",
      "\n",
      "Epoch 06233: loss did not improve\n",
      "Epoch 6234/10000\n",
      "2700/2700 [==============================] - 0s 28us/step - loss: 1.3151e-04 - mean_absolute_error: 0.0068\n",
      "\n",
      "Epoch 06234: loss did not improve\n",
      "Epoch 6235/10000\n",
      "2700/2700 [==============================] - 0s 29us/step - loss: 1.2605e-04 - mean_absolute_error: 0.0064\n",
      "\n",
      "Epoch 06235: loss did not improve\n",
      "Epoch 6236/10000\n",
      "2700/2700 [==============================] - 0s 27us/step - loss: 1.2582e-04 - mean_absolute_error: 0.0064\n",
      "\n",
      "Epoch 06236: loss did not improve\n",
      "Epoch 6237/10000\n",
      "2700/2700 [==============================] - 0s 30us/step - loss: 1.2558e-04 - mean_absolute_error: 0.0064\n",
      "\n",
      "Epoch 06237: loss did not improve\n",
      "Epoch 6238/10000\n",
      "2700/2700 [==============================] - 0s 29us/step - loss: 1.2529e-04 - mean_absolute_error: 0.0063\n",
      "\n",
      "Epoch 06238: loss did not improve\n",
      "Epoch 6239/10000\n",
      "2700/2700 [==============================] - 0s 28us/step - loss: 1.2843e-04 - mean_absolute_error: 0.0067\n",
      "\n",
      "Epoch 06239: loss did not improve\n",
      "Epoch 6240/10000\n",
      "2700/2700 [==============================] - 0s 26us/step - loss: 1.2836e-04 - mean_absolute_error: 0.0066\n",
      "\n",
      "Epoch 06240: loss did not improve\n",
      "Epoch 6241/10000\n",
      "2700/2700 [==============================] - 0s 28us/step - loss: 1.2788e-04 - mean_absolute_error: 0.0066\n",
      "\n",
      "Epoch 06241: loss did not improve\n",
      "Epoch 6242/10000\n",
      "2700/2700 [==============================] - 0s 28us/step - loss: 1.2547e-04 - mean_absolute_error: 0.0064\n",
      "\n",
      "Epoch 06242: loss did not improve\n",
      "Epoch 6243/10000\n",
      "2700/2700 [==============================] - 0s 29us/step - loss: 1.2756e-04 - mean_absolute_error: 0.0067\n",
      "\n",
      "Epoch 06243: loss did not improve\n",
      "Epoch 6244/10000\n",
      "2700/2700 [==============================] - 0s 26us/step - loss: 1.2619e-04 - mean_absolute_error: 0.0064\n",
      "\n",
      "Epoch 06244: loss did not improve\n",
      "Epoch 6245/10000\n",
      "2700/2700 [==============================] - 0s 28us/step - loss: 1.2787e-04 - mean_absolute_error: 0.0066\n",
      "\n",
      "Epoch 06245: loss did not improve\n",
      "Epoch 6246/10000\n",
      "2700/2700 [==============================] - 0s 28us/step - loss: 1.2589e-04 - mean_absolute_error: 0.0065\n",
      "\n",
      "Epoch 06246: loss did not improve\n",
      "Epoch 6247/10000\n",
      "2700/2700 [==============================] - 0s 30us/step - loss: 1.2941e-04 - mean_absolute_error: 0.0068\n",
      "\n",
      "Epoch 06247: loss did not improve\n",
      "Epoch 6248/10000\n",
      "2700/2700 [==============================] - 0s 29us/step - loss: 1.3404e-04 - mean_absolute_error: 0.0069\n",
      "\n",
      "Epoch 06248: loss did not improve\n",
      "Epoch 6249/10000\n",
      "2700/2700 [==============================] - 0s 26us/step - loss: 1.2712e-04 - mean_absolute_error: 0.0065\n",
      "\n",
      "Epoch 06249: loss did not improve\n",
      "Epoch 6250/10000\n",
      "2700/2700 [==============================] - 0s 29us/step - loss: 1.3069e-04 - mean_absolute_error: 0.0068\n",
      "\n",
      "Epoch 06250: loss did not improve\n",
      "Epoch 6251/10000\n",
      "2700/2700 [==============================] - 0s 29us/step - loss: 1.2879e-04 - mean_absolute_error: 0.0067\n",
      "\n",
      "Epoch 06251: loss did not improve\n",
      "Epoch 6252/10000\n",
      "2700/2700 [==============================] - 0s 28us/step - loss: 1.2683e-04 - mean_absolute_error: 0.0065\n",
      "\n",
      "Epoch 06252: loss did not improve\n",
      "Epoch 6253/10000\n",
      "2700/2700 [==============================] - 0s 28us/step - loss: 1.2858e-04 - mean_absolute_error: 0.0066\n",
      "\n",
      "Epoch 06253: loss did not improve\n",
      "Epoch 6254/10000\n",
      "2700/2700 [==============================] - 0s 27us/step - loss: 1.2747e-04 - mean_absolute_error: 0.0066\n",
      "\n",
      "Epoch 06254: loss did not improve\n",
      "Epoch 6255/10000\n",
      "2700/2700 [==============================] - 0s 28us/step - loss: 1.2703e-04 - mean_absolute_error: 0.0064\n",
      "\n",
      "Epoch 06255: loss did not improve\n",
      "Epoch 6256/10000\n",
      "2700/2700 [==============================] - 0s 28us/step - loss: 1.3069e-04 - mean_absolute_error: 0.0068\n",
      "\n",
      "Epoch 06256: loss did not improve\n",
      "Epoch 6257/10000\n",
      "2700/2700 [==============================] - 0s 25us/step - loss: 1.2592e-04 - mean_absolute_error: 0.0063\n",
      "\n",
      "Epoch 06257: loss did not improve\n",
      "Epoch 6258/10000\n",
      "2700/2700 [==============================] - 0s 27us/step - loss: 1.2918e-04 - mean_absolute_error: 0.0066\n",
      "\n",
      "Epoch 06258: loss did not improve\n",
      "Epoch 6259/10000\n",
      "2700/2700 [==============================] - 0s 30us/step - loss: 1.2559e-04 - mean_absolute_error: 0.0064\n",
      "\n",
      "Epoch 06259: loss did not improve\n",
      "Epoch 6260/10000\n",
      "2700/2700 [==============================] - 0s 29us/step - loss: 1.2525e-04 - mean_absolute_error: 0.0065\n",
      "\n",
      "Epoch 06260: loss improved from 0.00013 to 0.00013, saving model to keras/best_baseline_weights.hdf5\n",
      "Epoch 6261/10000\n",
      "2700/2700 [==============================] - 0s 28us/step - loss: 1.2621e-04 - mean_absolute_error: 0.0066\n",
      "\n",
      "Epoch 06261: loss did not improve\n",
      "Epoch 6262/10000\n",
      "2700/2700 [==============================] - 0s 26us/step - loss: 1.2596e-04 - mean_absolute_error: 0.0065\n",
      "\n",
      "Epoch 06262: loss did not improve\n",
      "Epoch 6263/10000\n",
      "2700/2700 [==============================] - 0s 29us/step - loss: 1.2792e-04 - mean_absolute_error: 0.0065\n",
      "\n",
      "Epoch 06263: loss did not improve\n",
      "Epoch 6264/10000\n",
      "2700/2700 [==============================] - 0s 29us/step - loss: 1.2785e-04 - mean_absolute_error: 0.0066\n",
      "\n",
      "Epoch 06264: loss did not improve\n",
      "Epoch 6265/10000\n",
      "2700/2700 [==============================] - 0s 27us/step - loss: 1.2950e-04 - mean_absolute_error: 0.0067\n",
      "\n",
      "Epoch 06265: loss did not improve\n",
      "Epoch 6266/10000\n",
      "2700/2700 [==============================] - 0s 28us/step - loss: 1.2841e-04 - mean_absolute_error: 0.0067\n",
      "\n",
      "Epoch 06266: loss did not improve\n",
      "Epoch 6267/10000\n",
      "2700/2700 [==============================] - 0s 27us/step - loss: 1.2703e-04 - mean_absolute_error: 0.0064\n",
      "\n",
      "Epoch 06267: loss did not improve\n",
      "Epoch 6268/10000\n",
      "2700/2700 [==============================] - 0s 28us/step - loss: 1.2763e-04 - mean_absolute_error: 0.0066\n",
      "\n",
      "Epoch 06268: loss did not improve\n",
      "Epoch 6269/10000\n",
      "2700/2700 [==============================] - 0s 28us/step - loss: 1.2601e-04 - mean_absolute_error: 0.0064\n",
      "\n",
      "Epoch 06269: loss did not improve\n",
      "Epoch 6270/10000\n",
      "2700/2700 [==============================] - 0s 27us/step - loss: 1.2571e-04 - mean_absolute_error: 0.0064\n",
      "\n",
      "Epoch 06270: loss did not improve\n",
      "Epoch 6271/10000\n",
      "2700/2700 [==============================] - 0s 29us/step - loss: 1.3012e-04 - mean_absolute_error: 0.0068\n",
      "\n",
      "Epoch 06271: loss did not improve\n",
      "Epoch 6272/10000\n",
      "2700/2700 [==============================] - 0s 28us/step - loss: 1.2589e-04 - mean_absolute_error: 0.0064\n",
      "\n",
      "Epoch 06272: loss did not improve\n",
      "Epoch 6273/10000\n",
      "2700/2700 [==============================] - 0s 28us/step - loss: 1.2789e-04 - mean_absolute_error: 0.0066\n",
      "\n",
      "Epoch 06273: loss did not improve\n",
      "Epoch 6274/10000\n",
      "2700/2700 [==============================] - 0s 27us/step - loss: 1.2520e-04 - mean_absolute_error: 0.0064\n",
      "\n",
      "Epoch 06274: loss improved from 0.00013 to 0.00013, saving model to keras/best_baseline_weights.hdf5\n",
      "Epoch 6275/10000\n",
      "2700/2700 [==============================] - 0s 31us/step - loss: 1.2618e-04 - mean_absolute_error: 0.0064\n",
      "\n",
      "Epoch 06275: loss did not improve\n",
      "Epoch 6276/10000\n",
      "2700/2700 [==============================] - 0s 30us/step - loss: 1.2684e-04 - mean_absolute_error: 0.0066\n",
      "\n",
      "Epoch 06276: loss did not improve\n",
      "Epoch 6277/10000\n",
      "2700/2700 [==============================] - 0s 29us/step - loss: 1.3016e-04 - mean_absolute_error: 0.0070\n",
      "\n",
      "Epoch 06277: loss did not improve\n",
      "Epoch 6278/10000\n",
      "2700/2700 [==============================] - 0s 28us/step - loss: 1.3149e-04 - mean_absolute_error: 0.0069\n",
      "\n",
      "Epoch 06278: loss did not improve\n",
      "Epoch 6279/10000\n",
      "2700/2700 [==============================] - 0s 26us/step - loss: 1.2518e-04 - mean_absolute_error: 0.0063\n",
      "\n",
      "Epoch 06279: loss improved from 0.00013 to 0.00013, saving model to keras/best_baseline_weights.hdf5\n",
      "Epoch 6280/10000\n",
      "2700/2700 [==============================] - 0s 28us/step - loss: 1.2631e-04 - mean_absolute_error: 0.0064\n",
      "\n",
      "Epoch 06280: loss did not improve\n",
      "Epoch 6281/10000\n",
      "2700/2700 [==============================] - 0s 30us/step - loss: 1.2721e-04 - mean_absolute_error: 0.0065\n",
      "\n",
      "Epoch 06281: loss did not improve\n",
      "Epoch 6282/10000\n",
      "2700/2700 [==============================] - 0s 30us/step - loss: 1.2612e-04 - mean_absolute_error: 0.0063\n",
      "\n",
      "Epoch 06282: loss did not improve\n",
      "Epoch 6283/10000\n",
      "2700/2700 [==============================] - 0s 30us/step - loss: 1.2991e-04 - mean_absolute_error: 0.0068\n",
      "\n",
      "Epoch 06283: loss did not improve\n",
      "Epoch 6284/10000\n",
      "2700/2700 [==============================] - 0s 28us/step - loss: 1.2612e-04 - mean_absolute_error: 0.0064\n",
      "\n",
      "Epoch 06284: loss did not improve\n",
      "Epoch 6285/10000\n",
      "2700/2700 [==============================] - 0s 28us/step - loss: 1.2492e-04 - mean_absolute_error: 0.0063\n",
      "\n",
      "Epoch 06285: loss improved from 0.00013 to 0.00012, saving model to keras/best_baseline_weights.hdf5\n",
      "Epoch 6286/10000\n",
      "2700/2700 [==============================] - 0s 28us/step - loss: 1.2619e-04 - mean_absolute_error: 0.0065\n",
      "\n",
      "Epoch 06286: loss did not improve\n",
      "Epoch 6287/10000\n",
      "2700/2700 [==============================] - 0s 29us/step - loss: 1.2837e-04 - mean_absolute_error: 0.0068\n",
      "\n",
      "Epoch 06287: loss did not improve\n",
      "Epoch 6288/10000\n",
      "2700/2700 [==============================] - 0s 28us/step - loss: 1.2581e-04 - mean_absolute_error: 0.0064\n",
      "\n",
      "Epoch 06288: loss did not improve\n",
      "Epoch 6289/10000\n",
      "2700/2700 [==============================] - 0s 25us/step - loss: 1.2604e-04 - mean_absolute_error: 0.0065\n",
      "\n",
      "Epoch 06289: loss did not improve\n",
      "Epoch 6290/10000\n",
      "2700/2700 [==============================] - 0s 29us/step - loss: 1.2747e-04 - mean_absolute_error: 0.0065\n",
      "\n",
      "Epoch 06290: loss did not improve\n",
      "Epoch 6291/10000\n",
      "2700/2700 [==============================] - 0s 29us/step - loss: 1.2493e-04 - mean_absolute_error: 0.0063\n",
      "\n",
      "Epoch 06291: loss did not improve\n",
      "Epoch 6292/10000\n",
      "2700/2700 [==============================] - 0s 28us/step - loss: 1.2434e-04 - mean_absolute_error: 0.0063\n",
      "\n",
      "Epoch 06292: loss improved from 0.00012 to 0.00012, saving model to keras/best_baseline_weights.hdf5\n",
      "Epoch 6293/10000\n",
      "2700/2700 [==============================] - 0s 27us/step - loss: 1.3069e-04 - mean_absolute_error: 0.0068\n",
      "\n",
      "Epoch 06293: loss did not improve\n",
      "Epoch 6294/10000\n",
      "2700/2700 [==============================] - 0s 26us/step - loss: 1.2584e-04 - mean_absolute_error: 0.0064\n",
      "\n",
      "Epoch 06294: loss did not improve\n",
      "Epoch 6295/10000\n",
      "2700/2700 [==============================] - 0s 29us/step - loss: 1.2688e-04 - mean_absolute_error: 0.0066\n",
      "\n",
      "Epoch 06295: loss did not improve\n",
      "Epoch 6296/10000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2700/2700 [==============================] - 0s 29us/step - loss: 1.2626e-04 - mean_absolute_error: 0.0065\n",
      "\n",
      "Epoch 06296: loss did not improve\n",
      "Epoch 6297/10000\n",
      "2700/2700 [==============================] - 0s 29us/step - loss: 1.2508e-04 - mean_absolute_error: 0.0063\n",
      "\n",
      "Epoch 06297: loss did not improve\n",
      "Epoch 6298/10000\n",
      "2700/2700 [==============================] - 0s 26us/step - loss: 1.2758e-04 - mean_absolute_error: 0.0065\n",
      "\n",
      "Epoch 06298: loss did not improve\n",
      "Epoch 6299/10000\n",
      "2700/2700 [==============================] - 0s 30us/step - loss: 1.2853e-04 - mean_absolute_error: 0.0067\n",
      "\n",
      "Epoch 06299: loss did not improve\n",
      "Epoch 6300/10000\n",
      "2700/2700 [==============================] - 0s 29us/step - loss: 1.2767e-04 - mean_absolute_error: 0.0066\n",
      "\n",
      "Epoch 06300: loss did not improve\n",
      "Epoch 6301/10000\n",
      "2700/2700 [==============================] - 0s 29us/step - loss: 1.2572e-04 - mean_absolute_error: 0.0064\n",
      "\n",
      "Epoch 06301: loss did not improve\n",
      "Epoch 6302/10000\n",
      "2700/2700 [==============================] - 0s 28us/step - loss: 1.2676e-04 - mean_absolute_error: 0.0066\n",
      "\n",
      "Epoch 06302: loss did not improve\n",
      "Epoch 6303/10000\n",
      "2700/2700 [==============================] - 0s 26us/step - loss: 1.2584e-04 - mean_absolute_error: 0.0064\n",
      "\n",
      "Epoch 06303: loss did not improve\n",
      "Epoch 6304/10000\n",
      "2700/2700 [==============================] - 0s 28us/step - loss: 1.2547e-04 - mean_absolute_error: 0.0064\n",
      "\n",
      "Epoch 06304: loss did not improve\n",
      "Epoch 6305/10000\n",
      "2700/2700 [==============================] - 0s 30us/step - loss: 1.2505e-04 - mean_absolute_error: 0.0063\n",
      "\n",
      "Epoch 06305: loss did not improve\n",
      "Epoch 6306/10000\n",
      "2700/2700 [==============================] - 0s 30us/step - loss: 1.2753e-04 - mean_absolute_error: 0.0065\n",
      "\n",
      "Epoch 06306: loss did not improve\n",
      "Epoch 6307/10000\n",
      "2700/2700 [==============================] - 0s 26us/step - loss: 1.2581e-04 - mean_absolute_error: 0.0064\n",
      "\n",
      "Epoch 06307: loss did not improve\n",
      "Epoch 6308/10000\n",
      "2700/2700 [==============================] - 0s 29us/step - loss: 1.2646e-04 - mean_absolute_error: 0.0066\n",
      "\n",
      "Epoch 06308: loss did not improve\n",
      "Epoch 6309/10000\n",
      "2700/2700 [==============================] - 0s 29us/step - loss: 1.3102e-04 - mean_absolute_error: 0.0068\n",
      "\n",
      "Epoch 06309: loss did not improve\n",
      "Epoch 6310/10000\n",
      "2700/2700 [==============================] - 0s 28us/step - loss: 1.2811e-04 - mean_absolute_error: 0.0067\n",
      "\n",
      "Epoch 06310: loss did not improve\n",
      "Epoch 6311/10000\n",
      "2700/2700 [==============================] - 0s 27us/step - loss: 1.2778e-04 - mean_absolute_error: 0.0066\n",
      "\n",
      "Epoch 06311: loss did not improve\n",
      "Epoch 6312/10000\n",
      "2700/2700 [==============================] - 0s 29us/step - loss: 1.3155e-04 - mean_absolute_error: 0.0068\n",
      "\n",
      "Epoch 06312: loss did not improve\n",
      "Epoch 6313/10000\n",
      "2700/2700 [==============================] - 0s 35us/step - loss: 1.2715e-04 - mean_absolute_error: 0.0065\n",
      "\n",
      "Epoch 06313: loss did not improve\n",
      "Epoch 6314/10000\n",
      "2700/2700 [==============================] - 0s 34us/step - loss: 1.2975e-04 - mean_absolute_error: 0.0066\n",
      "\n",
      "Epoch 06314: loss did not improve\n",
      "Epoch 6315/10000\n",
      "2700/2700 [==============================] - 0s 36us/step - loss: 1.2505e-04 - mean_absolute_error: 0.0063\n",
      "\n",
      "Epoch 06315: loss did not improve\n",
      "Epoch 6316/10000\n",
      "2700/2700 [==============================] - 0s 36us/step - loss: 1.2613e-04 - mean_absolute_error: 0.0065\n",
      "\n",
      "Epoch 06316: loss did not improve\n",
      "Epoch 6317/10000\n",
      "2700/2700 [==============================] - 0s 36us/step - loss: 1.2567e-04 - mean_absolute_error: 0.0064\n",
      "\n",
      "Epoch 06317: loss did not improve\n",
      "Epoch 6318/10000\n",
      "2700/2700 [==============================] - 0s 36us/step - loss: 1.2791e-04 - mean_absolute_error: 0.0066\n",
      "\n",
      "Epoch 06318: loss did not improve\n",
      "Epoch 6319/10000\n",
      "2700/2700 [==============================] - 0s 37us/step - loss: 1.2490e-04 - mean_absolute_error: 0.0063\n",
      "\n",
      "Epoch 06319: loss did not improve\n",
      "Epoch 6320/10000\n",
      "2700/2700 [==============================] - 0s 34us/step - loss: 1.2653e-04 - mean_absolute_error: 0.0066\n",
      "\n",
      "Epoch 06320: loss did not improve\n",
      "Epoch 6321/10000\n",
      "2700/2700 [==============================] - 0s 35us/step - loss: 1.2563e-04 - mean_absolute_error: 0.0065\n",
      "\n",
      "Epoch 06321: loss did not improve\n",
      "Epoch 6322/10000\n",
      "2700/2700 [==============================] - 0s 31us/step - loss: 1.2724e-04 - mean_absolute_error: 0.0064\n",
      "\n",
      "Epoch 06322: loss did not improve\n",
      "Epoch 6323/10000\n",
      "2700/2700 [==============================] - 0s 29us/step - loss: 1.2864e-04 - mean_absolute_error: 0.0067\n",
      "\n",
      "Epoch 06323: loss did not improve\n",
      "Epoch 6324/10000\n",
      "2700/2700 [==============================] - 0s 28us/step - loss: 1.2798e-04 - mean_absolute_error: 0.0066\n",
      "\n",
      "Epoch 06324: loss did not improve\n",
      "Epoch 6325/10000\n",
      "2700/2700 [==============================] - 0s 29us/step - loss: 1.3011e-04 - mean_absolute_error: 0.0068\n",
      "\n",
      "Epoch 06325: loss did not improve\n",
      "Epoch 6326/10000\n",
      "2700/2700 [==============================] - 0s 27us/step - loss: 1.2435e-04 - mean_absolute_error: 0.0063\n",
      "\n",
      "Epoch 06326: loss did not improve\n",
      "Epoch 6327/10000\n",
      "2700/2700 [==============================] - 0s 27us/step - loss: 1.2526e-04 - mean_absolute_error: 0.0064\n",
      "\n",
      "Epoch 06327: loss did not improve\n",
      "Epoch 6328/10000\n",
      "2700/2700 [==============================] - 0s 26us/step - loss: 1.2553e-04 - mean_absolute_error: 0.0064\n",
      "\n",
      "Epoch 06328: loss did not improve\n",
      "Epoch 6329/10000\n",
      "2700/2700 [==============================] - 0s 29us/step - loss: 1.2879e-04 - mean_absolute_error: 0.0067\n",
      "\n",
      "Epoch 06329: loss did not improve\n",
      "Epoch 6330/10000\n",
      "2700/2700 [==============================] - 0s 29us/step - loss: 1.2755e-04 - mean_absolute_error: 0.0066\n",
      "\n",
      "Epoch 06330: loss did not improve\n",
      "Epoch 6331/10000\n",
      "2700/2700 [==============================] - 0s 29us/step - loss: 1.2900e-04 - mean_absolute_error: 0.0067\n",
      "\n",
      "Epoch 06331: loss did not improve\n",
      "Epoch 6332/10000\n",
      "2700/2700 [==============================] - 0s 26us/step - loss: 1.2597e-04 - mean_absolute_error: 0.0064\n",
      "\n",
      "Epoch 06332: loss did not improve\n",
      "Epoch 6333/10000\n",
      "2700/2700 [==============================] - 0s 30us/step - loss: 1.2633e-04 - mean_absolute_error: 0.0065\n",
      "\n",
      "Epoch 06333: loss did not improve\n",
      "Epoch 6334/10000\n",
      "2700/2700 [==============================] - 0s 29us/step - loss: 1.3002e-04 - mean_absolute_error: 0.0068\n",
      "\n",
      "Epoch 06334: loss did not improve\n",
      "Epoch 6335/10000\n",
      "2700/2700 [==============================] - 0s 31us/step - loss: 1.2764e-04 - mean_absolute_error: 0.0065\n",
      "\n",
      "Epoch 06335: loss did not improve\n",
      "Epoch 6336/10000\n",
      "2700/2700 [==============================] - 0s 28us/step - loss: 1.2647e-04 - mean_absolute_error: 0.0065\n",
      "\n",
      "Epoch 06336: loss did not improve\n",
      "Epoch 6337/10000\n",
      "2700/2700 [==============================] - 0s 27us/step - loss: 1.2688e-04 - mean_absolute_error: 0.0065\n",
      "\n",
      "Epoch 06337: loss did not improve\n",
      "Epoch 6338/10000\n",
      "2700/2700 [==============================] - 0s 27us/step - loss: 1.3005e-04 - mean_absolute_error: 0.0066\n",
      "\n",
      "Epoch 06338: loss did not improve\n",
      "Epoch 6339/10000\n",
      "2700/2700 [==============================] - 0s 30us/step - loss: 1.2625e-04 - mean_absolute_error: 0.0064\n",
      "\n",
      "Epoch 06339: loss did not improve\n",
      "Epoch 6340/10000\n",
      "2700/2700 [==============================] - 0s 30us/step - loss: 1.2571e-04 - mean_absolute_error: 0.0065\n",
      "\n",
      "Epoch 06340: loss did not improve\n",
      "Epoch 6341/10000\n",
      "2700/2700 [==============================] - 0s 29us/step - loss: 1.2886e-04 - mean_absolute_error: 0.0066\n",
      "\n",
      "Epoch 06341: loss did not improve\n",
      "Epoch 6342/10000\n",
      "2700/2700 [==============================] - 0s 36us/step - loss: 1.2816e-04 - mean_absolute_error: 0.0066\n",
      "\n",
      "Epoch 06342: loss did not improve\n",
      "Epoch 6343/10000\n",
      "2700/2700 [==============================] - 0s 35us/step - loss: 1.2936e-04 - mean_absolute_error: 0.0069\n",
      "\n",
      "Epoch 06343: loss did not improve\n",
      "Epoch 6344/10000\n",
      "2700/2700 [==============================] - 0s 37us/step - loss: 1.2606e-04 - mean_absolute_error: 0.0065\n",
      "\n",
      "Epoch 06344: loss did not improve\n",
      "Epoch 6345/10000\n",
      "2700/2700 [==============================] - 0s 35us/step - loss: 1.2711e-04 - mean_absolute_error: 0.0066\n",
      "\n",
      "Epoch 06345: loss did not improve\n",
      "Epoch 6346/10000\n",
      "2700/2700 [==============================] - 0s 31us/step - loss: 1.3055e-04 - mean_absolute_error: 0.0068\n",
      "\n",
      "Epoch 06346: loss did not improve\n",
      "Epoch 6347/10000\n",
      "2700/2700 [==============================] - 0s 31us/step - loss: 1.2733e-04 - mean_absolute_error: 0.0065\n",
      "\n",
      "Epoch 06347: loss did not improve\n",
      "Epoch 6348/10000\n",
      "2700/2700 [==============================] - 0s 30us/step - loss: 1.2952e-04 - mean_absolute_error: 0.0066\n",
      "\n",
      "Epoch 06348: loss did not improve\n",
      "Epoch 6349/10000\n",
      "2700/2700 [==============================] - 0s 30us/step - loss: 1.2536e-04 - mean_absolute_error: 0.0064\n",
      "\n",
      "Epoch 06349: loss did not improve\n",
      "Epoch 6350/10000\n",
      "2700/2700 [==============================] - 0s 28us/step - loss: 1.2790e-04 - mean_absolute_error: 0.0066\n",
      "\n",
      "Epoch 06350: loss did not improve\n",
      "Epoch 6351/10000\n",
      "2700/2700 [==============================] - 0s 27us/step - loss: 1.2863e-04 - mean_absolute_error: 0.0067\n",
      "\n",
      "Epoch 06351: loss did not improve\n",
      "Epoch 6352/10000\n",
      "2700/2700 [==============================] - 0s 29us/step - loss: 1.2614e-04 - mean_absolute_error: 0.0064\n",
      "\n",
      "Epoch 06352: loss did not improve\n",
      "Epoch 6353/10000\n",
      "2700/2700 [==============================] - 0s 30us/step - loss: 1.2644e-04 - mean_absolute_error: 0.0064\n",
      "\n",
      "Epoch 06353: loss did not improve\n",
      "Epoch 6354/10000\n",
      "2700/2700 [==============================] - 0s 27us/step - loss: 1.2456e-04 - mean_absolute_error: 0.0064\n",
      "\n",
      "Epoch 06354: loss did not improve\n",
      "Epoch 6355/10000\n",
      "2700/2700 [==============================] - 0s 30us/step - loss: 1.2626e-04 - mean_absolute_error: 0.0065\n",
      "\n",
      "Epoch 06355: loss did not improve\n",
      "Epoch 6356/10000\n",
      "2700/2700 [==============================] - 0s 30us/step - loss: 1.2637e-04 - mean_absolute_error: 0.0064\n",
      "\n",
      "Epoch 06356: loss did not improve\n",
      "Epoch 6357/10000\n",
      "2700/2700 [==============================] - 0s 29us/step - loss: 1.2759e-04 - mean_absolute_error: 0.0066\n",
      "\n",
      "Epoch 06357: loss did not improve\n",
      "Epoch 6358/10000\n",
      "2700/2700 [==============================] - 0s 40us/step - loss: 1.2931e-04 - mean_absolute_error: 0.0068\n",
      "\n",
      "Epoch 06358: loss did not improve\n",
      "Epoch 6359/10000\n",
      "2700/2700 [==============================] - 0s 41us/step - loss: 1.2930e-04 - mean_absolute_error: 0.0067\n",
      "\n",
      "Epoch 06359: loss did not improve\n",
      "Epoch 6360/10000\n",
      "2700/2700 [==============================] - 0s 41us/step - loss: 1.2761e-04 - mean_absolute_error: 0.0066\n",
      "\n",
      "Epoch 06360: loss did not improve\n",
      "Epoch 6361/10000\n",
      "2700/2700 [==============================] - 0s 39us/step - loss: 1.2703e-04 - mean_absolute_error: 0.0065\n",
      "\n",
      "Epoch 06361: loss did not improve\n",
      "Epoch 6362/10000\n",
      "2700/2700 [==============================] - 0s 39us/step - loss: 1.3017e-04 - mean_absolute_error: 0.0068\n",
      "\n",
      "Epoch 06362: loss did not improve\n",
      "Epoch 6363/10000\n",
      "2700/2700 [==============================] - 0s 34us/step - loss: 1.2635e-04 - mean_absolute_error: 0.0065\n",
      "\n",
      "Epoch 06363: loss did not improve\n",
      "Epoch 6364/10000\n",
      "2700/2700 [==============================] - 0s 43us/step - loss: 1.2564e-04 - mean_absolute_error: 0.0064\n",
      "\n",
      "Epoch 06364: loss did not improve\n",
      "Epoch 6365/10000\n",
      "2700/2700 [==============================] - 0s 43us/step - loss: 1.2716e-04 - mean_absolute_error: 0.0065\n",
      "\n",
      "Epoch 06365: loss did not improve\n",
      "Epoch 6366/10000\n",
      "2700/2700 [==============================] - 0s 37us/step - loss: 1.2973e-04 - mean_absolute_error: 0.0068\n",
      "\n",
      "Epoch 06366: loss did not improve\n",
      "Epoch 6367/10000\n",
      "2700/2700 [==============================] - 0s 39us/step - loss: 1.2548e-04 - mean_absolute_error: 0.0064\n",
      "\n",
      "Epoch 06367: loss did not improve\n",
      "Epoch 6368/10000\n",
      "2700/2700 [==============================] - 0s 41us/step - loss: 1.2601e-04 - mean_absolute_error: 0.0065\n",
      "\n",
      "Epoch 06368: loss did not improve\n",
      "Epoch 6369/10000\n",
      "2700/2700 [==============================] - 0s 43us/step - loss: 1.2586e-04 - mean_absolute_error: 0.0065\n",
      "\n",
      "Epoch 06369: loss did not improve\n",
      "Epoch 6370/10000\n",
      "2700/2700 [==============================] - 0s 44us/step - loss: 1.2569e-04 - mean_absolute_error: 0.0063\n",
      "\n",
      "Epoch 06370: loss did not improve\n",
      "Epoch 6371/10000\n",
      "2700/2700 [==============================] - 0s 37us/step - loss: 1.2848e-04 - mean_absolute_error: 0.0067\n",
      "\n",
      "Epoch 06371: loss did not improve\n",
      "Epoch 6372/10000\n",
      "2700/2700 [==============================] - 0s 36us/step - loss: 1.2549e-04 - mean_absolute_error: 0.0064\n",
      "\n",
      "Epoch 06372: loss did not improve\n",
      "Epoch 6373/10000\n",
      "2700/2700 [==============================] - 0s 42us/step - loss: 1.2666e-04 - mean_absolute_error: 0.0065\n",
      "\n",
      "Epoch 06373: loss did not improve\n",
      "Epoch 6374/10000\n",
      "2700/2700 [==============================] - 0s 46us/step - loss: 1.3014e-04 - mean_absolute_error: 0.0068\n",
      "\n",
      "Epoch 06374: loss did not improve\n",
      "Epoch 6375/10000\n",
      "2700/2700 [==============================] - 0s 37us/step - loss: 1.2979e-04 - mean_absolute_error: 0.0068\n",
      "\n",
      "Epoch 06375: loss did not improve\n",
      "Epoch 6376/10000\n",
      "2700/2700 [==============================] - 0s 32us/step - loss: 1.2750e-04 - mean_absolute_error: 0.0067\n",
      "\n",
      "Epoch 06376: loss did not improve\n",
      "Epoch 6377/10000\n",
      "2700/2700 [==============================] - 0s 32us/step - loss: 1.2604e-04 - mean_absolute_error: 0.0065\n",
      "\n",
      "Epoch 06377: loss did not improve\n",
      "Epoch 6378/10000\n",
      "2700/2700 [==============================] - 0s 32us/step - loss: 1.2602e-04 - mean_absolute_error: 0.0064\n",
      "\n",
      "Epoch 06378: loss did not improve\n",
      "Epoch 6379/10000\n",
      "2700/2700 [==============================] - 0s 30us/step - loss: 1.2651e-04 - mean_absolute_error: 0.0066\n",
      "\n",
      "Epoch 06379: loss did not improve\n",
      "Epoch 6380/10000\n",
      "2700/2700 [==============================] - 0s 30us/step - loss: 1.2535e-04 - mean_absolute_error: 0.0063\n",
      "\n",
      "Epoch 06380: loss did not improve\n",
      "Epoch 6381/10000\n",
      "2700/2700 [==============================] - 0s 30us/step - loss: 1.2848e-04 - mean_absolute_error: 0.0066\n",
      "\n",
      "Epoch 06381: loss did not improve\n",
      "Epoch 6382/10000\n",
      "2700/2700 [==============================] - 0s 29us/step - loss: 1.2606e-04 - mean_absolute_error: 0.0065\n",
      "\n",
      "Epoch 06382: loss did not improve\n",
      "Epoch 6383/10000\n",
      "2700/2700 [==============================] - 0s 32us/step - loss: 1.2703e-04 - mean_absolute_error: 0.0066\n",
      "\n",
      "Epoch 06383: loss did not improve\n",
      "Epoch 6384/10000\n",
      "2700/2700 [==============================] - 0s 30us/step - loss: 1.2580e-04 - mean_absolute_error: 0.0064\n",
      "\n",
      "Epoch 06384: loss did not improve\n",
      "Epoch 6385/10000\n",
      "2700/2700 [==============================] - 0s 30us/step - loss: 1.2533e-04 - mean_absolute_error: 0.0065\n",
      "\n",
      "Epoch 06385: loss did not improve\n",
      "Epoch 6386/10000\n",
      "2700/2700 [==============================] - 0s 30us/step - loss: 1.2630e-04 - mean_absolute_error: 0.0064\n",
      "\n",
      "Epoch 06386: loss did not improve\n",
      "Epoch 6387/10000\n",
      "2700/2700 [==============================] - 0s 32us/step - loss: 1.2587e-04 - mean_absolute_error: 0.0065\n",
      "\n",
      "Epoch 06387: loss did not improve\n",
      "Epoch 6388/10000\n",
      "2700/2700 [==============================] - 0s 35us/step - loss: 1.2536e-04 - mean_absolute_error: 0.0064\n",
      "\n",
      "Epoch 06388: loss did not improve\n",
      "Epoch 6389/10000\n",
      "2700/2700 [==============================] - 0s 33us/step - loss: 1.2834e-04 - mean_absolute_error: 0.0067\n",
      "\n",
      "Epoch 06389: loss did not improve\n",
      "Epoch 6390/10000\n",
      "2700/2700 [==============================] - 0s 30us/step - loss: 1.2742e-04 - mean_absolute_error: 0.0067\n",
      "\n",
      "Epoch 06390: loss did not improve\n",
      "Epoch 6391/10000\n",
      "2700/2700 [==============================] - 0s 31us/step - loss: 1.2676e-04 - mean_absolute_error: 0.0066\n",
      "\n",
      "Epoch 06391: loss did not improve\n",
      "Epoch 6392/10000\n",
      "2700/2700 [==============================] - 0s 28us/step - loss: 1.2514e-04 - mean_absolute_error: 0.0064\n",
      "\n",
      "Epoch 06392: loss did not improve\n",
      "Epoch 6393/10000\n",
      "2700/2700 [==============================] - 0s 26us/step - loss: 1.2727e-04 - mean_absolute_error: 0.0065\n",
      "\n",
      "Epoch 06393: loss did not improve\n",
      "Epoch 6394/10000\n",
      "2700/2700 [==============================] - 0s 31us/step - loss: 1.2813e-04 - mean_absolute_error: 0.0066\n",
      "\n",
      "Epoch 06394: loss did not improve\n",
      "Epoch 6395/10000\n",
      "2700/2700 [==============================] - 0s 32us/step - loss: 1.2658e-04 - mean_absolute_error: 0.0065\n",
      "\n",
      "Epoch 06395: loss did not improve\n",
      "Epoch 6396/10000\n",
      "2700/2700 [==============================] - 0s 41us/step - loss: 1.2501e-04 - mean_absolute_error: 0.0064\n",
      "\n",
      "Epoch 06396: loss did not improve\n",
      "Epoch 6397/10000\n",
      "2700/2700 [==============================] - 0s 37us/step - loss: 1.2603e-04 - mean_absolute_error: 0.0065\n",
      "\n",
      "Epoch 06397: loss did not improve\n",
      "Epoch 6398/10000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2700/2700 [==============================] - 0s 31us/step - loss: 1.2777e-04 - mean_absolute_error: 0.0066\n",
      "\n",
      "Epoch 06398: loss did not improve\n",
      "Epoch 6399/10000\n",
      "2700/2700 [==============================] - 0s 29us/step - loss: 1.3239e-04 - mean_absolute_error: 0.0069\n",
      "\n",
      "Epoch 06399: loss did not improve\n",
      "Epoch 6400/10000\n",
      "2700/2700 [==============================] - 0s 30us/step - loss: 1.2493e-04 - mean_absolute_error: 0.0064\n",
      "\n",
      "Epoch 06400: loss did not improve\n",
      "Epoch 6401/10000\n",
      "2700/2700 [==============================] - 0s 27us/step - loss: 1.2757e-04 - mean_absolute_error: 0.0066\n",
      "\n",
      "Epoch 06401: loss did not improve\n",
      "Epoch 6402/10000\n",
      "2700/2700 [==============================] - 0s 31us/step - loss: 1.2534e-04 - mean_absolute_error: 0.0064\n",
      "\n",
      "Epoch 06402: loss did not improve\n",
      "Epoch 6403/10000\n",
      "2700/2700 [==============================] - 0s 29us/step - loss: 1.2629e-04 - mean_absolute_error: 0.0065\n",
      "\n",
      "Epoch 06403: loss did not improve\n",
      "Epoch 6404/10000\n",
      "2700/2700 [==============================] - 0s 31us/step - loss: 1.2606e-04 - mean_absolute_error: 0.0065\n",
      "\n",
      "Epoch 06404: loss did not improve\n",
      "Epoch 6405/10000\n",
      "2700/2700 [==============================] - 0s 31us/step - loss: 1.2780e-04 - mean_absolute_error: 0.0067\n",
      "\n",
      "Epoch 06405: loss did not improve\n",
      "Epoch 6406/10000\n",
      "2700/2700 [==============================] - 0s 34us/step - loss: 1.2888e-04 - mean_absolute_error: 0.0066\n",
      "\n",
      "Epoch 06406: loss did not improve\n",
      "Epoch 6407/10000\n",
      "2700/2700 [==============================] - 0s 32us/step - loss: 1.2730e-04 - mean_absolute_error: 0.0066\n",
      "\n",
      "Epoch 06407: loss did not improve\n",
      "Epoch 6408/10000\n",
      "2700/2700 [==============================] - 0s 35us/step - loss: 1.2834e-04 - mean_absolute_error: 0.0066\n",
      "\n",
      "Epoch 06408: loss did not improve\n",
      "Epoch 6409/10000\n",
      "2700/2700 [==============================] - 0s 34us/step - loss: 1.2725e-04 - mean_absolute_error: 0.0066\n",
      "\n",
      "Epoch 06409: loss did not improve\n",
      "Epoch 6410/10000\n",
      "2700/2700 [==============================] - 0s 36us/step - loss: 1.2704e-04 - mean_absolute_error: 0.0067\n",
      "\n",
      "Epoch 06410: loss did not improve\n",
      "Epoch 6411/10000\n",
      "2700/2700 [==============================] - 0s 39us/step - loss: 1.2681e-04 - mean_absolute_error: 0.0065\n",
      "\n",
      "Epoch 06411: loss did not improve\n",
      "Epoch 6412/10000\n",
      "2700/2700 [==============================] - 0s 35us/step - loss: 1.2803e-04 - mean_absolute_error: 0.0067\n",
      "\n",
      "Epoch 06412: loss did not improve\n",
      "Epoch 6413/10000\n",
      "2700/2700 [==============================] - 0s 30us/step - loss: 1.2529e-04 - mean_absolute_error: 0.0063\n",
      "\n",
      "Epoch 06413: loss did not improve\n",
      "Epoch 6414/10000\n",
      "2700/2700 [==============================] - 0s 32us/step - loss: 1.2469e-04 - mean_absolute_error: 0.0063\n",
      "\n",
      "Epoch 06414: loss did not improve\n",
      "Epoch 6415/10000\n",
      "2700/2700 [==============================] - 0s 32us/step - loss: 1.2496e-04 - mean_absolute_error: 0.0065\n",
      "\n",
      "Epoch 06415: loss did not improve\n",
      "Epoch 6416/10000\n",
      "2700/2700 [==============================] - 0s 29us/step - loss: 1.2521e-04 - mean_absolute_error: 0.0065\n",
      "\n",
      "Epoch 06416: loss did not improve\n",
      "Epoch 6417/10000\n",
      "2700/2700 [==============================] - 0s 29us/step - loss: 1.2869e-04 - mean_absolute_error: 0.0067\n",
      "\n",
      "Epoch 06417: loss did not improve\n",
      "Epoch 6418/10000\n",
      "2700/2700 [==============================] - 0s 31us/step - loss: 1.2707e-04 - mean_absolute_error: 0.0065\n",
      "\n",
      "Epoch 06418: loss did not improve\n",
      "Epoch 6419/10000\n",
      "2700/2700 [==============================] - 0s 31us/step - loss: 1.2467e-04 - mean_absolute_error: 0.0064\n",
      "\n",
      "Epoch 06419: loss did not improve\n",
      "Epoch 6420/10000\n",
      "2700/2700 [==============================] - 0s 30us/step - loss: 1.2810e-04 - mean_absolute_error: 0.0066\n",
      "\n",
      "Epoch 06420: loss did not improve\n",
      "Epoch 6421/10000\n",
      "2700/2700 [==============================] - 0s 31us/step - loss: 1.2508e-04 - mean_absolute_error: 0.0065\n",
      "\n",
      "Epoch 06421: loss did not improve\n",
      "Epoch 6422/10000\n",
      "2700/2700 [==============================] - 0s 36us/step - loss: 1.2523e-04 - mean_absolute_error: 0.0064\n",
      "\n",
      "Epoch 06422: loss did not improve\n",
      "Epoch 6423/10000\n",
      "2700/2700 [==============================] - 0s 33us/step - loss: 1.2613e-04 - mean_absolute_error: 0.0064\n",
      "\n",
      "Epoch 06423: loss did not improve\n",
      "Epoch 6424/10000\n",
      "2700/2700 [==============================] - 0s 41us/step - loss: 1.2844e-04 - mean_absolute_error: 0.0067\n",
      "\n",
      "Epoch 06424: loss did not improve\n",
      "Epoch 6425/10000\n",
      "2700/2700 [==============================] - 0s 36us/step - loss: 1.2488e-04 - mean_absolute_error: 0.0064\n",
      "\n",
      "Epoch 06425: loss did not improve\n",
      "Epoch 6426/10000\n",
      "2700/2700 [==============================] - 0s 35us/step - loss: 1.2716e-04 - mean_absolute_error: 0.0066\n",
      "\n",
      "Epoch 06426: loss did not improve\n",
      "Epoch 6427/10000\n",
      "2700/2700 [==============================] - 0s 30us/step - loss: 1.2774e-04 - mean_absolute_error: 0.0067\n",
      "\n",
      "Epoch 06427: loss did not improve\n",
      "Epoch 6428/10000\n",
      "2700/2700 [==============================] - 0s 29us/step - loss: 1.2833e-04 - mean_absolute_error: 0.0065\n",
      "\n",
      "Epoch 06428: loss did not improve\n",
      "Epoch 6429/10000\n",
      "2700/2700 [==============================] - 0s 32us/step - loss: 1.2676e-04 - mean_absolute_error: 0.0066\n",
      "\n",
      "Epoch 06429: loss did not improve\n",
      "Epoch 6430/10000\n",
      "2700/2700 [==============================] - 0s 30us/step - loss: 1.2670e-04 - mean_absolute_error: 0.0066\n",
      "\n",
      "Epoch 06430: loss did not improve\n",
      "Epoch 6431/10000\n",
      "2700/2700 [==============================] - 0s 36us/step - loss: 1.2992e-04 - mean_absolute_error: 0.0068\n",
      "\n",
      "Epoch 06431: loss did not improve\n",
      "Epoch 6432/10000\n",
      "2700/2700 [==============================] - 0s 33us/step - loss: 1.2630e-04 - mean_absolute_error: 0.0065\n",
      "\n",
      "Epoch 06432: loss did not improve\n",
      "Epoch 6433/10000\n",
      "2700/2700 [==============================] - 0s 31us/step - loss: 1.3268e-04 - mean_absolute_error: 0.0070\n",
      "\n",
      "Epoch 06433: loss did not improve\n",
      "Epoch 6434/10000\n",
      "2700/2700 [==============================] - 0s 37us/step - loss: 1.2623e-04 - mean_absolute_error: 0.0066\n",
      "\n",
      "Epoch 06434: loss did not improve\n",
      "Epoch 6435/10000\n",
      "2700/2700 [==============================] - 0s 36us/step - loss: 1.2526e-04 - mean_absolute_error: 0.0064\n",
      "\n",
      "Epoch 06435: loss did not improve\n",
      "Epoch 6436/10000\n",
      "2700/2700 [==============================] - 0s 33us/step - loss: 1.2530e-04 - mean_absolute_error: 0.0064\n",
      "\n",
      "Epoch 06436: loss did not improve\n",
      "Epoch 6437/10000\n",
      "2700/2700 [==============================] - 0s 31us/step - loss: 1.2868e-04 - mean_absolute_error: 0.0067\n",
      "\n",
      "Epoch 06437: loss did not improve\n",
      "Epoch 6438/10000\n",
      "2700/2700 [==============================] - 0s 36us/step - loss: 1.2523e-04 - mean_absolute_error: 0.0064\n",
      "\n",
      "Epoch 06438: loss did not improve\n",
      "Epoch 6439/10000\n",
      "2700/2700 [==============================] - 0s 32us/step - loss: 1.2557e-04 - mean_absolute_error: 0.0064\n",
      "\n",
      "Epoch 06439: loss did not improve\n",
      "Epoch 6440/10000\n",
      "2700/2700 [==============================] - 0s 34us/step - loss: 1.2536e-04 - mean_absolute_error: 0.0063\n",
      "\n",
      "Epoch 06440: loss did not improve\n",
      "Epoch 6441/10000\n",
      "2700/2700 [==============================] - 0s 32us/step - loss: 1.2625e-04 - mean_absolute_error: 0.0067\n",
      "\n",
      "Epoch 06441: loss did not improve\n",
      "Epoch 6442/10000\n",
      "2700/2700 [==============================] - 0s 38us/step - loss: 1.2678e-04 - mean_absolute_error: 0.0066\n",
      "\n",
      "Epoch 06442: loss did not improve\n",
      "Epoch 6443/10000\n",
      "2700/2700 [==============================] - 0s 46us/step - loss: 1.2985e-04 - mean_absolute_error: 0.0068\n",
      "\n",
      "Epoch 06443: loss did not improve\n",
      "Epoch 6444/10000\n",
      "2700/2700 [==============================] - 0s 34us/step - loss: 1.2529e-04 - mean_absolute_error: 0.0064\n",
      "\n",
      "Epoch 06444: loss did not improve\n",
      "Epoch 6445/10000\n",
      "2700/2700 [==============================] - 0s 31us/step - loss: 1.2584e-04 - mean_absolute_error: 0.0065\n",
      "\n",
      "Epoch 06445: loss did not improve\n",
      "Epoch 6446/10000\n",
      "2700/2700 [==============================] - 0s 35us/step - loss: 1.2563e-04 - mean_absolute_error: 0.0064\n",
      "\n",
      "Epoch 06446: loss did not improve\n",
      "Epoch 6447/10000\n",
      "2700/2700 [==============================] - 0s 28us/step - loss: 1.2676e-04 - mean_absolute_error: 0.0065\n",
      "\n",
      "Epoch 06447: loss did not improve\n",
      "Epoch 6448/10000\n",
      "2700/2700 [==============================] - 0s 32us/step - loss: 1.2579e-04 - mean_absolute_error: 0.0065\n",
      "\n",
      "Epoch 06448: loss did not improve\n",
      "Epoch 6449/10000\n",
      "2700/2700 [==============================] - 0s 29us/step - loss: 1.2754e-04 - mean_absolute_error: 0.0066\n",
      "\n",
      "Epoch 06449: loss did not improve\n",
      "Epoch 6450/10000\n",
      "2700/2700 [==============================] - 0s 28us/step - loss: 1.2670e-04 - mean_absolute_error: 0.0066\n",
      "\n",
      "Epoch 06450: loss did not improve\n",
      "Epoch 6451/10000\n",
      "2700/2700 [==============================] - 0s 30us/step - loss: 1.3003e-04 - mean_absolute_error: 0.0068\n",
      "\n",
      "Epoch 06451: loss did not improve\n",
      "Epoch 6452/10000\n",
      "2700/2700 [==============================] - 0s 35us/step - loss: 1.2594e-04 - mean_absolute_error: 0.0065\n",
      "\n",
      "Epoch 06452: loss did not improve\n",
      "Epoch 6453/10000\n",
      "2700/2700 [==============================] - 0s 31us/step - loss: 1.2975e-04 - mean_absolute_error: 0.0068\n",
      "\n",
      "Epoch 06453: loss did not improve\n",
      "Epoch 6454/10000\n",
      "2700/2700 [==============================] - 0s 35us/step - loss: 1.2739e-04 - mean_absolute_error: 0.0066\n",
      "\n",
      "Epoch 06454: loss did not improve\n",
      "Epoch 6455/10000\n",
      "2700/2700 [==============================] - 0s 33us/step - loss: 1.2516e-04 - mean_absolute_error: 0.0065\n",
      "\n",
      "Epoch 06455: loss did not improve\n",
      "Epoch 6456/10000\n",
      "2700/2700 [==============================] - 0s 31us/step - loss: 1.2379e-04 - mean_absolute_error: 0.0063\n",
      "\n",
      "Epoch 06456: loss improved from 0.00012 to 0.00012, saving model to keras/best_baseline_weights.hdf5\n",
      "Epoch 6457/10000\n",
      "2700/2700 [==============================] - 0s 34us/step - loss: 1.2592e-04 - mean_absolute_error: 0.0065\n",
      "\n",
      "Epoch 06457: loss did not improve\n",
      "Epoch 6458/10000\n",
      "2700/2700 [==============================] - 0s 29us/step - loss: 1.2675e-04 - mean_absolute_error: 0.0066\n",
      "\n",
      "Epoch 06458: loss did not improve\n",
      "Epoch 6459/10000\n",
      "2700/2700 [==============================] - 0s 31us/step - loss: 1.2460e-04 - mean_absolute_error: 0.0063\n",
      "\n",
      "Epoch 06459: loss did not improve\n",
      "Epoch 6460/10000\n",
      "2700/2700 [==============================] - 0s 30us/step - loss: 1.2833e-04 - mean_absolute_error: 0.0067\n",
      "\n",
      "Epoch 06460: loss did not improve\n",
      "Epoch 6461/10000\n",
      "2700/2700 [==============================] - 0s 32us/step - loss: 1.2593e-04 - mean_absolute_error: 0.0065\n",
      "\n",
      "Epoch 06461: loss did not improve\n",
      "Epoch 6462/10000\n",
      "2700/2700 [==============================] - 0s 30us/step - loss: 1.2462e-04 - mean_absolute_error: 0.0063\n",
      "\n",
      "Epoch 06462: loss did not improve\n",
      "Epoch 6463/10000\n",
      "2700/2700 [==============================] - 0s 28us/step - loss: 1.2518e-04 - mean_absolute_error: 0.0064\n",
      "\n",
      "Epoch 06463: loss did not improve\n",
      "Epoch 6464/10000\n",
      "2700/2700 [==============================] - 0s 31us/step - loss: 1.2614e-04 - mean_absolute_error: 0.0065\n",
      "\n",
      "Epoch 06464: loss did not improve\n",
      "Epoch 6465/10000\n",
      "2700/2700 [==============================] - 0s 34us/step - loss: 1.2909e-04 - mean_absolute_error: 0.0069\n",
      "\n",
      "Epoch 06465: loss did not improve\n",
      "Epoch 6466/10000\n",
      "2700/2700 [==============================] - 0s 34us/step - loss: 1.2525e-04 - mean_absolute_error: 0.0064\n",
      "\n",
      "Epoch 06466: loss did not improve\n",
      "Epoch 6467/10000\n",
      "2700/2700 [==============================] - 0s 31us/step - loss: 1.2563e-04 - mean_absolute_error: 0.0065\n",
      "\n",
      "Epoch 06467: loss did not improve\n",
      "Epoch 6468/10000\n",
      "2700/2700 [==============================] - 0s 32us/step - loss: 1.2718e-04 - mean_absolute_error: 0.0065\n",
      "\n",
      "Epoch 06468: loss did not improve\n",
      "Epoch 6469/10000\n",
      "2700/2700 [==============================] - 0s 32us/step - loss: 1.2880e-04 - mean_absolute_error: 0.0067\n",
      "\n",
      "Epoch 06469: loss did not improve\n",
      "Epoch 6470/10000\n",
      "2700/2700 [==============================] - 0s 32us/step - loss: 1.2664e-04 - mean_absolute_error: 0.0066\n",
      "\n",
      "Epoch 06470: loss did not improve\n",
      "Epoch 6471/10000\n",
      "2700/2700 [==============================] - 0s 29us/step - loss: 1.2774e-04 - mean_absolute_error: 0.0067\n",
      "\n",
      "Epoch 06471: loss did not improve\n",
      "Epoch 6472/10000\n",
      "2700/2700 [==============================] - 0s 32us/step - loss: 1.2699e-04 - mean_absolute_error: 0.0065\n",
      "\n",
      "Epoch 06472: loss did not improve\n",
      "Epoch 6473/10000\n",
      "2700/2700 [==============================] - 0s 31us/step - loss: 1.2351e-04 - mean_absolute_error: 0.0063\n",
      "\n",
      "Epoch 06473: loss improved from 0.00012 to 0.00012, saving model to keras/best_baseline_weights.hdf5\n",
      "Epoch 6474/10000\n",
      "2700/2700 [==============================] - 0s 34us/step - loss: 1.2687e-04 - mean_absolute_error: 0.0065\n",
      "\n",
      "Epoch 06474: loss did not improve\n",
      "Epoch 6475/10000\n",
      "2700/2700 [==============================] - 0s 30us/step - loss: 1.2628e-04 - mean_absolute_error: 0.0067\n",
      "\n",
      "Epoch 06475: loss did not improve\n",
      "Epoch 6476/10000\n",
      "2700/2700 [==============================] - 0s 30us/step - loss: 1.2577e-04 - mean_absolute_error: 0.0064\n",
      "\n",
      "Epoch 06476: loss did not improve\n",
      "Epoch 6477/10000\n",
      "2700/2700 [==============================] - 0s 30us/step - loss: 1.2742e-04 - mean_absolute_error: 0.0066\n",
      "\n",
      "Epoch 06477: loss did not improve\n",
      "Epoch 6478/10000\n",
      "2700/2700 [==============================] - 0s 31us/step - loss: 1.2798e-04 - mean_absolute_error: 0.0067\n",
      "\n",
      "Epoch 06478: loss did not improve\n",
      "Epoch 6479/10000\n",
      "2700/2700 [==============================] - 0s 32us/step - loss: 1.2730e-04 - mean_absolute_error: 0.0065\n",
      "\n",
      "Epoch 06479: loss did not improve\n",
      "Epoch 6480/10000\n",
      "2700/2700 [==============================] - 0s 31us/step - loss: 1.2670e-04 - mean_absolute_error: 0.0065\n",
      "\n",
      "Epoch 06480: loss did not improve\n",
      "Epoch 6481/10000\n",
      "2700/2700 [==============================] - 0s 35us/step - loss: 1.2630e-04 - mean_absolute_error: 0.0065\n",
      "\n",
      "Epoch 06481: loss did not improve\n",
      "Epoch 6482/10000\n",
      "2700/2700 [==============================] - 0s 31us/step - loss: 1.2484e-04 - mean_absolute_error: 0.0064\n",
      "\n",
      "Epoch 06482: loss did not improve\n",
      "Epoch 6483/10000\n",
      "2700/2700 [==============================] - 0s 37us/step - loss: 1.2559e-04 - mean_absolute_error: 0.0065\n",
      "\n",
      "Epoch 06483: loss did not improve\n",
      "Epoch 6484/10000\n",
      "2700/2700 [==============================] - 0s 40us/step - loss: 1.2515e-04 - mean_absolute_error: 0.0064\n",
      "\n",
      "Epoch 06484: loss did not improve\n",
      "Epoch 6485/10000\n",
      "2700/2700 [==============================] - 0s 32us/step - loss: 1.2476e-04 - mean_absolute_error: 0.0064\n",
      "\n",
      "Epoch 06485: loss did not improve\n",
      "Epoch 6486/10000\n",
      "2700/2700 [==============================] - 0s 33us/step - loss: 1.2458e-04 - mean_absolute_error: 0.0063\n",
      "\n",
      "Epoch 06486: loss did not improve\n",
      "Epoch 6487/10000\n",
      "2700/2700 [==============================] - 0s 34us/step - loss: 1.2569e-04 - mean_absolute_error: 0.0064\n",
      "\n",
      "Epoch 06487: loss did not improve\n",
      "Epoch 6488/10000\n",
      "2700/2700 [==============================] - 0s 30us/step - loss: 1.2539e-04 - mean_absolute_error: 0.0063\n",
      "\n",
      "Epoch 06488: loss did not improve\n",
      "Epoch 6489/10000\n",
      "2700/2700 [==============================] - 0s 30us/step - loss: 1.2718e-04 - mean_absolute_error: 0.0066\n",
      "\n",
      "Epoch 06489: loss did not improve\n",
      "Epoch 6490/10000\n",
      "2700/2700 [==============================] - 0s 33us/step - loss: 1.2546e-04 - mean_absolute_error: 0.0065\n",
      "\n",
      "Epoch 06490: loss did not improve\n",
      "Epoch 6491/10000\n",
      "2700/2700 [==============================] - 0s 31us/step - loss: 1.2516e-04 - mean_absolute_error: 0.0064\n",
      "\n",
      "Epoch 06491: loss did not improve\n",
      "Epoch 6492/10000\n",
      "2700/2700 [==============================] - 0s 36us/step - loss: 1.2631e-04 - mean_absolute_error: 0.0065\n",
      "\n",
      "Epoch 06492: loss did not improve\n",
      "Epoch 6493/10000\n",
      "2700/2700 [==============================] - 0s 30us/step - loss: 1.2669e-04 - mean_absolute_error: 0.0066\n",
      "\n",
      "Epoch 06493: loss did not improve\n",
      "Epoch 6494/10000\n",
      "2700/2700 [==============================] - 0s 31us/step - loss: 1.2726e-04 - mean_absolute_error: 0.0066\n",
      "\n",
      "Epoch 06494: loss did not improve\n",
      "Epoch 6495/10000\n",
      "2700/2700 [==============================] - 0s 30us/step - loss: 1.2674e-04 - mean_absolute_error: 0.0066\n",
      "\n",
      "Epoch 06495: loss did not improve\n",
      "Epoch 6496/10000\n",
      "2700/2700 [==============================] - 0s 36us/step - loss: 1.2524e-04 - mean_absolute_error: 0.0064\n",
      "\n",
      "Epoch 06496: loss did not improve\n",
      "Epoch 6497/10000\n",
      "2700/2700 [==============================] - 0s 34us/step - loss: 1.2821e-04 - mean_absolute_error: 0.0067\n",
      "\n",
      "Epoch 06497: loss did not improve\n",
      "Epoch 6498/10000\n",
      "2700/2700 [==============================] - 0s 30us/step - loss: 1.2489e-04 - mean_absolute_error: 0.0063\n",
      "\n",
      "Epoch 06498: loss did not improve\n",
      "Epoch 6499/10000\n",
      "2700/2700 [==============================] - 0s 34us/step - loss: 1.2687e-04 - mean_absolute_error: 0.0066\n",
      "\n",
      "Epoch 06499: loss did not improve\n",
      "Epoch 6500/10000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2700/2700 [==============================] - 0s 32us/step - loss: 1.2823e-04 - mean_absolute_error: 0.0067\n",
      "\n",
      "Epoch 06500: loss did not improve\n",
      "Epoch 6501/10000\n",
      "2700/2700 [==============================] - 0s 29us/step - loss: 1.2406e-04 - mean_absolute_error: 0.0064\n",
      "\n",
      "Epoch 06501: loss did not improve\n",
      "Epoch 6502/10000\n",
      "2700/2700 [==============================] - 0s 26us/step - loss: 1.2594e-04 - mean_absolute_error: 0.0066\n",
      "\n",
      "Epoch 06502: loss did not improve\n",
      "Epoch 6503/10000\n",
      "2700/2700 [==============================] - 0s 27us/step - loss: 1.2624e-04 - mean_absolute_error: 0.0065\n",
      "\n",
      "Epoch 06503: loss did not improve\n",
      "Epoch 6504/10000\n",
      "2700/2700 [==============================] - 0s 29us/step - loss: 1.3004e-04 - mean_absolute_error: 0.0070\n",
      "\n",
      "Epoch 06504: loss did not improve\n",
      "Epoch 6505/10000\n",
      "2700/2700 [==============================] - 0s 30us/step - loss: 1.2529e-04 - mean_absolute_error: 0.0065\n",
      "\n",
      "Epoch 06505: loss did not improve\n",
      "Epoch 6506/10000\n",
      "2700/2700 [==============================] - 0s 31us/step - loss: 1.2584e-04 - mean_absolute_error: 0.0065\n",
      "\n",
      "Epoch 06506: loss did not improve\n",
      "Epoch 6507/10000\n",
      "2700/2700 [==============================] - 0s 26us/step - loss: 1.2667e-04 - mean_absolute_error: 0.0066\n",
      "\n",
      "Epoch 06507: loss did not improve\n",
      "Epoch 6508/10000\n",
      "2700/2700 [==============================] - 0s 36us/step - loss: 1.2544e-04 - mean_absolute_error: 0.0065\n",
      "\n",
      "Epoch 06508: loss did not improve\n",
      "Epoch 6509/10000\n",
      "2700/2700 [==============================] - 0s 31us/step - loss: 1.2622e-04 - mean_absolute_error: 0.0066\n",
      "\n",
      "Epoch 06509: loss did not improve\n",
      "Epoch 6510/10000\n",
      "2700/2700 [==============================] - 0s 33us/step - loss: 1.2629e-04 - mean_absolute_error: 0.0065\n",
      "\n",
      "Epoch 06510: loss did not improve\n",
      "Epoch 6511/10000\n",
      "2700/2700 [==============================] - 0s 33us/step - loss: 1.2421e-04 - mean_absolute_error: 0.0064\n",
      "\n",
      "Epoch 06511: loss did not improve\n",
      "Epoch 6512/10000\n",
      "2700/2700 [==============================] - 0s 32us/step - loss: 1.2483e-04 - mean_absolute_error: 0.0064\n",
      "\n",
      "Epoch 06512: loss did not improve\n",
      "Epoch 6513/10000\n",
      "2700/2700 [==============================] - 0s 35us/step - loss: 1.2631e-04 - mean_absolute_error: 0.0066\n",
      "\n",
      "Epoch 06513: loss did not improve\n",
      "Epoch 6514/10000\n",
      "2700/2700 [==============================] - 0s 34us/step - loss: 1.2626e-04 - mean_absolute_error: 0.0065\n",
      "\n",
      "Epoch 06514: loss did not improve\n",
      "Epoch 6515/10000\n",
      "2700/2700 [==============================] - 0s 34us/step - loss: 1.2486e-04 - mean_absolute_error: 0.0064\n",
      "\n",
      "Epoch 06515: loss did not improve\n",
      "Epoch 6516/10000\n",
      "2700/2700 [==============================] - 0s 29us/step - loss: 1.2467e-04 - mean_absolute_error: 0.0064\n",
      "\n",
      "Epoch 06516: loss did not improve\n",
      "Epoch 6517/10000\n",
      "2700/2700 [==============================] - 0s 34us/step - loss: 1.2679e-04 - mean_absolute_error: 0.0065\n",
      "\n",
      "Epoch 06517: loss did not improve\n",
      "Epoch 6518/10000\n",
      "2700/2700 [==============================] - 0s 31us/step - loss: 1.2557e-04 - mean_absolute_error: 0.0064\n",
      "\n",
      "Epoch 06518: loss did not improve\n",
      "Epoch 6519/10000\n",
      "2700/2700 [==============================] - 0s 30us/step - loss: 1.2669e-04 - mean_absolute_error: 0.0065\n",
      "\n",
      "Epoch 06519: loss did not improve\n",
      "Epoch 6520/10000\n",
      "2700/2700 [==============================] - 0s 33us/step - loss: 1.2666e-04 - mean_absolute_error: 0.0066\n",
      "\n",
      "Epoch 06520: loss did not improve\n",
      "Epoch 6521/10000\n",
      "2700/2700 [==============================] - 0s 35us/step - loss: 1.2803e-04 - mean_absolute_error: 0.0067\n",
      "\n",
      "Epoch 06521: loss did not improve\n",
      "Epoch 6522/10000\n",
      "2700/2700 [==============================] - 0s 34us/step - loss: 1.2602e-04 - mean_absolute_error: 0.0066\n",
      "\n",
      "Epoch 06522: loss did not improve\n",
      "Epoch 6523/10000\n",
      "2700/2700 [==============================] - 0s 29us/step - loss: 1.2632e-04 - mean_absolute_error: 0.0065\n",
      "\n",
      "Epoch 06523: loss did not improve\n",
      "Epoch 6524/10000\n",
      "2700/2700 [==============================] - 0s 30us/step - loss: 1.2629e-04 - mean_absolute_error: 0.0065\n",
      "\n",
      "Epoch 06524: loss did not improve\n",
      "Epoch 6525/10000\n",
      "2700/2700 [==============================] - 0s 35us/step - loss: 1.2848e-04 - mean_absolute_error: 0.0068\n",
      "\n",
      "Epoch 06525: loss did not improve\n",
      "Epoch 6526/10000\n",
      "2700/2700 [==============================] - 0s 32us/step - loss: 1.2843e-04 - mean_absolute_error: 0.0067\n",
      "\n",
      "Epoch 06526: loss did not improve\n",
      "Epoch 6527/10000\n",
      "2700/2700 [==============================] - 0s 30us/step - loss: 1.2734e-04 - mean_absolute_error: 0.0067\n",
      "\n",
      "Epoch 06527: loss did not improve\n",
      "Epoch 6528/10000\n",
      "2700/2700 [==============================] - 0s 33us/step - loss: 1.2510e-04 - mean_absolute_error: 0.0065\n",
      "\n",
      "Epoch 06528: loss did not improve\n",
      "Epoch 6529/10000\n",
      "2700/2700 [==============================] - 0s 32us/step - loss: 1.2558e-04 - mean_absolute_error: 0.0064\n",
      "\n",
      "Epoch 06529: loss did not improve\n",
      "Epoch 6530/10000\n",
      "2700/2700 [==============================] - 0s 35us/step - loss: 1.2561e-04 - mean_absolute_error: 0.0065\n",
      "\n",
      "Epoch 06530: loss did not improve\n",
      "Epoch 6531/10000\n",
      "2700/2700 [==============================] - 0s 32us/step - loss: 1.2585e-04 - mean_absolute_error: 0.0064\n",
      "\n",
      "Epoch 06531: loss did not improve\n",
      "Epoch 6532/10000\n",
      "2700/2700 [==============================] - 0s 33us/step - loss: 1.2561e-04 - mean_absolute_error: 0.0065\n",
      "\n",
      "Epoch 06532: loss did not improve\n",
      "Epoch 6533/10000\n",
      "2700/2700 [==============================] - 0s 32us/step - loss: 1.2509e-04 - mean_absolute_error: 0.0064\n",
      "\n",
      "Epoch 06533: loss did not improve\n",
      "Epoch 6534/10000\n",
      "2700/2700 [==============================] - 0s 28us/step - loss: 1.2568e-04 - mean_absolute_error: 0.0064\n",
      "\n",
      "Epoch 06534: loss did not improve\n",
      "Epoch 6535/10000\n",
      "2700/2700 [==============================] - 0s 33us/step - loss: 1.2620e-04 - mean_absolute_error: 0.0065\n",
      "\n",
      "Epoch 06535: loss did not improve\n",
      "Epoch 6536/10000\n",
      "2700/2700 [==============================] - 0s 34us/step - loss: 1.2654e-04 - mean_absolute_error: 0.0066\n",
      "\n",
      "Epoch 06536: loss did not improve\n",
      "Epoch 6537/10000\n",
      "2700/2700 [==============================] - 0s 34us/step - loss: 1.2588e-04 - mean_absolute_error: 0.0064\n",
      "\n",
      "Epoch 06537: loss did not improve\n",
      "Epoch 6538/10000\n",
      "2700/2700 [==============================] - 0s 34us/step - loss: 1.2548e-04 - mean_absolute_error: 0.0066\n",
      "\n",
      "Epoch 06538: loss did not improve\n",
      "Epoch 6539/10000\n",
      "2700/2700 [==============================] - 0s 29us/step - loss: 1.2627e-04 - mean_absolute_error: 0.0066\n",
      "\n",
      "Epoch 06539: loss did not improve\n",
      "Epoch 6540/10000\n",
      "2700/2700 [==============================] - 0s 33us/step - loss: 1.2379e-04 - mean_absolute_error: 0.0063\n",
      "\n",
      "Epoch 06540: loss did not improve\n",
      "Epoch 6541/10000\n",
      "2700/2700 [==============================] - 0s 35us/step - loss: 1.2650e-04 - mean_absolute_error: 0.0065\n",
      "\n",
      "Epoch 06541: loss did not improve\n",
      "Epoch 6542/10000\n",
      "2700/2700 [==============================] - 0s 33us/step - loss: 1.2768e-04 - mean_absolute_error: 0.0067\n",
      "\n",
      "Epoch 06542: loss did not improve\n",
      "Epoch 6543/10000\n",
      "2700/2700 [==============================] - 0s 35us/step - loss: 1.2462e-04 - mean_absolute_error: 0.0064\n",
      "\n",
      "Epoch 06543: loss did not improve\n",
      "Epoch 6544/10000\n",
      "2700/2700 [==============================] - 0s 37us/step - loss: 1.2587e-04 - mean_absolute_error: 0.0063\n",
      "\n",
      "Epoch 06544: loss did not improve\n",
      "Epoch 6545/10000\n",
      "2700/2700 [==============================] - 0s 40us/step - loss: 1.2498e-04 - mean_absolute_error: 0.0064\n",
      "\n",
      "Epoch 06545: loss did not improve\n",
      "Epoch 6546/10000\n",
      "2700/2700 [==============================] - 0s 31us/step - loss: 1.2666e-04 - mean_absolute_error: 0.0066\n",
      "\n",
      "Epoch 06546: loss did not improve\n",
      "Epoch 6547/10000\n",
      "2700/2700 [==============================] - 0s 31us/step - loss: 1.2999e-04 - mean_absolute_error: 0.0068\n",
      "\n",
      "Epoch 06547: loss did not improve\n",
      "Epoch 6548/10000\n",
      "2700/2700 [==============================] - 0s 31us/step - loss: 1.2493e-04 - mean_absolute_error: 0.0064\n",
      "\n",
      "Epoch 06548: loss did not improve\n",
      "Epoch 6549/10000\n",
      "2700/2700 [==============================] - 0s 34us/step - loss: 1.2470e-04 - mean_absolute_error: 0.0064\n",
      "\n",
      "Epoch 06549: loss did not improve\n",
      "Epoch 6550/10000\n",
      "2700/2700 [==============================] - 0s 35us/step - loss: 1.2415e-04 - mean_absolute_error: 0.0063\n",
      "\n",
      "Epoch 06550: loss did not improve\n",
      "Epoch 6551/10000\n",
      "2700/2700 [==============================] - 0s 35us/step - loss: 1.2545e-04 - mean_absolute_error: 0.0065\n",
      "\n",
      "Epoch 06551: loss did not improve\n",
      "Epoch 6552/10000\n",
      "2700/2700 [==============================] - 0s 36us/step - loss: 1.2638e-04 - mean_absolute_error: 0.0066\n",
      "\n",
      "Epoch 06552: loss did not improve\n",
      "Epoch 6553/10000\n",
      "2700/2700 [==============================] - 0s 36us/step - loss: 1.2460e-04 - mean_absolute_error: 0.0064\n",
      "\n",
      "Epoch 06553: loss did not improve\n",
      "Epoch 6554/10000\n",
      "2700/2700 [==============================] - 0s 34us/step - loss: 1.2596e-04 - mean_absolute_error: 0.0064\n",
      "\n",
      "Epoch 06554: loss did not improve\n",
      "Epoch 6555/10000\n",
      "2700/2700 [==============================] - 0s 29us/step - loss: 1.2581e-04 - mean_absolute_error: 0.0065\n",
      "\n",
      "Epoch 06555: loss did not improve\n",
      "Epoch 6556/10000\n",
      "2700/2700 [==============================] - 0s 30us/step - loss: 1.2827e-04 - mean_absolute_error: 0.0067\n",
      "\n",
      "Epoch 06556: loss did not improve\n",
      "Epoch 6557/10000\n",
      "2700/2700 [==============================] - 0s 31us/step - loss: 1.2536e-04 - mean_absolute_error: 0.0065\n",
      "\n",
      "Epoch 06557: loss did not improve\n",
      "Epoch 6558/10000\n",
      "2700/2700 [==============================] - 0s 36us/step - loss: 1.2590e-04 - mean_absolute_error: 0.0064\n",
      "\n",
      "Epoch 06558: loss did not improve\n",
      "Epoch 6559/10000\n",
      "2700/2700 [==============================] - 0s 32us/step - loss: 1.2439e-04 - mean_absolute_error: 0.0064\n",
      "\n",
      "Epoch 06559: loss did not improve\n",
      "Epoch 6560/10000\n",
      "2700/2700 [==============================] - 0s 35us/step - loss: 1.2492e-04 - mean_absolute_error: 0.0065\n",
      "\n",
      "Epoch 06560: loss did not improve\n",
      "Epoch 6561/10000\n",
      "2700/2700 [==============================] - 0s 32us/step - loss: 1.2757e-04 - mean_absolute_error: 0.0066\n",
      "\n",
      "Epoch 06561: loss did not improve\n",
      "Epoch 6562/10000\n",
      "2700/2700 [==============================] - 0s 29us/step - loss: 1.2590e-04 - mean_absolute_error: 0.0065\n",
      "\n",
      "Epoch 06562: loss did not improve\n",
      "Epoch 6563/10000\n",
      "2700/2700 [==============================] - 0s 32us/step - loss: 1.2623e-04 - mean_absolute_error: 0.0065\n",
      "\n",
      "Epoch 06563: loss did not improve\n",
      "Epoch 6564/10000\n",
      "2700/2700 [==============================] - 0s 33us/step - loss: 1.2727e-04 - mean_absolute_error: 0.0067\n",
      "\n",
      "Epoch 06564: loss did not improve\n",
      "Epoch 6565/10000\n",
      "2700/2700 [==============================] - 0s 34us/step - loss: 1.2604e-04 - mean_absolute_error: 0.0066\n",
      "\n",
      "Epoch 06565: loss did not improve\n",
      "Epoch 6566/10000\n",
      "2700/2700 [==============================] - 0s 33us/step - loss: 1.2493e-04 - mean_absolute_error: 0.0065\n",
      "\n",
      "Epoch 06566: loss did not improve\n",
      "Epoch 6567/10000\n",
      "2700/2700 [==============================] - 0s 34us/step - loss: 1.2368e-04 - mean_absolute_error: 0.0062\n",
      "\n",
      "Epoch 06567: loss did not improve\n",
      "Epoch 6568/10000\n",
      "2700/2700 [==============================] - 0s 31us/step - loss: 1.2755e-04 - mean_absolute_error: 0.0068\n",
      "\n",
      "Epoch 06568: loss did not improve\n",
      "Epoch 6569/10000\n",
      "2700/2700 [==============================] - 0s 29us/step - loss: 1.2531e-04 - mean_absolute_error: 0.0066\n",
      "\n",
      "Epoch 06569: loss did not improve\n",
      "Epoch 6570/10000\n",
      "2700/2700 [==============================] - ETA: 0s - loss: 1.2303e-04 - mean_absolute_error: 0.006 - 0s 32us/step - loss: 1.2457e-04 - mean_absolute_error: 0.0063\n",
      "\n",
      "Epoch 06570: loss did not improve\n",
      "Epoch 6571/10000\n",
      "2700/2700 [==============================] - 0s 34us/step - loss: 1.2544e-04 - mean_absolute_error: 0.0065\n",
      "\n",
      "Epoch 06571: loss did not improve\n",
      "Epoch 6572/10000\n",
      "2700/2700 [==============================] - 0s 34us/step - loss: 1.2525e-04 - mean_absolute_error: 0.0064\n",
      "\n",
      "Epoch 06572: loss did not improve\n",
      "Epoch 6573/10000\n",
      "2700/2700 [==============================] - 0s 36us/step - loss: 1.2467e-04 - mean_absolute_error: 0.0064\n",
      "\n",
      "Epoch 06573: loss did not improve\n",
      "Epoch 6574/10000\n",
      "2700/2700 [==============================] - 0s 33us/step - loss: 1.2405e-04 - mean_absolute_error: 0.0064\n",
      "\n",
      "Epoch 06574: loss did not improve\n",
      "Epoch 6575/10000\n",
      "2700/2700 [==============================] - 0s 31us/step - loss: 1.2551e-04 - mean_absolute_error: 0.0066\n",
      "\n",
      "Epoch 06575: loss did not improve\n",
      "Epoch 6576/10000\n",
      "2700/2700 [==============================] - 0s 35us/step - loss: 1.2441e-04 - mean_absolute_error: 0.0063\n",
      "\n",
      "Epoch 06576: loss did not improve\n",
      "Epoch 6577/10000\n",
      "2700/2700 [==============================] - 0s 31us/step - loss: 1.2429e-04 - mean_absolute_error: 0.0065\n",
      "\n",
      "Epoch 06577: loss did not improve\n",
      "Epoch 6578/10000\n",
      "2700/2700 [==============================] - 0s 30us/step - loss: 1.2424e-04 - mean_absolute_error: 0.0063\n",
      "\n",
      "Epoch 06578: loss did not improve\n",
      "Epoch 6579/10000\n",
      "2700/2700 [==============================] - 0s 31us/step - loss: 1.2598e-04 - mean_absolute_error: 0.0066\n",
      "\n",
      "Epoch 06579: loss did not improve\n",
      "Epoch 6580/10000\n",
      "2700/2700 [==============================] - 0s 33us/step - loss: 1.2391e-04 - mean_absolute_error: 0.0064\n",
      "\n",
      "Epoch 06580: loss did not improve\n",
      "Epoch 6581/10000\n",
      "2700/2700 [==============================] - 0s 32us/step - loss: 1.2611e-04 - mean_absolute_error: 0.0065\n",
      "\n",
      "Epoch 06581: loss did not improve\n",
      "Epoch 6582/10000\n",
      "2700/2700 [==============================] - 0s 33us/step - loss: 1.2659e-04 - mean_absolute_error: 0.0066\n",
      "\n",
      "Epoch 06582: loss did not improve\n",
      "Epoch 6583/10000\n",
      "2700/2700 [==============================] - 0s 31us/step - loss: 1.2608e-04 - mean_absolute_error: 0.0065\n",
      "\n",
      "Epoch 06583: loss did not improve\n",
      "Epoch 6584/10000\n",
      "2700/2700 [==============================] - 0s 27us/step - loss: 1.2585e-04 - mean_absolute_error: 0.0064\n",
      "\n",
      "Epoch 06584: loss did not improve\n",
      "Epoch 6585/10000\n",
      "2700/2700 [==============================] - 0s 36us/step - loss: 1.2840e-04 - mean_absolute_error: 0.0068\n",
      "\n",
      "Epoch 06585: loss did not improve\n",
      "Epoch 6586/10000\n",
      "2700/2700 [==============================] - 0s 30us/step - loss: 1.2531e-04 - mean_absolute_error: 0.0064\n",
      "\n",
      "Epoch 06586: loss did not improve\n",
      "Epoch 6587/10000\n",
      "2700/2700 [==============================] - 0s 33us/step - loss: 1.2520e-04 - mean_absolute_error: 0.0065\n",
      "\n",
      "Epoch 06587: loss did not improve\n",
      "Epoch 6588/10000\n",
      "2700/2700 [==============================] - 0s 31us/step - loss: 1.2751e-04 - mean_absolute_error: 0.0066\n",
      "\n",
      "Epoch 06588: loss did not improve\n",
      "Epoch 6589/10000\n",
      "2700/2700 [==============================] - 0s 31us/step - loss: 1.2610e-04 - mean_absolute_error: 0.0065\n",
      "\n",
      "Epoch 06589: loss did not improve\n",
      "Epoch 6590/10000\n",
      "2700/2700 [==============================] - 0s 31us/step - loss: 1.2614e-04 - mean_absolute_error: 0.0065\n",
      "\n",
      "Epoch 06590: loss did not improve\n",
      "Epoch 6591/10000\n",
      "2700/2700 [==============================] - 0s 29us/step - loss: 1.2464e-04 - mean_absolute_error: 0.0064\n",
      "\n",
      "Epoch 06591: loss did not improve\n",
      "Epoch 6592/10000\n",
      "2700/2700 [==============================] - 0s 35us/step - loss: 1.2742e-04 - mean_absolute_error: 0.0065\n",
      "\n",
      "Epoch 06592: loss did not improve\n",
      "Epoch 6593/10000\n",
      "2700/2700 [==============================] - 0s 34us/step - loss: 1.2413e-04 - mean_absolute_error: 0.0064\n",
      "\n",
      "Epoch 06593: loss did not improve\n",
      "Epoch 6594/10000\n",
      "2700/2700 [==============================] - 0s 33us/step - loss: 1.2776e-04 - mean_absolute_error: 0.0066\n",
      "\n",
      "Epoch 06594: loss did not improve\n",
      "Epoch 6595/10000\n",
      "2700/2700 [==============================] - 0s 34us/step - loss: 1.2507e-04 - mean_absolute_error: 0.0065\n",
      "\n",
      "Epoch 06595: loss did not improve\n",
      "Epoch 6596/10000\n",
      "2700/2700 [==============================] - 0s 32us/step - loss: 1.2602e-04 - mean_absolute_error: 0.0066\n",
      "\n",
      "Epoch 06596: loss did not improve\n",
      "Epoch 6597/10000\n",
      "2700/2700 [==============================] - 0s 33us/step - loss: 1.2569e-04 - mean_absolute_error: 0.0065\n",
      "\n",
      "Epoch 06597: loss did not improve\n",
      "Epoch 6598/10000\n",
      "2700/2700 [==============================] - 0s 30us/step - loss: 1.2604e-04 - mean_absolute_error: 0.0065\n",
      "\n",
      "Epoch 06598: loss did not improve\n",
      "Epoch 6599/10000\n",
      "2700/2700 [==============================] - 0s 29us/step - loss: 1.2628e-04 - mean_absolute_error: 0.0065\n",
      "\n",
      "Epoch 06599: loss did not improve\n",
      "Epoch 6600/10000\n",
      "2700/2700 [==============================] - 0s 37us/step - loss: 1.2463e-04 - mean_absolute_error: 0.0063\n",
      "\n",
      "Epoch 06600: loss did not improve\n",
      "Epoch 6601/10000\n",
      "2700/2700 [==============================] - 0s 34us/step - loss: 1.2674e-04 - mean_absolute_error: 0.0066\n",
      "\n",
      "Epoch 06601: loss did not improve\n",
      "Epoch 6602/10000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2700/2700 [==============================] - 0s 34us/step - loss: 1.2457e-04 - mean_absolute_error: 0.0064\n",
      "\n",
      "Epoch 06602: loss did not improve\n",
      "Epoch 6603/10000\n",
      "2700/2700 [==============================] - 0s 31us/step - loss: 1.2633e-04 - mean_absolute_error: 0.0066\n",
      "\n",
      "Epoch 06603: loss did not improve\n",
      "Epoch 6604/10000\n",
      "2700/2700 [==============================] - 0s 28us/step - loss: 1.2341e-04 - mean_absolute_error: 0.0063\n",
      "\n",
      "Epoch 06604: loss improved from 0.00012 to 0.00012, saving model to keras/best_baseline_weights.hdf5\n",
      "Epoch 6605/10000\n",
      "2700/2700 [==============================] - 0s 32us/step - loss: 1.2645e-04 - mean_absolute_error: 0.0066\n",
      "\n",
      "Epoch 06605: loss did not improve\n",
      "Epoch 6606/10000\n",
      "2700/2700 [==============================] - 0s 32us/step - loss: 1.2821e-04 - mean_absolute_error: 0.0066\n",
      "\n",
      "Epoch 06606: loss did not improve\n",
      "Epoch 6607/10000\n",
      "2700/2700 [==============================] - 0s 34us/step - loss: 1.2667e-04 - mean_absolute_error: 0.0065\n",
      "\n",
      "Epoch 06607: loss did not improve\n",
      "Epoch 6608/10000\n",
      "2700/2700 [==============================] - 0s 33us/step - loss: 1.2747e-04 - mean_absolute_error: 0.0066\n",
      "\n",
      "Epoch 06608: loss did not improve\n",
      "Epoch 6609/10000\n",
      "2700/2700 [==============================] - 0s 28us/step - loss: 1.2461e-04 - mean_absolute_error: 0.0064\n",
      "\n",
      "Epoch 06609: loss did not improve\n",
      "Epoch 6610/10000\n",
      "2700/2700 [==============================] - 0s 30us/step - loss: 1.2614e-04 - mean_absolute_error: 0.0065\n",
      "\n",
      "Epoch 06610: loss did not improve\n",
      "Epoch 6611/10000\n",
      "2700/2700 [==============================] - 0s 29us/step - loss: 1.2613e-04 - mean_absolute_error: 0.0066\n",
      "\n",
      "Epoch 06611: loss did not improve\n",
      "Epoch 6612/10000\n",
      "2700/2700 [==============================] - 0s 30us/step - loss: 1.2895e-04 - mean_absolute_error: 0.0067\n",
      "\n",
      "Epoch 06612: loss did not improve\n",
      "Epoch 6613/10000\n",
      "2700/2700 [==============================] - 0s 31us/step - loss: 1.2513e-04 - mean_absolute_error: 0.0065\n",
      "\n",
      "Epoch 06613: loss did not improve\n",
      "Epoch 6614/10000\n",
      "2700/2700 [==============================] - 0s 27us/step - loss: 1.2602e-04 - mean_absolute_error: 0.0065\n",
      "\n",
      "Epoch 06614: loss did not improve\n",
      "Epoch 6615/10000\n",
      "2700/2700 [==============================] - 0s 33us/step - loss: 1.2399e-04 - mean_absolute_error: 0.0063\n",
      "\n",
      "Epoch 06615: loss did not improve\n",
      "Epoch 6616/10000\n",
      "2700/2700 [==============================] - 0s 30us/step - loss: 1.2469e-04 - mean_absolute_error: 0.0064\n",
      "\n",
      "Epoch 06616: loss did not improve\n",
      "Epoch 6617/10000\n",
      "2700/2700 [==============================] - 0s 32us/step - loss: 1.2942e-04 - mean_absolute_error: 0.0069\n",
      "\n",
      "Epoch 06617: loss did not improve\n",
      "Epoch 6618/10000\n",
      "2700/2700 [==============================] - 0s 31us/step - loss: 1.2364e-04 - mean_absolute_error: 0.0063\n",
      "\n",
      "Epoch 06618: loss did not improve\n",
      "Epoch 6619/10000\n",
      "2700/2700 [==============================] - 0s 30us/step - loss: 1.2659e-04 - mean_absolute_error: 0.0066\n",
      "\n",
      "Epoch 06619: loss did not improve\n",
      "Epoch 6620/10000\n",
      "2700/2700 [==============================] - 0s 27us/step - loss: 1.2478e-04 - mean_absolute_error: 0.0064\n",
      "\n",
      "Epoch 06620: loss did not improve\n",
      "Epoch 6621/10000\n",
      "2700/2700 [==============================] - 0s 31us/step - loss: 1.2429e-04 - mean_absolute_error: 0.0064\n",
      "\n",
      "Epoch 06621: loss did not improve\n",
      "Epoch 6622/10000\n",
      "2700/2700 [==============================] - 0s 35us/step - loss: 1.2600e-04 - mean_absolute_error: 0.0066\n",
      "\n",
      "Epoch 06622: loss did not improve\n",
      "Epoch 6623/10000\n",
      "2700/2700 [==============================] - 0s 30us/step - loss: 1.2373e-04 - mean_absolute_error: 0.0063\n",
      "\n",
      "Epoch 06623: loss did not improve\n",
      "Epoch 6624/10000\n",
      "2700/2700 [==============================] - 0s 32us/step - loss: 1.2450e-04 - mean_absolute_error: 0.0063\n",
      "\n",
      "Epoch 06624: loss did not improve\n",
      "Epoch 6625/10000\n",
      "2700/2700 [==============================] - 0s 32us/step - loss: 1.2502e-04 - mean_absolute_error: 0.0064\n",
      "\n",
      "Epoch 06625: loss did not improve\n",
      "Epoch 6626/10000\n",
      "2700/2700 [==============================] - 0s 30us/step - loss: 1.2715e-04 - mean_absolute_error: 0.0067\n",
      "\n",
      "Epoch 06626: loss did not improve\n",
      "Epoch 6627/10000\n",
      "2700/2700 [==============================] - 0s 31us/step - loss: 1.2680e-04 - mean_absolute_error: 0.0066\n",
      "\n",
      "Epoch 06627: loss did not improve\n",
      "Epoch 6628/10000\n",
      "2700/2700 [==============================] - 0s 30us/step - loss: 1.2561e-04 - mean_absolute_error: 0.0065\n",
      "\n",
      "Epoch 06628: loss did not improve\n",
      "Epoch 6629/10000\n",
      "2700/2700 [==============================] - 0s 32us/step - loss: 1.2384e-04 - mean_absolute_error: 0.0063\n",
      "\n",
      "Epoch 06629: loss did not improve\n",
      "Epoch 6630/10000\n",
      "2700/2700 [==============================] - 0s 32us/step - loss: 1.2359e-04 - mean_absolute_error: 0.0064\n",
      "\n",
      "Epoch 06630: loss did not improve\n",
      "Epoch 6631/10000\n",
      "2700/2700 [==============================] - 0s 31us/step - loss: 1.2373e-04 - mean_absolute_error: 0.0064\n",
      "\n",
      "Epoch 06631: loss did not improve\n",
      "Epoch 6632/10000\n",
      "2700/2700 [==============================] - 0s 26us/step - loss: 1.2905e-04 - mean_absolute_error: 0.0067\n",
      "\n",
      "Epoch 06632: loss did not improve\n",
      "Epoch 6633/10000\n",
      "2700/2700 [==============================] - 0s 33us/step - loss: 1.2781e-04 - mean_absolute_error: 0.0066\n",
      "\n",
      "Epoch 06633: loss did not improve\n",
      "Epoch 6634/10000\n",
      "2700/2700 [==============================] - 0s 30us/step - loss: 1.2538e-04 - mean_absolute_error: 0.0065\n",
      "\n",
      "Epoch 06634: loss did not improve\n",
      "Epoch 6635/10000\n",
      "2700/2700 [==============================] - 0s 29us/step - loss: 1.2413e-04 - mean_absolute_error: 0.0063\n",
      "\n",
      "Epoch 06635: loss did not improve\n",
      "Epoch 6636/10000\n",
      "2700/2700 [==============================] - 0s 30us/step - loss: 1.2645e-04 - mean_absolute_error: 0.0065\n",
      "\n",
      "Epoch 06636: loss did not improve\n",
      "Epoch 6637/10000\n",
      "2700/2700 [==============================] - 0s 28us/step - loss: 1.2671e-04 - mean_absolute_error: 0.0065\n",
      "\n",
      "Epoch 06637: loss did not improve\n",
      "Epoch 6638/10000\n",
      "2700/2700 [==============================] - 0s 32us/step - loss: 1.2537e-04 - mean_absolute_error: 0.0065\n",
      "\n",
      "Epoch 06638: loss did not improve\n",
      "Epoch 6639/10000\n",
      "2700/2700 [==============================] - 0s 31us/step - loss: 1.2804e-04 - mean_absolute_error: 0.0066\n",
      "\n",
      "Epoch 06639: loss did not improve\n",
      "Epoch 6640/10000\n",
      "2700/2700 [==============================] - 0s 30us/step - loss: 1.2498e-04 - mean_absolute_error: 0.0064\n",
      "\n",
      "Epoch 06640: loss did not improve\n",
      "Epoch 6641/10000\n",
      "2700/2700 [==============================] - 0s 35us/step - loss: 1.2644e-04 - mean_absolute_error: 0.0066\n",
      "\n",
      "Epoch 06641: loss did not improve\n",
      "Epoch 6642/10000\n",
      "2700/2700 [==============================] - 0s 31us/step - loss: 1.2760e-04 - mean_absolute_error: 0.0067\n",
      "\n",
      "Epoch 06642: loss did not improve\n",
      "Epoch 6643/10000\n",
      "2700/2700 [==============================] - 0s 31us/step - loss: 1.2992e-04 - mean_absolute_error: 0.0069\n",
      "\n",
      "Epoch 06643: loss did not improve\n",
      "Epoch 6644/10000\n",
      "2700/2700 [==============================] - 0s 31us/step - loss: 1.2573e-04 - mean_absolute_error: 0.0065\n",
      "\n",
      "Epoch 06644: loss did not improve\n",
      "Epoch 6645/10000\n",
      "2700/2700 [==============================] - 0s 34us/step - loss: 1.2534e-04 - mean_absolute_error: 0.0065\n",
      "\n",
      "Epoch 06645: loss did not improve\n",
      "Epoch 6646/10000\n",
      "2700/2700 [==============================] - 0s 31us/step - loss: 1.2695e-04 - mean_absolute_error: 0.0067\n",
      "\n",
      "Epoch 06646: loss did not improve\n",
      "Epoch 6647/10000\n",
      "2700/2700 [==============================] - 0s 30us/step - loss: 1.2547e-04 - mean_absolute_error: 0.0065\n",
      "\n",
      "Epoch 06647: loss did not improve\n",
      "Epoch 6648/10000\n",
      "2700/2700 [==============================] - 0s 30us/step - loss: 1.2410e-04 - mean_absolute_error: 0.0064\n",
      "\n",
      "Epoch 06648: loss did not improve\n",
      "Epoch 6649/10000\n",
      "2700/2700 [==============================] - 0s 31us/step - loss: 1.2481e-04 - mean_absolute_error: 0.0065\n",
      "\n",
      "Epoch 06649: loss did not improve\n",
      "Epoch 6650/10000\n",
      "2700/2700 [==============================] - 0s 30us/step - loss: 1.2640e-04 - mean_absolute_error: 0.0065\n",
      "\n",
      "Epoch 06650: loss did not improve\n",
      "Epoch 6651/10000\n",
      "2700/2700 [==============================] - 0s 31us/step - loss: 1.2398e-04 - mean_absolute_error: 0.0063\n",
      "\n",
      "Epoch 06651: loss did not improve\n",
      "Epoch 6652/10000\n",
      "2700/2700 [==============================] - 0s 32us/step - loss: 1.2566e-04 - mean_absolute_error: 0.0065\n",
      "\n",
      "Epoch 06652: loss did not improve\n",
      "Epoch 6653/10000\n",
      "2700/2700 [==============================] - 0s 29us/step - loss: 1.2568e-04 - mean_absolute_error: 0.0066\n",
      "\n",
      "Epoch 06653: loss did not improve\n",
      "Epoch 6654/10000\n",
      "2700/2700 [==============================] - 0s 30us/step - loss: 1.2520e-04 - mean_absolute_error: 0.0064\n",
      "\n",
      "Epoch 06654: loss did not improve\n",
      "Epoch 6655/10000\n",
      "2700/2700 [==============================] - 0s 28us/step - loss: 1.2586e-04 - mean_absolute_error: 0.0065\n",
      "\n",
      "Epoch 06655: loss did not improve\n",
      "Epoch 6656/10000\n",
      "2700/2700 [==============================] - 0s 26us/step - loss: 1.2398e-04 - mean_absolute_error: 0.0064\n",
      "\n",
      "Epoch 06656: loss did not improve\n",
      "Epoch 6657/10000\n",
      "2700/2700 [==============================] - 0s 28us/step - loss: 1.2932e-04 - mean_absolute_error: 0.0069\n",
      "\n",
      "Epoch 06657: loss did not improve\n",
      "Epoch 6658/10000\n",
      "2700/2700 [==============================] - 0s 29us/step - loss: 1.2436e-04 - mean_absolute_error: 0.0065\n",
      "\n",
      "Epoch 06658: loss did not improve\n",
      "Epoch 6659/10000\n",
      "2700/2700 [==============================] - 0s 28us/step - loss: 1.2565e-04 - mean_absolute_error: 0.0065\n",
      "\n",
      "Epoch 06659: loss did not improve\n",
      "Epoch 6660/10000\n",
      "2700/2700 [==============================] - 0s 26us/step - loss: 1.2848e-04 - mean_absolute_error: 0.0068\n",
      "\n",
      "Epoch 06660: loss did not improve\n",
      "Epoch 6661/10000\n",
      "2700/2700 [==============================] - 0s 29us/step - loss: 1.2454e-04 - mean_absolute_error: 0.0063\n",
      "\n",
      "Epoch 06661: loss did not improve\n",
      "Epoch 6662/10000\n",
      "2700/2700 [==============================] - 0s 28us/step - loss: 1.2502e-04 - mean_absolute_error: 0.0064\n",
      "\n",
      "Epoch 06662: loss did not improve\n",
      "Epoch 6663/10000\n",
      "2700/2700 [==============================] - 0s 30us/step - loss: 1.2640e-04 - mean_absolute_error: 0.0067\n",
      "\n",
      "Epoch 06663: loss did not improve\n",
      "Epoch 6664/10000\n",
      "2700/2700 [==============================] - 0s 26us/step - loss: 1.2485e-04 - mean_absolute_error: 0.0064\n",
      "\n",
      "Epoch 06664: loss did not improve\n",
      "Epoch 6665/10000\n",
      "2700/2700 [==============================] - 0s 29us/step - loss: 1.2850e-04 - mean_absolute_error: 0.0068\n",
      "\n",
      "Epoch 06665: loss did not improve\n",
      "Epoch 6666/10000\n",
      "2700/2700 [==============================] - 0s 29us/step - loss: 1.2497e-04 - mean_absolute_error: 0.0064\n",
      "\n",
      "Epoch 06666: loss did not improve\n",
      "Epoch 6667/10000\n",
      "2700/2700 [==============================] - 0s 28us/step - loss: 1.2866e-04 - mean_absolute_error: 0.0067\n",
      "\n",
      "Epoch 06667: loss did not improve\n",
      "Epoch 6668/10000\n",
      "2700/2700 [==============================] - 0s 33us/step - loss: 1.3084e-04 - mean_absolute_error: 0.0070\n",
      "\n",
      "Epoch 06668: loss did not improve\n",
      "Epoch 6669/10000\n",
      "2700/2700 [==============================] - 0s 35us/step - loss: 1.2639e-04 - mean_absolute_error: 0.0066\n",
      "\n",
      "Epoch 06669: loss did not improve\n",
      "Epoch 6670/10000\n",
      "2700/2700 [==============================] - 0s 34us/step - loss: 1.2447e-04 - mean_absolute_error: 0.0064\n",
      "\n",
      "Epoch 06670: loss did not improve\n",
      "Epoch 6671/10000\n",
      "2700/2700 [==============================] - 0s 33us/step - loss: 1.2533e-04 - mean_absolute_error: 0.0065\n",
      "\n",
      "Epoch 06671: loss did not improve\n",
      "Epoch 6672/10000\n",
      "2700/2700 [==============================] - 0s 38us/step - loss: 1.2475e-04 - mean_absolute_error: 0.0064\n",
      "\n",
      "Epoch 06672: loss did not improve\n",
      "Epoch 6673/10000\n",
      "2700/2700 [==============================] - 0s 31us/step - loss: 1.2382e-04 - mean_absolute_error: 0.0065\n",
      "\n",
      "Epoch 06673: loss did not improve\n",
      "Epoch 6674/10000\n",
      "2700/2700 [==============================] - 0s 31us/step - loss: 1.2566e-04 - mean_absolute_error: 0.0066\n",
      "\n",
      "Epoch 06674: loss did not improve\n",
      "Epoch 6675/10000\n",
      "2700/2700 [==============================] - 0s 43us/step - loss: 1.2476e-04 - mean_absolute_error: 0.0064\n",
      "\n",
      "Epoch 06675: loss did not improve\n",
      "Epoch 6676/10000\n",
      "2700/2700 [==============================] - 0s 36us/step - loss: 1.2468e-04 - mean_absolute_error: 0.0064\n",
      "\n",
      "Epoch 06676: loss did not improve\n",
      "Epoch 6677/10000\n",
      "2700/2700 [==============================] - 0s 30us/step - loss: 1.2322e-04 - mean_absolute_error: 0.0062\n",
      "\n",
      "Epoch 06677: loss improved from 0.00012 to 0.00012, saving model to keras/best_baseline_weights.hdf5\n",
      "Epoch 6678/10000\n",
      "2700/2700 [==============================] - 0s 36us/step - loss: 1.2400e-04 - mean_absolute_error: 0.0064\n",
      "\n",
      "Epoch 06678: loss did not improve\n",
      "Epoch 6679/10000\n",
      "2700/2700 [==============================] - 0s 32us/step - loss: 1.2784e-04 - mean_absolute_error: 0.0067\n",
      "\n",
      "Epoch 06679: loss did not improve\n",
      "Epoch 6680/10000\n",
      "2700/2700 [==============================] - 0s 33us/step - loss: 1.2341e-04 - mean_absolute_error: 0.0063\n",
      "\n",
      "Epoch 06680: loss did not improve\n",
      "Epoch 6681/10000\n",
      "2700/2700 [==============================] - 0s 32us/step - loss: 1.2464e-04 - mean_absolute_error: 0.0064\n",
      "\n",
      "Epoch 06681: loss did not improve\n",
      "Epoch 6682/10000\n",
      "2700/2700 [==============================] - 0s 33us/step - loss: 1.2525e-04 - mean_absolute_error: 0.0065\n",
      "\n",
      "Epoch 06682: loss did not improve\n",
      "Epoch 6683/10000\n",
      "2700/2700 [==============================] - 0s 29us/step - loss: 1.2632e-04 - mean_absolute_error: 0.0065\n",
      "\n",
      "Epoch 06683: loss did not improve\n",
      "Epoch 6684/10000\n",
      "2700/2700 [==============================] - 0s 30us/step - loss: 1.2414e-04 - mean_absolute_error: 0.0063\n",
      "\n",
      "Epoch 06684: loss did not improve\n",
      "Epoch 6685/10000\n",
      "2700/2700 [==============================] - 0s 31us/step - loss: 1.2686e-04 - mean_absolute_error: 0.0066\n",
      "\n",
      "Epoch 06685: loss did not improve\n",
      "Epoch 6686/10000\n",
      "2700/2700 [==============================] - 0s 32us/step - loss: 1.2867e-04 - mean_absolute_error: 0.0067\n",
      "\n",
      "Epoch 06686: loss did not improve\n",
      "Epoch 6687/10000\n",
      "2700/2700 [==============================] - 0s 32us/step - loss: 1.2493e-04 - mean_absolute_error: 0.0064\n",
      "\n",
      "Epoch 06687: loss did not improve\n",
      "Epoch 6688/10000\n",
      "2700/2700 [==============================] - 0s 37us/step - loss: 1.2362e-04 - mean_absolute_error: 0.0063\n",
      "\n",
      "Epoch 06688: loss did not improve\n",
      "Epoch 6689/10000\n",
      "2700/2700 [==============================] - 0s 35us/step - loss: 1.2639e-04 - mean_absolute_error: 0.0066\n",
      "\n",
      "Epoch 06689: loss did not improve\n",
      "Epoch 6690/10000\n",
      "2700/2700 [==============================] - 0s 34us/step - loss: 1.2783e-04 - mean_absolute_error: 0.0068\n",
      "\n",
      "Epoch 06690: loss did not improve\n",
      "Epoch 6691/10000\n",
      "2700/2700 [==============================] - 0s 32us/step - loss: 1.2515e-04 - mean_absolute_error: 0.0064\n",
      "\n",
      "Epoch 06691: loss did not improve\n",
      "Epoch 6692/10000\n",
      "2700/2700 [==============================] - 0s 29us/step - loss: 1.2484e-04 - mean_absolute_error: 0.0064\n",
      "\n",
      "Epoch 06692: loss did not improve\n",
      "Epoch 6693/10000\n",
      "2700/2700 [==============================] - 0s 32us/step - loss: 1.2880e-04 - mean_absolute_error: 0.0069\n",
      "\n",
      "Epoch 06693: loss did not improve\n",
      "Epoch 6694/10000\n",
      "2700/2700 [==============================] - 0s 31us/step - loss: 1.2452e-04 - mean_absolute_error: 0.0063\n",
      "\n",
      "Epoch 06694: loss did not improve\n",
      "Epoch 6695/10000\n",
      "2700/2700 [==============================] - 0s 31us/step - loss: 1.2490e-04 - mean_absolute_error: 0.0064\n",
      "\n",
      "Epoch 06695: loss did not improve\n",
      "Epoch 6696/10000\n",
      "2700/2700 [==============================] - 0s 33us/step - loss: 1.2452e-04 - mean_absolute_error: 0.0064\n",
      "\n",
      "Epoch 06696: loss did not improve\n",
      "Epoch 6697/10000\n",
      "2700/2700 [==============================] - 0s 40us/step - loss: 1.2342e-04 - mean_absolute_error: 0.0063\n",
      "\n",
      "Epoch 06697: loss did not improve\n",
      "Epoch 6698/10000\n",
      "2700/2700 [==============================] - 0s 36us/step - loss: 1.2847e-04 - mean_absolute_error: 0.0067\n",
      "\n",
      "Epoch 06698: loss did not improve\n",
      "Epoch 6699/10000\n",
      "2700/2700 [==============================] - 0s 32us/step - loss: 1.2516e-04 - mean_absolute_error: 0.0065\n",
      "\n",
      "Epoch 06699: loss did not improve\n",
      "Epoch 6700/10000\n",
      "2700/2700 [==============================] - 0s 34us/step - loss: 1.2503e-04 - mean_absolute_error: 0.0065\n",
      "\n",
      "Epoch 06700: loss did not improve\n",
      "Epoch 6701/10000\n",
      "2700/2700 [==============================] - 0s 31us/step - loss: 1.2579e-04 - mean_absolute_error: 0.0066\n",
      "\n",
      "Epoch 06701: loss did not improve\n",
      "Epoch 6702/10000\n",
      "2700/2700 [==============================] - 0s 32us/step - loss: 1.2512e-04 - mean_absolute_error: 0.0065\n",
      "\n",
      "Epoch 06702: loss did not improve\n",
      "Epoch 6703/10000\n",
      "2700/2700 [==============================] - 0s 33us/step - loss: 1.2694e-04 - mean_absolute_error: 0.0067\n",
      "\n",
      "Epoch 06703: loss did not improve\n",
      "Epoch 6704/10000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2700/2700 [==============================] - 0s 32us/step - loss: 1.2511e-04 - mean_absolute_error: 0.0064\n",
      "\n",
      "Epoch 06704: loss did not improve\n",
      "Epoch 6705/10000\n",
      "2700/2700 [==============================] - 0s 31us/step - loss: 1.2660e-04 - mean_absolute_error: 0.0066\n",
      "\n",
      "Epoch 06705: loss did not improve\n",
      "Epoch 6706/10000\n",
      "2700/2700 [==============================] - 0s 29us/step - loss: 1.2328e-04 - mean_absolute_error: 0.0063\n",
      "\n",
      "Epoch 06706: loss did not improve\n",
      "Epoch 6707/10000\n",
      "2700/2700 [==============================] - 0s 39us/step - loss: 1.2469e-04 - mean_absolute_error: 0.0064\n",
      "\n",
      "Epoch 06707: loss did not improve\n",
      "Epoch 6708/10000\n",
      "2700/2700 [==============================] - 0s 35us/step - loss: 1.2805e-04 - mean_absolute_error: 0.0067\n",
      "\n",
      "Epoch 06708: loss did not improve\n",
      "Epoch 6709/10000\n",
      "2700/2700 [==============================] - 0s 29us/step - loss: 1.2608e-04 - mean_absolute_error: 0.0065\n",
      "\n",
      "Epoch 06709: loss did not improve\n",
      "Epoch 6710/10000\n",
      "2700/2700 [==============================] - 0s 29us/step - loss: 1.2379e-04 - mean_absolute_error: 0.0064\n",
      "\n",
      "Epoch 06710: loss did not improve\n",
      "Epoch 6711/10000\n",
      "2700/2700 [==============================] - 0s 31us/step - loss: 1.2805e-04 - mean_absolute_error: 0.0068\n",
      "\n",
      "Epoch 06711: loss did not improve\n",
      "Epoch 6712/10000\n",
      "2700/2700 [==============================] - 0s 32us/step - loss: 1.2667e-04 - mean_absolute_error: 0.0066\n",
      "\n",
      "Epoch 06712: loss did not improve\n",
      "Epoch 6713/10000\n",
      "2700/2700 [==============================] - 0s 36us/step - loss: 1.2640e-04 - mean_absolute_error: 0.0066\n",
      "\n",
      "Epoch 06713: loss did not improve\n",
      "Epoch 6714/10000\n",
      "2700/2700 [==============================] - 0s 34us/step - loss: 1.2718e-04 - mean_absolute_error: 0.0067\n",
      "\n",
      "Epoch 06714: loss did not improve\n",
      "Epoch 6715/10000\n",
      "2700/2700 [==============================] - 0s 32us/step - loss: 1.2528e-04 - mean_absolute_error: 0.0066\n",
      "\n",
      "Epoch 06715: loss did not improve\n",
      "Epoch 6716/10000\n",
      "2700/2700 [==============================] - 0s 32us/step - loss: 1.2662e-04 - mean_absolute_error: 0.0066\n",
      "\n",
      "Epoch 06716: loss did not improve\n",
      "Epoch 6717/10000\n",
      "2700/2700 [==============================] - 0s 30us/step - loss: 1.2653e-04 - mean_absolute_error: 0.0066\n",
      "\n",
      "Epoch 06717: loss did not improve\n",
      "Epoch 6718/10000\n",
      "2700/2700 [==============================] - 0s 34us/step - loss: 1.2442e-04 - mean_absolute_error: 0.0064\n",
      "\n",
      "Epoch 06718: loss did not improve\n",
      "Epoch 6719/10000\n",
      "2700/2700 [==============================] - 0s 34us/step - loss: 1.2863e-04 - mean_absolute_error: 0.0070\n",
      "\n",
      "Epoch 06719: loss did not improve\n",
      "Epoch 6720/10000\n",
      "2700/2700 [==============================] - 0s 32us/step - loss: 1.2349e-04 - mean_absolute_error: 0.0063\n",
      "\n",
      "Epoch 06720: loss did not improve\n",
      "Epoch 6721/10000\n",
      "2700/2700 [==============================] - 0s 31us/step - loss: 1.2529e-04 - mean_absolute_error: 0.0066\n",
      "\n",
      "Epoch 06721: loss did not improve\n",
      "Epoch 6722/10000\n",
      "2700/2700 [==============================] - 0s 34us/step - loss: 1.2694e-04 - mean_absolute_error: 0.0066\n",
      "\n",
      "Epoch 06722: loss did not improve\n",
      "Epoch 6723/10000\n",
      "2700/2700 [==============================] - 0s 32us/step - loss: 1.2418e-04 - mean_absolute_error: 0.0064\n",
      "\n",
      "Epoch 06723: loss did not improve\n",
      "Epoch 6724/10000\n",
      "2700/2700 [==============================] - 0s 40us/step - loss: 1.2476e-04 - mean_absolute_error: 0.0064\n",
      "\n",
      "Epoch 06724: loss did not improve\n",
      "Epoch 6725/10000\n",
      "2700/2700 [==============================] - 0s 33us/step - loss: 1.2740e-04 - mean_absolute_error: 0.0066\n",
      "\n",
      "Epoch 06725: loss did not improve\n",
      "Epoch 6726/10000\n",
      "2700/2700 [==============================] - 0s 30us/step - loss: 1.2552e-04 - mean_absolute_error: 0.0065\n",
      "\n",
      "Epoch 06726: loss did not improve\n",
      "Epoch 6727/10000\n",
      "2700/2700 [==============================] - 0s 29us/step - loss: 1.2453e-04 - mean_absolute_error: 0.0064\n",
      "\n",
      "Epoch 06727: loss did not improve\n",
      "Epoch 6728/10000\n",
      "2700/2700 [==============================] - 0s 32us/step - loss: 1.2414e-04 - mean_absolute_error: 0.0064\n",
      "\n",
      "Epoch 06728: loss did not improve\n",
      "Epoch 6729/10000\n",
      "2700/2700 [==============================] - 0s 34us/step - loss: 1.2637e-04 - mean_absolute_error: 0.0065\n",
      "\n",
      "Epoch 06729: loss did not improve\n",
      "Epoch 6730/10000\n",
      "2700/2700 [==============================] - 0s 33us/step - loss: 1.2533e-04 - mean_absolute_error: 0.0064\n",
      "\n",
      "Epoch 06730: loss did not improve\n",
      "Epoch 6731/10000\n",
      "2700/2700 [==============================] - 0s 34us/step - loss: 1.2658e-04 - mean_absolute_error: 0.0065\n",
      "\n",
      "Epoch 06731: loss did not improve\n",
      "Epoch 6732/10000\n",
      "2700/2700 [==============================] - 0s 31us/step - loss: 1.2438e-04 - mean_absolute_error: 0.0064\n",
      "\n",
      "Epoch 06732: loss did not improve\n",
      "Epoch 6733/10000\n",
      "2700/2700 [==============================] - 0s 32us/step - loss: 1.2451e-04 - mean_absolute_error: 0.0065\n",
      "\n",
      "Epoch 06733: loss did not improve\n",
      "Epoch 6734/10000\n",
      "2700/2700 [==============================] - 0s 32us/step - loss: 1.2426e-04 - mean_absolute_error: 0.0065\n",
      "\n",
      "Epoch 06734: loss did not improve\n",
      "Epoch 6735/10000\n",
      "2700/2700 [==============================] - 0s 32us/step - loss: 1.2423e-04 - mean_absolute_error: 0.0063\n",
      "\n",
      "Epoch 06735: loss did not improve\n",
      "Epoch 6736/10000\n",
      "2700/2700 [==============================] - 0s 33us/step - loss: 1.2697e-04 - mean_absolute_error: 0.0066\n",
      "\n",
      "Epoch 06736: loss did not improve\n",
      "Epoch 6737/10000\n",
      "2700/2700 [==============================] - 0s 32us/step - loss: 1.2356e-04 - mean_absolute_error: 0.0064\n",
      "\n",
      "Epoch 06737: loss did not improve\n",
      "Epoch 6738/10000\n",
      "2700/2700 [==============================] - 0s 31us/step - loss: 1.2518e-04 - mean_absolute_error: 0.0065\n",
      "\n",
      "Epoch 06738: loss did not improve\n",
      "Epoch 6739/10000\n",
      "2700/2700 [==============================] - 0s 34us/step - loss: 1.2936e-04 - mean_absolute_error: 0.0068\n",
      "\n",
      "Epoch 06739: loss did not improve\n",
      "Epoch 6740/10000\n",
      "2700/2700 [==============================] - 0s 30us/step - loss: 1.2819e-04 - mean_absolute_error: 0.0067\n",
      "\n",
      "Epoch 06740: loss did not improve\n",
      "Epoch 6741/10000\n",
      "2700/2700 [==============================] - 0s 33us/step - loss: 1.2537e-04 - mean_absolute_error: 0.0065\n",
      "\n",
      "Epoch 06741: loss did not improve\n",
      "Epoch 6742/10000\n",
      "2700/2700 [==============================] - 0s 37us/step - loss: 1.2462e-04 - mean_absolute_error: 0.0064\n",
      "\n",
      "Epoch 06742: loss did not improve\n",
      "Epoch 6743/10000\n",
      "2700/2700 [==============================] - 0s 35us/step - loss: 1.2582e-04 - mean_absolute_error: 0.0067\n",
      "\n",
      "Epoch 06743: loss did not improve\n",
      "Epoch 6744/10000\n",
      "2700/2700 [==============================] - 0s 29us/step - loss: 1.2565e-04 - mean_absolute_error: 0.0066\n",
      "\n",
      "Epoch 06744: loss did not improve\n",
      "Epoch 6745/10000\n",
      "2700/2700 [==============================] - 0s 31us/step - loss: 1.2532e-04 - mean_absolute_error: 0.0065\n",
      "\n",
      "Epoch 06745: loss did not improve\n",
      "Epoch 6746/10000\n",
      "2700/2700 [==============================] - 0s 32us/step - loss: 1.2278e-04 - mean_absolute_error: 0.0063\n",
      "\n",
      "Epoch 06746: loss improved from 0.00012 to 0.00012, saving model to keras/best_baseline_weights.hdf5\n",
      "Epoch 6747/10000\n",
      "2700/2700 [==============================] - 0s 33us/step - loss: 1.2429e-04 - mean_absolute_error: 0.0064\n",
      "\n",
      "Epoch 06747: loss did not improve\n",
      "Epoch 6748/10000\n",
      "2700/2700 [==============================] - 0s 36us/step - loss: 1.2640e-04 - mean_absolute_error: 0.0065\n",
      "\n",
      "Epoch 06748: loss did not improve\n",
      "Epoch 6749/10000\n",
      "2700/2700 [==============================] - 0s 33us/step - loss: 1.2540e-04 - mean_absolute_error: 0.0065\n",
      "\n",
      "Epoch 06749: loss did not improve\n",
      "Epoch 6750/10000\n",
      "2700/2700 [==============================] - 0s 33us/step - loss: 1.2417e-04 - mean_absolute_error: 0.0064\n",
      "\n",
      "Epoch 06750: loss did not improve\n",
      "Epoch 6751/10000\n",
      "2700/2700 [==============================] - 0s 32us/step - loss: 1.2418e-04 - mean_absolute_error: 0.0063\n",
      "\n",
      "Epoch 06751: loss did not improve\n",
      "Epoch 6752/10000\n",
      "2700/2700 [==============================] - 0s 32us/step - loss: 1.2899e-04 - mean_absolute_error: 0.0067\n",
      "\n",
      "Epoch 06752: loss did not improve\n",
      "Epoch 6753/10000\n",
      "2700/2700 [==============================] - 0s 33us/step - loss: 1.2752e-04 - mean_absolute_error: 0.0068\n",
      "\n",
      "Epoch 06753: loss did not improve\n",
      "Epoch 6754/10000\n",
      "2700/2700 [==============================] - 0s 32us/step - loss: 1.2519e-04 - mean_absolute_error: 0.0066\n",
      "\n",
      "Epoch 06754: loss did not improve\n",
      "Epoch 6755/10000\n",
      "2700/2700 [==============================] - 0s 43us/step - loss: 1.2542e-04 - mean_absolute_error: 0.0065\n",
      "\n",
      "Epoch 06755: loss did not improve\n",
      "Epoch 6756/10000\n",
      "2700/2700 [==============================] - 0s 33us/step - loss: 1.2368e-04 - mean_absolute_error: 0.0063\n",
      "\n",
      "Epoch 06756: loss did not improve\n",
      "Epoch 6757/10000\n",
      "2700/2700 [==============================] - 0s 33us/step - loss: 1.2699e-04 - mean_absolute_error: 0.0067\n",
      "\n",
      "Epoch 06757: loss did not improve\n",
      "Epoch 6758/10000\n",
      "2700/2700 [==============================] - 0s 29us/step - loss: 1.3205e-04 - mean_absolute_error: 0.0070\n",
      "\n",
      "Epoch 06758: loss did not improve\n",
      "Epoch 6759/10000\n",
      "2700/2700 [==============================] - 0s 29us/step - loss: 1.2526e-04 - mean_absolute_error: 0.0065\n",
      "\n",
      "Epoch 06759: loss did not improve\n",
      "Epoch 6760/10000\n",
      "2700/2700 [==============================] - 0s 30us/step - loss: 1.2322e-04 - mean_absolute_error: 0.0063\n",
      "\n",
      "Epoch 06760: loss did not improve\n",
      "Epoch 6761/10000\n",
      "2700/2700 [==============================] - 0s 27us/step - loss: 1.2388e-04 - mean_absolute_error: 0.0063\n",
      "\n",
      "Epoch 06761: loss did not improve\n",
      "Epoch 6762/10000\n",
      "2700/2700 [==============================] - 0s 37us/step - loss: 1.3049e-04 - mean_absolute_error: 0.0068\n",
      "\n",
      "Epoch 06762: loss did not improve\n",
      "Epoch 6763/10000\n",
      "2700/2700 [==============================] - 0s 34us/step - loss: 1.2530e-04 - mean_absolute_error: 0.0065\n",
      "\n",
      "Epoch 06763: loss did not improve\n",
      "Epoch 6764/10000\n",
      "2700/2700 [==============================] - 0s 34us/step - loss: 1.2411e-04 - mean_absolute_error: 0.0064\n",
      "\n",
      "Epoch 06764: loss did not improve\n",
      "Epoch 6765/10000\n",
      "2700/2700 [==============================] - 0s 31us/step - loss: 1.2265e-04 - mean_absolute_error: 0.0063\n",
      "\n",
      "Epoch 06765: loss improved from 0.00012 to 0.00012, saving model to keras/best_baseline_weights.hdf5\n",
      "Epoch 6766/10000\n",
      "2700/2700 [==============================] - 0s 34us/step - loss: 1.2507e-04 - mean_absolute_error: 0.0064\n",
      "\n",
      "Epoch 06766: loss did not improve\n",
      "Epoch 6767/10000\n",
      "2700/2700 [==============================] - 0s 31us/step - loss: 1.2597e-04 - mean_absolute_error: 0.0065\n",
      "\n",
      "Epoch 06767: loss did not improve\n",
      "Epoch 6768/10000\n",
      "2700/2700 [==============================] - 0s 33us/step - loss: 1.2599e-04 - mean_absolute_error: 0.0066\n",
      "\n",
      "Epoch 06768: loss did not improve\n",
      "Epoch 6769/10000\n",
      "2700/2700 [==============================] - 0s 33us/step - loss: 1.2775e-04 - mean_absolute_error: 0.0068\n",
      "\n",
      "Epoch 06769: loss did not improve\n",
      "Epoch 6770/10000\n",
      "2700/2700 [==============================] - 0s 35us/step - loss: 1.2943e-04 - mean_absolute_error: 0.0068\n",
      "\n",
      "Epoch 06770: loss did not improve\n",
      "Epoch 6771/10000\n",
      "2700/2700 [==============================] - 0s 31us/step - loss: 1.2624e-04 - mean_absolute_error: 0.0065\n",
      "\n",
      "Epoch 06771: loss did not improve\n",
      "Epoch 6772/10000\n",
      "2700/2700 [==============================] - 0s 30us/step - loss: 1.2428e-04 - mean_absolute_error: 0.0064\n",
      "\n",
      "Epoch 06772: loss did not improve\n",
      "Epoch 6773/10000\n",
      "2700/2700 [==============================] - 0s 33us/step - loss: 1.2414e-04 - mean_absolute_error: 0.0064\n",
      "\n",
      "Epoch 06773: loss did not improve\n",
      "Epoch 6774/10000\n",
      "2700/2700 [==============================] - 0s 32us/step - loss: 1.2396e-04 - mean_absolute_error: 0.0065\n",
      "\n",
      "Epoch 06774: loss did not improve\n",
      "Epoch 6775/10000\n",
      "2700/2700 [==============================] - 0s 34us/step - loss: 1.2326e-04 - mean_absolute_error: 0.0063\n",
      "\n",
      "Epoch 06775: loss did not improve\n",
      "Epoch 6776/10000\n",
      "2700/2700 [==============================] - 0s 38us/step - loss: 1.2424e-04 - mean_absolute_error: 0.0064\n",
      "\n",
      "Epoch 06776: loss did not improve\n",
      "Epoch 6777/10000\n",
      "2700/2700 [==============================] - 0s 32us/step - loss: 1.2468e-04 - mean_absolute_error: 0.0064\n",
      "\n",
      "Epoch 06777: loss did not improve\n",
      "Epoch 6778/10000\n",
      "2700/2700 [==============================] - 0s 32us/step - loss: 1.2827e-04 - mean_absolute_error: 0.0067\n",
      "\n",
      "Epoch 06778: loss did not improve\n",
      "Epoch 6779/10000\n",
      "2700/2700 [==============================] - 0s 31us/step - loss: 1.2479e-04 - mean_absolute_error: 0.0065\n",
      "\n",
      "Epoch 06779: loss did not improve\n",
      "Epoch 6780/10000\n",
      "2700/2700 [==============================] - 0s 31us/step - loss: 1.2511e-04 - mean_absolute_error: 0.0064\n",
      "\n",
      "Epoch 06780: loss did not improve\n",
      "Epoch 6781/10000\n",
      "2700/2700 [==============================] - 0s 36us/step - loss: 1.2368e-04 - mean_absolute_error: 0.0063\n",
      "\n",
      "Epoch 06781: loss did not improve\n",
      "Epoch 6782/10000\n",
      "2700/2700 [==============================] - 0s 36us/step - loss: 1.2555e-04 - mean_absolute_error: 0.0065\n",
      "\n",
      "Epoch 06782: loss did not improve\n",
      "Epoch 6783/10000\n",
      "2700/2700 [==============================] - 0s 34us/step - loss: 1.2503e-04 - mean_absolute_error: 0.0065\n",
      "\n",
      "Epoch 06783: loss did not improve\n",
      "Epoch 6784/10000\n",
      "2700/2700 [==============================] - 0s 30us/step - loss: 1.2810e-04 - mean_absolute_error: 0.0068\n",
      "\n",
      "Epoch 06784: loss did not improve\n",
      "Epoch 6785/10000\n",
      "2700/2700 [==============================] - 0s 31us/step - loss: 1.2755e-04 - mean_absolute_error: 0.0066\n",
      "\n",
      "Epoch 06785: loss did not improve\n",
      "Epoch 6786/10000\n",
      "2700/2700 [==============================] - 0s 29us/step - loss: 1.2398e-04 - mean_absolute_error: 0.0064\n",
      "\n",
      "Epoch 06786: loss did not improve\n",
      "Epoch 6787/10000\n",
      "2700/2700 [==============================] - 0s 34us/step - loss: 1.2649e-04 - mean_absolute_error: 0.0067\n",
      "\n",
      "Epoch 06787: loss did not improve\n",
      "Epoch 6788/10000\n",
      "2700/2700 [==============================] - 0s 35us/step - loss: 1.2992e-04 - mean_absolute_error: 0.0068\n",
      "\n",
      "Epoch 06788: loss did not improve\n",
      "Epoch 6789/10000\n",
      "2700/2700 [==============================] - 0s 34us/step - loss: 1.2417e-04 - mean_absolute_error: 0.0064\n",
      "\n",
      "Epoch 06789: loss did not improve\n",
      "Epoch 6790/10000\n",
      "2700/2700 [==============================] - 0s 30us/step - loss: 1.2406e-04 - mean_absolute_error: 0.0064\n",
      "\n",
      "Epoch 06790: loss did not improve\n",
      "Epoch 6791/10000\n",
      "2700/2700 [==============================] - 0s 33us/step - loss: 1.2312e-04 - mean_absolute_error: 0.0064\n",
      "\n",
      "Epoch 06791: loss did not improve\n",
      "Epoch 6792/10000\n",
      "2700/2700 [==============================] - 0s 30us/step - loss: 1.2478e-04 - mean_absolute_error: 0.0064\n",
      "\n",
      "Epoch 06792: loss did not improve\n",
      "Epoch 6793/10000\n",
      "2700/2700 [==============================] - 0s 33us/step - loss: 1.2345e-04 - mean_absolute_error: 0.0063\n",
      "\n",
      "Epoch 06793: loss did not improve\n",
      "Epoch 6794/10000\n",
      "2700/2700 [==============================] - 0s 33us/step - loss: 1.2570e-04 - mean_absolute_error: 0.0065\n",
      "\n",
      "Epoch 06794: loss did not improve\n",
      "Epoch 6795/10000\n",
      "2700/2700 [==============================] - 0s 33us/step - loss: 1.2290e-04 - mean_absolute_error: 0.0063\n",
      "\n",
      "Epoch 06795: loss did not improve\n",
      "Epoch 6796/10000\n",
      "2700/2700 [==============================] - 0s 33us/step - loss: 1.2401e-04 - mean_absolute_error: 0.0064\n",
      "\n",
      "Epoch 06796: loss did not improve\n",
      "Epoch 6797/10000\n",
      "2700/2700 [==============================] - 0s 39us/step - loss: 1.2599e-04 - mean_absolute_error: 0.0066\n",
      "\n",
      "Epoch 06797: loss did not improve\n",
      "Epoch 6798/10000\n",
      "2700/2700 [==============================] - 0s 32us/step - loss: 1.2470e-04 - mean_absolute_error: 0.0064\n",
      "\n",
      "Epoch 06798: loss did not improve\n",
      "Epoch 6799/10000\n",
      "2700/2700 [==============================] - 0s 30us/step - loss: 1.2303e-04 - mean_absolute_error: 0.0063\n",
      "\n",
      "Epoch 06799: loss did not improve\n",
      "Epoch 6800/10000\n",
      "2700/2700 [==============================] - 0s 33us/step - loss: 1.2381e-04 - mean_absolute_error: 0.0063\n",
      "\n",
      "Epoch 06800: loss did not improve\n",
      "Epoch 6801/10000\n",
      "2700/2700 [==============================] - 0s 34us/step - loss: 1.2443e-04 - mean_absolute_error: 0.0065\n",
      "\n",
      "Epoch 06801: loss did not improve\n",
      "Epoch 6802/10000\n",
      "2700/2700 [==============================] - 0s 32us/step - loss: 1.2414e-04 - mean_absolute_error: 0.0064\n",
      "\n",
      "Epoch 06802: loss did not improve\n",
      "Epoch 6803/10000\n",
      "2700/2700 [==============================] - 0s 33us/step - loss: 1.2561e-04 - mean_absolute_error: 0.0065\n",
      "\n",
      "Epoch 06803: loss did not improve\n",
      "Epoch 6804/10000\n",
      "2700/2700 [==============================] - 0s 37us/step - loss: 1.2276e-04 - mean_absolute_error: 0.0063\n",
      "\n",
      "Epoch 06804: loss did not improve\n",
      "Epoch 6805/10000\n",
      "2700/2700 [==============================] - 0s 36us/step - loss: 1.2607e-04 - mean_absolute_error: 0.0066\n",
      "\n",
      "Epoch 06805: loss did not improve\n",
      "Epoch 6806/10000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2700/2700 [==============================] - 0s 38us/step - loss: 1.2420e-04 - mean_absolute_error: 0.0064\n",
      "\n",
      "Epoch 06806: loss did not improve\n",
      "Epoch 6807/10000\n",
      "2700/2700 [==============================] - 0s 29us/step - loss: 1.2608e-04 - mean_absolute_error: 0.0066\n",
      "\n",
      "Epoch 06807: loss did not improve\n",
      "Epoch 6808/10000\n",
      "2700/2700 [==============================] - 0s 32us/step - loss: 1.2445e-04 - mean_absolute_error: 0.0065\n",
      "\n",
      "Epoch 06808: loss did not improve\n",
      "Epoch 6809/10000\n",
      "2700/2700 [==============================] - 0s 28us/step - loss: 1.2643e-04 - mean_absolute_error: 0.0066\n",
      "\n",
      "Epoch 06809: loss did not improve\n",
      "Epoch 6810/10000\n",
      "2700/2700 [==============================] - 0s 27us/step - loss: 1.2656e-04 - mean_absolute_error: 0.0067\n",
      "\n",
      "Epoch 06810: loss did not improve\n",
      "Epoch 6811/10000\n",
      "2700/2700 [==============================] - 0s 29us/step - loss: 1.2465e-04 - mean_absolute_error: 0.0065\n",
      "\n",
      "Epoch 06811: loss did not improve\n",
      "Epoch 6812/10000\n",
      "2700/2700 [==============================] - 0s 29us/step - loss: 1.2628e-04 - mean_absolute_error: 0.0066\n",
      "\n",
      "Epoch 06812: loss did not improve\n",
      "Epoch 6813/10000\n",
      "2700/2700 [==============================] - 0s 29us/step - loss: 1.2466e-04 - mean_absolute_error: 0.0064\n",
      "\n",
      "Epoch 06813: loss did not improve\n",
      "Epoch 6814/10000\n",
      "2700/2700 [==============================] - 0s 31us/step - loss: 1.2478e-04 - mean_absolute_error: 0.0065\n",
      "\n",
      "Epoch 06814: loss did not improve\n",
      "Epoch 6815/10000\n",
      "2700/2700 [==============================] - 0s 44us/step - loss: 1.2586e-04 - mean_absolute_error: 0.0066\n",
      "\n",
      "Epoch 06815: loss did not improve\n",
      "Epoch 6816/10000\n",
      "2700/2700 [==============================] - 0s 44us/step - loss: 1.2743e-04 - mean_absolute_error: 0.0067\n",
      "\n",
      "Epoch 06816: loss did not improve\n",
      "Epoch 6817/10000\n",
      "2700/2700 [==============================] - 0s 33us/step - loss: 1.2790e-04 - mean_absolute_error: 0.0068\n",
      "\n",
      "Epoch 06817: loss did not improve\n",
      "Epoch 6818/10000\n",
      "2700/2700 [==============================] - 0s 27us/step - loss: 1.2373e-04 - mean_absolute_error: 0.0065\n",
      "\n",
      "Epoch 06818: loss did not improve\n",
      "Epoch 6819/10000\n",
      "2700/2700 [==============================] - 0s 28us/step - loss: 1.2452e-04 - mean_absolute_error: 0.0065\n",
      "\n",
      "Epoch 06819: loss did not improve\n",
      "Epoch 6820/10000\n",
      "2700/2700 [==============================] - 0s 28us/step - loss: 1.2488e-04 - mean_absolute_error: 0.0065\n",
      "\n",
      "Epoch 06820: loss did not improve\n",
      "Epoch 6821/10000\n",
      "2700/2700 [==============================] - 0s 30us/step - loss: 1.2541e-04 - mean_absolute_error: 0.0066\n",
      "\n",
      "Epoch 06821: loss did not improve\n",
      "Epoch 6822/10000\n",
      "2700/2700 [==============================] - 0s 28us/step - loss: 1.2517e-04 - mean_absolute_error: 0.0065\n",
      "\n",
      "Epoch 06822: loss did not improve\n",
      "Epoch 6823/10000\n",
      "2700/2700 [==============================] - 0s 51us/step - loss: 1.2432e-04 - mean_absolute_error: 0.0065\n",
      "\n",
      "Epoch 06823: loss did not improve\n",
      "Epoch 6824/10000\n",
      "2700/2700 [==============================] - 0s 39us/step - loss: 1.2365e-04 - mean_absolute_error: 0.0064\n",
      "\n",
      "Epoch 06824: loss did not improve\n",
      "Epoch 6825/10000\n",
      "2700/2700 [==============================] - 0s 37us/step - loss: 1.2773e-04 - mean_absolute_error: 0.0066\n",
      "\n",
      "Epoch 06825: loss did not improve\n",
      "Epoch 6826/10000\n",
      "2700/2700 [==============================] - 0s 34us/step - loss: 1.3121e-04 - mean_absolute_error: 0.0071\n",
      "\n",
      "Epoch 06826: loss did not improve\n",
      "Epoch 6827/10000\n",
      "2700/2700 [==============================] - 0s 36us/step - loss: 1.2663e-04 - mean_absolute_error: 0.0067\n",
      "\n",
      "Epoch 06827: loss did not improve\n",
      "Epoch 6828/10000\n",
      "2700/2700 [==============================] - 0s 42us/step - loss: 1.2607e-04 - mean_absolute_error: 0.0066\n",
      "\n",
      "Epoch 06828: loss did not improve\n",
      "Epoch 6829/10000\n",
      "2700/2700 [==============================] - 0s 36us/step - loss: 1.2619e-04 - mean_absolute_error: 0.0066\n",
      "\n",
      "Epoch 06829: loss did not improve\n",
      "Epoch 6830/10000\n",
      "2700/2700 [==============================] - 0s 34us/step - loss: 1.2657e-04 - mean_absolute_error: 0.0066\n",
      "\n",
      "Epoch 06830: loss did not improve\n",
      "Epoch 6831/10000\n",
      "2700/2700 [==============================] - 0s 31us/step - loss: 1.2494e-04 - mean_absolute_error: 0.0064\n",
      "\n",
      "Epoch 06831: loss did not improve\n",
      "Epoch 6832/10000\n",
      "2700/2700 [==============================] - 0s 26us/step - loss: 1.2545e-04 - mean_absolute_error: 0.0065\n",
      "\n",
      "Epoch 06832: loss did not improve\n",
      "Epoch 6833/10000\n",
      "2700/2700 [==============================] - 0s 29us/step - loss: 1.2592e-04 - mean_absolute_error: 0.0066\n",
      "\n",
      "Epoch 06833: loss did not improve\n",
      "Epoch 6834/10000\n",
      "2700/2700 [==============================] - 0s 29us/step - loss: 1.2514e-04 - mean_absolute_error: 0.0065\n",
      "\n",
      "Epoch 06834: loss did not improve\n",
      "Epoch 6835/10000\n",
      "2700/2700 [==============================] - 0s 29us/step - loss: 1.2522e-04 - mean_absolute_error: 0.0065\n",
      "\n",
      "Epoch 06835: loss did not improve\n",
      "Epoch 6836/10000\n",
      "2700/2700 [==============================] - 0s 26us/step - loss: 1.2442e-04 - mean_absolute_error: 0.0064\n",
      "\n",
      "Epoch 06836: loss did not improve\n",
      "Epoch 6837/10000\n",
      "2700/2700 [==============================] - 0s 31us/step - loss: 1.2613e-04 - mean_absolute_error: 0.0065\n",
      "\n",
      "Epoch 06837: loss did not improve\n",
      "Epoch 6838/10000\n",
      "2700/2700 [==============================] - 0s 37us/step - loss: 1.2684e-04 - mean_absolute_error: 0.0067\n",
      "\n",
      "Epoch 06838: loss did not improve\n",
      "Epoch 6839/10000\n",
      "2700/2700 [==============================] - 0s 36us/step - loss: 1.2671e-04 - mean_absolute_error: 0.0066\n",
      "\n",
      "Epoch 06839: loss did not improve\n",
      "Epoch 6840/10000\n",
      "2700/2700 [==============================] - 0s 35us/step - loss: 1.2373e-04 - mean_absolute_error: 0.0064\n",
      "\n",
      "Epoch 06840: loss did not improve\n",
      "Epoch 6841/10000\n",
      "2700/2700 [==============================] - 0s 32us/step - loss: 1.2610e-04 - mean_absolute_error: 0.0066\n",
      "\n",
      "Epoch 06841: loss did not improve\n",
      "Epoch 6842/10000\n",
      "2700/2700 [==============================] - 0s 31us/step - loss: 1.2487e-04 - mean_absolute_error: 0.0065\n",
      "\n",
      "Epoch 06842: loss did not improve\n",
      "Epoch 6843/10000\n",
      "2700/2700 [==============================] - 0s 32us/step - loss: 1.2323e-04 - mean_absolute_error: 0.0063\n",
      "\n",
      "Epoch 06843: loss did not improve\n",
      "Epoch 6844/10000\n",
      "2700/2700 [==============================] - 0s 27us/step - loss: 1.2526e-04 - mean_absolute_error: 0.0066\n",
      "\n",
      "Epoch 06844: loss did not improve\n",
      "Epoch 6845/10000\n",
      "2700/2700 [==============================] - 0s 31us/step - loss: 1.2315e-04 - mean_absolute_error: 0.0063\n",
      "\n",
      "Epoch 06845: loss did not improve\n",
      "Epoch 6846/10000\n",
      "2700/2700 [==============================] - 0s 28us/step - loss: 1.2322e-04 - mean_absolute_error: 0.0063\n",
      "\n",
      "Epoch 06846: loss did not improve\n",
      "Epoch 6847/10000\n",
      "2700/2700 [==============================] - 0s 28us/step - loss: 1.2404e-04 - mean_absolute_error: 0.0064\n",
      "\n",
      "Epoch 06847: loss did not improve\n",
      "Epoch 6848/10000\n",
      "2700/2700 [==============================] - 0s 27us/step - loss: 1.2737e-04 - mean_absolute_error: 0.0067\n",
      "\n",
      "Epoch 06848: loss did not improve\n",
      "Epoch 6849/10000\n",
      "2700/2700 [==============================] - 0s 26us/step - loss: 1.2904e-04 - mean_absolute_error: 0.0068\n",
      "\n",
      "Epoch 06849: loss did not improve\n",
      "Epoch 6850/10000\n",
      "2700/2700 [==============================] - 0s 29us/step - loss: 1.2600e-04 - mean_absolute_error: 0.0066\n",
      "\n",
      "Epoch 06850: loss did not improve\n",
      "Epoch 6851/10000\n",
      "2700/2700 [==============================] - 0s 28us/step - loss: 1.2390e-04 - mean_absolute_error: 0.0064\n",
      "\n",
      "Epoch 06851: loss did not improve\n",
      "Epoch 6852/10000\n",
      "2700/2700 [==============================] - 0s 28us/step - loss: 1.2556e-04 - mean_absolute_error: 0.0064\n",
      "\n",
      "Epoch 06852: loss did not improve\n",
      "Epoch 6853/10000\n",
      "2700/2700 [==============================] - 0s 28us/step - loss: 1.2341e-04 - mean_absolute_error: 0.0063\n",
      "\n",
      "Epoch 06853: loss did not improve\n",
      "Epoch 6854/10000\n",
      "2700/2700 [==============================] - 0s 36us/step - loss: 1.2495e-04 - mean_absolute_error: 0.0065\n",
      "\n",
      "Epoch 06854: loss did not improve\n",
      "Epoch 6855/10000\n",
      "2700/2700 [==============================] - 0s 35us/step - loss: 1.2672e-04 - mean_absolute_error: 0.0067\n",
      "\n",
      "Epoch 06855: loss did not improve\n",
      "Epoch 6856/10000\n",
      "2700/2700 [==============================] - 0s 35us/step - loss: 1.2368e-04 - mean_absolute_error: 0.0064\n",
      "\n",
      "Epoch 06856: loss did not improve\n",
      "Epoch 6857/10000\n",
      "2700/2700 [==============================] - 0s 32us/step - loss: 1.2321e-04 - mean_absolute_error: 0.0063\n",
      "\n",
      "Epoch 06857: loss did not improve\n",
      "Epoch 6858/10000\n",
      "2700/2700 [==============================] - 0s 31us/step - loss: 1.2712e-04 - mean_absolute_error: 0.0067\n",
      "\n",
      "Epoch 06858: loss did not improve\n",
      "Epoch 6859/10000\n",
      "2700/2700 [==============================] - 0s 26us/step - loss: 1.2339e-04 - mean_absolute_error: 0.0064\n",
      "\n",
      "Epoch 06859: loss did not improve\n",
      "Epoch 6860/10000\n",
      "2700/2700 [==============================] - 0s 26us/step - loss: 1.2312e-04 - mean_absolute_error: 0.0063\n",
      "\n",
      "Epoch 06860: loss did not improve\n",
      "Epoch 6861/10000\n",
      "2700/2700 [==============================] - 0s 29us/step - loss: 1.2385e-04 - mean_absolute_error: 0.0064\n",
      "\n",
      "Epoch 06861: loss did not improve\n",
      "Epoch 6862/10000\n",
      "2700/2700 [==============================] - 0s 28us/step - loss: 1.2684e-04 - mean_absolute_error: 0.0066\n",
      "\n",
      "Epoch 06862: loss did not improve\n",
      "Epoch 6863/10000\n",
      "2700/2700 [==============================] - 0s 29us/step - loss: 1.2351e-04 - mean_absolute_error: 0.0064\n",
      "\n",
      "Epoch 06863: loss did not improve\n",
      "Epoch 6864/10000\n",
      "2700/2700 [==============================] - 0s 27us/step - loss: 1.2761e-04 - mean_absolute_error: 0.0066\n",
      "\n",
      "Epoch 06864: loss did not improve\n",
      "Epoch 6865/10000\n",
      "2700/2700 [==============================] - 0s 30us/step - loss: 1.2770e-04 - mean_absolute_error: 0.0066\n",
      "\n",
      "Epoch 06865: loss did not improve\n",
      "Epoch 6866/10000\n",
      "2700/2700 [==============================] - 0s 31us/step - loss: 1.2281e-04 - mean_absolute_error: 0.0062\n",
      "\n",
      "Epoch 06866: loss did not improve\n",
      "Epoch 6867/10000\n",
      "2700/2700 [==============================] - 0s 30us/step - loss: 1.2331e-04 - mean_absolute_error: 0.0064\n",
      "\n",
      "Epoch 06867: loss did not improve\n",
      "Epoch 6868/10000\n",
      "2700/2700 [==============================] - 0s 28us/step - loss: 1.2297e-04 - mean_absolute_error: 0.0063\n",
      "\n",
      "Epoch 06868: loss did not improve\n",
      "Epoch 6869/10000\n",
      "2700/2700 [==============================] - 0s 27us/step - loss: 1.2386e-04 - mean_absolute_error: 0.0064\n",
      "\n",
      "Epoch 06869: loss did not improve\n",
      "Epoch 6870/10000\n",
      "2700/2700 [==============================] - 0s 30us/step - loss: 1.2477e-04 - mean_absolute_error: 0.0065\n",
      "\n",
      "Epoch 06870: loss did not improve\n",
      "Epoch 6871/10000\n",
      "2700/2700 [==============================] - 0s 29us/step - loss: 1.2415e-04 - mean_absolute_error: 0.0065\n",
      "\n",
      "Epoch 06871: loss did not improve\n",
      "Epoch 6872/10000\n",
      "2700/2700 [==============================] - 0s 28us/step - loss: 1.2444e-04 - mean_absolute_error: 0.0063\n",
      "\n",
      "Epoch 06872: loss did not improve\n",
      "Epoch 6873/10000\n",
      "2700/2700 [==============================] - 0s 27us/step - loss: 1.2572e-04 - mean_absolute_error: 0.0067\n",
      "\n",
      "Epoch 06873: loss did not improve\n",
      "Epoch 6874/10000\n",
      "2700/2700 [==============================] - 0s 28us/step - loss: 1.2792e-04 - mean_absolute_error: 0.0068\n",
      "\n",
      "Epoch 06874: loss did not improve\n",
      "Epoch 6875/10000\n",
      "2700/2700 [==============================] - 0s 27us/step - loss: 1.2719e-04 - mean_absolute_error: 0.0067\n",
      "\n",
      "Epoch 06875: loss did not improve\n",
      "Epoch 6876/10000\n",
      "2700/2700 [==============================] - 0s 30us/step - loss: 1.2282e-04 - mean_absolute_error: 0.0063\n",
      "\n",
      "Epoch 06876: loss did not improve\n",
      "Epoch 6877/10000\n",
      "2700/2700 [==============================] - 0s 27us/step - loss: 1.2412e-04 - mean_absolute_error: 0.0065\n",
      "\n",
      "Epoch 06877: loss did not improve\n",
      "Epoch 6878/10000\n",
      "2700/2700 [==============================] - 0s 29us/step - loss: 1.2518e-04 - mean_absolute_error: 0.0066\n",
      "\n",
      "Epoch 06878: loss did not improve\n",
      "Epoch 6879/10000\n",
      "2700/2700 [==============================] - 0s 28us/step - loss: 1.2591e-04 - mean_absolute_error: 0.0066\n",
      "\n",
      "Epoch 06879: loss did not improve\n",
      "Epoch 6880/10000\n",
      "2700/2700 [==============================] - 0s 28us/step - loss: 1.2977e-04 - mean_absolute_error: 0.0070\n",
      "\n",
      "Epoch 06880: loss did not improve\n",
      "Epoch 6881/10000\n",
      "2700/2700 [==============================] - 0s 27us/step - loss: 1.2579e-04 - mean_absolute_error: 0.0066\n",
      "\n",
      "Epoch 06881: loss did not improve\n",
      "Epoch 6882/10000\n",
      "2700/2700 [==============================] - 0s 30us/step - loss: 1.2742e-04 - mean_absolute_error: 0.0066\n",
      "\n",
      "Epoch 06882: loss did not improve\n",
      "Epoch 6883/10000\n",
      "2700/2700 [==============================] - 0s 30us/step - loss: 1.2772e-04 - mean_absolute_error: 0.0067\n",
      "\n",
      "Epoch 06883: loss did not improve\n",
      "Epoch 6884/10000\n",
      "2700/2700 [==============================] - 0s 30us/step - loss: 1.2538e-04 - mean_absolute_error: 0.0065\n",
      "\n",
      "Epoch 06884: loss did not improve\n",
      "Epoch 6885/10000\n",
      "2700/2700 [==============================] - 0s 28us/step - loss: 1.2506e-04 - mean_absolute_error: 0.0066\n",
      "\n",
      "Epoch 06885: loss did not improve\n",
      "Epoch 6886/10000\n",
      "2700/2700 [==============================] - 0s 28us/step - loss: 1.2331e-04 - mean_absolute_error: 0.0064\n",
      "\n",
      "Epoch 06886: loss did not improve\n",
      "Epoch 6887/10000\n",
      "2700/2700 [==============================] - 0s 28us/step - loss: 1.2400e-04 - mean_absolute_error: 0.0064\n",
      "\n",
      "Epoch 06887: loss did not improve\n",
      "Epoch 6888/10000\n",
      "2700/2700 [==============================] - 0s 30us/step - loss: 1.2619e-04 - mean_absolute_error: 0.0066\n",
      "\n",
      "Epoch 06888: loss did not improve\n",
      "Epoch 6889/10000\n",
      "2700/2700 [==============================] - 0s 31us/step - loss: 1.2832e-04 - mean_absolute_error: 0.0067\n",
      "\n",
      "Epoch 06889: loss did not improve\n",
      "Epoch 6890/10000\n",
      "2700/2700 [==============================] - 0s 30us/step - loss: 1.2616e-04 - mean_absolute_error: 0.0065\n",
      "\n",
      "Epoch 06890: loss did not improve\n",
      "Epoch 6891/10000\n",
      "2700/2700 [==============================] - 0s 27us/step - loss: 1.2610e-04 - mean_absolute_error: 0.0066\n",
      "\n",
      "Epoch 06891: loss did not improve\n",
      "Epoch 6892/10000\n",
      "2700/2700 [==============================] - 0s 28us/step - loss: 1.2322e-04 - mean_absolute_error: 0.0063\n",
      "\n",
      "Epoch 06892: loss did not improve\n",
      "Epoch 6893/10000\n",
      "2700/2700 [==============================] - 0s 30us/step - loss: 1.2412e-04 - mean_absolute_error: 0.0064\n",
      "\n",
      "Epoch 06893: loss did not improve\n",
      "Epoch 6894/10000\n",
      "2700/2700 [==============================] - 0s 26us/step - loss: 1.2411e-04 - mean_absolute_error: 0.0064\n",
      "\n",
      "Epoch 06894: loss did not improve\n",
      "Epoch 6895/10000\n",
      "2700/2700 [==============================] - 0s 27us/step - loss: 1.2550e-04 - mean_absolute_error: 0.0065\n",
      "\n",
      "Epoch 06895: loss did not improve\n",
      "Epoch 6896/10000\n",
      "2700/2700 [==============================] - 0s 29us/step - loss: 1.2752e-04 - mean_absolute_error: 0.0067\n",
      "\n",
      "Epoch 06896: loss did not improve\n",
      "Epoch 6897/10000\n",
      "2700/2700 [==============================] - 0s 28us/step - loss: 1.2361e-04 - mean_absolute_error: 0.0064\n",
      "\n",
      "Epoch 06897: loss did not improve\n",
      "Epoch 6898/10000\n",
      "2700/2700 [==============================] - 0s 29us/step - loss: 1.2583e-04 - mean_absolute_error: 0.0066\n",
      "\n",
      "Epoch 06898: loss did not improve\n",
      "Epoch 6899/10000\n",
      "2700/2700 [==============================] - 0s 26us/step - loss: 1.2580e-04 - mean_absolute_error: 0.0066\n",
      "\n",
      "Epoch 06899: loss did not improve\n",
      "Epoch 6900/10000\n",
      "2700/2700 [==============================] - 0s 30us/step - loss: 1.2574e-04 - mean_absolute_error: 0.0065\n",
      "\n",
      "Epoch 06900: loss did not improve\n",
      "Epoch 6901/10000\n",
      "2700/2700 [==============================] - 0s 28us/step - loss: 1.2431e-04 - mean_absolute_error: 0.0065\n",
      "\n",
      "Epoch 06901: loss did not improve\n",
      "Epoch 6902/10000\n",
      "2700/2700 [==============================] - 0s 40us/step - loss: 1.2476e-04 - mean_absolute_error: 0.0064\n",
      "\n",
      "Epoch 06902: loss did not improve\n",
      "Epoch 6903/10000\n",
      "2700/2700 [==============================] - 0s 30us/step - loss: 1.2449e-04 - mean_absolute_error: 0.0065\n",
      "\n",
      "Epoch 06903: loss did not improve\n",
      "Epoch 6904/10000\n",
      "2700/2700 [==============================] - 0s 33us/step - loss: 1.2478e-04 - mean_absolute_error: 0.0065\n",
      "\n",
      "Epoch 06904: loss did not improve\n",
      "Epoch 6905/10000\n",
      "2700/2700 [==============================] - 0s 28us/step - loss: 1.2264e-04 - mean_absolute_error: 0.0063\n",
      "\n",
      "Epoch 06905: loss improved from 0.00012 to 0.00012, saving model to keras/best_baseline_weights.hdf5\n",
      "Epoch 6906/10000\n",
      "2700/2700 [==============================] - 0s 28us/step - loss: 1.2398e-04 - mean_absolute_error: 0.0064\n",
      "\n",
      "Epoch 06906: loss did not improve\n",
      "Epoch 6907/10000\n",
      "2700/2700 [==============================] - 0s 33us/step - loss: 1.2854e-04 - mean_absolute_error: 0.0069\n",
      "\n",
      "Epoch 06907: loss did not improve\n",
      "Epoch 6908/10000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2700/2700 [==============================] - 0s 32us/step - loss: 1.2521e-04 - mean_absolute_error: 0.0065\n",
      "\n",
      "Epoch 06908: loss did not improve\n",
      "Epoch 6909/10000\n",
      "2700/2700 [==============================] - 0s 34us/step - loss: 1.2452e-04 - mean_absolute_error: 0.0065\n",
      "\n",
      "Epoch 06909: loss did not improve\n",
      "Epoch 6910/10000\n",
      "2700/2700 [==============================] - 0s 35us/step - loss: 1.2562e-04 - mean_absolute_error: 0.0065\n",
      "\n",
      "Epoch 06910: loss did not improve\n",
      "Epoch 6911/10000\n",
      "2700/2700 [==============================] - 0s 28us/step - loss: 1.2773e-04 - mean_absolute_error: 0.0069\n",
      "\n",
      "Epoch 06911: loss did not improve\n",
      "Epoch 6912/10000\n",
      "2700/2700 [==============================] - 0s 29us/step - loss: 1.2590e-04 - mean_absolute_error: 0.0067\n",
      "\n",
      "Epoch 06912: loss did not improve\n",
      "Epoch 6913/10000\n",
      "2700/2700 [==============================] - 0s 27us/step - loss: 1.2573e-04 - mean_absolute_error: 0.0065\n",
      "\n",
      "Epoch 06913: loss did not improve\n",
      "Epoch 6914/10000\n",
      "2700/2700 [==============================] - 0s 30us/step - loss: 1.2514e-04 - mean_absolute_error: 0.0065\n",
      "\n",
      "Epoch 06914: loss did not improve\n",
      "Epoch 6915/10000\n",
      "2700/2700 [==============================] - 0s 27us/step - loss: 1.2685e-04 - mean_absolute_error: 0.0066\n",
      "\n",
      "Epoch 06915: loss did not improve\n",
      "Epoch 6916/10000\n",
      "2700/2700 [==============================] - 0s 27us/step - loss: 1.2431e-04 - mean_absolute_error: 0.0064\n",
      "\n",
      "Epoch 06916: loss did not improve\n",
      "Epoch 6917/10000\n",
      "2700/2700 [==============================] - 0s 27us/step - loss: 1.2509e-04 - mean_absolute_error: 0.0065\n",
      "\n",
      "Epoch 06917: loss did not improve\n",
      "Epoch 6918/10000\n",
      "2700/2700 [==============================] - 0s 27us/step - loss: 1.2560e-04 - mean_absolute_error: 0.0066\n",
      "\n",
      "Epoch 06918: loss did not improve\n",
      "Epoch 6919/10000\n",
      "2700/2700 [==============================] - 0s 30us/step - loss: 1.2361e-04 - mean_absolute_error: 0.0064\n",
      "\n",
      "Epoch 06919: loss did not improve\n",
      "Epoch 6920/10000\n",
      "2700/2700 [==============================] - 0s 29us/step - loss: 1.2365e-04 - mean_absolute_error: 0.0064\n",
      "\n",
      "Epoch 06920: loss did not improve\n",
      "Epoch 6921/10000\n",
      "2700/2700 [==============================] - 0s 26us/step - loss: 1.3060e-04 - mean_absolute_error: 0.0070\n",
      "\n",
      "Epoch 06921: loss did not improve\n",
      "Epoch 6922/10000\n",
      "2700/2700 [==============================] - 0s 30us/step - loss: 1.2523e-04 - mean_absolute_error: 0.0065\n",
      "\n",
      "Epoch 06922: loss did not improve\n",
      "Epoch 6923/10000\n",
      "2700/2700 [==============================] - 0s 30us/step - loss: 1.2306e-04 - mean_absolute_error: 0.0064\n",
      "\n",
      "Epoch 06923: loss did not improve\n",
      "Epoch 6924/10000\n",
      "2700/2700 [==============================] - 0s 30us/step - loss: 1.2379e-04 - mean_absolute_error: 0.0064\n",
      "\n",
      "Epoch 06924: loss did not improve\n",
      "Epoch 6925/10000\n",
      "2700/2700 [==============================] - 0s 29us/step - loss: 1.2635e-04 - mean_absolute_error: 0.0066\n",
      "\n",
      "Epoch 06925: loss did not improve\n",
      "Epoch 6926/10000\n",
      "2700/2700 [==============================] - 0s 27us/step - loss: 1.2423e-04 - mean_absolute_error: 0.0065\n",
      "\n",
      "Epoch 06926: loss did not improve\n",
      "Epoch 6927/10000\n",
      "2700/2700 [==============================] - 0s 29us/step - loss: 1.2967e-04 - mean_absolute_error: 0.0068\n",
      "\n",
      "Epoch 06927: loss did not improve\n",
      "Epoch 6928/10000\n",
      "2700/2700 [==============================] - 0s 31us/step - loss: 1.2464e-04 - mean_absolute_error: 0.0064\n",
      "\n",
      "Epoch 06928: loss did not improve\n",
      "Epoch 6929/10000\n",
      "2700/2700 [==============================] - 0s 29us/step - loss: 1.2484e-04 - mean_absolute_error: 0.0065\n",
      "\n",
      "Epoch 06929: loss did not improve\n",
      "Epoch 6930/10000\n",
      "2700/2700 [==============================] - 0s 29us/step - loss: 1.2500e-04 - mean_absolute_error: 0.0065\n",
      "\n",
      "Epoch 06930: loss did not improve\n",
      "Epoch 6931/10000\n",
      "2700/2700 [==============================] - 0s 28us/step - loss: 1.2594e-04 - mean_absolute_error: 0.0067\n",
      "\n",
      "Epoch 06931: loss did not improve\n",
      "Epoch 6932/10000\n",
      "2700/2700 [==============================] - 0s 28us/step - loss: 1.2593e-04 - mean_absolute_error: 0.0066\n",
      "\n",
      "Epoch 06932: loss did not improve\n",
      "Epoch 6933/10000\n",
      "2700/2700 [==============================] - 0s 28us/step - loss: 1.2406e-04 - mean_absolute_error: 0.0065\n",
      "\n",
      "Epoch 06933: loss did not improve\n",
      "Epoch 6934/10000\n",
      "2700/2700 [==============================] - 0s 30us/step - loss: 1.2453e-04 - mean_absolute_error: 0.0064\n",
      "\n",
      "Epoch 06934: loss did not improve\n",
      "Epoch 6935/10000\n",
      "2700/2700 [==============================] - 0s 28us/step - loss: 1.2639e-04 - mean_absolute_error: 0.0066\n",
      "\n",
      "Epoch 06935: loss did not improve\n",
      "Epoch 6936/10000\n",
      "2700/2700 [==============================] - 0s 30us/step - loss: 1.2850e-04 - mean_absolute_error: 0.0068\n",
      "\n",
      "Epoch 06936: loss did not improve\n",
      "Epoch 6937/10000\n",
      "2700/2700 [==============================] - 0s 31us/step - loss: 1.2321e-04 - mean_absolute_error: 0.0063\n",
      "\n",
      "Epoch 06937: loss did not improve\n",
      "Epoch 6938/10000\n",
      "2700/2700 [==============================] - 0s 28us/step - loss: 1.2357e-04 - mean_absolute_error: 0.0064\n",
      "\n",
      "Epoch 06938: loss did not improve\n",
      "Epoch 6939/10000\n",
      "2700/2700 [==============================] - 0s 30us/step - loss: 1.2566e-04 - mean_absolute_error: 0.0065\n",
      "\n",
      "Epoch 06939: loss did not improve\n",
      "Epoch 6940/10000\n",
      "2700/2700 [==============================] - 0s 27us/step - loss: 1.2445e-04 - mean_absolute_error: 0.0064\n",
      "\n",
      "Epoch 06940: loss did not improve\n",
      "Epoch 6941/10000\n",
      "2700/2700 [==============================] - 0s 30us/step - loss: 1.2376e-04 - mean_absolute_error: 0.0064\n",
      "\n",
      "Epoch 06941: loss did not improve\n",
      "Epoch 6942/10000\n",
      "2700/2700 [==============================] - 0s 30us/step - loss: 1.2514e-04 - mean_absolute_error: 0.0066\n",
      "\n",
      "Epoch 06942: loss did not improve\n",
      "Epoch 6943/10000\n",
      "2700/2700 [==============================] - 0s 29us/step - loss: 1.2701e-04 - mean_absolute_error: 0.0068\n",
      "\n",
      "Epoch 06943: loss did not improve\n",
      "Epoch 6944/10000\n",
      "2700/2700 [==============================] - 0s 30us/step - loss: 1.2460e-04 - mean_absolute_error: 0.0065\n",
      "\n",
      "Epoch 06944: loss did not improve\n",
      "Epoch 6945/10000\n",
      "2700/2700 [==============================] - 0s 26us/step - loss: 1.2368e-04 - mean_absolute_error: 0.0063\n",
      "\n",
      "Epoch 06945: loss did not improve\n",
      "Epoch 6946/10000\n",
      "2700/2700 [==============================] - 0s 30us/step - loss: 1.2476e-04 - mean_absolute_error: 0.0064\n",
      "\n",
      "Epoch 06946: loss did not improve\n",
      "Epoch 6947/10000\n",
      "2700/2700 [==============================] - 0s 31us/step - loss: 1.2566e-04 - mean_absolute_error: 0.0067\n",
      "\n",
      "Epoch 06947: loss did not improve\n",
      "Epoch 6948/10000\n",
      "2700/2700 [==============================] - 0s 29us/step - loss: 1.2528e-04 - mean_absolute_error: 0.0066\n",
      "\n",
      "Epoch 06948: loss did not improve\n",
      "Epoch 6949/10000\n",
      "2700/2700 [==============================] - 0s 29us/step - loss: 1.2875e-04 - mean_absolute_error: 0.0067\n",
      "\n",
      "Epoch 06949: loss did not improve\n",
      "Epoch 6950/10000\n",
      "2700/2700 [==============================] - 0s 30us/step - loss: 1.2434e-04 - mean_absolute_error: 0.0065\n",
      "\n",
      "Epoch 06950: loss did not improve\n",
      "Epoch 6951/10000\n",
      "2700/2700 [==============================] - 0s 29us/step - loss: 1.2341e-04 - mean_absolute_error: 0.0064\n",
      "\n",
      "Epoch 06951: loss did not improve\n",
      "Epoch 6952/10000\n",
      "2700/2700 [==============================] - 0s 31us/step - loss: 1.2409e-04 - mean_absolute_error: 0.0064\n",
      "\n",
      "Epoch 06952: loss did not improve\n",
      "Epoch 6953/10000\n",
      "2700/2700 [==============================] - 0s 29us/step - loss: 1.2326e-04 - mean_absolute_error: 0.0063\n",
      "\n",
      "Epoch 06953: loss did not improve\n",
      "Epoch 6954/10000\n",
      "2700/2700 [==============================] - 0s 29us/step - loss: 1.2545e-04 - mean_absolute_error: 0.0066\n",
      "\n",
      "Epoch 06954: loss did not improve\n",
      "Epoch 6955/10000\n",
      "2700/2700 [==============================] - 0s 28us/step - loss: 1.2728e-04 - mean_absolute_error: 0.0067\n",
      "\n",
      "Epoch 06955: loss did not improve\n",
      "Epoch 6956/10000\n",
      "2700/2700 [==============================] - 0s 28us/step - loss: 1.3154e-04 - mean_absolute_error: 0.0070\n",
      "\n",
      "Epoch 06956: loss did not improve\n",
      "Epoch 6957/10000\n",
      "2700/2700 [==============================] - 0s 28us/step - loss: 1.2412e-04 - mean_absolute_error: 0.0064\n",
      "\n",
      "Epoch 06957: loss did not improve\n",
      "Epoch 6958/10000\n",
      "2700/2700 [==============================] - 0s 31us/step - loss: 1.2372e-04 - mean_absolute_error: 0.0064\n",
      "\n",
      "Epoch 06958: loss did not improve\n",
      "Epoch 6959/10000\n",
      "2700/2700 [==============================] - 0s 25us/step - loss: 1.2501e-04 - mean_absolute_error: 0.0065\n",
      "\n",
      "Epoch 06959: loss did not improve\n",
      "Epoch 6960/10000\n",
      "2700/2700 [==============================] - 0s 29us/step - loss: 1.2606e-04 - mean_absolute_error: 0.0066\n",
      "\n",
      "Epoch 06960: loss did not improve\n",
      "Epoch 6961/10000\n",
      "2700/2700 [==============================] - 0s 30us/step - loss: 1.2725e-04 - mean_absolute_error: 0.0067\n",
      "\n",
      "Epoch 06961: loss did not improve\n",
      "Epoch 6962/10000\n",
      "2700/2700 [==============================] - 0s 29us/step - loss: 1.2665e-04 - mean_absolute_error: 0.0067\n",
      "\n",
      "Epoch 06962: loss did not improve\n",
      "Epoch 6963/10000\n",
      "2700/2700 [==============================] - 0s 28us/step - loss: 1.2381e-04 - mean_absolute_error: 0.0063\n",
      "\n",
      "Epoch 06963: loss did not improve\n",
      "Epoch 6964/10000\n",
      "2700/2700 [==============================] - 0s 28us/step - loss: 1.2339e-04 - mean_absolute_error: 0.0064\n",
      "\n",
      "Epoch 06964: loss did not improve\n",
      "Epoch 6965/10000\n",
      "2700/2700 [==============================] - 0s 28us/step - loss: 1.2549e-04 - mean_absolute_error: 0.0067\n",
      "\n",
      "Epoch 06965: loss did not improve\n",
      "Epoch 6966/10000\n",
      "2700/2700 [==============================] - 0s 29us/step - loss: 1.2569e-04 - mean_absolute_error: 0.0065\n",
      "\n",
      "Epoch 06966: loss did not improve\n",
      "Epoch 6967/10000\n",
      "2700/2700 [==============================] - 0s 31us/step - loss: 1.2412e-04 - mean_absolute_error: 0.0064\n",
      "\n",
      "Epoch 06967: loss did not improve\n",
      "Epoch 6968/10000\n",
      "2700/2700 [==============================] - 0s 30us/step - loss: 1.2334e-04 - mean_absolute_error: 0.0064\n",
      "\n",
      "Epoch 06968: loss did not improve\n",
      "Epoch 6969/10000\n",
      "2700/2700 [==============================] - 0s 29us/step - loss: 1.2680e-04 - mean_absolute_error: 0.0067\n",
      "\n",
      "Epoch 06969: loss did not improve\n",
      "Epoch 6970/10000\n",
      "2700/2700 [==============================] - 0s 32us/step - loss: 1.2684e-04 - mean_absolute_error: 0.0068\n",
      "\n",
      "Epoch 06970: loss did not improve\n",
      "Epoch 6971/10000\n",
      "2700/2700 [==============================] - 0s 30us/step - loss: 1.2314e-04 - mean_absolute_error: 0.0063\n",
      "\n",
      "Epoch 06971: loss did not improve\n",
      "Epoch 6972/10000\n",
      "2700/2700 [==============================] - 0s 28us/step - loss: 1.2496e-04 - mean_absolute_error: 0.0066\n",
      "\n",
      "Epoch 06972: loss did not improve\n",
      "Epoch 6973/10000\n",
      "2700/2700 [==============================] - 0s 26us/step - loss: 1.2372e-04 - mean_absolute_error: 0.0065\n",
      "\n",
      "Epoch 06973: loss did not improve\n",
      "Epoch 6974/10000\n",
      "2700/2700 [==============================] - 0s 27us/step - loss: 1.2482e-04 - mean_absolute_error: 0.0066\n",
      "\n",
      "Epoch 06974: loss did not improve\n",
      "Epoch 6975/10000\n",
      "2700/2700 [==============================] - 0s 28us/step - loss: 1.2622e-04 - mean_absolute_error: 0.0066\n",
      "\n",
      "Epoch 06975: loss did not improve\n",
      "Epoch 6976/10000\n",
      "2700/2700 [==============================] - 0s 28us/step - loss: 1.2527e-04 - mean_absolute_error: 0.0065\n",
      "\n",
      "Epoch 06976: loss did not improve\n",
      "Epoch 6977/10000\n",
      "2700/2700 [==============================] - 0s 27us/step - loss: 1.2257e-04 - mean_absolute_error: 0.0063\n",
      "\n",
      "Epoch 06977: loss improved from 0.00012 to 0.00012, saving model to keras/best_baseline_weights.hdf5\n",
      "Epoch 6978/10000\n",
      "2700/2700 [==============================] - 0s 25us/step - loss: 1.2352e-04 - mean_absolute_error: 0.0064\n",
      "\n",
      "Epoch 06978: loss did not improve\n",
      "Epoch 6979/10000\n",
      "2700/2700 [==============================] - 0s 28us/step - loss: 1.2307e-04 - mean_absolute_error: 0.0064\n",
      "\n",
      "Epoch 06979: loss did not improve\n",
      "Epoch 6980/10000\n",
      "2700/2700 [==============================] - 0s 28us/step - loss: 1.2600e-04 - mean_absolute_error: 0.0066\n",
      "\n",
      "Epoch 06980: loss did not improve\n",
      "Epoch 6981/10000\n",
      "2700/2700 [==============================] - 0s 28us/step - loss: 1.2450e-04 - mean_absolute_error: 0.0065\n",
      "\n",
      "Epoch 06981: loss did not improve\n",
      "Epoch 6982/10000\n",
      "2700/2700 [==============================] - 0s 28us/step - loss: 1.2453e-04 - mean_absolute_error: 0.0065\n",
      "\n",
      "Epoch 06982: loss did not improve\n",
      "Epoch 6983/10000\n",
      "2700/2700 [==============================] - 0s 28us/step - loss: 1.2314e-04 - mean_absolute_error: 0.0064\n",
      "\n",
      "Epoch 06983: loss did not improve\n",
      "Epoch 6984/10000\n",
      "2700/2700 [==============================] - 0s 32us/step - loss: 1.2729e-04 - mean_absolute_error: 0.0068\n",
      "\n",
      "Epoch 06984: loss did not improve\n",
      "Epoch 6985/10000\n",
      "2700/2700 [==============================] - 0s 29us/step - loss: 1.2243e-04 - mean_absolute_error: 0.0063\n",
      "\n",
      "Epoch 06985: loss improved from 0.00012 to 0.00012, saving model to keras/best_baseline_weights.hdf5\n",
      "Epoch 6986/10000\n",
      "2700/2700 [==============================] - 0s 30us/step - loss: 1.2613e-04 - mean_absolute_error: 0.0067\n",
      "\n",
      "Epoch 06986: loss did not improve\n",
      "Epoch 6987/10000\n",
      "2700/2700 [==============================] - 0s 26us/step - loss: 1.2697e-04 - mean_absolute_error: 0.0066\n",
      "\n",
      "Epoch 06987: loss did not improve\n",
      "Epoch 6988/10000\n",
      "2700/2700 [==============================] - 0s 30us/step - loss: 1.2762e-04 - mean_absolute_error: 0.0067\n",
      "\n",
      "Epoch 06988: loss did not improve\n",
      "Epoch 6989/10000\n",
      "2700/2700 [==============================] - 0s 29us/step - loss: 1.2373e-04 - mean_absolute_error: 0.0064\n",
      "\n",
      "Epoch 06989: loss did not improve\n",
      "Epoch 6990/10000\n",
      "2700/2700 [==============================] - 0s 28us/step - loss: 1.2313e-04 - mean_absolute_error: 0.0063\n",
      "\n",
      "Epoch 06990: loss did not improve\n",
      "Epoch 6991/10000\n",
      "2700/2700 [==============================] - 0s 27us/step - loss: 1.2386e-04 - mean_absolute_error: 0.0064\n",
      "\n",
      "Epoch 06991: loss did not improve\n",
      "Epoch 6992/10000\n",
      "2700/2700 [==============================] - 0s 29us/step - loss: 1.2702e-04 - mean_absolute_error: 0.0066\n",
      "\n",
      "Epoch 06992: loss did not improve\n",
      "Epoch 6993/10000\n",
      "2700/2700 [==============================] - 0s 28us/step - loss: 1.2460e-04 - mean_absolute_error: 0.0065\n",
      "\n",
      "Epoch 06993: loss did not improve\n",
      "Epoch 6994/10000\n",
      "2700/2700 [==============================] - 0s 28us/step - loss: 1.2247e-04 - mean_absolute_error: 0.0063\n",
      "\n",
      "Epoch 06994: loss did not improve\n",
      "Epoch 6995/10000\n",
      "2700/2700 [==============================] - 0s 26us/step - loss: 1.2376e-04 - mean_absolute_error: 0.0065\n",
      "\n",
      "Epoch 06995: loss did not improve\n",
      "Epoch 6996/10000\n",
      "2700/2700 [==============================] - 0s 27us/step - loss: 1.2670e-04 - mean_absolute_error: 0.0067\n",
      "\n",
      "Epoch 06996: loss did not improve\n",
      "Epoch 6997/10000\n",
      "2700/2700 [==============================] - 0s 29us/step - loss: 1.2369e-04 - mean_absolute_error: 0.0064\n",
      "\n",
      "Epoch 06997: loss did not improve\n",
      "Epoch 6998/10000\n",
      "2700/2700 [==============================] - 0s 29us/step - loss: 1.2349e-04 - mean_absolute_error: 0.0064\n",
      "\n",
      "Epoch 06998: loss did not improve\n",
      "Epoch 6999/10000\n",
      "2700/2700 [==============================] - 0s 26us/step - loss: 1.2556e-04 - mean_absolute_error: 0.0065\n",
      "\n",
      "Epoch 06999: loss did not improve\n",
      "Epoch 7000/10000\n",
      "2700/2700 [==============================] - 0s 30us/step - loss: 1.2628e-04 - mean_absolute_error: 0.0066\n",
      "\n",
      "Epoch 07000: loss did not improve\n",
      "Epoch 7001/10000\n",
      "2700/2700 [==============================] - 0s 29us/step - loss: 1.2320e-04 - mean_absolute_error: 0.0063\n",
      "\n",
      "Epoch 07001: loss did not improve\n",
      "Epoch 7002/10000\n",
      "2700/2700 [==============================] - 0s 28us/step - loss: 1.2534e-04 - mean_absolute_error: 0.0065\n",
      "\n",
      "Epoch 07002: loss did not improve\n",
      "Epoch 7003/10000\n",
      "2700/2700 [==============================] - 0s 26us/step - loss: 1.2353e-04 - mean_absolute_error: 0.0063\n",
      "\n",
      "Epoch 07003: loss did not improve\n",
      "Epoch 7004/10000\n",
      "2700/2700 [==============================] - 0s 31us/step - loss: 1.2476e-04 - mean_absolute_error: 0.0065\n",
      "\n",
      "Epoch 07004: loss did not improve\n",
      "Epoch 7005/10000\n",
      "2700/2700 [==============================] - 0s 28us/step - loss: 1.2516e-04 - mean_absolute_error: 0.0066\n",
      "\n",
      "Epoch 07005: loss did not improve\n",
      "Epoch 7006/10000\n",
      "2700/2700 [==============================] - 0s 28us/step - loss: 1.2574e-04 - mean_absolute_error: 0.0066\n",
      "\n",
      "Epoch 07006: loss did not improve\n",
      "Epoch 7007/10000\n",
      "2700/2700 [==============================] - 0s 29us/step - loss: 1.2355e-04 - mean_absolute_error: 0.0064\n",
      "\n",
      "Epoch 07007: loss did not improve\n",
      "Epoch 7008/10000\n",
      "2700/2700 [==============================] - 0s 27us/step - loss: 1.2761e-04 - mean_absolute_error: 0.0067\n",
      "\n",
      "Epoch 07008: loss did not improve\n",
      "Epoch 7009/10000\n",
      "2700/2700 [==============================] - 0s 27us/step - loss: 1.2581e-04 - mean_absolute_error: 0.0067\n",
      "\n",
      "Epoch 07009: loss did not improve\n",
      "Epoch 7010/10000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2700/2700 [==============================] - 0s 30us/step - loss: 1.2380e-04 - mean_absolute_error: 0.0065\n",
      "\n",
      "Epoch 07010: loss did not improve\n",
      "Epoch 7011/10000\n",
      "2700/2700 [==============================] - 0s 27us/step - loss: 1.2212e-04 - mean_absolute_error: 0.0062\n",
      "\n",
      "Epoch 07011: loss improved from 0.00012 to 0.00012, saving model to keras/best_baseline_weights.hdf5\n",
      "Epoch 7012/10000\n",
      "2700/2700 [==============================] - 0s 27us/step - loss: 1.2388e-04 - mean_absolute_error: 0.0064\n",
      "\n",
      "Epoch 07012: loss did not improve\n",
      "Epoch 7013/10000\n",
      "2700/2700 [==============================] - 0s 30us/step - loss: 1.2707e-04 - mean_absolute_error: 0.0067\n",
      "\n",
      "Epoch 07013: loss did not improve\n",
      "Epoch 7014/10000\n",
      "2700/2700 [==============================] - 0s 28us/step - loss: 1.2565e-04 - mean_absolute_error: 0.0065\n",
      "\n",
      "Epoch 07014: loss did not improve\n",
      "Epoch 7015/10000\n",
      "2700/2700 [==============================] - 0s 27us/step - loss: 1.2383e-04 - mean_absolute_error: 0.0064\n",
      "\n",
      "Epoch 07015: loss did not improve\n",
      "Epoch 7016/10000\n",
      "2700/2700 [==============================] - 0s 29us/step - loss: 1.2558e-04 - mean_absolute_error: 0.0065\n",
      "\n",
      "Epoch 07016: loss did not improve\n",
      "Epoch 7017/10000\n",
      "2700/2700 [==============================] - 0s 28us/step - loss: 1.2311e-04 - mean_absolute_error: 0.0064\n",
      "\n",
      "Epoch 07017: loss did not improve\n",
      "Epoch 7018/10000\n",
      "2700/2700 [==============================] - 0s 31us/step - loss: 1.2347e-04 - mean_absolute_error: 0.0063\n",
      "\n",
      "Epoch 07018: loss did not improve\n",
      "Epoch 7019/10000\n",
      "2700/2700 [==============================] - 0s 28us/step - loss: 1.2479e-04 - mean_absolute_error: 0.0065\n",
      "\n",
      "Epoch 07019: loss did not improve\n",
      "Epoch 7020/10000\n",
      "2700/2700 [==============================] - 0s 29us/step - loss: 1.2568e-04 - mean_absolute_error: 0.0066\n",
      "\n",
      "Epoch 07020: loss did not improve\n",
      "Epoch 7021/10000\n",
      "2700/2700 [==============================] - 0s 26us/step - loss: 1.2606e-04 - mean_absolute_error: 0.0065\n",
      "\n",
      "Epoch 07021: loss did not improve\n",
      "Epoch 7022/10000\n",
      "2700/2700 [==============================] - 0s 27us/step - loss: 1.2380e-04 - mean_absolute_error: 0.0064\n",
      "\n",
      "Epoch 07022: loss did not improve\n",
      "Epoch 7023/10000\n",
      "2700/2700 [==============================] - 0s 28us/step - loss: 1.2429e-04 - mean_absolute_error: 0.0064\n",
      "\n",
      "Epoch 07023: loss did not improve\n",
      "Epoch 7024/10000\n",
      "2700/2700 [==============================] - 0s 29us/step - loss: 1.2532e-04 - mean_absolute_error: 0.0066\n",
      "\n",
      "Epoch 07024: loss did not improve\n",
      "Epoch 7025/10000\n",
      "2700/2700 [==============================] - 0s 26us/step - loss: 1.2434e-04 - mean_absolute_error: 0.0066\n",
      "\n",
      "Epoch 07025: loss did not improve\n",
      "Epoch 7026/10000\n",
      "2700/2700 [==============================] - 0s 29us/step - loss: 1.2668e-04 - mean_absolute_error: 0.0067\n",
      "\n",
      "Epoch 07026: loss did not improve\n",
      "Epoch 7027/10000\n",
      "2700/2700 [==============================] - 0s 27us/step - loss: 1.2304e-04 - mean_absolute_error: 0.0063\n",
      "\n",
      "Epoch 07027: loss did not improve\n",
      "Epoch 7028/10000\n",
      "2700/2700 [==============================] - 0s 29us/step - loss: 1.2435e-04 - mean_absolute_error: 0.0065\n",
      "\n",
      "Epoch 07028: loss did not improve\n",
      "Epoch 7029/10000\n",
      "2700/2700 [==============================] - 0s 26us/step - loss: 1.2245e-04 - mean_absolute_error: 0.0063\n",
      "\n",
      "Epoch 07029: loss did not improve\n",
      "Epoch 7030/10000\n",
      "2700/2700 [==============================] - 0s 29us/step - loss: 1.2927e-04 - mean_absolute_error: 0.0068\n",
      "\n",
      "Epoch 07030: loss did not improve\n",
      "Epoch 7031/10000\n",
      "2700/2700 [==============================] - 0s 29us/step - loss: 1.2267e-04 - mean_absolute_error: 0.0063\n",
      "\n",
      "Epoch 07031: loss did not improve\n",
      "Epoch 7032/10000\n",
      "2700/2700 [==============================] - 0s 29us/step - loss: 1.2349e-04 - mean_absolute_error: 0.0064\n",
      "\n",
      "Epoch 07032: loss did not improve\n",
      "Epoch 7033/10000\n",
      "2700/2700 [==============================] - 0s 29us/step - loss: 1.2333e-04 - mean_absolute_error: 0.0063\n",
      "\n",
      "Epoch 07033: loss did not improve\n",
      "Epoch 7034/10000\n",
      "2700/2700 [==============================] - 0s 28us/step - loss: 1.2324e-04 - mean_absolute_error: 0.0064\n",
      "\n",
      "Epoch 07034: loss did not improve\n",
      "Epoch 7035/10000\n",
      "2700/2700 [==============================] - 0s 30us/step - loss: 1.2545e-04 - mean_absolute_error: 0.0066\n",
      "\n",
      "Epoch 07035: loss did not improve\n",
      "Epoch 7036/10000\n",
      "2700/2700 [==============================] - 0s 30us/step - loss: 1.2520e-04 - mean_absolute_error: 0.0064\n",
      "\n",
      "Epoch 07036: loss did not improve\n",
      "Epoch 7037/10000\n",
      "2700/2700 [==============================] - 0s 28us/step - loss: 1.2750e-04 - mean_absolute_error: 0.0068\n",
      "\n",
      "Epoch 07037: loss did not improve\n",
      "Epoch 7038/10000\n",
      "2700/2700 [==============================] - 0s 26us/step - loss: 1.2296e-04 - mean_absolute_error: 0.0064\n",
      "\n",
      "Epoch 07038: loss did not improve\n",
      "Epoch 7039/10000\n",
      "2700/2700 [==============================] - 0s 27us/step - loss: 1.2273e-04 - mean_absolute_error: 0.0063\n",
      "\n",
      "Epoch 07039: loss did not improve\n",
      "Epoch 7040/10000\n",
      "2700/2700 [==============================] - 0s 30us/step - loss: 1.2599e-04 - mean_absolute_error: 0.0066\n",
      "\n",
      "Epoch 07040: loss did not improve\n",
      "Epoch 7041/10000\n",
      "2700/2700 [==============================] - 0s 27us/step - loss: 1.2310e-04 - mean_absolute_error: 0.0064\n",
      "\n",
      "Epoch 07041: loss did not improve\n",
      "Epoch 7042/10000\n",
      "2700/2700 [==============================] - 0s 27us/step - loss: 1.2816e-04 - mean_absolute_error: 0.0068\n",
      "\n",
      "Epoch 07042: loss did not improve\n",
      "Epoch 7043/10000\n",
      "2700/2700 [==============================] - 0s 30us/step - loss: 1.2581e-04 - mean_absolute_error: 0.0067\n",
      "\n",
      "Epoch 07043: loss did not improve\n",
      "Epoch 7044/10000\n",
      "2700/2700 [==============================] - 0s 28us/step - loss: 1.2635e-04 - mean_absolute_error: 0.0066\n",
      "\n",
      "Epoch 07044: loss did not improve\n",
      "Epoch 7045/10000\n",
      "2700/2700 [==============================] - 0s 28us/step - loss: 1.2442e-04 - mean_absolute_error: 0.0064\n",
      "\n",
      "Epoch 07045: loss did not improve\n",
      "Epoch 7046/10000\n",
      "2700/2700 [==============================] - 0s 31us/step - loss: 1.2262e-04 - mean_absolute_error: 0.0064\n",
      "\n",
      "Epoch 07046: loss did not improve\n",
      "Epoch 7047/10000\n",
      "2700/2700 [==============================] - 0s 30us/step - loss: 1.2517e-04 - mean_absolute_error: 0.0064\n",
      "\n",
      "Epoch 07047: loss did not improve\n",
      "Epoch 7048/10000\n",
      "2700/2700 [==============================] - 0s 39us/step - loss: 1.2389e-04 - mean_absolute_error: 0.0064\n",
      "\n",
      "Epoch 07048: loss did not improve\n",
      "Epoch 7049/10000\n",
      "2700/2700 [==============================] - 0s 29us/step - loss: 1.2348e-04 - mean_absolute_error: 0.0063\n",
      "\n",
      "Epoch 07049: loss did not improve\n",
      "Epoch 7050/10000\n",
      "2700/2700 [==============================] - 0s 28us/step - loss: 1.2744e-04 - mean_absolute_error: 0.0067\n",
      "\n",
      "Epoch 07050: loss did not improve\n",
      "Epoch 7051/10000\n",
      "2700/2700 [==============================] - 0s 27us/step - loss: 1.2475e-04 - mean_absolute_error: 0.0066\n",
      "\n",
      "Epoch 07051: loss did not improve\n",
      "Epoch 7052/10000\n",
      "2700/2700 [==============================] - 0s 40us/step - loss: 1.2256e-04 - mean_absolute_error: 0.0063\n",
      "\n",
      "Epoch 07052: loss did not improve\n",
      "Epoch 7053/10000\n",
      "2700/2700 [==============================] - 0s 33us/step - loss: 1.2443e-04 - mean_absolute_error: 0.0064\n",
      "\n",
      "Epoch 07053: loss did not improve\n",
      "Epoch 7054/10000\n",
      "2700/2700 [==============================] - 0s 31us/step - loss: 1.2594e-04 - mean_absolute_error: 0.0066\n",
      "\n",
      "Epoch 07054: loss did not improve\n",
      "Epoch 7055/10000\n",
      "2700/2700 [==============================] - 0s 42us/step - loss: 1.2538e-04 - mean_absolute_error: 0.0066\n",
      "\n",
      "Epoch 07055: loss did not improve\n",
      "Epoch 7056/10000\n",
      "2700/2700 [==============================] - 0s 36us/step - loss: 1.2646e-04 - mean_absolute_error: 0.0067\n",
      "\n",
      "Epoch 07056: loss did not improve\n",
      "Epoch 7057/10000\n",
      "2700/2700 [==============================] - 0s 28us/step - loss: 1.2219e-04 - mean_absolute_error: 0.0062\n",
      "\n",
      "Epoch 07057: loss did not improve\n",
      "Epoch 7058/10000\n",
      "2700/2700 [==============================] - 0s 33us/step - loss: 1.2399e-04 - mean_absolute_error: 0.0065\n",
      "\n",
      "Epoch 07058: loss did not improve\n",
      "Epoch 7059/10000\n",
      "2700/2700 [==============================] - 0s 29us/step - loss: 1.2358e-04 - mean_absolute_error: 0.0064\n",
      "\n",
      "Epoch 07059: loss did not improve\n",
      "Epoch 7060/10000\n",
      "2700/2700 [==============================] - 0s 31us/step - loss: 1.2612e-04 - mean_absolute_error: 0.0065\n",
      "\n",
      "Epoch 07060: loss did not improve\n",
      "Epoch 7061/10000\n",
      "2700/2700 [==============================] - 0s 32us/step - loss: 1.2299e-04 - mean_absolute_error: 0.0064\n",
      "\n",
      "Epoch 07061: loss did not improve\n",
      "Epoch 7062/10000\n",
      "2700/2700 [==============================] - 0s 25us/step - loss: 1.2371e-04 - mean_absolute_error: 0.0065\n",
      "\n",
      "Epoch 07062: loss did not improve\n",
      "Epoch 7063/10000\n",
      "2700/2700 [==============================] - 0s 27us/step - loss: 1.2458e-04 - mean_absolute_error: 0.0065\n",
      "\n",
      "Epoch 07063: loss did not improve\n",
      "Epoch 7064/10000\n",
      "2700/2700 [==============================] - 0s 27us/step - loss: 1.2794e-04 - mean_absolute_error: 0.0067\n",
      "\n",
      "Epoch 07064: loss did not improve\n",
      "Epoch 7065/10000\n",
      "2700/2700 [==============================] - 0s 28us/step - loss: 1.2325e-04 - mean_absolute_error: 0.0064\n",
      "\n",
      "Epoch 07065: loss did not improve\n",
      "Epoch 7066/10000\n",
      "2700/2700 [==============================] - 0s 28us/step - loss: 1.2448e-04 - mean_absolute_error: 0.0065\n",
      "\n",
      "Epoch 07066: loss did not improve\n",
      "Epoch 7067/10000\n",
      "2700/2700 [==============================] - 0s 30us/step - loss: 1.2523e-04 - mean_absolute_error: 0.0066\n",
      "\n",
      "Epoch 07067: loss did not improve\n",
      "Epoch 7068/10000\n",
      "2700/2700 [==============================] - 0s 32us/step - loss: 1.2511e-04 - mean_absolute_error: 0.0066\n",
      "\n",
      "Epoch 07068: loss did not improve\n",
      "Epoch 7069/10000\n",
      "2700/2700 [==============================] - 0s 29us/step - loss: 1.2385e-04 - mean_absolute_error: 0.0065\n",
      "\n",
      "Epoch 07069: loss did not improve\n",
      "Epoch 7070/10000\n",
      "2700/2700 [==============================] - 0s 28us/step - loss: 1.2419e-04 - mean_absolute_error: 0.0064\n",
      "\n",
      "Epoch 07070: loss did not improve\n",
      "Epoch 7071/10000\n",
      "2700/2700 [==============================] - 0s 27us/step - loss: 1.2500e-04 - mean_absolute_error: 0.0065\n",
      "\n",
      "Epoch 07071: loss did not improve\n",
      "Epoch 7072/10000\n",
      "2700/2700 [==============================] - 0s 31us/step - loss: 1.2421e-04 - mean_absolute_error: 0.0064\n",
      "\n",
      "Epoch 07072: loss did not improve\n",
      "Epoch 7073/10000\n",
      "2700/2700 [==============================] - 0s 27us/step - loss: 1.2468e-04 - mean_absolute_error: 0.0065\n",
      "\n",
      "Epoch 07073: loss did not improve\n",
      "Epoch 7074/10000\n",
      "2700/2700 [==============================] - 0s 30us/step - loss: 1.2335e-04 - mean_absolute_error: 0.0064\n",
      "\n",
      "Epoch 07074: loss did not improve\n",
      "Epoch 7075/10000\n",
      "2700/2700 [==============================] - 0s 30us/step - loss: 1.2347e-04 - mean_absolute_error: 0.0066\n",
      "\n",
      "Epoch 07075: loss did not improve\n",
      "Epoch 7076/10000\n",
      "2700/2700 [==============================] - 0s 26us/step - loss: 1.2574e-04 - mean_absolute_error: 0.0066\n",
      "\n",
      "Epoch 07076: loss did not improve\n",
      "Epoch 7077/10000\n",
      "2700/2700 [==============================] - 0s 30us/step - loss: 1.2660e-04 - mean_absolute_error: 0.0067\n",
      "\n",
      "Epoch 07077: loss did not improve\n",
      "Epoch 7078/10000\n",
      "2700/2700 [==============================] - 0s 28us/step - loss: 1.2809e-04 - mean_absolute_error: 0.0068\n",
      "\n",
      "Epoch 07078: loss did not improve\n",
      "Epoch 7079/10000\n",
      "2700/2700 [==============================] - 0s 28us/step - loss: 1.2233e-04 - mean_absolute_error: 0.0063\n",
      "\n",
      "Epoch 07079: loss did not improve\n",
      "Epoch 7080/10000\n",
      "2700/2700 [==============================] - 0s 26us/step - loss: 1.2289e-04 - mean_absolute_error: 0.0064\n",
      "\n",
      "Epoch 07080: loss did not improve\n",
      "Epoch 7081/10000\n",
      "2700/2700 [==============================] - 0s 28us/step - loss: 1.2576e-04 - mean_absolute_error: 0.0066\n",
      "\n",
      "Epoch 07081: loss did not improve\n",
      "Epoch 7082/10000\n",
      "2700/2700 [==============================] - 0s 28us/step - loss: 1.2475e-04 - mean_absolute_error: 0.0065\n",
      "\n",
      "Epoch 07082: loss did not improve\n",
      "Epoch 7083/10000\n",
      "2700/2700 [==============================] - 0s 30us/step - loss: 1.2208e-04 - mean_absolute_error: 0.0063\n",
      "\n",
      "Epoch 07083: loss improved from 0.00012 to 0.00012, saving model to keras/best_baseline_weights.hdf5\n",
      "Epoch 7084/10000\n",
      "2700/2700 [==============================] - 0s 29us/step - loss: 1.2269e-04 - mean_absolute_error: 0.0063\n",
      "\n",
      "Epoch 07084: loss did not improve\n",
      "Epoch 7085/10000\n",
      "2700/2700 [==============================] - 0s 25us/step - loss: 1.2454e-04 - mean_absolute_error: 0.0066\n",
      "\n",
      "Epoch 07085: loss did not improve\n",
      "Epoch 7086/10000\n",
      "2700/2700 [==============================] - 0s 28us/step - loss: 1.2637e-04 - mean_absolute_error: 0.0068\n",
      "\n",
      "Epoch 07086: loss did not improve\n",
      "Epoch 7087/10000\n",
      "2700/2700 [==============================] - 0s 33us/step - loss: 1.2402e-04 - mean_absolute_error: 0.0065\n",
      "\n",
      "Epoch 07087: loss did not improve\n",
      "Epoch 7088/10000\n",
      "2700/2700 [==============================] - 0s 27us/step - loss: 1.2560e-04 - mean_absolute_error: 0.0066\n",
      "\n",
      "Epoch 07088: loss did not improve\n",
      "Epoch 7089/10000\n",
      "2700/2700 [==============================] - 0s 27us/step - loss: 1.2291e-04 - mean_absolute_error: 0.0063\n",
      "\n",
      "Epoch 07089: loss did not improve\n",
      "Epoch 7090/10000\n",
      "2700/2700 [==============================] - 0s 30us/step - loss: 1.2563e-04 - mean_absolute_error: 0.0066\n",
      "\n",
      "Epoch 07090: loss did not improve\n",
      "Epoch 7091/10000\n",
      "2700/2700 [==============================] - 0s 30us/step - loss: 1.2571e-04 - mean_absolute_error: 0.0067\n",
      "\n",
      "Epoch 07091: loss did not improve\n",
      "Epoch 7092/10000\n",
      "2700/2700 [==============================] - 0s 29us/step - loss: 1.2214e-04 - mean_absolute_error: 0.0063\n",
      "\n",
      "Epoch 07092: loss did not improve\n",
      "Epoch 7093/10000\n",
      "2700/2700 [==============================] - 0s 32us/step - loss: 1.2806e-04 - mean_absolute_error: 0.0068\n",
      "\n",
      "Epoch 07093: loss did not improve\n",
      "Epoch 7094/10000\n",
      "2700/2700 [==============================] - 0s 30us/step - loss: 1.2293e-04 - mean_absolute_error: 0.0064\n",
      "\n",
      "Epoch 07094: loss did not improve\n",
      "Epoch 7095/10000\n",
      "2700/2700 [==============================] - 0s 30us/step - loss: 1.2326e-04 - mean_absolute_error: 0.0064\n",
      "\n",
      "Epoch 07095: loss did not improve\n",
      "Epoch 7096/10000\n",
      "2700/2700 [==============================] - 0s 29us/step - loss: 1.2282e-04 - mean_absolute_error: 0.0064\n",
      "\n",
      "Epoch 07096: loss did not improve\n",
      "Epoch 7097/10000\n",
      "2700/2700 [==============================] - 0s 28us/step - loss: 1.2453e-04 - mean_absolute_error: 0.0065\n",
      "\n",
      "Epoch 07097: loss did not improve\n",
      "Epoch 7098/10000\n",
      "2700/2700 [==============================] - 0s 30us/step - loss: 1.2354e-04 - mean_absolute_error: 0.0065\n",
      "\n",
      "Epoch 07098: loss did not improve\n",
      "Epoch 7099/10000\n",
      "2700/2700 [==============================] - 0s 27us/step - loss: 1.2471e-04 - mean_absolute_error: 0.0065\n",
      "\n",
      "Epoch 07099: loss did not improve\n",
      "Epoch 7100/10000\n",
      "2700/2700 [==============================] - 0s 29us/step - loss: 1.2399e-04 - mean_absolute_error: 0.0065\n",
      "\n",
      "Epoch 07100: loss did not improve\n",
      "Epoch 7101/10000\n",
      "2700/2700 [==============================] - 0s 30us/step - loss: 1.2437e-04 - mean_absolute_error: 0.0065\n",
      "\n",
      "Epoch 07101: loss did not improve\n",
      "Epoch 7102/10000\n",
      "2700/2700 [==============================] - 0s 28us/step - loss: 1.2207e-04 - mean_absolute_error: 0.0063\n",
      "\n",
      "Epoch 07102: loss improved from 0.00012 to 0.00012, saving model to keras/best_baseline_weights.hdf5\n",
      "Epoch 7103/10000\n",
      "2700/2700 [==============================] - 0s 28us/step - loss: 1.2431e-04 - mean_absolute_error: 0.0065\n",
      "\n",
      "Epoch 07103: loss did not improve\n",
      "Epoch 7104/10000\n",
      "2700/2700 [==============================] - 0s 28us/step - loss: 1.2433e-04 - mean_absolute_error: 0.0065\n",
      "\n",
      "Epoch 07104: loss did not improve\n",
      "Epoch 7105/10000\n",
      "2700/2700 [==============================] - 0s 29us/step - loss: 1.2353e-04 - mean_absolute_error: 0.0064\n",
      "\n",
      "Epoch 07105: loss did not improve\n",
      "Epoch 7106/10000\n",
      "2700/2700 [==============================] - 0s 29us/step - loss: 1.2509e-04 - mean_absolute_error: 0.0065\n",
      "\n",
      "Epoch 07106: loss did not improve\n",
      "Epoch 7107/10000\n",
      "2700/2700 [==============================] - 0s 29us/step - loss: 1.2725e-04 - mean_absolute_error: 0.0066\n",
      "\n",
      "Epoch 07107: loss did not improve\n",
      "Epoch 7108/10000\n",
      "2700/2700 [==============================] - 0s 27us/step - loss: 1.2349e-04 - mean_absolute_error: 0.0064\n",
      "\n",
      "Epoch 07108: loss did not improve\n",
      "Epoch 7109/10000\n",
      "2700/2700 [==============================] - 0s 28us/step - loss: 1.2590e-04 - mean_absolute_error: 0.0066\n",
      "\n",
      "Epoch 07109: loss did not improve\n",
      "Epoch 7110/10000\n",
      "2700/2700 [==============================] - 0s 29us/step - loss: 1.2430e-04 - mean_absolute_error: 0.0065\n",
      "\n",
      "Epoch 07110: loss did not improve\n",
      "Epoch 7111/10000\n",
      "2700/2700 [==============================] - 0s 29us/step - loss: 1.2346e-04 - mean_absolute_error: 0.0065\n",
      "\n",
      "Epoch 07111: loss did not improve\n",
      "Epoch 7112/10000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2700/2700 [==============================] - 0s 26us/step - loss: 1.2626e-04 - mean_absolute_error: 0.0066\n",
      "\n",
      "Epoch 07112: loss did not improve\n",
      "Epoch 7113/10000\n",
      "2700/2700 [==============================] - 0s 30us/step - loss: 1.2333e-04 - mean_absolute_error: 0.0065\n",
      "\n",
      "Epoch 07113: loss did not improve\n",
      "Epoch 7114/10000\n",
      "2700/2700 [==============================] - 0s 27us/step - loss: 1.2290e-04 - mean_absolute_error: 0.0064\n",
      "\n",
      "Epoch 07114: loss did not improve\n",
      "Epoch 7115/10000\n",
      "2700/2700 [==============================] - 0s 29us/step - loss: 1.2747e-04 - mean_absolute_error: 0.0067\n",
      "\n",
      "Epoch 07115: loss did not improve\n",
      "Epoch 7116/10000\n",
      "2700/2700 [==============================] - 0s 30us/step - loss: 1.2353e-04 - mean_absolute_error: 0.0065\n",
      "\n",
      "Epoch 07116: loss did not improve\n",
      "Epoch 7117/10000\n",
      "2700/2700 [==============================] - 0s 31us/step - loss: 1.2351e-04 - mean_absolute_error: 0.0064\n",
      "\n",
      "Epoch 07117: loss did not improve\n",
      "Epoch 7118/10000\n",
      "2700/2700 [==============================] - 0s 29us/step - loss: 1.2466e-04 - mean_absolute_error: 0.0066\n",
      "\n",
      "Epoch 07118: loss did not improve\n",
      "Epoch 7119/10000\n",
      "2700/2700 [==============================] - 0s 30us/step - loss: 1.2374e-04 - mean_absolute_error: 0.0065\n",
      "\n",
      "Epoch 07119: loss did not improve\n",
      "Epoch 7120/10000\n",
      "2700/2700 [==============================] - 0s 28us/step - loss: 1.2296e-04 - mean_absolute_error: 0.0064\n",
      "\n",
      "Epoch 07120: loss did not improve\n",
      "Epoch 7121/10000\n",
      "2700/2700 [==============================] - 0s 28us/step - loss: 1.2507e-04 - mean_absolute_error: 0.0066\n",
      "\n",
      "Epoch 07121: loss did not improve\n",
      "Epoch 7122/10000\n",
      "2700/2700 [==============================] - 0s 27us/step - loss: 1.2251e-04 - mean_absolute_error: 0.0063\n",
      "\n",
      "Epoch 07122: loss did not improve\n",
      "Epoch 7123/10000\n",
      "2700/2700 [==============================] - 0s 28us/step - loss: 1.2310e-04 - mean_absolute_error: 0.0064\n",
      "\n",
      "Epoch 07123: loss did not improve\n",
      "Epoch 7124/10000\n",
      "2700/2700 [==============================] - 0s 30us/step - loss: 1.2628e-04 - mean_absolute_error: 0.0066\n",
      "\n",
      "Epoch 07124: loss did not improve\n",
      "Epoch 7125/10000\n",
      "2700/2700 [==============================] - 0s 31us/step - loss: 1.2433e-04 - mean_absolute_error: 0.0065\n",
      "\n",
      "Epoch 07125: loss did not improve\n",
      "Epoch 7126/10000\n",
      "2700/2700 [==============================] - 0s 29us/step - loss: 1.2688e-04 - mean_absolute_error: 0.0067\n",
      "\n",
      "Epoch 07126: loss did not improve\n",
      "Epoch 7127/10000\n",
      "2700/2700 [==============================] - 0s 26us/step - loss: 1.2387e-04 - mean_absolute_error: 0.0064\n",
      "\n",
      "Epoch 07127: loss did not improve\n",
      "Epoch 7128/10000\n",
      "2700/2700 [==============================] - 0s 28us/step - loss: 1.2229e-04 - mean_absolute_error: 0.0063\n",
      "\n",
      "Epoch 07128: loss did not improve\n",
      "Epoch 7129/10000\n",
      "2700/2700 [==============================] - 0s 27us/step - loss: 1.2279e-04 - mean_absolute_error: 0.0063\n",
      "\n",
      "Epoch 07129: loss did not improve\n",
      "Epoch 7130/10000\n",
      "2700/2700 [==============================] - 0s 26us/step - loss: 1.2271e-04 - mean_absolute_error: 0.0064\n",
      "\n",
      "Epoch 07130: loss did not improve\n",
      "Epoch 7131/10000\n",
      "2700/2700 [==============================] - 0s 28us/step - loss: 1.2455e-04 - mean_absolute_error: 0.0065\n",
      "\n",
      "Epoch 07131: loss did not improve\n",
      "Epoch 7132/10000\n",
      "2700/2700 [==============================] - 0s 28us/step - loss: 1.2328e-04 - mean_absolute_error: 0.0063\n",
      "\n",
      "Epoch 07132: loss did not improve\n",
      "Epoch 7133/10000\n",
      "2700/2700 [==============================] - 0s 28us/step - loss: 1.2410e-04 - mean_absolute_error: 0.0065\n",
      "\n",
      "Epoch 07133: loss did not improve\n",
      "Epoch 7134/10000\n",
      "2700/2700 [==============================] - 0s 30us/step - loss: 1.2655e-04 - mean_absolute_error: 0.0066\n",
      "\n",
      "Epoch 07134: loss did not improve\n",
      "Epoch 7135/10000\n",
      "2700/2700 [==============================] - 0s 36us/step - loss: 1.2277e-04 - mean_absolute_error: 0.0063\n",
      "\n",
      "Epoch 07135: loss did not improve\n",
      "Epoch 7136/10000\n",
      "2700/2700 [==============================] - 0s 37us/step - loss: 1.2984e-04 - mean_absolute_error: 0.0069\n",
      "\n",
      "Epoch 07136: loss did not improve\n",
      "Epoch 7137/10000\n",
      "2700/2700 [==============================] - 0s 42us/step - loss: 1.2328e-04 - mean_absolute_error: 0.0064\n",
      "\n",
      "Epoch 07137: loss did not improve\n",
      "Epoch 7138/10000\n",
      "2700/2700 [==============================] - 0s 36us/step - loss: 1.2551e-04 - mean_absolute_error: 0.0066\n",
      "\n",
      "Epoch 07138: loss did not improve\n",
      "Epoch 7139/10000\n",
      "2700/2700 [==============================] - 0s 27us/step - loss: 1.2776e-04 - mean_absolute_error: 0.0069\n",
      "\n",
      "Epoch 07139: loss did not improve\n",
      "Epoch 7140/10000\n",
      "2700/2700 [==============================] - 0s 30us/step - loss: 1.2390e-04 - mean_absolute_error: 0.0064\n",
      "\n",
      "Epoch 07140: loss did not improve\n",
      "Epoch 7141/10000\n",
      "2700/2700 [==============================] - 0s 37us/step - loss: 1.2421e-04 - mean_absolute_error: 0.0064\n",
      "\n",
      "Epoch 07141: loss did not improve\n",
      "Epoch 7142/10000\n",
      "2700/2700 [==============================] - 0s 41us/step - loss: 1.2453e-04 - mean_absolute_error: 0.0065\n",
      "\n",
      "Epoch 07142: loss did not improve\n",
      "Epoch 7143/10000\n",
      "2700/2700 [==============================] - 0s 36us/step - loss: 1.2451e-04 - mean_absolute_error: 0.0064\n",
      "\n",
      "Epoch 07143: loss did not improve\n",
      "Epoch 7144/10000\n",
      "2700/2700 [==============================] - 0s 35us/step - loss: 1.2391e-04 - mean_absolute_error: 0.0065\n",
      "\n",
      "Epoch 07144: loss did not improve\n",
      "Epoch 7145/10000\n",
      "2700/2700 [==============================] - 0s 29us/step - loss: 1.2514e-04 - mean_absolute_error: 0.0066\n",
      "\n",
      "Epoch 07145: loss did not improve\n",
      "Epoch 7146/10000\n",
      "2700/2700 [==============================] - 0s 29us/step - loss: 1.2339e-04 - mean_absolute_error: 0.0065\n",
      "\n",
      "Epoch 07146: loss did not improve\n",
      "Epoch 7147/10000\n",
      "2700/2700 [==============================] - 0s 29us/step - loss: 1.2571e-04 - mean_absolute_error: 0.0066\n",
      "\n",
      "Epoch 07147: loss did not improve\n",
      "Epoch 7148/10000\n",
      "2700/2700 [==============================] - 0s 27us/step - loss: 1.2522e-04 - mean_absolute_error: 0.0065\n",
      "\n",
      "Epoch 07148: loss did not improve\n",
      "Epoch 7149/10000\n",
      "2700/2700 [==============================] - 0s 29us/step - loss: 1.2301e-04 - mean_absolute_error: 0.0064\n",
      "\n",
      "Epoch 07149: loss did not improve\n",
      "Epoch 7150/10000\n",
      "2700/2700 [==============================] - 0s 30us/step - loss: 1.2260e-04 - mean_absolute_error: 0.0063\n",
      "\n",
      "Epoch 07150: loss did not improve\n",
      "Epoch 7151/10000\n",
      "2700/2700 [==============================] - 0s 29us/step - loss: 1.2801e-04 - mean_absolute_error: 0.0068\n",
      "\n",
      "Epoch 07151: loss did not improve\n",
      "Epoch 7152/10000\n",
      "2700/2700 [==============================] - 0s 28us/step - loss: 1.2479e-04 - mean_absolute_error: 0.0065\n",
      "\n",
      "Epoch 07152: loss did not improve\n",
      "Epoch 7153/10000\n",
      "2700/2700 [==============================] - 0s 29us/step - loss: 1.2327e-04 - mean_absolute_error: 0.0063\n",
      "\n",
      "Epoch 07153: loss did not improve\n",
      "Epoch 7154/10000\n",
      "2700/2700 [==============================] - 0s 31us/step - loss: 1.2648e-04 - mean_absolute_error: 0.0068\n",
      "\n",
      "Epoch 07154: loss did not improve\n",
      "Epoch 7155/10000\n",
      "2700/2700 [==============================] - 0s 32us/step - loss: 1.2714e-04 - mean_absolute_error: 0.0068\n",
      "\n",
      "Epoch 07155: loss did not improve\n",
      "Epoch 7156/10000\n",
      "2700/2700 [==============================] - 0s 38us/step - loss: 1.2511e-04 - mean_absolute_error: 0.0065\n",
      "\n",
      "Epoch 07156: loss did not improve\n",
      "Epoch 7157/10000\n",
      "2700/2700 [==============================] - 0s 36us/step - loss: 1.2539e-04 - mean_absolute_error: 0.0066\n",
      "\n",
      "Epoch 07157: loss did not improve\n",
      "Epoch 7158/10000\n",
      "2700/2700 [==============================] - 0s 34us/step - loss: 1.2271e-04 - mean_absolute_error: 0.0063\n",
      "\n",
      "Epoch 07158: loss did not improve\n",
      "Epoch 7159/10000\n",
      "2700/2700 [==============================] - 0s 37us/step - loss: 1.2254e-04 - mean_absolute_error: 0.0065\n",
      "\n",
      "Epoch 07159: loss did not improve\n",
      "Epoch 7160/10000\n",
      "2700/2700 [==============================] - 0s 36us/step - loss: 1.2600e-04 - mean_absolute_error: 0.0066\n",
      "\n",
      "Epoch 07160: loss did not improve\n",
      "Epoch 7161/10000\n",
      "2700/2700 [==============================] - 0s 31us/step - loss: 1.2355e-04 - mean_absolute_error: 0.0065\n",
      "\n",
      "Epoch 07161: loss did not improve\n",
      "Epoch 7162/10000\n",
      "2700/2700 [==============================] - 0s 25us/step - loss: 1.2387e-04 - mean_absolute_error: 0.0065\n",
      "\n",
      "Epoch 07162: loss did not improve\n",
      "Epoch 7163/10000\n",
      "2700/2700 [==============================] - 0s 28us/step - loss: 1.2419e-04 - mean_absolute_error: 0.0066\n",
      "\n",
      "Epoch 07163: loss did not improve\n",
      "Epoch 7164/10000\n",
      "2700/2700 [==============================] - 0s 31us/step - loss: 1.2379e-04 - mean_absolute_error: 0.0064\n",
      "\n",
      "Epoch 07164: loss did not improve\n",
      "Epoch 7165/10000\n",
      "2700/2700 [==============================] - 0s 28us/step - loss: 1.2292e-04 - mean_absolute_error: 0.0063\n",
      "\n",
      "Epoch 07165: loss did not improve\n",
      "Epoch 7166/10000\n",
      "2700/2700 [==============================] - 0s 27us/step - loss: 1.2812e-04 - mean_absolute_error: 0.0067\n",
      "\n",
      "Epoch 07166: loss did not improve\n",
      "Epoch 7167/10000\n",
      "2700/2700 [==============================] - 0s 27us/step - loss: 1.2364e-04 - mean_absolute_error: 0.0064\n",
      "\n",
      "Epoch 07167: loss did not improve\n",
      "Epoch 7168/10000\n",
      "2700/2700 [==============================] - 0s 27us/step - loss: 1.2271e-04 - mean_absolute_error: 0.0063\n",
      "\n",
      "Epoch 07168: loss did not improve\n",
      "Epoch 7169/10000\n",
      "2700/2700 [==============================] - 0s 28us/step - loss: 1.2331e-04 - mean_absolute_error: 0.0064\n",
      "\n",
      "Epoch 07169: loss did not improve\n",
      "Epoch 7170/10000\n",
      "2700/2700 [==============================] - 0s 29us/step - loss: 1.2723e-04 - mean_absolute_error: 0.0067\n",
      "\n",
      "Epoch 07170: loss did not improve\n",
      "Epoch 7171/10000\n",
      "2700/2700 [==============================] - 0s 35us/step - loss: 1.2448e-04 - mean_absolute_error: 0.0065\n",
      "\n",
      "Epoch 07171: loss did not improve\n",
      "Epoch 7172/10000\n",
      "2700/2700 [==============================] - 0s 34us/step - loss: 1.2206e-04 - mean_absolute_error: 0.0064\n",
      "\n",
      "Epoch 07172: loss improved from 0.00012 to 0.00012, saving model to keras/best_baseline_weights.hdf5\n",
      "Epoch 7173/10000\n",
      "2700/2700 [==============================] - 0s 36us/step - loss: 1.2515e-04 - mean_absolute_error: 0.0065\n",
      "\n",
      "Epoch 07173: loss did not improve\n",
      "Epoch 7174/10000\n",
      "2700/2700 [==============================] - 0s 30us/step - loss: 1.2417e-04 - mean_absolute_error: 0.0065\n",
      "\n",
      "Epoch 07174: loss did not improve\n",
      "Epoch 7175/10000\n",
      "2700/2700 [==============================] - 0s 29us/step - loss: 1.2475e-04 - mean_absolute_error: 0.0065\n",
      "\n",
      "Epoch 07175: loss did not improve\n",
      "Epoch 7176/10000\n",
      "2700/2700 [==============================] - 0s 28us/step - loss: 1.2292e-04 - mean_absolute_error: 0.0064\n",
      "\n",
      "Epoch 07176: loss did not improve\n",
      "Epoch 7177/10000\n",
      "2700/2700 [==============================] - 0s 26us/step - loss: 1.2511e-04 - mean_absolute_error: 0.0066\n",
      "\n",
      "Epoch 07177: loss did not improve\n",
      "Epoch 7178/10000\n",
      "2700/2700 [==============================] - 0s 28us/step - loss: 1.2666e-04 - mean_absolute_error: 0.0067\n",
      "\n",
      "Epoch 07178: loss did not improve\n",
      "Epoch 7179/10000\n",
      "2700/2700 [==============================] - 0s 32us/step - loss: 1.2683e-04 - mean_absolute_error: 0.0067\n",
      "\n",
      "Epoch 07179: loss did not improve\n",
      "Epoch 7180/10000\n",
      "2700/2700 [==============================] - 0s 28us/step - loss: 1.2301e-04 - mean_absolute_error: 0.0064\n",
      "\n",
      "Epoch 07180: loss did not improve\n",
      "Epoch 7181/10000\n",
      "2700/2700 [==============================] - 0s 27us/step - loss: 1.2857e-04 - mean_absolute_error: 0.0069\n",
      "\n",
      "Epoch 07181: loss did not improve\n",
      "Epoch 7182/10000\n",
      "2700/2700 [==============================] - 0s 28us/step - loss: 1.2450e-04 - mean_absolute_error: 0.0065\n",
      "\n",
      "Epoch 07182: loss did not improve\n",
      "Epoch 7183/10000\n",
      "2700/2700 [==============================] - 0s 31us/step - loss: 1.2239e-04 - mean_absolute_error: 0.0064\n",
      "\n",
      "Epoch 07183: loss did not improve\n",
      "Epoch 7184/10000\n",
      "2700/2700 [==============================] - 0s 31us/step - loss: 1.2267e-04 - mean_absolute_error: 0.0063\n",
      "\n",
      "Epoch 07184: loss did not improve\n",
      "Epoch 7185/10000\n",
      "2700/2700 [==============================] - 0s 31us/step - loss: 1.2263e-04 - mean_absolute_error: 0.0063\n",
      "\n",
      "Epoch 07185: loss did not improve\n",
      "Epoch 7186/10000\n",
      "2700/2700 [==============================] - 0s 28us/step - loss: 1.2427e-04 - mean_absolute_error: 0.0065\n",
      "\n",
      "Epoch 07186: loss did not improve\n",
      "Epoch 7187/10000\n",
      "2700/2700 [==============================] - 0s 27us/step - loss: 1.2535e-04 - mean_absolute_error: 0.0067\n",
      "\n",
      "Epoch 07187: loss did not improve\n",
      "Epoch 7188/10000\n",
      "2700/2700 [==============================] - 0s 29us/step - loss: 1.2607e-04 - mean_absolute_error: 0.0066\n",
      "\n",
      "Epoch 07188: loss did not improve\n",
      "Epoch 7189/10000\n",
      "2700/2700 [==============================] - 0s 29us/step - loss: 1.2301e-04 - mean_absolute_error: 0.0064\n",
      "\n",
      "Epoch 07189: loss did not improve\n",
      "Epoch 7190/10000\n",
      "2700/2700 [==============================] - 0s 29us/step - loss: 1.2289e-04 - mean_absolute_error: 0.0064\n",
      "\n",
      "Epoch 07190: loss did not improve\n",
      "Epoch 7191/10000\n",
      "2700/2700 [==============================] - 0s 27us/step - loss: 1.2378e-04 - mean_absolute_error: 0.0064\n",
      "\n",
      "Epoch 07191: loss did not improve\n",
      "Epoch 7192/10000\n",
      "2700/2700 [==============================] - 0s 29us/step - loss: 1.2343e-04 - mean_absolute_error: 0.0064\n",
      "\n",
      "Epoch 07192: loss did not improve\n",
      "Epoch 7193/10000\n",
      "2700/2700 [==============================] - 0s 29us/step - loss: 1.2447e-04 - mean_absolute_error: 0.0065\n",
      "\n",
      "Epoch 07193: loss did not improve\n",
      "Epoch 7194/10000\n",
      "2700/2700 [==============================] - 0s 28us/step - loss: 1.2641e-04 - mean_absolute_error: 0.0066\n",
      "\n",
      "Epoch 07194: loss did not improve\n",
      "Epoch 7195/10000\n",
      "2700/2700 [==============================] - 0s 37us/step - loss: 1.2363e-04 - mean_absolute_error: 0.0064\n",
      "\n",
      "Epoch 07195: loss did not improve\n",
      "Epoch 7196/10000\n",
      "2700/2700 [==============================] - 0s 31us/step - loss: 1.2393e-04 - mean_absolute_error: 0.0064\n",
      "\n",
      "Epoch 07196: loss did not improve\n",
      "Epoch 7197/10000\n",
      "2700/2700 [==============================] - 0s 28us/step - loss: 1.2290e-04 - mean_absolute_error: 0.0063\n",
      "\n",
      "Epoch 07197: loss did not improve\n",
      "Epoch 7198/10000\n",
      "2700/2700 [==============================] - 0s 29us/step - loss: 1.2403e-04 - mean_absolute_error: 0.0065\n",
      "\n",
      "Epoch 07198: loss did not improve\n",
      "Epoch 7199/10000\n",
      "2700/2700 [==============================] - 0s 30us/step - loss: 1.2408e-04 - mean_absolute_error: 0.0065\n",
      "\n",
      "Epoch 07199: loss did not improve\n",
      "Epoch 7200/10000\n",
      "2700/2700 [==============================] - 0s 28us/step - loss: 1.2671e-04 - mean_absolute_error: 0.0067\n",
      "\n",
      "Epoch 07200: loss did not improve\n",
      "Epoch 7201/10000\n",
      "2700/2700 [==============================] - 0s 31us/step - loss: 1.2484e-04 - mean_absolute_error: 0.0066\n",
      "\n",
      "Epoch 07201: loss did not improve\n",
      "Epoch 7202/10000\n",
      "2700/2700 [==============================] - 0s 28us/step - loss: 1.2236e-04 - mean_absolute_error: 0.0064\n",
      "\n",
      "Epoch 07202: loss did not improve\n",
      "Epoch 7203/10000\n",
      "2700/2700 [==============================] - 0s 28us/step - loss: 1.2799e-04 - mean_absolute_error: 0.0068\n",
      "\n",
      "Epoch 07203: loss did not improve\n",
      "Epoch 7204/10000\n",
      "2700/2700 [==============================] - 0s 30us/step - loss: 1.2429e-04 - mean_absolute_error: 0.0065\n",
      "\n",
      "Epoch 07204: loss did not improve\n",
      "Epoch 7205/10000\n",
      "2700/2700 [==============================] - 0s 30us/step - loss: 1.2566e-04 - mean_absolute_error: 0.0066\n",
      "\n",
      "Epoch 07205: loss did not improve\n",
      "Epoch 7206/10000\n",
      "2700/2700 [==============================] - 0s 28us/step - loss: 1.2488e-04 - mean_absolute_error: 0.0066\n",
      "\n",
      "Epoch 07206: loss did not improve\n",
      "Epoch 7207/10000\n",
      "2700/2700 [==============================] - 0s 30us/step - loss: 1.2456e-04 - mean_absolute_error: 0.0065\n",
      "\n",
      "Epoch 07207: loss did not improve\n",
      "Epoch 7208/10000\n",
      "2700/2700 [==============================] - 0s 28us/step - loss: 1.2446e-04 - mean_absolute_error: 0.0065\n",
      "\n",
      "Epoch 07208: loss did not improve\n",
      "Epoch 7209/10000\n",
      "2700/2700 [==============================] - 0s 31us/step - loss: 1.2589e-04 - mean_absolute_error: 0.0066\n",
      "\n",
      "Epoch 07209: loss did not improve\n",
      "Epoch 7210/10000\n",
      "2700/2700 [==============================] - 0s 30us/step - loss: 1.2371e-04 - mean_absolute_error: 0.0065\n",
      "\n",
      "Epoch 07210: loss did not improve\n",
      "Epoch 7211/10000\n",
      "2700/2700 [==============================] - 0s 28us/step - loss: 1.2437e-04 - mean_absolute_error: 0.0067\n",
      "\n",
      "Epoch 07211: loss did not improve\n",
      "Epoch 7212/10000\n",
      "2700/2700 [==============================] - 0s 31us/step - loss: 1.2375e-04 - mean_absolute_error: 0.0065\n",
      "\n",
      "Epoch 07212: loss did not improve\n",
      "Epoch 7213/10000\n",
      "2700/2700 [==============================] - 0s 30us/step - loss: 1.2304e-04 - mean_absolute_error: 0.0064\n",
      "\n",
      "Epoch 07213: loss did not improve\n",
      "Epoch 7214/10000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2700/2700 [==============================] - 0s 28us/step - loss: 1.2562e-04 - mean_absolute_error: 0.0066\n",
      "\n",
      "Epoch 07214: loss did not improve\n",
      "Epoch 7215/10000\n",
      "2700/2700 [==============================] - 0s 30us/step - loss: 1.2402e-04 - mean_absolute_error: 0.0064\n",
      "\n",
      "Epoch 07215: loss did not improve\n",
      "Epoch 7216/10000\n",
      "2700/2700 [==============================] - 0s 26us/step - loss: 1.2659e-04 - mean_absolute_error: 0.0066\n",
      "\n",
      "Epoch 07216: loss did not improve\n",
      "Epoch 7217/10000\n",
      "2700/2700 [==============================] - 0s 30us/step - loss: 1.2487e-04 - mean_absolute_error: 0.0066\n",
      "\n",
      "Epoch 07217: loss did not improve\n",
      "Epoch 7218/10000\n",
      "2700/2700 [==============================] - 0s 31us/step - loss: 1.2867e-04 - mean_absolute_error: 0.0067\n",
      "\n",
      "Epoch 07218: loss did not improve\n",
      "Epoch 7219/10000\n",
      "2700/2700 [==============================] - 0s 28us/step - loss: 1.2900e-04 - mean_absolute_error: 0.0068\n",
      "\n",
      "Epoch 07219: loss did not improve\n",
      "Epoch 7220/10000\n",
      "2700/2700 [==============================] - 0s 29us/step - loss: 1.2490e-04 - mean_absolute_error: 0.0065\n",
      "\n",
      "Epoch 07220: loss did not improve\n",
      "Epoch 7221/10000\n",
      "2700/2700 [==============================] - 0s 31us/step - loss: 1.2153e-04 - mean_absolute_error: 0.0063\n",
      "\n",
      "Epoch 07221: loss improved from 0.00012 to 0.00012, saving model to keras/best_baseline_weights.hdf5\n",
      "Epoch 7222/10000\n",
      "2700/2700 [==============================] - 0s 38us/step - loss: 1.2428e-04 - mean_absolute_error: 0.0064\n",
      "\n",
      "Epoch 07222: loss did not improve\n",
      "Epoch 7223/10000\n",
      "2700/2700 [==============================] - 0s 35us/step - loss: 1.2526e-04 - mean_absolute_error: 0.0066\n",
      "\n",
      "Epoch 07223: loss did not improve\n",
      "Epoch 7224/10000\n",
      "2700/2700 [==============================] - 0s 35us/step - loss: 1.2243e-04 - mean_absolute_error: 0.0064\n",
      "\n",
      "Epoch 07224: loss did not improve\n",
      "Epoch 7225/10000\n",
      "2700/2700 [==============================] - 0s 28us/step - loss: 1.2385e-04 - mean_absolute_error: 0.0066\n",
      "\n",
      "Epoch 07225: loss did not improve\n",
      "Epoch 7226/10000\n",
      "2700/2700 [==============================] - 0s 29us/step - loss: 1.2419e-04 - mean_absolute_error: 0.0065\n",
      "\n",
      "Epoch 07226: loss did not improve\n",
      "Epoch 7227/10000\n",
      "2700/2700 [==============================] - 0s 29us/step - loss: 1.2421e-04 - mean_absolute_error: 0.0066\n",
      "\n",
      "Epoch 07227: loss did not improve\n",
      "Epoch 7228/10000\n",
      "2700/2700 [==============================] - 0s 29us/step - loss: 1.2642e-04 - mean_absolute_error: 0.0066\n",
      "\n",
      "Epoch 07228: loss did not improve\n",
      "Epoch 7229/10000\n",
      "2700/2700 [==============================] - 0s 27us/step - loss: 1.2300e-04 - mean_absolute_error: 0.0064\n",
      "\n",
      "Epoch 07229: loss did not improve\n",
      "Epoch 7230/10000\n",
      "2700/2700 [==============================] - 0s 29us/step - loss: 1.2303e-04 - mean_absolute_error: 0.0064\n",
      "\n",
      "Epoch 07230: loss did not improve\n",
      "Epoch 7231/10000\n",
      "2700/2700 [==============================] - 0s 31us/step - loss: 1.2503e-04 - mean_absolute_error: 0.0065\n",
      "\n",
      "Epoch 07231: loss did not improve\n",
      "Epoch 7232/10000\n",
      "2700/2700 [==============================] - 0s 32us/step - loss: 1.2435e-04 - mean_absolute_error: 0.0065\n",
      "\n",
      "Epoch 07232: loss did not improve\n",
      "Epoch 7233/10000\n",
      "2700/2700 [==============================] - 0s 30us/step - loss: 1.2326e-04 - mean_absolute_error: 0.0064\n",
      "\n",
      "Epoch 07233: loss did not improve\n",
      "Epoch 7234/10000\n",
      "2700/2700 [==============================] - 0s 30us/step - loss: 1.2369e-04 - mean_absolute_error: 0.0064\n",
      "\n",
      "Epoch 07234: loss did not improve\n",
      "Epoch 7235/10000\n",
      "2700/2700 [==============================] - 0s 29us/step - loss: 1.2313e-04 - mean_absolute_error: 0.0064\n",
      "\n",
      "Epoch 07235: loss did not improve\n",
      "Epoch 7236/10000\n",
      "2700/2700 [==============================] - 0s 29us/step - loss: 1.2753e-04 - mean_absolute_error: 0.0068\n",
      "\n",
      "Epoch 07236: loss did not improve\n",
      "Epoch 7237/10000\n",
      "2700/2700 [==============================] - 0s 28us/step - loss: 1.2723e-04 - mean_absolute_error: 0.0068\n",
      "\n",
      "Epoch 07237: loss did not improve\n",
      "Epoch 7238/10000\n",
      "2700/2700 [==============================] - 0s 29us/step - loss: 1.2420e-04 - mean_absolute_error: 0.0065\n",
      "\n",
      "Epoch 07238: loss did not improve\n",
      "Epoch 7239/10000\n",
      "2700/2700 [==============================] - 0s 31us/step - loss: 1.2454e-04 - mean_absolute_error: 0.0067\n",
      "\n",
      "Epoch 07239: loss did not improve\n",
      "Epoch 7240/10000\n",
      "2700/2700 [==============================] - 0s 27us/step - loss: 1.2388e-04 - mean_absolute_error: 0.0065\n",
      "\n",
      "Epoch 07240: loss did not improve\n",
      "Epoch 7241/10000\n",
      "2700/2700 [==============================] - 0s 28us/step - loss: 1.2342e-04 - mean_absolute_error: 0.0064\n",
      "\n",
      "Epoch 07241: loss did not improve\n",
      "Epoch 7242/10000\n",
      "2700/2700 [==============================] - 0s 29us/step - loss: 1.2680e-04 - mean_absolute_error: 0.0066\n",
      "\n",
      "Epoch 07242: loss did not improve\n",
      "Epoch 7243/10000\n",
      "2700/2700 [==============================] - 0s 28us/step - loss: 1.2633e-04 - mean_absolute_error: 0.0067\n",
      "\n",
      "Epoch 07243: loss did not improve\n",
      "Epoch 7244/10000\n",
      "2700/2700 [==============================] - 0s 27us/step - loss: 1.2490e-04 - mean_absolute_error: 0.0065\n",
      "\n",
      "Epoch 07244: loss did not improve\n",
      "Epoch 7245/10000\n",
      "2700/2700 [==============================] - 0s 28us/step - loss: 1.2250e-04 - mean_absolute_error: 0.0064\n",
      "\n",
      "Epoch 07245: loss did not improve\n",
      "Epoch 7246/10000\n",
      "2700/2700 [==============================] - 0s 44us/step - loss: 1.2268e-04 - mean_absolute_error: 0.0064\n",
      "\n",
      "Epoch 07246: loss did not improve\n",
      "Epoch 7247/10000\n",
      "2700/2700 [==============================] - 0s 28us/step - loss: 1.2348e-04 - mean_absolute_error: 0.0065\n",
      "\n",
      "Epoch 07247: loss did not improve\n",
      "Epoch 7248/10000\n",
      "2700/2700 [==============================] - 0s 28us/step - loss: 1.2357e-04 - mean_absolute_error: 0.0065\n",
      "\n",
      "Epoch 07248: loss did not improve\n",
      "Epoch 7249/10000\n",
      "2700/2700 [==============================] - 0s 29us/step - loss: 1.2486e-04 - mean_absolute_error: 0.0065\n",
      "\n",
      "Epoch 07249: loss did not improve\n",
      "Epoch 7250/10000\n",
      "2700/2700 [==============================] - 0s 29us/step - loss: 1.2538e-04 - mean_absolute_error: 0.0067\n",
      "\n",
      "Epoch 07250: loss did not improve\n",
      "Epoch 7251/10000\n",
      "2700/2700 [==============================] - 0s 28us/step - loss: 1.2271e-04 - mean_absolute_error: 0.0064\n",
      "\n",
      "Epoch 07251: loss did not improve\n",
      "Epoch 7252/10000\n",
      "2700/2700 [==============================] - 0s 28us/step - loss: 1.2377e-04 - mean_absolute_error: 0.0065\n",
      "\n",
      "Epoch 07252: loss did not improve\n",
      "Epoch 7253/10000\n",
      "2700/2700 [==============================] - 0s 31us/step - loss: 1.2479e-04 - mean_absolute_error: 0.0066\n",
      "\n",
      "Epoch 07253: loss did not improve\n",
      "Epoch 7254/10000\n",
      "2700/2700 [==============================] - 0s 25us/step - loss: 1.2578e-04 - mean_absolute_error: 0.0066\n",
      "\n",
      "Epoch 07254: loss did not improve\n",
      "Epoch 7255/10000\n",
      "2700/2700 [==============================] - 0s 30us/step - loss: 1.2260e-04 - mean_absolute_error: 0.0063\n",
      "\n",
      "Epoch 07255: loss did not improve\n",
      "Epoch 7256/10000\n",
      "2700/2700 [==============================] - 0s 29us/step - loss: 1.2434e-04 - mean_absolute_error: 0.0065\n",
      "\n",
      "Epoch 07256: loss did not improve\n",
      "Epoch 7257/10000\n",
      "2700/2700 [==============================] - 0s 30us/step - loss: 1.2467e-04 - mean_absolute_error: 0.0065\n",
      "\n",
      "Epoch 07257: loss did not improve\n",
      "Epoch 7258/10000\n",
      "2700/2700 [==============================] - 0s 28us/step - loss: 1.2371e-04 - mean_absolute_error: 0.0065\n",
      "\n",
      "Epoch 07258: loss did not improve\n",
      "Epoch 7259/10000\n",
      "2700/2700 [==============================] - 0s 37us/step - loss: 1.2256e-04 - mean_absolute_error: 0.0063\n",
      "\n",
      "Epoch 07259: loss did not improve\n",
      "Epoch 7260/10000\n",
      "2700/2700 [==============================] - 0s 27us/step - loss: 1.2322e-04 - mean_absolute_error: 0.0064\n",
      "\n",
      "Epoch 07260: loss did not improve\n",
      "Epoch 7261/10000\n",
      "2700/2700 [==============================] - 0s 33us/step - loss: 1.2844e-04 - mean_absolute_error: 0.0069\n",
      "\n",
      "Epoch 07261: loss did not improve\n",
      "Epoch 7262/10000\n",
      "2700/2700 [==============================] - 0s 29us/step - loss: 1.2482e-04 - mean_absolute_error: 0.0065\n",
      "\n",
      "Epoch 07262: loss did not improve\n",
      "Epoch 7263/10000\n",
      "2700/2700 [==============================] - 0s 29us/step - loss: 1.2271e-04 - mean_absolute_error: 0.0064\n",
      "\n",
      "Epoch 07263: loss did not improve\n",
      "Epoch 7264/10000\n",
      "2700/2700 [==============================] - 0s 32us/step - loss: 1.2437e-04 - mean_absolute_error: 0.0064\n",
      "\n",
      "Epoch 07264: loss did not improve\n",
      "Epoch 7265/10000\n",
      "2700/2700 [==============================] - 0s 28us/step - loss: 1.2335e-04 - mean_absolute_error: 0.0064\n",
      "\n",
      "Epoch 07265: loss did not improve\n",
      "Epoch 7266/10000\n",
      "2700/2700 [==============================] - 0s 27us/step - loss: 1.2370e-04 - mean_absolute_error: 0.0065\n",
      "\n",
      "Epoch 07266: loss did not improve\n",
      "Epoch 7267/10000\n",
      "2700/2700 [==============================] - 0s 28us/step - loss: 1.2336e-04 - mean_absolute_error: 0.0065\n",
      "\n",
      "Epoch 07267: loss did not improve\n",
      "Epoch 7268/10000\n",
      "2700/2700 [==============================] - 0s 28us/step - loss: 1.2423e-04 - mean_absolute_error: 0.0065\n",
      "\n",
      "Epoch 07268: loss did not improve\n",
      "Epoch 7269/10000\n",
      "2700/2700 [==============================] - 0s 26us/step - loss: 1.2246e-04 - mean_absolute_error: 0.0063\n",
      "\n",
      "Epoch 07269: loss did not improve\n",
      "Epoch 7270/10000\n",
      "2700/2700 [==============================] - 0s 29us/step - loss: 1.2244e-04 - mean_absolute_error: 0.0063\n",
      "\n",
      "Epoch 07270: loss did not improve\n",
      "Epoch 7271/10000\n",
      "2700/2700 [==============================] - 0s 29us/step - loss: 1.2866e-04 - mean_absolute_error: 0.0069\n",
      "\n",
      "Epoch 07271: loss did not improve\n",
      "Epoch 7272/10000\n",
      "2700/2700 [==============================] - 0s 30us/step - loss: 1.2713e-04 - mean_absolute_error: 0.0068\n",
      "\n",
      "Epoch 07272: loss did not improve\n",
      "Epoch 7273/10000\n",
      "2700/2700 [==============================] - 0s 28us/step - loss: 1.2444e-04 - mean_absolute_error: 0.0066\n",
      "\n",
      "Epoch 07273: loss did not improve\n",
      "Epoch 7274/10000\n",
      "2700/2700 [==============================] - 0s 28us/step - loss: 1.2649e-04 - mean_absolute_error: 0.0068\n",
      "\n",
      "Epoch 07274: loss did not improve\n",
      "Epoch 7275/10000\n",
      "2700/2700 [==============================] - 0s 28us/step - loss: 1.2289e-04 - mean_absolute_error: 0.0064\n",
      "\n",
      "Epoch 07275: loss did not improve\n",
      "Epoch 7276/10000\n",
      "2700/2700 [==============================] - 0s 29us/step - loss: 1.2469e-04 - mean_absolute_error: 0.0065\n",
      "\n",
      "Epoch 07276: loss did not improve\n",
      "Epoch 7277/10000\n",
      "2700/2700 [==============================] - 0s 28us/step - loss: 1.2454e-04 - mean_absolute_error: 0.0064\n",
      "\n",
      "Epoch 07277: loss did not improve\n",
      "Epoch 7278/10000\n",
      "2700/2700 [==============================] - 0s 26us/step - loss: 1.2497e-04 - mean_absolute_error: 0.0066\n",
      "\n",
      "Epoch 07278: loss did not improve\n",
      "Epoch 7279/10000\n",
      "2700/2700 [==============================] - 0s 29us/step - loss: 1.2200e-04 - mean_absolute_error: 0.0063\n",
      "\n",
      "Epoch 07279: loss did not improve\n",
      "Epoch 7280/10000\n",
      "2700/2700 [==============================] - 0s 29us/step - loss: 1.2469e-04 - mean_absolute_error: 0.0065\n",
      "\n",
      "Epoch 07280: loss did not improve\n",
      "Epoch 7281/10000\n",
      "2700/2700 [==============================] - 0s 31us/step - loss: 1.2423e-04 - mean_absolute_error: 0.0066\n",
      "\n",
      "Epoch 07281: loss did not improve\n",
      "Epoch 7282/10000\n",
      "2700/2700 [==============================] - 0s 26us/step - loss: 1.2295e-04 - mean_absolute_error: 0.0065\n",
      "\n",
      "Epoch 07282: loss did not improve\n",
      "Epoch 7283/10000\n",
      "2700/2700 [==============================] - 0s 28us/step - loss: 1.2584e-04 - mean_absolute_error: 0.0066\n",
      "\n",
      "Epoch 07283: loss did not improve\n",
      "Epoch 7284/10000\n",
      "2700/2700 [==============================] - 0s 29us/step - loss: 1.2330e-04 - mean_absolute_error: 0.0063\n",
      "\n",
      "Epoch 07284: loss did not improve\n",
      "Epoch 7285/10000\n",
      "2700/2700 [==============================] - 0s 28us/step - loss: 1.2628e-04 - mean_absolute_error: 0.0066\n",
      "\n",
      "Epoch 07285: loss did not improve\n",
      "Epoch 7286/10000\n",
      "2700/2700 [==============================] - 0s 31us/step - loss: 1.2427e-04 - mean_absolute_error: 0.0065\n",
      "\n",
      "Epoch 07286: loss did not improve\n",
      "Epoch 7287/10000\n",
      "2700/2700 [==============================] - 0s 27us/step - loss: 1.2511e-04 - mean_absolute_error: 0.0066\n",
      "\n",
      "Epoch 07287: loss did not improve\n",
      "Epoch 7288/10000\n",
      "2700/2700 [==============================] - 0s 29us/step - loss: 1.2428e-04 - mean_absolute_error: 0.0066\n",
      "\n",
      "Epoch 07288: loss did not improve\n",
      "Epoch 7289/10000\n",
      "2700/2700 [==============================] - 0s 29us/step - loss: 1.2524e-04 - mean_absolute_error: 0.0067\n",
      "\n",
      "Epoch 07289: loss did not improve\n",
      "Epoch 7290/10000\n",
      "2700/2700 [==============================] - 0s 29us/step - loss: 1.2420e-04 - mean_absolute_error: 0.0065\n",
      "\n",
      "Epoch 07290: loss did not improve\n",
      "Epoch 7291/10000\n",
      "2700/2700 [==============================] - 0s 27us/step - loss: 1.2342e-04 - mean_absolute_error: 0.0065\n",
      "\n",
      "Epoch 07291: loss did not improve\n",
      "Epoch 7292/10000\n",
      "2700/2700 [==============================] - 0s 28us/step - loss: 1.2499e-04 - mean_absolute_error: 0.0065\n",
      "\n",
      "Epoch 07292: loss did not improve\n",
      "Epoch 7293/10000\n",
      "2700/2700 [==============================] - 0s 30us/step - loss: 1.2282e-04 - mean_absolute_error: 0.0064\n",
      "\n",
      "Epoch 07293: loss did not improve\n",
      "Epoch 7294/10000\n",
      "2700/2700 [==============================] - 0s 32us/step - loss: 1.2403e-04 - mean_absolute_error: 0.0064\n",
      "\n",
      "Epoch 07294: loss did not improve\n",
      "Epoch 7295/10000\n",
      "2700/2700 [==============================] - 0s 29us/step - loss: 1.2333e-04 - mean_absolute_error: 0.0064\n",
      "\n",
      "Epoch 07295: loss did not improve\n",
      "Epoch 7296/10000\n",
      "2700/2700 [==============================] - 0s 29us/step - loss: 1.2242e-04 - mean_absolute_error: 0.0063\n",
      "\n",
      "Epoch 07296: loss did not improve\n",
      "Epoch 7297/10000\n",
      "2700/2700 [==============================] - 0s 29us/step - loss: 1.2653e-04 - mean_absolute_error: 0.0067\n",
      "\n",
      "Epoch 07297: loss did not improve\n",
      "Epoch 7298/10000\n",
      "2700/2700 [==============================] - 0s 27us/step - loss: 1.2139e-04 - mean_absolute_error: 0.0063\n",
      "\n",
      "Epoch 07298: loss improved from 0.00012 to 0.00012, saving model to keras/best_baseline_weights.hdf5\n",
      "Epoch 7299/10000\n",
      "2700/2700 [==============================] - 0s 30us/step - loss: 1.2391e-04 - mean_absolute_error: 0.0065\n",
      "\n",
      "Epoch 07299: loss did not improve\n",
      "Epoch 7300/10000\n",
      "2700/2700 [==============================] - 0s 29us/step - loss: 1.2364e-04 - mean_absolute_error: 0.0065\n",
      "\n",
      "Epoch 07300: loss did not improve\n",
      "Epoch 7301/10000\n",
      "2700/2700 [==============================] - 0s 27us/step - loss: 1.2374e-04 - mean_absolute_error: 0.0065\n",
      "\n",
      "Epoch 07301: loss did not improve\n",
      "Epoch 7302/10000\n",
      "2700/2700 [==============================] - 0s 31us/step - loss: 1.2485e-04 - mean_absolute_error: 0.0066\n",
      "\n",
      "Epoch 07302: loss did not improve\n",
      "Epoch 7303/10000\n",
      "2700/2700 [==============================] - 0s 30us/step - loss: 1.2239e-04 - mean_absolute_error: 0.0063\n",
      "\n",
      "Epoch 07303: loss did not improve\n",
      "Epoch 7304/10000\n",
      "2700/2700 [==============================] - 0s 29us/step - loss: 1.2596e-04 - mean_absolute_error: 0.0066\n",
      "\n",
      "Epoch 07304: loss did not improve\n",
      "Epoch 7305/10000\n",
      "2700/2700 [==============================] - 0s 30us/step - loss: 1.2595e-04 - mean_absolute_error: 0.0067\n",
      "\n",
      "Epoch 07305: loss did not improve\n",
      "Epoch 7306/10000\n",
      "2700/2700 [==============================] - 0s 28us/step - loss: 1.2373e-04 - mean_absolute_error: 0.0065\n",
      "\n",
      "Epoch 07306: loss did not improve\n",
      "Epoch 7307/10000\n",
      "2700/2700 [==============================] - 0s 30us/step - loss: 1.2263e-04 - mean_absolute_error: 0.0063\n",
      "\n",
      "Epoch 07307: loss did not improve\n",
      "Epoch 7308/10000\n",
      "2700/2700 [==============================] - 0s 33us/step - loss: 1.2262e-04 - mean_absolute_error: 0.0064\n",
      "\n",
      "Epoch 07308: loss did not improve\n",
      "Epoch 7309/10000\n",
      "2700/2700 [==============================] - 0s 28us/step - loss: 1.2442e-04 - mean_absolute_error: 0.0064\n",
      "\n",
      "Epoch 07309: loss did not improve\n",
      "Epoch 7310/10000\n",
      "2700/2700 [==============================] - 0s 29us/step - loss: 1.2735e-04 - mean_absolute_error: 0.0067\n",
      "\n",
      "Epoch 07310: loss did not improve\n",
      "Epoch 7311/10000\n",
      "2700/2700 [==============================] - 0s 26us/step - loss: 1.2636e-04 - mean_absolute_error: 0.0067\n",
      "\n",
      "Epoch 07311: loss did not improve\n",
      "Epoch 7312/10000\n",
      "2700/2700 [==============================] - 0s 29us/step - loss: 1.2197e-04 - mean_absolute_error: 0.0062\n",
      "\n",
      "Epoch 07312: loss did not improve\n",
      "Epoch 7313/10000\n",
      "2700/2700 [==============================] - 0s 29us/step - loss: 1.2328e-04 - mean_absolute_error: 0.0064\n",
      "\n",
      "Epoch 07313: loss did not improve\n",
      "Epoch 7314/10000\n",
      "2700/2700 [==============================] - 0s 28us/step - loss: 1.2394e-04 - mean_absolute_error: 0.0064\n",
      "\n",
      "Epoch 07314: loss did not improve\n",
      "Epoch 7315/10000\n",
      "2700/2700 [==============================] - 0s 27us/step - loss: 1.2756e-04 - mean_absolute_error: 0.0068\n",
      "\n",
      "Epoch 07315: loss did not improve\n",
      "Epoch 7316/10000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2700/2700 [==============================] - 0s 30us/step - loss: 1.2324e-04 - mean_absolute_error: 0.0065\n",
      "\n",
      "Epoch 07316: loss did not improve\n",
      "Epoch 7317/10000\n",
      "2700/2700 [==============================] - 0s 27us/step - loss: 1.2340e-04 - mean_absolute_error: 0.0065\n",
      "\n",
      "Epoch 07317: loss did not improve\n",
      "Epoch 7318/10000\n",
      "2700/2700 [==============================] - 0s 28us/step - loss: 1.2367e-04 - mean_absolute_error: 0.0064\n",
      "\n",
      "Epoch 07318: loss did not improve\n",
      "Epoch 7319/10000\n",
      "2700/2700 [==============================] - 0s 27us/step - loss: 1.2318e-04 - mean_absolute_error: 0.0065\n",
      "\n",
      "Epoch 07319: loss did not improve\n",
      "Epoch 7320/10000\n",
      "2700/2700 [==============================] - 0s 30us/step - loss: 1.2242e-04 - mean_absolute_error: 0.0064\n",
      "\n",
      "Epoch 07320: loss did not improve\n",
      "Epoch 7321/10000\n",
      "2700/2700 [==============================] - 0s 27us/step - loss: 1.2478e-04 - mean_absolute_error: 0.0066\n",
      "\n",
      "Epoch 07321: loss did not improve\n",
      "Epoch 7322/10000\n",
      "2700/2700 [==============================] - 0s 30us/step - loss: 1.2147e-04 - mean_absolute_error: 0.0063\n",
      "\n",
      "Epoch 07322: loss did not improve\n",
      "Epoch 7323/10000\n",
      "2700/2700 [==============================] - 0s 30us/step - loss: 1.2317e-04 - mean_absolute_error: 0.0065\n",
      "\n",
      "Epoch 07323: loss did not improve\n",
      "Epoch 7324/10000\n",
      "2700/2700 [==============================] - 0s 27us/step - loss: 1.2279e-04 - mean_absolute_error: 0.0064\n",
      "\n",
      "Epoch 07324: loss did not improve\n",
      "Epoch 7325/10000\n",
      "2700/2700 [==============================] - 0s 30us/step - loss: 1.2429e-04 - mean_absolute_error: 0.0065\n",
      "\n",
      "Epoch 07325: loss did not improve\n",
      "Epoch 7326/10000\n",
      "2700/2700 [==============================] - 0s 27us/step - loss: 1.2338e-04 - mean_absolute_error: 0.0064\n",
      "\n",
      "Epoch 07326: loss did not improve\n",
      "Epoch 7327/10000\n",
      "2700/2700 [==============================] - 0s 28us/step - loss: 1.2546e-04 - mean_absolute_error: 0.0067\n",
      "\n",
      "Epoch 07327: loss did not improve\n",
      "Epoch 7328/10000\n",
      "2700/2700 [==============================] - 0s 26us/step - loss: 1.2331e-04 - mean_absolute_error: 0.0065\n",
      "\n",
      "Epoch 07328: loss did not improve\n",
      "Epoch 7329/10000\n",
      "2700/2700 [==============================] - 0s 28us/step - loss: 1.2312e-04 - mean_absolute_error: 0.0063\n",
      "\n",
      "Epoch 07329: loss did not improve\n",
      "Epoch 7330/10000\n",
      "2700/2700 [==============================] - 0s 27us/step - loss: 1.2227e-04 - mean_absolute_error: 0.0063\n",
      "\n",
      "Epoch 07330: loss did not improve\n",
      "Epoch 7331/10000\n",
      "2700/2700 [==============================] - 0s 28us/step - loss: 1.2384e-04 - mean_absolute_error: 0.0065\n",
      "\n",
      "Epoch 07331: loss did not improve\n",
      "Epoch 7332/10000\n",
      "2700/2700 [==============================] - 0s 26us/step - loss: 1.2705e-04 - mean_absolute_error: 0.0069\n",
      "\n",
      "Epoch 07332: loss did not improve\n",
      "Epoch 7333/10000\n",
      "2700/2700 [==============================] - 0s 31us/step - loss: 1.2409e-04 - mean_absolute_error: 0.0065\n",
      "\n",
      "Epoch 07333: loss did not improve\n",
      "Epoch 7334/10000\n",
      "2700/2700 [==============================] - 0s 28us/step - loss: 1.2223e-04 - mean_absolute_error: 0.0063\n",
      "\n",
      "Epoch 07334: loss did not improve\n",
      "Epoch 7335/10000\n",
      "2700/2700 [==============================] - 0s 27us/step - loss: 1.2296e-04 - mean_absolute_error: 0.0064\n",
      "\n",
      "Epoch 07335: loss did not improve\n",
      "Epoch 7336/10000\n",
      "2700/2700 [==============================] - 0s 26us/step - loss: 1.2269e-04 - mean_absolute_error: 0.0065\n",
      "\n",
      "Epoch 07336: loss did not improve\n",
      "Epoch 7337/10000\n",
      "2700/2700 [==============================] - 0s 29us/step - loss: 1.2383e-04 - mean_absolute_error: 0.0064\n",
      "\n",
      "Epoch 07337: loss did not improve\n",
      "Epoch 7338/10000\n",
      "2700/2700 [==============================] - 0s 29us/step - loss: 1.2251e-04 - mean_absolute_error: 0.0064\n",
      "\n",
      "Epoch 07338: loss did not improve\n",
      "Epoch 7339/10000\n",
      "2700/2700 [==============================] - 0s 28us/step - loss: 1.2445e-04 - mean_absolute_error: 0.0065\n",
      "\n",
      "Epoch 07339: loss did not improve\n",
      "Epoch 7340/10000\n",
      "2700/2700 [==============================] - 0s 26us/step - loss: 1.2575e-04 - mean_absolute_error: 0.0066\n",
      "\n",
      "Epoch 07340: loss did not improve\n",
      "Epoch 7341/10000\n",
      "2700/2700 [==============================] - 0s 29us/step - loss: 1.2481e-04 - mean_absolute_error: 0.0065\n",
      "\n",
      "Epoch 07341: loss did not improve\n",
      "Epoch 7342/10000\n",
      "2700/2700 [==============================] - 0s 28us/step - loss: 1.2381e-04 - mean_absolute_error: 0.0065\n",
      "\n",
      "Epoch 07342: loss did not improve\n",
      "Epoch 7343/10000\n",
      "2700/2700 [==============================] - 0s 31us/step - loss: 1.2296e-04 - mean_absolute_error: 0.0064\n",
      "\n",
      "Epoch 07343: loss did not improve\n",
      "Epoch 7344/10000\n",
      "2700/2700 [==============================] - 0s 31us/step - loss: 1.2299e-04 - mean_absolute_error: 0.0064\n",
      "\n",
      "Epoch 07344: loss did not improve\n",
      "Epoch 7345/10000\n",
      "2700/2700 [==============================] - 0s 28us/step - loss: 1.2351e-04 - mean_absolute_error: 0.0064\n",
      "\n",
      "Epoch 07345: loss did not improve\n",
      "Epoch 7346/10000\n",
      "2700/2700 [==============================] - 0s 28us/step - loss: 1.2493e-04 - mean_absolute_error: 0.0066\n",
      "\n",
      "Epoch 07346: loss did not improve\n",
      "Epoch 7347/10000\n",
      "2700/2700 [==============================] - 0s 30us/step - loss: 1.2370e-04 - mean_absolute_error: 0.0064\n",
      "\n",
      "Epoch 07347: loss did not improve\n",
      "Epoch 7348/10000\n",
      "2700/2700 [==============================] - 0s 28us/step - loss: 1.2327e-04 - mean_absolute_error: 0.0065\n",
      "\n",
      "Epoch 07348: loss did not improve\n",
      "Epoch 7349/10000\n",
      "2700/2700 [==============================] - 0s 27us/step - loss: 1.2502e-04 - mean_absolute_error: 0.0066\n",
      "\n",
      "Epoch 07349: loss did not improve\n",
      "Epoch 7350/10000\n",
      "2700/2700 [==============================] - 0s 29us/step - loss: 1.2309e-04 - mean_absolute_error: 0.0064\n",
      "\n",
      "Epoch 07350: loss did not improve\n",
      "Epoch 7351/10000\n",
      "2700/2700 [==============================] - 0s 30us/step - loss: 1.2553e-04 - mean_absolute_error: 0.0066\n",
      "\n",
      "Epoch 07351: loss did not improve\n",
      "Epoch 7352/10000\n",
      "2700/2700 [==============================] - 0s 28us/step - loss: 1.2610e-04 - mean_absolute_error: 0.0066\n",
      "\n",
      "Epoch 07352: loss did not improve\n",
      "Epoch 7353/10000\n",
      "2700/2700 [==============================] - 0s 30us/step - loss: 1.2363e-04 - mean_absolute_error: 0.0064\n",
      "\n",
      "Epoch 07353: loss did not improve\n",
      "Epoch 7354/10000\n",
      "2700/2700 [==============================] - 0s 27us/step - loss: 1.2172e-04 - mean_absolute_error: 0.0063\n",
      "\n",
      "Epoch 07354: loss did not improve\n",
      "Epoch 7355/10000\n",
      "2700/2700 [==============================] - 0s 29us/step - loss: 1.2267e-04 - mean_absolute_error: 0.0064\n",
      "\n",
      "Epoch 07355: loss did not improve\n",
      "Epoch 7356/10000\n",
      "2700/2700 [==============================] - 0s 28us/step - loss: 1.2425e-04 - mean_absolute_error: 0.0066\n",
      "\n",
      "Epoch 07356: loss did not improve\n",
      "Epoch 7357/10000\n",
      "2700/2700 [==============================] - 0s 29us/step - loss: 1.2346e-04 - mean_absolute_error: 0.0064\n",
      "\n",
      "Epoch 07357: loss did not improve\n",
      "Epoch 7358/10000\n",
      "2700/2700 [==============================] - 0s 26us/step - loss: 1.2294e-04 - mean_absolute_error: 0.0064\n",
      "\n",
      "Epoch 07358: loss did not improve\n",
      "Epoch 7359/10000\n",
      "2700/2700 [==============================] - 0s 29us/step - loss: 1.2413e-04 - mean_absolute_error: 0.0066\n",
      "\n",
      "Epoch 07359: loss did not improve\n",
      "Epoch 7360/10000\n",
      "2700/2700 [==============================] - 0s 30us/step - loss: 1.2380e-04 - mean_absolute_error: 0.0064\n",
      "\n",
      "Epoch 07360: loss did not improve\n",
      "Epoch 7361/10000\n",
      "2700/2700 [==============================] - 0s 28us/step - loss: 1.2275e-04 - mean_absolute_error: 0.0065\n",
      "\n",
      "Epoch 07361: loss did not improve\n",
      "Epoch 7362/10000\n",
      "2700/2700 [==============================] - 0s 27us/step - loss: 1.2361e-04 - mean_absolute_error: 0.0064\n",
      "\n",
      "Epoch 07362: loss did not improve\n",
      "Epoch 7363/10000\n",
      "2700/2700 [==============================] - 0s 29us/step - loss: 1.2359e-04 - mean_absolute_error: 0.0065\n",
      "\n",
      "Epoch 07363: loss did not improve\n",
      "Epoch 7364/10000\n",
      "2700/2700 [==============================] - 0s 29us/step - loss: 1.2283e-04 - mean_absolute_error: 0.0065\n",
      "\n",
      "Epoch 07364: loss did not improve\n",
      "Epoch 7365/10000\n",
      "2700/2700 [==============================] - 0s 28us/step - loss: 1.2239e-04 - mean_absolute_error: 0.0065\n",
      "\n",
      "Epoch 07365: loss did not improve\n",
      "Epoch 7366/10000\n",
      "2700/2700 [==============================] - 0s 32us/step - loss: 1.2172e-04 - mean_absolute_error: 0.0063\n",
      "\n",
      "Epoch 07366: loss did not improve\n",
      "Epoch 7367/10000\n",
      "2700/2700 [==============================] - 0s 27us/step - loss: 1.2286e-04 - mean_absolute_error: 0.0064\n",
      "\n",
      "Epoch 07367: loss did not improve\n",
      "Epoch 7368/10000\n",
      "2700/2700 [==============================] - 0s 29us/step - loss: 1.2329e-04 - mean_absolute_error: 0.0064\n",
      "\n",
      "Epoch 07368: loss did not improve\n",
      "Epoch 7369/10000\n",
      "2700/2700 [==============================] - 0s 28us/step - loss: 1.2502e-04 - mean_absolute_error: 0.0066\n",
      "\n",
      "Epoch 07369: loss did not improve\n",
      "Epoch 7370/10000\n",
      "2700/2700 [==============================] - 0s 28us/step - loss: 1.2530e-04 - mean_absolute_error: 0.0065\n",
      "\n",
      "Epoch 07370: loss did not improve\n",
      "Epoch 7371/10000\n",
      "2700/2700 [==============================] - 0s 26us/step - loss: 1.2326e-04 - mean_absolute_error: 0.0063\n",
      "\n",
      "Epoch 07371: loss did not improve\n",
      "Epoch 7372/10000\n",
      "2700/2700 [==============================] - 0s 29us/step - loss: 1.2355e-04 - mean_absolute_error: 0.0064: 0s - loss: 1.1218e-04 - mean_absolute_error: 0.006\n",
      "\n",
      "Epoch 07372: loss did not improve\n",
      "Epoch 7373/10000\n",
      "2700/2700 [==============================] - 0s 28us/step - loss: 1.2532e-04 - mean_absolute_error: 0.0066\n",
      "\n",
      "Epoch 07373: loss did not improve\n",
      "Epoch 7374/10000\n",
      "2700/2700 [==============================] - 0s 29us/step - loss: 1.2767e-04 - mean_absolute_error: 0.0068\n",
      "\n",
      "Epoch 07374: loss did not improve\n",
      "Epoch 7375/10000\n",
      "2700/2700 [==============================] - 0s 25us/step - loss: 1.2447e-04 - mean_absolute_error: 0.0065\n",
      "\n",
      "Epoch 07375: loss did not improve\n",
      "Epoch 7376/10000\n",
      "2700/2700 [==============================] - 0s 31us/step - loss: 1.2447e-04 - mean_absolute_error: 0.0065\n",
      "\n",
      "Epoch 07376: loss did not improve\n",
      "Epoch 7377/10000\n",
      "2700/2700 [==============================] - 0s 28us/step - loss: 1.2574e-04 - mean_absolute_error: 0.0066\n",
      "\n",
      "Epoch 07377: loss did not improve\n",
      "Epoch 7378/10000\n",
      "2700/2700 [==============================] - 0s 28us/step - loss: 1.2360e-04 - mean_absolute_error: 0.0064\n",
      "\n",
      "Epoch 07378: loss did not improve\n",
      "Epoch 7379/10000\n",
      "2700/2700 [==============================] - 0s 27us/step - loss: 1.2173e-04 - mean_absolute_error: 0.0062\n",
      "\n",
      "Epoch 07379: loss did not improve\n",
      "Epoch 7380/10000\n",
      "2700/2700 [==============================] - 0s 31us/step - loss: 1.2162e-04 - mean_absolute_error: 0.0063\n",
      "\n",
      "Epoch 07380: loss did not improve\n",
      "Epoch 7381/10000\n",
      "2700/2700 [==============================] - 0s 28us/step - loss: 1.2731e-04 - mean_absolute_error: 0.0067\n",
      "\n",
      "Epoch 07381: loss did not improve\n",
      "Epoch 7382/10000\n",
      "2700/2700 [==============================] - 0s 28us/step - loss: 1.2651e-04 - mean_absolute_error: 0.0068\n",
      "\n",
      "Epoch 07382: loss did not improve\n",
      "Epoch 7383/10000\n",
      "2700/2700 [==============================] - 0s 29us/step - loss: 1.2444e-04 - mean_absolute_error: 0.0065\n",
      "\n",
      "Epoch 07383: loss did not improve\n",
      "Epoch 7384/10000\n",
      "2700/2700 [==============================] - 0s 26us/step - loss: 1.2316e-04 - mean_absolute_error: 0.0063\n",
      "\n",
      "Epoch 07384: loss did not improve\n",
      "Epoch 7385/10000\n",
      "2700/2700 [==============================] - 0s 30us/step - loss: 1.3111e-04 - mean_absolute_error: 0.0070\n",
      "\n",
      "Epoch 07385: loss did not improve\n",
      "Epoch 7386/10000\n",
      "2700/2700 [==============================] - 0s 28us/step - loss: 1.2300e-04 - mean_absolute_error: 0.0064\n",
      "\n",
      "Epoch 07386: loss did not improve\n",
      "Epoch 7387/10000\n",
      "2700/2700 [==============================] - 0s 29us/step - loss: 1.2831e-04 - mean_absolute_error: 0.0070\n",
      "\n",
      "Epoch 07387: loss did not improve\n",
      "Epoch 7388/10000\n",
      "2700/2700 [==============================] - 0s 26us/step - loss: 1.2452e-04 - mean_absolute_error: 0.0065\n",
      "\n",
      "Epoch 07388: loss did not improve\n",
      "Epoch 7389/10000\n",
      "2700/2700 [==============================] - 0s 29us/step - loss: 1.2406e-04 - mean_absolute_error: 0.0065\n",
      "\n",
      "Epoch 07389: loss did not improve\n",
      "Epoch 7390/10000\n",
      "2700/2700 [==============================] - 0s 28us/step - loss: 1.2182e-04 - mean_absolute_error: 0.0063\n",
      "\n",
      "Epoch 07390: loss did not improve\n",
      "Epoch 7391/10000\n",
      "2700/2700 [==============================] - 0s 29us/step - loss: 1.2175e-04 - mean_absolute_error: 0.0063\n",
      "\n",
      "Epoch 07391: loss did not improve\n",
      "Epoch 7392/10000\n",
      "2700/2700 [==============================] - 0s 26us/step - loss: 1.2263e-04 - mean_absolute_error: 0.0063\n",
      "\n",
      "Epoch 07392: loss did not improve\n",
      "Epoch 7393/10000\n",
      "2700/2700 [==============================] - 0s 29us/step - loss: 1.2349e-04 - mean_absolute_error: 0.0065\n",
      "\n",
      "Epoch 07393: loss did not improve\n",
      "Epoch 7394/10000\n",
      "2700/2700 [==============================] - 0s 28us/step - loss: 1.2350e-04 - mean_absolute_error: 0.0065\n",
      "\n",
      "Epoch 07394: loss did not improve\n",
      "Epoch 7395/10000\n",
      "2700/2700 [==============================] - 0s 32us/step - loss: 1.2311e-04 - mean_absolute_error: 0.0064\n",
      "\n",
      "Epoch 07395: loss did not improve\n",
      "Epoch 7396/10000\n",
      "2700/2700 [==============================] - 0s 30us/step - loss: 1.2201e-04 - mean_absolute_error: 0.0063\n",
      "\n",
      "Epoch 07396: loss did not improve\n",
      "Epoch 7397/10000\n",
      "2700/2700 [==============================] - 0s 26us/step - loss: 1.2510e-04 - mean_absolute_error: 0.0065\n",
      "\n",
      "Epoch 07397: loss did not improve\n",
      "Epoch 7398/10000\n",
      "2700/2700 [==============================] - 0s 28us/step - loss: 1.2452e-04 - mean_absolute_error: 0.0066\n",
      "\n",
      "Epoch 07398: loss did not improve\n",
      "Epoch 7399/10000\n",
      "2700/2700 [==============================] - 0s 28us/step - loss: 1.2407e-04 - mean_absolute_error: 0.0066\n",
      "\n",
      "Epoch 07399: loss did not improve\n",
      "Epoch 7400/10000\n",
      "2700/2700 [==============================] - 0s 28us/step - loss: 1.2371e-04 - mean_absolute_error: 0.0066\n",
      "\n",
      "Epoch 07400: loss did not improve\n",
      "Epoch 7401/10000\n",
      "2700/2700 [==============================] - 0s 26us/step - loss: 1.2247e-04 - mean_absolute_error: 0.0063\n",
      "\n",
      "Epoch 07401: loss did not improve\n",
      "Epoch 7402/10000\n",
      "2700/2700 [==============================] - 0s 30us/step - loss: 1.2248e-04 - mean_absolute_error: 0.0063\n",
      "\n",
      "Epoch 07402: loss did not improve\n",
      "Epoch 7403/10000\n",
      "2700/2700 [==============================] - 0s 28us/step - loss: 1.2493e-04 - mean_absolute_error: 0.0066\n",
      "\n",
      "Epoch 07403: loss did not improve\n",
      "Epoch 7404/10000\n",
      "2700/2700 [==============================] - 0s 28us/step - loss: 1.2548e-04 - mean_absolute_error: 0.0066\n",
      "\n",
      "Epoch 07404: loss did not improve\n",
      "Epoch 7405/10000\n",
      "2700/2700 [==============================] - 0s 27us/step - loss: 1.2320e-04 - mean_absolute_error: 0.0064\n",
      "\n",
      "Epoch 07405: loss did not improve\n",
      "Epoch 7406/10000\n",
      "2700/2700 [==============================] - 0s 29us/step - loss: 1.2466e-04 - mean_absolute_error: 0.0065\n",
      "\n",
      "Epoch 07406: loss did not improve\n",
      "Epoch 7407/10000\n",
      "2700/2700 [==============================] - 0s 29us/step - loss: 1.2259e-04 - mean_absolute_error: 0.0063\n",
      "\n",
      "Epoch 07407: loss did not improve\n",
      "Epoch 7408/10000\n",
      "2700/2700 [==============================] - 0s 27us/step - loss: 1.2362e-04 - mean_absolute_error: 0.0064\n",
      "\n",
      "Epoch 07408: loss did not improve\n",
      "Epoch 7409/10000\n",
      "2700/2700 [==============================] - 0s 27us/step - loss: 1.2338e-04 - mean_absolute_error: 0.0064\n",
      "\n",
      "Epoch 07409: loss did not improve\n",
      "Epoch 7410/10000\n",
      "2700/2700 [==============================] - 0s 30us/step - loss: 1.2367e-04 - mean_absolute_error: 0.0065\n",
      "\n",
      "Epoch 07410: loss did not improve\n",
      "Epoch 7411/10000\n",
      "2700/2700 [==============================] - 0s 28us/step - loss: 1.2192e-04 - mean_absolute_error: 0.0063\n",
      "\n",
      "Epoch 07411: loss did not improve\n",
      "Epoch 7412/10000\n",
      "2700/2700 [==============================] - 0s 30us/step - loss: 1.2340e-04 - mean_absolute_error: 0.0064\n",
      "\n",
      "Epoch 07412: loss did not improve\n",
      "Epoch 7413/10000\n",
      "2700/2700 [==============================] - 0s 29us/step - loss: 1.2681e-04 - mean_absolute_error: 0.0067\n",
      "\n",
      "Epoch 07413: loss did not improve\n",
      "Epoch 7414/10000\n",
      "2700/2700 [==============================] - 0s 28us/step - loss: 1.2351e-04 - mean_absolute_error: 0.0065\n",
      "\n",
      "Epoch 07414: loss did not improve\n",
      "Epoch 7415/10000\n",
      "2700/2700 [==============================] - 0s 30us/step - loss: 1.2417e-04 - mean_absolute_error: 0.0066\n",
      "\n",
      "Epoch 07415: loss did not improve\n",
      "Epoch 7416/10000\n",
      "2700/2700 [==============================] - 0s 29us/step - loss: 1.2354e-04 - mean_absolute_error: 0.0064\n",
      "\n",
      "Epoch 07416: loss did not improve\n",
      "Epoch 7417/10000\n",
      "2700/2700 [==============================] - 0s 31us/step - loss: 1.2267e-04 - mean_absolute_error: 0.0064\n",
      "\n",
      "Epoch 07417: loss did not improve\n",
      "Epoch 7418/10000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2700/2700 [==============================] - 0s 30us/step - loss: 1.2406e-04 - mean_absolute_error: 0.0065\n",
      "\n",
      "Epoch 07418: loss did not improve\n",
      "Epoch 7419/10000\n",
      "2700/2700 [==============================] - 0s 29us/step - loss: 1.2209e-04 - mean_absolute_error: 0.0064\n",
      "\n",
      "Epoch 07419: loss did not improve\n",
      "Epoch 7420/10000\n",
      "2700/2700 [==============================] - 0s 29us/step - loss: 1.2176e-04 - mean_absolute_error: 0.0063\n",
      "\n",
      "Epoch 07420: loss did not improve\n",
      "Epoch 7421/10000\n",
      "2700/2700 [==============================] - 0s 27us/step - loss: 1.2272e-04 - mean_absolute_error: 0.0064\n",
      "\n",
      "Epoch 07421: loss did not improve\n",
      "Epoch 7422/10000\n",
      "2700/2700 [==============================] - 0s 28us/step - loss: 1.2323e-04 - mean_absolute_error: 0.0065\n",
      "\n",
      "Epoch 07422: loss did not improve\n",
      "Epoch 7423/10000\n",
      "2700/2700 [==============================] - 0s 27us/step - loss: 1.2577e-04 - mean_absolute_error: 0.0066\n",
      "\n",
      "Epoch 07423: loss did not improve\n",
      "Epoch 7424/10000\n",
      "2700/2700 [==============================] - 0s 29us/step - loss: 1.2183e-04 - mean_absolute_error: 0.0063\n",
      "\n",
      "Epoch 07424: loss did not improve\n",
      "Epoch 7425/10000\n",
      "2700/2700 [==============================] - 0s 27us/step - loss: 1.2362e-04 - mean_absolute_error: 0.0065\n",
      "\n",
      "Epoch 07425: loss did not improve\n",
      "Epoch 7426/10000\n",
      "2700/2700 [==============================] - 0s 29us/step - loss: 1.2429e-04 - mean_absolute_error: 0.0065\n",
      "\n",
      "Epoch 07426: loss did not improve\n",
      "Epoch 7427/10000\n",
      "2700/2700 [==============================] - 0s 25us/step - loss: 1.2337e-04 - mean_absolute_error: 0.0064\n",
      "\n",
      "Epoch 07427: loss did not improve\n",
      "Epoch 7428/10000\n",
      "2700/2700 [==============================] - 0s 31us/step - loss: 1.2466e-04 - mean_absolute_error: 0.0065\n",
      "\n",
      "Epoch 07428: loss did not improve\n",
      "Epoch 7429/10000\n",
      "2700/2700 [==============================] - 0s 29us/step - loss: 1.2282e-04 - mean_absolute_error: 0.0064\n",
      "\n",
      "Epoch 07429: loss did not improve\n",
      "Epoch 7430/10000\n",
      "2700/2700 [==============================] - 0s 28us/step - loss: 1.2385e-04 - mean_absolute_error: 0.0065\n",
      "\n",
      "Epoch 07430: loss did not improve\n",
      "Epoch 7431/10000\n",
      "2700/2700 [==============================] - 0s 26us/step - loss: 1.2297e-04 - mean_absolute_error: 0.0065\n",
      "\n",
      "Epoch 07431: loss did not improve\n",
      "Epoch 7432/10000\n",
      "2700/2700 [==============================] - 0s 30us/step - loss: 1.2677e-04 - mean_absolute_error: 0.0067\n",
      "\n",
      "Epoch 07432: loss did not improve\n",
      "Epoch 7433/10000\n",
      "2700/2700 [==============================] - 0s 28us/step - loss: 1.2203e-04 - mean_absolute_error: 0.0063\n",
      "\n",
      "Epoch 07433: loss did not improve\n",
      "Epoch 7434/10000\n",
      "2700/2700 [==============================] - 0s 28us/step - loss: 1.2307e-04 - mean_absolute_error: 0.0064\n",
      "\n",
      "Epoch 07434: loss did not improve\n",
      "Epoch 7435/10000\n",
      "2700/2700 [==============================] - 0s 31us/step - loss: 1.2378e-04 - mean_absolute_error: 0.0066\n",
      "\n",
      "Epoch 07435: loss did not improve\n",
      "Epoch 7436/10000\n",
      "2700/2700 [==============================] - 0s 29us/step - loss: 1.2297e-04 - mean_absolute_error: 0.0064\n",
      "\n",
      "Epoch 07436: loss did not improve\n",
      "Epoch 7437/10000\n",
      "2700/2700 [==============================] - 0s 31us/step - loss: 1.2427e-04 - mean_absolute_error: 0.0067\n",
      "\n",
      "Epoch 07437: loss did not improve\n",
      "Epoch 7438/10000\n",
      "2700/2700 [==============================] - 0s 30us/step - loss: 1.2434e-04 - mean_absolute_error: 0.0066\n",
      "\n",
      "Epoch 07438: loss did not improve\n",
      "Epoch 7439/10000\n",
      "2700/2700 [==============================] - 0s 28us/step - loss: 1.2460e-04 - mean_absolute_error: 0.0066\n",
      "\n",
      "Epoch 07439: loss did not improve\n",
      "Epoch 7440/10000\n",
      "2700/2700 [==============================] - 0s 26us/step - loss: 1.2366e-04 - mean_absolute_error: 0.0064\n",
      "\n",
      "Epoch 07440: loss did not improve\n",
      "Epoch 7441/10000\n",
      "2700/2700 [==============================] - 0s 29us/step - loss: 1.2622e-04 - mean_absolute_error: 0.0066\n",
      "\n",
      "Epoch 07441: loss did not improve\n",
      "Epoch 7442/10000\n",
      "2700/2700 [==============================] - 0s 28us/step - loss: 1.2490e-04 - mean_absolute_error: 0.0066\n",
      "\n",
      "Epoch 07442: loss did not improve\n",
      "Epoch 7443/10000\n",
      "2700/2700 [==============================] - 0s 28us/step - loss: 1.2424e-04 - mean_absolute_error: 0.0067\n",
      "\n",
      "Epoch 07443: loss did not improve\n",
      "Epoch 7444/10000\n",
      "2700/2700 [==============================] - 0s 30us/step - loss: 1.2774e-04 - mean_absolute_error: 0.0068\n",
      "\n",
      "Epoch 07444: loss did not improve\n",
      "Epoch 7445/10000\n",
      "2700/2700 [==============================] - 0s 28us/step - loss: 1.2519e-04 - mean_absolute_error: 0.0066\n",
      "\n",
      "Epoch 07445: loss did not improve\n",
      "Epoch 7446/10000\n",
      "2700/2700 [==============================] - 0s 32us/step - loss: 1.2268e-04 - mean_absolute_error: 0.0065\n",
      "\n",
      "Epoch 07446: loss did not improve\n",
      "Epoch 7447/10000\n",
      "2700/2700 [==============================] - 0s 35us/step - loss: 1.2411e-04 - mean_absolute_error: 0.0067\n",
      "\n",
      "Epoch 07447: loss did not improve\n",
      "Epoch 7448/10000\n",
      "2700/2700 [==============================] - 0s 36us/step - loss: 1.2248e-04 - mean_absolute_error: 0.0064\n",
      "\n",
      "Epoch 07448: loss did not improve\n",
      "Epoch 7449/10000\n",
      "2700/2700 [==============================] - 0s 37us/step - loss: 1.2555e-04 - mean_absolute_error: 0.0065\n",
      "\n",
      "Epoch 07449: loss did not improve\n",
      "Epoch 7450/10000\n",
      "2700/2700 [==============================] - 0s 36us/step - loss: 1.2719e-04 - mean_absolute_error: 0.0068: 0s - loss: 1.2608e-04 - mean_absolute_error: 0.006\n",
      "\n",
      "Epoch 07450: loss did not improve\n",
      "Epoch 7451/10000\n",
      "2700/2700 [==============================] - 0s 29us/step - loss: 1.2466e-04 - mean_absolute_error: 0.0066\n",
      "\n",
      "Epoch 07451: loss did not improve\n",
      "Epoch 7452/10000\n",
      "2700/2700 [==============================] - ETA: 0s - loss: 1.3084e-04 - mean_absolute_error: 0.006 - 0s 28us/step - loss: 1.2335e-04 - mean_absolute_error: 0.0064\n",
      "\n",
      "Epoch 07452: loss did not improve\n",
      "Epoch 7453/10000\n",
      "2700/2700 [==============================] - 0s 30us/step - loss: 1.2260e-04 - mean_absolute_error: 0.0065\n",
      "\n",
      "Epoch 07453: loss did not improve\n",
      "Epoch 7454/10000\n",
      "2700/2700 [==============================] - 0s 29us/step - loss: 1.2316e-04 - mean_absolute_error: 0.0065\n",
      "\n",
      "Epoch 07454: loss did not improve\n",
      "Epoch 7455/10000\n",
      "2700/2700 [==============================] - 0s 31us/step - loss: 1.2445e-04 - mean_absolute_error: 0.0066\n",
      "\n",
      "Epoch 07455: loss did not improve\n",
      "Epoch 7456/10000\n",
      "2700/2700 [==============================] - 0s 29us/step - loss: 1.2383e-04 - mean_absolute_error: 0.0065\n",
      "\n",
      "Epoch 07456: loss did not improve\n",
      "Epoch 7457/10000\n",
      "2700/2700 [==============================] - 0s 28us/step - loss: 1.2305e-04 - mean_absolute_error: 0.0064\n",
      "\n",
      "Epoch 07457: loss did not improve\n",
      "Epoch 7458/10000\n",
      "2700/2700 [==============================] - 0s 28us/step - loss: 1.2548e-04 - mean_absolute_error: 0.0066\n",
      "\n",
      "Epoch 07458: loss did not improve\n",
      "Epoch 7459/10000\n",
      "2700/2700 [==============================] - 0s 29us/step - loss: 1.2380e-04 - mean_absolute_error: 0.0063\n",
      "\n",
      "Epoch 07459: loss did not improve\n",
      "Epoch 7460/10000\n",
      "2700/2700 [==============================] - 0s 29us/step - loss: 1.2351e-04 - mean_absolute_error: 0.0065\n",
      "\n",
      "Epoch 07460: loss did not improve\n",
      "Epoch 7461/10000\n",
      "2700/2700 [==============================] - 0s 29us/step - loss: 1.2348e-04 - mean_absolute_error: 0.0065\n",
      "\n",
      "Epoch 07461: loss did not improve\n",
      "Epoch 7462/10000\n",
      "2700/2700 [==============================] - 0s 29us/step - loss: 1.2197e-04 - mean_absolute_error: 0.0064\n",
      "\n",
      "Epoch 07462: loss did not improve\n",
      "Epoch 7463/10000\n",
      "2700/2700 [==============================] - 0s 28us/step - loss: 1.2408e-04 - mean_absolute_error: 0.0065\n",
      "\n",
      "Epoch 07463: loss did not improve\n",
      "Epoch 7464/10000\n",
      "2700/2700 [==============================] - 0s 29us/step - loss: 1.2167e-04 - mean_absolute_error: 0.0063\n",
      "\n",
      "Epoch 07464: loss did not improve\n",
      "Epoch 7465/10000\n",
      "2700/2700 [==============================] - 0s 31us/step - loss: 1.2254e-04 - mean_absolute_error: 0.0064\n",
      "\n",
      "Epoch 07465: loss did not improve\n",
      "Epoch 7466/10000\n",
      "2700/2700 [==============================] - 0s 30us/step - loss: 1.2529e-04 - mean_absolute_error: 0.0066\n",
      "\n",
      "Epoch 07466: loss did not improve\n",
      "Epoch 7467/10000\n",
      "2700/2700 [==============================] - 0s 25us/step - loss: 1.2385e-04 - mean_absolute_error: 0.0065\n",
      "\n",
      "Epoch 07467: loss did not improve\n",
      "Epoch 7468/10000\n",
      "2700/2700 [==============================] - 0s 30us/step - loss: 1.2353e-04 - mean_absolute_error: 0.0065\n",
      "\n",
      "Epoch 07468: loss did not improve\n",
      "Epoch 7469/10000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2700/2700 [==============================] - 0s 33us/step - loss: 1.2250e-04 - mean_absolute_error: 0.0063\n",
      "\n",
      "Epoch 07469: loss did not improve\n",
      "Epoch 7470/10000\n",
      "2700/2700 [==============================] - 0s 31us/step - loss: 1.2276e-04 - mean_absolute_error: 0.0065\n",
      "\n",
      "Epoch 07470: loss did not improve\n",
      "Epoch 7471/10000\n",
      "2700/2700 [==============================] - 0s 30us/step - loss: 1.2304e-04 - mean_absolute_error: 0.0064\n",
      "\n",
      "Epoch 07471: loss did not improve\n",
      "Epoch 7472/10000\n",
      "2700/2700 [==============================] - 0s 27us/step - loss: 1.2392e-04 - mean_absolute_error: 0.0064\n",
      "\n",
      "Epoch 07472: loss did not improve\n",
      "Epoch 7473/10000\n",
      "2700/2700 [==============================] - 0s 30us/step - loss: 1.2582e-04 - mean_absolute_error: 0.0066\n",
      "\n",
      "Epoch 07473: loss did not improve\n",
      "Epoch 7474/10000\n",
      "2700/2700 [==============================] - 0s 31us/step - loss: 1.2342e-04 - mean_absolute_error: 0.0064\n",
      "\n",
      "Epoch 07474: loss did not improve\n",
      "Epoch 7475/10000\n",
      "2700/2700 [==============================] - 0s 31us/step - loss: 1.2662e-04 - mean_absolute_error: 0.0067\n",
      "\n",
      "Epoch 07475: loss did not improve\n",
      "Epoch 7476/10000\n",
      "2700/2700 [==============================] - 0s 30us/step - loss: 1.2333e-04 - mean_absolute_error: 0.0065\n",
      "\n",
      "Epoch 07476: loss did not improve\n",
      "Epoch 7477/10000\n",
      "2700/2700 [==============================] - 0s 31us/step - loss: 1.2178e-04 - mean_absolute_error: 0.0062\n",
      "\n",
      "Epoch 07477: loss did not improve\n",
      "Epoch 7478/10000\n",
      "2700/2700 [==============================] - 0s 28us/step - loss: 1.2408e-04 - mean_absolute_error: 0.0064\n",
      "\n",
      "Epoch 07478: loss did not improve\n",
      "Epoch 7479/10000\n",
      "2700/2700 [==============================] - 0s 30us/step - loss: 1.2180e-04 - mean_absolute_error: 0.0063\n",
      "\n",
      "Epoch 07479: loss did not improve\n",
      "Epoch 7480/10000\n",
      "2700/2700 [==============================] - 0s 28us/step - loss: 1.2552e-04 - mean_absolute_error: 0.0066\n",
      "\n",
      "Epoch 07480: loss did not improve\n",
      "Epoch 7481/10000\n",
      "2700/2700 [==============================] - 0s 31us/step - loss: 1.2465e-04 - mean_absolute_error: 0.0066\n",
      "\n",
      "Epoch 07481: loss did not improve\n",
      "Epoch 7482/10000\n",
      "2700/2700 [==============================] - 0s 29us/step - loss: 1.2304e-04 - mean_absolute_error: 0.0064\n",
      "\n",
      "Epoch 07482: loss did not improve\n",
      "Epoch 7483/10000\n",
      "2700/2700 [==============================] - 0s 29us/step - loss: 1.2240e-04 - mean_absolute_error: 0.0064\n",
      "\n",
      "Epoch 07483: loss did not improve\n",
      "Epoch 7484/10000\n",
      "2700/2700 [==============================] - 0s 29us/step - loss: 1.2452e-04 - mean_absolute_error: 0.0066\n",
      "\n",
      "Epoch 07484: loss did not improve\n",
      "Epoch 7485/10000\n",
      "2700/2700 [==============================] - 0s 29us/step - loss: 1.2350e-04 - mean_absolute_error: 0.0065\n",
      "\n",
      "Epoch 07485: loss did not improve\n",
      "Epoch 7486/10000\n",
      "2700/2700 [==============================] - 0s 30us/step - loss: 1.2484e-04 - mean_absolute_error: 0.0067\n",
      "\n",
      "Epoch 07486: loss did not improve\n",
      "Epoch 7487/10000\n",
      "2700/2700 [==============================] - 0s 28us/step - loss: 1.2257e-04 - mean_absolute_error: 0.0064\n",
      "\n",
      "Epoch 07487: loss did not improve\n",
      "Epoch 7488/10000\n",
      "2700/2700 [==============================] - 0s 28us/step - loss: 1.2380e-04 - mean_absolute_error: 0.0065\n",
      "\n",
      "Epoch 07488: loss did not improve\n",
      "Epoch 7489/10000\n",
      "2700/2700 [==============================] - 0s 28us/step - loss: 1.2236e-04 - mean_absolute_error: 0.0063\n",
      "\n",
      "Epoch 07489: loss did not improve\n",
      "Epoch 7490/10000\n",
      "2700/2700 [==============================] - 0s 31us/step - loss: 1.2524e-04 - mean_absolute_error: 0.0066\n",
      "\n",
      "Epoch 07490: loss did not improve\n",
      "Epoch 7491/10000\n",
      "2700/2700 [==============================] - 0s 30us/step - loss: 1.2311e-04 - mean_absolute_error: 0.0064\n",
      "\n",
      "Epoch 07491: loss did not improve\n",
      "Epoch 7492/10000\n",
      "2700/2700 [==============================] - 0s 27us/step - loss: 1.2465e-04 - mean_absolute_error: 0.0065\n",
      "\n",
      "Epoch 07492: loss did not improve\n",
      "Epoch 7493/10000\n",
      "2700/2700 [==============================] - 0s 28us/step - loss: 1.2169e-04 - mean_absolute_error: 0.0064\n",
      "\n",
      "Epoch 07493: loss did not improve\n",
      "Epoch 7494/10000\n",
      "2700/2700 [==============================] - 0s 28us/step - loss: 1.2376e-04 - mean_absolute_error: 0.0064\n",
      "\n",
      "Epoch 07494: loss did not improve\n",
      "Epoch 7495/10000\n",
      "2700/2700 [==============================] - 0s 32us/step - loss: 1.2329e-04 - mean_absolute_error: 0.0064\n",
      "\n",
      "Epoch 07495: loss did not improve\n",
      "Epoch 7496/10000\n",
      "2700/2700 [==============================] - 0s 30us/step - loss: 1.2152e-04 - mean_absolute_error: 0.0063\n",
      "\n",
      "Epoch 07496: loss did not improve\n",
      "Epoch 7497/10000\n",
      "2700/2700 [==============================] - 0s 28us/step - loss: 1.2330e-04 - mean_absolute_error: 0.0065\n",
      "\n",
      "Epoch 07497: loss did not improve\n",
      "Epoch 7498/10000\n",
      "2700/2700 [==============================] - 0s 30us/step - loss: 1.2180e-04 - mean_absolute_error: 0.0064\n",
      "\n",
      "Epoch 07498: loss did not improve\n",
      "Epoch 7499/10000\n",
      "2700/2700 [==============================] - 0s 30us/step - loss: 1.2310e-04 - mean_absolute_error: 0.0064\n",
      "\n",
      "Epoch 07499: loss did not improve\n",
      "Epoch 7500/10000\n",
      "2700/2700 [==============================] - 0s 29us/step - loss: 1.2304e-04 - mean_absolute_error: 0.0065\n",
      "\n",
      "Epoch 07500: loss did not improve\n",
      "Epoch 7501/10000\n",
      "2700/2700 [==============================] - 0s 29us/step - loss: 1.2404e-04 - mean_absolute_error: 0.0065\n",
      "\n",
      "Epoch 07501: loss did not improve\n",
      "Epoch 7502/10000\n",
      "2700/2700 [==============================] - 0s 29us/step - loss: 1.2186e-04 - mean_absolute_error: 0.0063\n",
      "\n",
      "Epoch 07502: loss did not improve\n",
      "Epoch 7503/10000\n",
      "2700/2700 [==============================] - 0s 30us/step - loss: 1.2327e-04 - mean_absolute_error: 0.0064\n",
      "\n",
      "Epoch 07503: loss did not improve\n",
      "Epoch 7504/10000\n",
      "2700/2700 [==============================] - 0s 28us/step - loss: 1.2466e-04 - mean_absolute_error: 0.0065\n",
      "\n",
      "Epoch 07504: loss did not improve\n",
      "Epoch 7505/10000\n",
      "2700/2700 [==============================] - 0s 31us/step - loss: 1.2523e-04 - mean_absolute_error: 0.0066\n",
      "\n",
      "Epoch 07505: loss did not improve\n",
      "Epoch 7506/10000\n",
      "2700/2700 [==============================] - 0s 31us/step - loss: 1.2367e-04 - mean_absolute_error: 0.0065\n",
      "\n",
      "Epoch 07506: loss did not improve\n",
      "Epoch 7507/10000\n",
      "2700/2700 [==============================] - 0s 28us/step - loss: 1.2344e-04 - mean_absolute_error: 0.0064\n",
      "\n",
      "Epoch 07507: loss did not improve\n",
      "Epoch 7508/10000\n",
      "2700/2700 [==============================] - 0s 31us/step - loss: 1.2366e-04 - mean_absolute_error: 0.0066\n",
      "\n",
      "Epoch 07508: loss did not improve\n",
      "Epoch 7509/10000\n",
      "2700/2700 [==============================] - 0s 29us/step - loss: 1.2263e-04 - mean_absolute_error: 0.0064\n",
      "\n",
      "Epoch 07509: loss did not improve\n",
      "Epoch 7510/10000\n",
      "2700/2700 [==============================] - 0s 28us/step - loss: 1.2375e-04 - mean_absolute_error: 0.0066\n",
      "\n",
      "Epoch 07510: loss did not improve\n",
      "Epoch 7511/10000\n",
      "2700/2700 [==============================] - 0s 25us/step - loss: 1.2423e-04 - mean_absolute_error: 0.0064\n",
      "\n",
      "Epoch 07511: loss did not improve\n",
      "Epoch 7512/10000\n",
      "2700/2700 [==============================] - 0s 30us/step - loss: 1.2293e-04 - mean_absolute_error: 0.0064\n",
      "\n",
      "Epoch 07512: loss did not improve\n",
      "Epoch 7513/10000\n",
      "2700/2700 [==============================] - 0s 29us/step - loss: 1.2255e-04 - mean_absolute_error: 0.0064\n",
      "\n",
      "Epoch 07513: loss did not improve\n",
      "Epoch 7514/10000\n",
      "2700/2700 [==============================] - 0s 30us/step - loss: 1.2173e-04 - mean_absolute_error: 0.0063\n",
      "\n",
      "Epoch 07514: loss did not improve\n",
      "Epoch 7515/10000\n",
      "2700/2700 [==============================] - 0s 32us/step - loss: 1.2405e-04 - mean_absolute_error: 0.0065\n",
      "\n",
      "Epoch 07515: loss did not improve\n",
      "Epoch 7516/10000\n",
      "2700/2700 [==============================] - 0s 33us/step - loss: 1.2187e-04 - mean_absolute_error: 0.0063\n",
      "\n",
      "Epoch 07516: loss did not improve\n",
      "Epoch 7517/10000\n",
      "2700/2700 [==============================] - 0s 27us/step - loss: 1.2347e-04 - mean_absolute_error: 0.0064\n",
      "\n",
      "Epoch 07517: loss did not improve\n",
      "Epoch 7518/10000\n",
      "2700/2700 [==============================] - 0s 31us/step - loss: 1.2255e-04 - mean_absolute_error: 0.0064\n",
      "\n",
      "Epoch 07518: loss did not improve\n",
      "Epoch 7519/10000\n",
      "2700/2700 [==============================] - 0s 30us/step - loss: 1.2286e-04 - mean_absolute_error: 0.0064\n",
      "\n",
      "Epoch 07519: loss did not improve\n",
      "Epoch 7520/10000\n",
      "2700/2700 [==============================] - 0s 32us/step - loss: 1.2288e-04 - mean_absolute_error: 0.0064\n",
      "\n",
      "Epoch 07520: loss did not improve\n",
      "Epoch 7521/10000\n",
      "2700/2700 [==============================] - 0s 30us/step - loss: 1.2355e-04 - mean_absolute_error: 0.0064\n",
      "\n",
      "Epoch 07521: loss did not improve\n",
      "Epoch 7522/10000\n",
      "2700/2700 [==============================] - 0s 29us/step - loss: 1.2322e-04 - mean_absolute_error: 0.0064\n",
      "\n",
      "Epoch 07522: loss did not improve\n",
      "Epoch 7523/10000\n",
      "2700/2700 [==============================] - 0s 27us/step - loss: 1.2582e-04 - mean_absolute_error: 0.0066\n",
      "\n",
      "Epoch 07523: loss did not improve\n",
      "Epoch 7524/10000\n",
      "2700/2700 [==============================] - 0s 29us/step - loss: 1.2222e-04 - mean_absolute_error: 0.0064\n",
      "\n",
      "Epoch 07524: loss did not improve\n",
      "Epoch 7525/10000\n",
      "2700/2700 [==============================] - 0s 29us/step - loss: 1.2446e-04 - mean_absolute_error: 0.0066\n",
      "\n",
      "Epoch 07525: loss did not improve\n",
      "Epoch 7526/10000\n",
      "2700/2700 [==============================] - 0s 28us/step - loss: 1.2509e-04 - mean_absolute_error: 0.0066\n",
      "\n",
      "Epoch 07526: loss did not improve\n",
      "Epoch 7527/10000\n",
      "2700/2700 [==============================] - 0s 26us/step - loss: 1.2248e-04 - mean_absolute_error: 0.0064\n",
      "\n",
      "Epoch 07527: loss did not improve\n",
      "Epoch 7528/10000\n",
      "2700/2700 [==============================] - 0s 31us/step - loss: 1.2271e-04 - mean_absolute_error: 0.0065\n",
      "\n",
      "Epoch 07528: loss did not improve\n",
      "Epoch 7529/10000\n",
      "2700/2700 [==============================] - 0s 27us/step - loss: 1.2354e-04 - mean_absolute_error: 0.0064\n",
      "\n",
      "Epoch 07529: loss did not improve\n",
      "Epoch 7530/10000\n",
      "2700/2700 [==============================] - 0s 30us/step - loss: 1.2411e-04 - mean_absolute_error: 0.0065\n",
      "\n",
      "Epoch 07530: loss did not improve\n",
      "Epoch 7531/10000\n",
      "2700/2700 [==============================] - 0s 26us/step - loss: 1.2305e-04 - mean_absolute_error: 0.0064\n",
      "\n",
      "Epoch 07531: loss did not improve\n",
      "Epoch 7532/10000\n",
      "2700/2700 [==============================] - 0s 28us/step - loss: 1.2278e-04 - mean_absolute_error: 0.0063\n",
      "\n",
      "Epoch 07532: loss did not improve\n",
      "Epoch 7533/10000\n",
      "2700/2700 [==============================] - 0s 31us/step - loss: 1.2351e-04 - mean_absolute_error: 0.0064\n",
      "\n",
      "Epoch 07533: loss did not improve\n",
      "Epoch 7534/10000\n",
      "2700/2700 [==============================] - 0s 28us/step - loss: 1.2238e-04 - mean_absolute_error: 0.0064\n",
      "\n",
      "Epoch 07534: loss did not improve\n",
      "Epoch 7535/10000\n",
      "2700/2700 [==============================] - 0s 35us/step - loss: 1.2405e-04 - mean_absolute_error: 0.0065\n",
      "\n",
      "Epoch 07535: loss did not improve\n",
      "Epoch 7536/10000\n",
      "2700/2700 [==============================] - 0s 26us/step - loss: 1.2534e-04 - mean_absolute_error: 0.0066\n",
      "\n",
      "Epoch 07536: loss did not improve\n",
      "Epoch 7537/10000\n",
      "2700/2700 [==============================] - 0s 30us/step - loss: 1.2177e-04 - mean_absolute_error: 0.0064\n",
      "\n",
      "Epoch 07537: loss did not improve\n",
      "Epoch 7538/10000\n",
      "2700/2700 [==============================] - 0s 30us/step - loss: 1.2543e-04 - mean_absolute_error: 0.0067\n",
      "\n",
      "Epoch 07538: loss did not improve\n",
      "Epoch 7539/10000\n",
      "2700/2700 [==============================] - 0s 28us/step - loss: 1.2446e-04 - mean_absolute_error: 0.0065\n",
      "\n",
      "Epoch 07539: loss did not improve\n",
      "Epoch 7540/10000\n",
      "2700/2700 [==============================] - 0s 28us/step - loss: 1.2322e-04 - mean_absolute_error: 0.0065\n",
      "\n",
      "Epoch 07540: loss did not improve\n",
      "Epoch 7541/10000\n",
      "2700/2700 [==============================] - 0s 28us/step - loss: 1.2337e-04 - mean_absolute_error: 0.0064\n",
      "\n",
      "Epoch 07541: loss did not improve\n",
      "Epoch 7542/10000\n",
      "2700/2700 [==============================] - 0s 29us/step - loss: 1.2351e-04 - mean_absolute_error: 0.0064\n",
      "\n",
      "Epoch 07542: loss did not improve\n",
      "Epoch 7543/10000\n",
      "2700/2700 [==============================] - 0s 28us/step - loss: 1.2210e-04 - mean_absolute_error: 0.0063\n",
      "\n",
      "Epoch 07543: loss did not improve\n",
      "Epoch 7544/10000\n",
      "2700/2700 [==============================] - 0s 29us/step - loss: 1.2615e-04 - mean_absolute_error: 0.0068\n",
      "\n",
      "Epoch 07544: loss did not improve\n",
      "Epoch 7545/10000\n",
      "2700/2700 [==============================] - 0s 28us/step - loss: 1.2221e-04 - mean_absolute_error: 0.0064\n",
      "\n",
      "Epoch 07545: loss did not improve\n",
      "Epoch 7546/10000\n",
      "2700/2700 [==============================] - 0s 29us/step - loss: 1.2196e-04 - mean_absolute_error: 0.0063\n",
      "\n",
      "Epoch 07546: loss did not improve\n",
      "Epoch 7547/10000\n",
      "2700/2700 [==============================] - 0s 28us/step - loss: 1.2244e-04 - mean_absolute_error: 0.0063\n",
      "\n",
      "Epoch 07547: loss did not improve\n",
      "Epoch 7548/10000\n",
      "2700/2700 [==============================] - 0s 28us/step - loss: 1.2424e-04 - mean_absolute_error: 0.0066\n",
      "\n",
      "Epoch 07548: loss did not improve\n",
      "Epoch 7549/10000\n",
      "2700/2700 [==============================] - 0s 26us/step - loss: 1.2340e-04 - mean_absolute_error: 0.0064\n",
      "\n",
      "Epoch 07549: loss did not improve\n",
      "Epoch 7550/10000\n",
      "2700/2700 [==============================] - 0s 28us/step - loss: 1.2401e-04 - mean_absolute_error: 0.0066\n",
      "\n",
      "Epoch 07550: loss did not improve\n",
      "Epoch 7551/10000\n",
      "2700/2700 [==============================] - 0s 28us/step - loss: 1.2429e-04 - mean_absolute_error: 0.0067\n",
      "\n",
      "Epoch 07551: loss did not improve\n",
      "Epoch 7552/10000\n",
      "2700/2700 [==============================] - 0s 28us/step - loss: 1.2359e-04 - mean_absolute_error: 0.0066\n",
      "\n",
      "Epoch 07552: loss did not improve\n",
      "Epoch 7553/10000\n",
      "2700/2700 [==============================] - 0s 26us/step - loss: 1.2402e-04 - mean_absolute_error: 0.0065\n",
      "\n",
      "Epoch 07553: loss did not improve\n",
      "Epoch 7554/10000\n",
      "2700/2700 [==============================] - 0s 30us/step - loss: 1.2342e-04 - mean_absolute_error: 0.0065\n",
      "\n",
      "Epoch 07554: loss did not improve\n",
      "Epoch 7555/10000\n",
      "2700/2700 [==============================] - 0s 28us/step - loss: 1.2349e-04 - mean_absolute_error: 0.0065\n",
      "\n",
      "Epoch 07555: loss did not improve\n",
      "Epoch 7556/10000\n",
      "2700/2700 [==============================] - 0s 28us/step - loss: 1.2171e-04 - mean_absolute_error: 0.0063\n",
      "\n",
      "Epoch 07556: loss did not improve\n",
      "Epoch 7557/10000\n",
      "2700/2700 [==============================] - 0s 28us/step - loss: 1.2226e-04 - mean_absolute_error: 0.0063\n",
      "\n",
      "Epoch 07557: loss did not improve\n",
      "Epoch 7558/10000\n",
      "2700/2700 [==============================] - 0s 30us/step - loss: 1.2230e-04 - mean_absolute_error: 0.0064\n",
      "\n",
      "Epoch 07558: loss did not improve\n",
      "Epoch 7559/10000\n",
      "2700/2700 [==============================] - 0s 28us/step - loss: 1.2257e-04 - mean_absolute_error: 0.0065\n",
      "\n",
      "Epoch 07559: loss did not improve\n",
      "Epoch 7560/10000\n",
      "2700/2700 [==============================] - 0s 29us/step - loss: 1.2275e-04 - mean_absolute_error: 0.0065\n",
      "\n",
      "Epoch 07560: loss did not improve\n",
      "Epoch 7561/10000\n",
      "2700/2700 [==============================] - 0s 30us/step - loss: 1.2474e-04 - mean_absolute_error: 0.0066\n",
      "\n",
      "Epoch 07561: loss did not improve\n",
      "Epoch 7562/10000\n",
      "2700/2700 [==============================] - 0s 29us/step - loss: 1.2344e-04 - mean_absolute_error: 0.0065\n",
      "\n",
      "Epoch 07562: loss did not improve\n",
      "Epoch 7563/10000\n",
      "2700/2700 [==============================] - 0s 28us/step - loss: 1.2152e-04 - mean_absolute_error: 0.0063\n",
      "\n",
      "Epoch 07563: loss did not improve\n",
      "Epoch 7564/10000\n",
      "2700/2700 [==============================] - 0s 28us/step - loss: 1.2557e-04 - mean_absolute_error: 0.0067\n",
      "\n",
      "Epoch 07564: loss did not improve\n",
      "Epoch 7565/10000\n",
      "2700/2700 [==============================] - 0s 28us/step - loss: 1.2239e-04 - mean_absolute_error: 0.0063\n",
      "\n",
      "Epoch 07565: loss did not improve\n",
      "Epoch 7566/10000\n",
      "2700/2700 [==============================] - 0s 27us/step - loss: 1.2886e-04 - mean_absolute_error: 0.0070\n",
      "\n",
      "Epoch 07566: loss did not improve\n",
      "Epoch 7567/10000\n",
      "2700/2700 [==============================] - 0s 30us/step - loss: 1.2296e-04 - mean_absolute_error: 0.0064\n",
      "\n",
      "Epoch 07567: loss did not improve\n",
      "Epoch 7568/10000\n",
      "2700/2700 [==============================] - 0s 28us/step - loss: 1.2459e-04 - mean_absolute_error: 0.0066\n",
      "\n",
      "Epoch 07568: loss did not improve\n",
      "Epoch 7569/10000\n",
      "2700/2700 [==============================] - 0s 29us/step - loss: 1.2362e-04 - mean_absolute_error: 0.0065\n",
      "\n",
      "Epoch 07569: loss did not improve\n",
      "Epoch 7570/10000\n",
      "2700/2700 [==============================] - 0s 29us/step - loss: 1.2399e-04 - mean_absolute_error: 0.0065\n",
      "\n",
      "Epoch 07570: loss did not improve\n",
      "Epoch 7571/10000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2700/2700 [==============================] - 0s 28us/step - loss: 1.2337e-04 - mean_absolute_error: 0.0065\n",
      "\n",
      "Epoch 07571: loss did not improve\n",
      "Epoch 7572/10000\n",
      "2700/2700 [==============================] - 0s 28us/step - loss: 1.2117e-04 - mean_absolute_error: 0.0063\n",
      "\n",
      "Epoch 07572: loss improved from 0.00012 to 0.00012, saving model to keras/best_baseline_weights.hdf5\n",
      "Epoch 7573/10000\n",
      "2700/2700 [==============================] - 0s 29us/step - loss: 1.2280e-04 - mean_absolute_error: 0.0065\n",
      "\n",
      "Epoch 07573: loss did not improve\n",
      "Epoch 7574/10000\n",
      "2700/2700 [==============================] - 0s 30us/step - loss: 1.2404e-04 - mean_absolute_error: 0.0066\n",
      "\n",
      "Epoch 07574: loss did not improve\n",
      "Epoch 7575/10000\n",
      "2700/2700 [==============================] - 0s 28us/step - loss: 1.2325e-04 - mean_absolute_error: 0.0065\n",
      "\n",
      "Epoch 07575: loss did not improve\n",
      "Epoch 7576/10000\n",
      "2700/2700 [==============================] - 0s 27us/step - loss: 1.2284e-04 - mean_absolute_error: 0.0064\n",
      "\n",
      "Epoch 07576: loss did not improve\n",
      "Epoch 7577/10000\n",
      "2700/2700 [==============================] - 0s 28us/step - loss: 1.2228e-04 - mean_absolute_error: 0.0064\n",
      "\n",
      "Epoch 07577: loss did not improve\n",
      "Epoch 7578/10000\n",
      "2700/2700 [==============================] - 0s 28us/step - loss: 1.2172e-04 - mean_absolute_error: 0.0062\n",
      "\n",
      "Epoch 07578: loss did not improve\n",
      "Epoch 7579/10000\n",
      "2700/2700 [==============================] - 0s 30us/step - loss: 1.2148e-04 - mean_absolute_error: 0.0063\n",
      "\n",
      "Epoch 07579: loss did not improve\n",
      "Epoch 7580/10000\n",
      "2700/2700 [==============================] - 0s 26us/step - loss: 1.2402e-04 - mean_absolute_error: 0.0065\n",
      "\n",
      "Epoch 07580: loss did not improve\n",
      "Epoch 7581/10000\n",
      "2700/2700 [==============================] - 0s 28us/step - loss: 1.2631e-04 - mean_absolute_error: 0.0068\n",
      "\n",
      "Epoch 07581: loss did not improve\n",
      "Epoch 7582/10000\n",
      "2700/2700 [==============================] - 0s 27us/step - loss: 1.2568e-04 - mean_absolute_error: 0.0066\n",
      "\n",
      "Epoch 07582: loss did not improve\n",
      "Epoch 7583/10000\n",
      "2700/2700 [==============================] - 0s 29us/step - loss: 1.2315e-04 - mean_absolute_error: 0.0065\n",
      "\n",
      "Epoch 07583: loss did not improve\n",
      "Epoch 7584/10000\n",
      "2700/2700 [==============================] - 0s 27us/step - loss: 1.2484e-04 - mean_absolute_error: 0.0067\n",
      "\n",
      "Epoch 07584: loss did not improve\n",
      "Epoch 7585/10000\n",
      "2700/2700 [==============================] - 0s 30us/step - loss: 1.2262e-04 - mean_absolute_error: 0.0064\n",
      "\n",
      "Epoch 07585: loss did not improve\n",
      "Epoch 7586/10000\n",
      "2700/2700 [==============================] - 0s 28us/step - loss: 1.2291e-04 - mean_absolute_error: 0.0065\n",
      "\n",
      "Epoch 07586: loss did not improve\n",
      "Epoch 7587/10000\n",
      "2700/2700 [==============================] - 0s 30us/step - loss: 1.2260e-04 - mean_absolute_error: 0.0064\n",
      "\n",
      "Epoch 07587: loss did not improve\n",
      "Epoch 7588/10000\n",
      "2700/2700 [==============================] - 0s 26us/step - loss: 1.2276e-04 - mean_absolute_error: 0.0064\n",
      "\n",
      "Epoch 07588: loss did not improve\n",
      "Epoch 7589/10000\n",
      "2700/2700 [==============================] - 0s 26us/step - loss: 1.2355e-04 - mean_absolute_error: 0.0065\n",
      "\n",
      "Epoch 07589: loss did not improve\n",
      "Epoch 7590/10000\n",
      "2700/2700 [==============================] - 0s 35us/step - loss: 1.2671e-04 - mean_absolute_error: 0.0067\n",
      "\n",
      "Epoch 07590: loss did not improve\n",
      "Epoch 7591/10000\n",
      "2700/2700 [==============================] - 0s 42us/step - loss: 1.2452e-04 - mean_absolute_error: 0.0066\n",
      "\n",
      "Epoch 07591: loss did not improve\n",
      "Epoch 7592/10000\n",
      "2700/2700 [==============================] - 0s 38us/step - loss: 1.2154e-04 - mean_absolute_error: 0.0063\n",
      "\n",
      "Epoch 07592: loss did not improve\n",
      "Epoch 7593/10000\n",
      "2700/2700 [==============================] - 0s 34us/step - loss: 1.2291e-04 - mean_absolute_error: 0.0064\n",
      "\n",
      "Epoch 07593: loss did not improve\n",
      "Epoch 7594/10000\n",
      "2700/2700 [==============================] - 0s 29us/step - loss: 1.2221e-04 - mean_absolute_error: 0.0064\n",
      "\n",
      "Epoch 07594: loss did not improve\n",
      "Epoch 7595/10000\n",
      "2700/2700 [==============================] - 0s 34us/step - loss: 1.2240e-04 - mean_absolute_error: 0.0064\n",
      "\n",
      "Epoch 07595: loss did not improve\n",
      "Epoch 7596/10000\n",
      "2700/2700 [==============================] - 0s 32us/step - loss: 1.2193e-04 - mean_absolute_error: 0.0064\n",
      "\n",
      "Epoch 07596: loss did not improve\n",
      "Epoch 7597/10000\n",
      "2700/2700 [==============================] - 0s 28us/step - loss: 1.2362e-04 - mean_absolute_error: 0.0065\n",
      "\n",
      "Epoch 07597: loss did not improve\n",
      "Epoch 7598/10000\n",
      "2700/2700 [==============================] - 0s 31us/step - loss: 1.2787e-04 - mean_absolute_error: 0.0068\n",
      "\n",
      "Epoch 07598: loss did not improve\n",
      "Epoch 7599/10000\n",
      "2700/2700 [==============================] - 0s 31us/step - loss: 1.2389e-04 - mean_absolute_error: 0.0065\n",
      "\n",
      "Epoch 07599: loss did not improve\n",
      "Epoch 7600/10000\n",
      "2700/2700 [==============================] - 0s 29us/step - loss: 1.2204e-04 - mean_absolute_error: 0.0064\n",
      "\n",
      "Epoch 07600: loss did not improve\n",
      "Epoch 7601/10000\n",
      "2700/2700 [==============================] - 0s 28us/step - loss: 1.2285e-04 - mean_absolute_error: 0.0064\n",
      "\n",
      "Epoch 07601: loss did not improve\n",
      "Epoch 7602/10000\n",
      "2700/2700 [==============================] - 0s 33us/step - loss: 1.2162e-04 - mean_absolute_error: 0.0062\n",
      "\n",
      "Epoch 07602: loss did not improve\n",
      "Epoch 7603/10000\n",
      "2700/2700 [==============================] - 0s 28us/step - loss: 1.2262e-04 - mean_absolute_error: 0.0065\n",
      "\n",
      "Epoch 07603: loss did not improve\n",
      "Epoch 7604/10000\n",
      "2700/2700 [==============================] - 0s 32us/step - loss: 1.2189e-04 - mean_absolute_error: 0.0063\n",
      "\n",
      "Epoch 07604: loss did not improve\n",
      "Epoch 7605/10000\n",
      "2700/2700 [==============================] - 0s 29us/step - loss: 1.2266e-04 - mean_absolute_error: 0.0064\n",
      "\n",
      "Epoch 07605: loss did not improve\n",
      "Epoch 7606/10000\n",
      "2700/2700 [==============================] - 0s 29us/step - loss: 1.2370e-04 - mean_absolute_error: 0.0065\n",
      "\n",
      "Epoch 07606: loss did not improve\n",
      "Epoch 7607/10000\n",
      "2700/2700 [==============================] - 0s 29us/step - loss: 1.2500e-04 - mean_absolute_error: 0.0066\n",
      "\n",
      "Epoch 07607: loss did not improve\n",
      "Epoch 7608/10000\n",
      "2700/2700 [==============================] - 0s 26us/step - loss: 1.2209e-04 - mean_absolute_error: 0.0064\n",
      "\n",
      "Epoch 07608: loss did not improve\n",
      "Epoch 7609/10000\n",
      "2700/2700 [==============================] - 0s 28us/step - loss: 1.2075e-04 - mean_absolute_error: 0.0062\n",
      "\n",
      "Epoch 07609: loss improved from 0.00012 to 0.00012, saving model to keras/best_baseline_weights.hdf5\n",
      "Epoch 7610/10000\n",
      "2700/2700 [==============================] - 0s 37us/step - loss: 1.2218e-04 - mean_absolute_error: 0.0064\n",
      "\n",
      "Epoch 07610: loss did not improve\n",
      "Epoch 7611/10000\n",
      "2700/2700 [==============================] - 0s 31us/step - loss: 1.2445e-04 - mean_absolute_error: 0.0066\n",
      "\n",
      "Epoch 07611: loss did not improve\n",
      "Epoch 7612/10000\n",
      "2700/2700 [==============================] - 0s 33us/step - loss: 1.2318e-04 - mean_absolute_error: 0.0065\n",
      "\n",
      "Epoch 07612: loss did not improve\n",
      "Epoch 7613/10000\n",
      "2700/2700 [==============================] - 0s 30us/step - loss: 1.2356e-04 - mean_absolute_error: 0.0066\n",
      "\n",
      "Epoch 07613: loss did not improve\n",
      "Epoch 7614/10000\n",
      "2700/2700 [==============================] - 0s 30us/step - loss: 1.2362e-04 - mean_absolute_error: 0.0065\n",
      "\n",
      "Epoch 07614: loss did not improve\n",
      "Epoch 7615/10000\n",
      "2700/2700 [==============================] - 0s 27us/step - loss: 1.2147e-04 - mean_absolute_error: 0.0065\n",
      "\n",
      "Epoch 07615: loss did not improve\n",
      "Epoch 7616/10000\n",
      "2700/2700 [==============================] - 0s 30us/step - loss: 1.2106e-04 - mean_absolute_error: 0.0063\n",
      "\n",
      "Epoch 07616: loss did not improve\n",
      "Epoch 7617/10000\n",
      "2700/2700 [==============================] - 0s 42us/step - loss: 1.2241e-04 - mean_absolute_error: 0.0064\n",
      "\n",
      "Epoch 07617: loss did not improve\n",
      "Epoch 7618/10000\n",
      "2700/2700 [==============================] - 0s 33us/step - loss: 1.2298e-04 - mean_absolute_error: 0.0064\n",
      "\n",
      "Epoch 07618: loss did not improve\n",
      "Epoch 7619/10000\n",
      "2700/2700 [==============================] - 0s 30us/step - loss: 1.2180e-04 - mean_absolute_error: 0.0063\n",
      "\n",
      "Epoch 07619: loss did not improve\n",
      "Epoch 7620/10000\n",
      "2700/2700 [==============================] - 0s 33us/step - loss: 1.2440e-04 - mean_absolute_error: 0.0066\n",
      "\n",
      "Epoch 07620: loss did not improve\n",
      "Epoch 7621/10000\n",
      "2700/2700 [==============================] - 0s 29us/step - loss: 1.2239e-04 - mean_absolute_error: 0.0064\n",
      "\n",
      "Epoch 07621: loss did not improve\n",
      "Epoch 7622/10000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2700/2700 [==============================] - 0s 29us/step - loss: 1.2523e-04 - mean_absolute_error: 0.0066\n",
      "\n",
      "Epoch 07622: loss did not improve\n",
      "Epoch 7623/10000\n",
      "2700/2700 [==============================] - 0s 30us/step - loss: 1.2478e-04 - mean_absolute_error: 0.0065\n",
      "\n",
      "Epoch 07623: loss did not improve\n",
      "Epoch 7624/10000\n",
      "2700/2700 [==============================] - 0s 32us/step - loss: 1.2464e-04 - mean_absolute_error: 0.0065\n",
      "\n",
      "Epoch 07624: loss did not improve\n",
      "Epoch 7625/10000\n",
      "2700/2700 [==============================] - 0s 36us/step - loss: 1.2679e-04 - mean_absolute_error: 0.0067\n",
      "\n",
      "Epoch 07625: loss did not improve\n",
      "Epoch 7626/10000\n",
      "2700/2700 [==============================] - 0s 31us/step - loss: 1.2556e-04 - mean_absolute_error: 0.0066\n",
      "\n",
      "Epoch 07626: loss did not improve\n",
      "Epoch 7627/10000\n",
      "2700/2700 [==============================] - 0s 28us/step - loss: 1.2352e-04 - mean_absolute_error: 0.0065\n",
      "\n",
      "Epoch 07627: loss did not improve\n",
      "Epoch 7628/10000\n",
      "2700/2700 [==============================] - 0s 26us/step - loss: 1.2493e-04 - mean_absolute_error: 0.0066\n",
      "\n",
      "Epoch 07628: loss did not improve\n",
      "Epoch 7629/10000\n",
      "2700/2700 [==============================] - 0s 29us/step - loss: 1.2361e-04 - mean_absolute_error: 0.0066: 0s - loss: 1.3343e-04 - mean_absolute_error: 0.006\n",
      "\n",
      "Epoch 07629: loss did not improve\n",
      "Epoch 7630/10000\n",
      "2700/2700 [==============================] - 0s 32us/step - loss: 1.2167e-04 - mean_absolute_error: 0.0064\n",
      "\n",
      "Epoch 07630: loss did not improve\n",
      "Epoch 7631/10000\n",
      "2700/2700 [==============================] - 0s 29us/step - loss: 1.2396e-04 - mean_absolute_error: 0.0066\n",
      "\n",
      "Epoch 07631: loss did not improve\n",
      "Epoch 7632/10000\n",
      "2700/2700 [==============================] - 0s 29us/step - loss: 1.2185e-04 - mean_absolute_error: 0.0063\n",
      "\n",
      "Epoch 07632: loss did not improve\n",
      "Epoch 7633/10000\n",
      "2700/2700 [==============================] - 0s 27us/step - loss: 1.2310e-04 - mean_absolute_error: 0.0065\n",
      "\n",
      "Epoch 07633: loss did not improve\n",
      "Epoch 7634/10000\n",
      "2700/2700 [==============================] - 0s 28us/step - loss: 1.2631e-04 - mean_absolute_error: 0.0067\n",
      "\n",
      "Epoch 07634: loss did not improve\n",
      "Epoch 7635/10000\n",
      "2700/2700 [==============================] - 0s 29us/step - loss: 1.2474e-04 - mean_absolute_error: 0.0066\n",
      "\n",
      "Epoch 07635: loss did not improve\n",
      "Epoch 7636/10000\n",
      "2700/2700 [==============================] - 0s 28us/step - loss: 1.2308e-04 - mean_absolute_error: 0.0064\n",
      "\n",
      "Epoch 07636: loss did not improve\n",
      "Epoch 7637/10000\n",
      "2700/2700 [==============================] - 0s 27us/step - loss: 1.2200e-04 - mean_absolute_error: 0.0064\n",
      "\n",
      "Epoch 07637: loss did not improve\n",
      "Epoch 7638/10000\n",
      "2700/2700 [==============================] - 0s 28us/step - loss: 1.2303e-04 - mean_absolute_error: 0.0065\n",
      "\n",
      "Epoch 07638: loss did not improve\n",
      "Epoch 7639/10000\n",
      "2700/2700 [==============================] - 0s 31us/step - loss: 1.2162e-04 - mean_absolute_error: 0.0064\n",
      "\n",
      "Epoch 07639: loss did not improve\n",
      "Epoch 7640/10000\n",
      "2700/2700 [==============================] - 0s 28us/step - loss: 1.2534e-04 - mean_absolute_error: 0.0067\n",
      "\n",
      "Epoch 07640: loss did not improve\n",
      "Epoch 7641/10000\n",
      "2700/2700 [==============================] - 0s 26us/step - loss: 1.2212e-04 - mean_absolute_error: 0.0064\n",
      "\n",
      "Epoch 07641: loss did not improve\n",
      "Epoch 7642/10000\n",
      "2700/2700 [==============================] - 0s 29us/step - loss: 1.2151e-04 - mean_absolute_error: 0.0063\n",
      "\n",
      "Epoch 07642: loss did not improve\n",
      "Epoch 7643/10000\n",
      "2700/2700 [==============================] - 0s 29us/step - loss: 1.2459e-04 - mean_absolute_error: 0.0065\n",
      "\n",
      "Epoch 07643: loss did not improve\n",
      "Epoch 7644/10000\n",
      "2700/2700 [==============================] - 0s 27us/step - loss: 1.2329e-04 - mean_absolute_error: 0.0065\n",
      "\n",
      "Epoch 07644: loss did not improve\n",
      "Epoch 7645/10000\n",
      "2700/2700 [==============================] - 0s 28us/step - loss: 1.2863e-04 - mean_absolute_error: 0.0070\n",
      "\n",
      "Epoch 07645: loss did not improve\n",
      "Epoch 7646/10000\n",
      "2700/2700 [==============================] - 0s 30us/step - loss: 1.2555e-04 - mean_absolute_error: 0.0066\n",
      "\n",
      "Epoch 07646: loss did not improve\n",
      "Epoch 7647/10000\n",
      "2700/2700 [==============================] - 0s 28us/step - loss: 1.2268e-04 - mean_absolute_error: 0.0064\n",
      "\n",
      "Epoch 07647: loss did not improve\n",
      "Epoch 7648/10000\n",
      "2700/2700 [==============================] - 0s 37us/step - loss: 1.2348e-04 - mean_absolute_error: 0.0064\n",
      "\n",
      "Epoch 07648: loss did not improve\n",
      "Epoch 7649/10000\n",
      "2700/2700 [==============================] - 0s 30us/step - loss: 1.2367e-04 - mean_absolute_error: 0.0065\n",
      "\n",
      "Epoch 07649: loss did not improve\n",
      "Epoch 7650/10000\n",
      "2700/2700 [==============================] - 0s 32us/step - loss: 1.2322e-04 - mean_absolute_error: 0.0065\n",
      "\n",
      "Epoch 07650: loss did not improve\n",
      "Epoch 7651/10000\n",
      "2700/2700 [==============================] - 0s 30us/step - loss: 1.2104e-04 - mean_absolute_error: 0.0063\n",
      "\n",
      "Epoch 07651: loss did not improve\n",
      "Epoch 7652/10000\n",
      "2700/2700 [==============================] - 0s 26us/step - loss: 1.2154e-04 - mean_absolute_error: 0.0063\n",
      "\n",
      "Epoch 07652: loss did not improve\n",
      "Epoch 7653/10000\n",
      "2700/2700 [==============================] - 0s 29us/step - loss: 1.2294e-04 - mean_absolute_error: 0.0063\n",
      "\n",
      "Epoch 07653: loss did not improve\n",
      "Epoch 7654/10000\n",
      "2700/2700 [==============================] - 0s 35us/step - loss: 1.2171e-04 - mean_absolute_error: 0.0064\n",
      "\n",
      "Epoch 07654: loss did not improve\n",
      "Epoch 7655/10000\n",
      "2700/2700 [==============================] - 0s 36us/step - loss: 1.2779e-04 - mean_absolute_error: 0.0069\n",
      "\n",
      "Epoch 07655: loss did not improve\n",
      "Epoch 7656/10000\n",
      "2700/2700 [==============================] - 0s 28us/step - loss: 1.2309e-04 - mean_absolute_error: 0.0064\n",
      "\n",
      "Epoch 07656: loss did not improve\n",
      "Epoch 7657/10000\n",
      "2700/2700 [==============================] - 0s 28us/step - loss: 1.2333e-04 - mean_absolute_error: 0.0066\n",
      "\n",
      "Epoch 07657: loss did not improve\n",
      "Epoch 7658/10000\n",
      "2700/2700 [==============================] - 0s 33us/step - loss: 1.2276e-04 - mean_absolute_error: 0.0064\n",
      "\n",
      "Epoch 07658: loss did not improve\n",
      "Epoch 7659/10000\n",
      "2700/2700 [==============================] - 0s 28us/step - loss: 1.2548e-04 - mean_absolute_error: 0.0067\n",
      "\n",
      "Epoch 07659: loss did not improve\n",
      "Epoch 7660/10000\n",
      "2700/2700 [==============================] - 0s 29us/step - loss: 1.2494e-04 - mean_absolute_error: 0.0067\n",
      "\n",
      "Epoch 07660: loss did not improve\n",
      "Epoch 7661/10000\n",
      "2700/2700 [==============================] - 0s 33us/step - loss: 1.2286e-04 - mean_absolute_error: 0.0064\n",
      "\n",
      "Epoch 07661: loss did not improve\n",
      "Epoch 7662/10000\n",
      "2700/2700 [==============================] - 0s 29us/step - loss: 1.2477e-04 - mean_absolute_error: 0.0065\n",
      "\n",
      "Epoch 07662: loss did not improve\n",
      "Epoch 7663/10000\n",
      "2700/2700 [==============================] - 0s 28us/step - loss: 1.2313e-04 - mean_absolute_error: 0.0064\n",
      "\n",
      "Epoch 07663: loss did not improve\n",
      "Epoch 7664/10000\n",
      "2700/2700 [==============================] - 0s 28us/step - loss: 1.2301e-04 - mean_absolute_error: 0.0064\n",
      "\n",
      "Epoch 07664: loss did not improve\n",
      "Epoch 7665/10000\n",
      "2700/2700 [==============================] - 0s 28us/step - loss: 1.2166e-04 - mean_absolute_error: 0.0064\n",
      "\n",
      "Epoch 07665: loss did not improve\n",
      "Epoch 7666/10000\n",
      "2700/2700 [==============================] - 0s 30us/step - loss: 1.2390e-04 - mean_absolute_error: 0.0066\n",
      "\n",
      "Epoch 07666: loss did not improve\n",
      "Epoch 7667/10000\n",
      "2700/2700 [==============================] - 0s 27us/step - loss: 1.2495e-04 - mean_absolute_error: 0.0066\n",
      "\n",
      "Epoch 07667: loss did not improve\n",
      "Epoch 7668/10000\n",
      "2700/2700 [==============================] - 0s 30us/step - loss: 1.2699e-04 - mean_absolute_error: 0.0067\n",
      "\n",
      "Epoch 07668: loss did not improve\n",
      "Epoch 7669/10000\n",
      "2700/2700 [==============================] - 0s 28us/step - loss: 1.2369e-04 - mean_absolute_error: 0.0066\n",
      "\n",
      "Epoch 07669: loss did not improve\n",
      "Epoch 7670/10000\n",
      "2700/2700 [==============================] - 0s 30us/step - loss: 1.2220e-04 - mean_absolute_error: 0.0064\n",
      "\n",
      "Epoch 07670: loss did not improve\n",
      "Epoch 7671/10000\n",
      "2700/2700 [==============================] - 0s 31us/step - loss: 1.2162e-04 - mean_absolute_error: 0.0063\n",
      "\n",
      "Epoch 07671: loss did not improve\n",
      "Epoch 7672/10000\n",
      "2700/2700 [==============================] - 0s 26us/step - loss: 1.2444e-04 - mean_absolute_error: 0.0066\n",
      "\n",
      "Epoch 07672: loss did not improve\n",
      "Epoch 7673/10000\n",
      "2700/2700 [==============================] - 0s 29us/step - loss: 1.2405e-04 - mean_absolute_error: 0.0066\n",
      "\n",
      "Epoch 07673: loss did not improve\n",
      "Epoch 7674/10000\n",
      "2700/2700 [==============================] - 0s 29us/step - loss: 1.2355e-04 - mean_absolute_error: 0.0065\n",
      "\n",
      "Epoch 07674: loss did not improve\n",
      "Epoch 7675/10000\n",
      "2700/2700 [==============================] - 0s 36us/step - loss: 1.2681e-04 - mean_absolute_error: 0.0069\n",
      "\n",
      "Epoch 07675: loss did not improve\n",
      "Epoch 7676/10000\n",
      "2700/2700 [==============================] - 0s 33us/step - loss: 1.2456e-04 - mean_absolute_error: 0.0066\n",
      "\n",
      "Epoch 07676: loss did not improve\n",
      "Epoch 7677/10000\n",
      "2700/2700 [==============================] - 0s 29us/step - loss: 1.2333e-04 - mean_absolute_error: 0.0065\n",
      "\n",
      "Epoch 07677: loss did not improve\n",
      "Epoch 7678/10000\n",
      "2700/2700 [==============================] - 0s 28us/step - loss: 1.2485e-04 - mean_absolute_error: 0.0066\n",
      "\n",
      "Epoch 07678: loss did not improve\n",
      "Epoch 7679/10000\n",
      "2700/2700 [==============================] - 0s 28us/step - loss: 1.2390e-04 - mean_absolute_error: 0.0064\n",
      "\n",
      "Epoch 07679: loss did not improve\n",
      "Epoch 7680/10000\n",
      "2700/2700 [==============================] - 0s 31us/step - loss: 1.2031e-04 - mean_absolute_error: 0.0062\n",
      "\n",
      "Epoch 07680: loss improved from 0.00012 to 0.00012, saving model to keras/best_baseline_weights.hdf5\n",
      "Epoch 7681/10000\n",
      "2700/2700 [==============================] - 0s 27us/step - loss: 1.2211e-04 - mean_absolute_error: 0.0064\n",
      "\n",
      "Epoch 07681: loss did not improve\n",
      "Epoch 7682/10000\n",
      "2700/2700 [==============================] - 0s 31us/step - loss: 1.2384e-04 - mean_absolute_error: 0.0065\n",
      "\n",
      "Epoch 07682: loss did not improve\n",
      "Epoch 7683/10000\n",
      "2700/2700 [==============================] - 0s 34us/step - loss: 1.2375e-04 - mean_absolute_error: 0.0065\n",
      "\n",
      "Epoch 07683: loss did not improve\n",
      "Epoch 7684/10000\n",
      "2700/2700 [==============================] - 0s 38us/step - loss: 1.2182e-04 - mean_absolute_error: 0.0063\n",
      "\n",
      "Epoch 07684: loss did not improve\n",
      "Epoch 7685/10000\n",
      "2700/2700 [==============================] - 0s 34us/step - loss: 1.2144e-04 - mean_absolute_error: 0.0064\n",
      "\n",
      "Epoch 07685: loss did not improve\n",
      "Epoch 7686/10000\n",
      "2700/2700 [==============================] - 0s 26us/step - loss: 1.2219e-04 - mean_absolute_error: 0.0065\n",
      "\n",
      "Epoch 07686: loss did not improve\n",
      "Epoch 7687/10000\n",
      "2700/2700 [==============================] - 0s 29us/step - loss: 1.2214e-04 - mean_absolute_error: 0.0063\n",
      "\n",
      "Epoch 07687: loss did not improve\n",
      "Epoch 7688/10000\n",
      "2700/2700 [==============================] - 0s 28us/step - loss: 1.2536e-04 - mean_absolute_error: 0.0067\n",
      "\n",
      "Epoch 07688: loss did not improve\n",
      "Epoch 7689/10000\n",
      "2700/2700 [==============================] - 0s 28us/step - loss: 1.2268e-04 - mean_absolute_error: 0.0064\n",
      "\n",
      "Epoch 07689: loss did not improve\n",
      "Epoch 7690/10000\n",
      "2700/2700 [==============================] - 0s 28us/step - loss: 1.2367e-04 - mean_absolute_error: 0.0064\n",
      "\n",
      "Epoch 07690: loss did not improve\n",
      "Epoch 7691/10000\n",
      "2700/2700 [==============================] - 0s 31us/step - loss: 1.2227e-04 - mean_absolute_error: 0.0063\n",
      "\n",
      "Epoch 07691: loss did not improve\n",
      "Epoch 7692/10000\n",
      "2700/2700 [==============================] - 0s 30us/step - loss: 1.2189e-04 - mean_absolute_error: 0.0064\n",
      "\n",
      "Epoch 07692: loss did not improve\n",
      "Epoch 7693/10000\n",
      "2700/2700 [==============================] - 0s 29us/step - loss: 1.2178e-04 - mean_absolute_error: 0.0064\n",
      "\n",
      "Epoch 07693: loss did not improve\n",
      "Epoch 7694/10000\n",
      "2700/2700 [==============================] - 0s 29us/step - loss: 1.2476e-04 - mean_absolute_error: 0.0066\n",
      "\n",
      "Epoch 07694: loss did not improve\n",
      "Epoch 7695/10000\n",
      "2700/2700 [==============================] - 0s 26us/step - loss: 1.2156e-04 - mean_absolute_error: 0.0062\n",
      "\n",
      "Epoch 07695: loss did not improve\n",
      "Epoch 7696/10000\n",
      "2700/2700 [==============================] - 0s 28us/step - loss: 1.2177e-04 - mean_absolute_error: 0.0063\n",
      "\n",
      "Epoch 07696: loss did not improve\n",
      "Epoch 7697/10000\n",
      "2700/2700 [==============================] - 0s 31us/step - loss: 1.2570e-04 - mean_absolute_error: 0.0067\n",
      "\n",
      "Epoch 07697: loss did not improve\n",
      "Epoch 7698/10000\n",
      "2700/2700 [==============================] - 0s 31us/step - loss: 1.2578e-04 - mean_absolute_error: 0.0066\n",
      "\n",
      "Epoch 07698: loss did not improve\n",
      "Epoch 7699/10000\n",
      "2700/2700 [==============================] - 0s 31us/step - loss: 1.2497e-04 - mean_absolute_error: 0.0066\n",
      "\n",
      "Epoch 07699: loss did not improve\n",
      "Epoch 7700/10000\n",
      "2700/2700 [==============================] - 0s 27us/step - loss: 1.2552e-04 - mean_absolute_error: 0.0066\n",
      "\n",
      "Epoch 07700: loss did not improve\n",
      "Epoch 7701/10000\n",
      "2700/2700 [==============================] - 0s 33us/step - loss: 1.2251e-04 - mean_absolute_error: 0.0064\n",
      "\n",
      "Epoch 07701: loss did not improve\n",
      "Epoch 7702/10000\n",
      "2700/2700 [==============================] - 0s 31us/step - loss: 1.2356e-04 - mean_absolute_error: 0.0066\n",
      "\n",
      "Epoch 07702: loss did not improve\n",
      "Epoch 7703/10000\n",
      "2700/2700 [==============================] - 0s 29us/step - loss: 1.2342e-04 - mean_absolute_error: 0.0065\n",
      "\n",
      "Epoch 07703: loss did not improve\n",
      "Epoch 7704/10000\n",
      "2700/2700 [==============================] - 0s 28us/step - loss: 1.2484e-04 - mean_absolute_error: 0.0066\n",
      "\n",
      "Epoch 07704: loss did not improve\n",
      "Epoch 7705/10000\n",
      "2700/2700 [==============================] - 0s 31us/step - loss: 1.2220e-04 - mean_absolute_error: 0.0065\n",
      "\n",
      "Epoch 07705: loss did not improve\n",
      "Epoch 7706/10000\n",
      "2700/2700 [==============================] - 0s 29us/step - loss: 1.2344e-04 - mean_absolute_error: 0.0064\n",
      "\n",
      "Epoch 07706: loss did not improve\n",
      "Epoch 7707/10000\n",
      "2700/2700 [==============================] - 0s 28us/step - loss: 1.2249e-04 - mean_absolute_error: 0.0065\n",
      "\n",
      "Epoch 07707: loss did not improve\n",
      "Epoch 7708/10000\n",
      "2700/2700 [==============================] - 0s 28us/step - loss: 1.2275e-04 - mean_absolute_error: 0.0064\n",
      "\n",
      "Epoch 07708: loss did not improve\n",
      "Epoch 7709/10000\n",
      "2700/2700 [==============================] - 0s 31us/step - loss: 1.2120e-04 - mean_absolute_error: 0.0064\n",
      "\n",
      "Epoch 07709: loss did not improve\n",
      "Epoch 7710/10000\n",
      "2700/2700 [==============================] - 0s 27us/step - loss: 1.2379e-04 - mean_absolute_error: 0.0065\n",
      "\n",
      "Epoch 07710: loss did not improve\n",
      "Epoch 7711/10000\n",
      "2700/2700 [==============================] - 0s 28us/step - loss: 1.2403e-04 - mean_absolute_error: 0.0065\n",
      "\n",
      "Epoch 07711: loss did not improve\n",
      "Epoch 7712/10000\n",
      "2700/2700 [==============================] - 0s 27us/step - loss: 1.2269e-04 - mean_absolute_error: 0.0064\n",
      "\n",
      "Epoch 07712: loss did not improve\n",
      "Epoch 7713/10000\n",
      "2700/2700 [==============================] - 0s 27us/step - loss: 1.2194e-04 - mean_absolute_error: 0.0063\n",
      "\n",
      "Epoch 07713: loss did not improve\n",
      "Epoch 7714/10000\n",
      "2700/2700 [==============================] - 0s 30us/step - loss: 1.2215e-04 - mean_absolute_error: 0.0064\n",
      "\n",
      "Epoch 07714: loss did not improve\n",
      "Epoch 7715/10000\n",
      "2700/2700 [==============================] - 0s 29us/step - loss: 1.2433e-04 - mean_absolute_error: 0.0066\n",
      "\n",
      "Epoch 07715: loss did not improve\n",
      "Epoch 7716/10000\n",
      "2700/2700 [==============================] - 0s 29us/step - loss: 1.2489e-04 - mean_absolute_error: 0.0066\n",
      "\n",
      "Epoch 07716: loss did not improve\n",
      "Epoch 7717/10000\n",
      "2700/2700 [==============================] - 0s 28us/step - loss: 1.2395e-04 - mean_absolute_error: 0.0065\n",
      "\n",
      "Epoch 07717: loss did not improve\n",
      "Epoch 7718/10000\n",
      "2700/2700 [==============================] - 0s 28us/step - loss: 1.2712e-04 - mean_absolute_error: 0.0068\n",
      "\n",
      "Epoch 07718: loss did not improve\n",
      "Epoch 7719/10000\n",
      "2700/2700 [==============================] - 0s 30us/step - loss: 1.2988e-04 - mean_absolute_error: 0.0070\n",
      "\n",
      "Epoch 07719: loss did not improve\n",
      "Epoch 7720/10000\n",
      "2700/2700 [==============================] - 0s 29us/step - loss: 1.2503e-04 - mean_absolute_error: 0.0066\n",
      "\n",
      "Epoch 07720: loss did not improve\n",
      "Epoch 7721/10000\n",
      "2700/2700 [==============================] - 0s 31us/step - loss: 1.2312e-04 - mean_absolute_error: 0.0065\n",
      "\n",
      "Epoch 07721: loss did not improve\n",
      "Epoch 7722/10000\n",
      "2700/2700 [==============================] - 0s 28us/step - loss: 1.2143e-04 - mean_absolute_error: 0.0062\n",
      "\n",
      "Epoch 07722: loss did not improve\n",
      "Epoch 7723/10000\n",
      "2700/2700 [==============================] - 0s 28us/step - loss: 1.2355e-04 - mean_absolute_error: 0.0066\n",
      "\n",
      "Epoch 07723: loss did not improve\n",
      "Epoch 7724/10000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2700/2700 [==============================] - 0s 32us/step - loss: 1.2286e-04 - mean_absolute_error: 0.0064\n",
      "\n",
      "Epoch 07724: loss did not improve\n",
      "Epoch 7725/10000\n",
      "2700/2700 [==============================] - 0s 28us/step - loss: 1.2127e-04 - mean_absolute_error: 0.0063\n",
      "\n",
      "Epoch 07725: loss did not improve\n",
      "Epoch 7726/10000\n",
      "2700/2700 [==============================] - 0s 27us/step - loss: 1.2663e-04 - mean_absolute_error: 0.0068\n",
      "\n",
      "Epoch 07726: loss did not improve\n",
      "Epoch 7727/10000\n",
      "2700/2700 [==============================] - 0s 29us/step - loss: 1.2225e-04 - mean_absolute_error: 0.0064\n",
      "\n",
      "Epoch 07727: loss did not improve\n",
      "Epoch 7728/10000\n",
      "2700/2700 [==============================] - 0s 28us/step - loss: 1.2733e-04 - mean_absolute_error: 0.0068\n",
      "\n",
      "Epoch 07728: loss did not improve\n",
      "Epoch 7729/10000\n",
      "2700/2700 [==============================] - 0s 28us/step - loss: 1.2234e-04 - mean_absolute_error: 0.0064\n",
      "\n",
      "Epoch 07729: loss did not improve\n",
      "Epoch 7730/10000\n",
      "2700/2700 [==============================] - 0s 28us/step - loss: 1.2397e-04 - mean_absolute_error: 0.0065\n",
      "\n",
      "Epoch 07730: loss did not improve\n",
      "Epoch 7731/10000\n",
      "2700/2700 [==============================] - 0s 28us/step - loss: 1.2470e-04 - mean_absolute_error: 0.0066\n",
      "\n",
      "Epoch 07731: loss did not improve\n",
      "Epoch 7732/10000\n",
      "2700/2700 [==============================] - 0s 28us/step - loss: 1.2176e-04 - mean_absolute_error: 0.0064\n",
      "\n",
      "Epoch 07732: loss did not improve\n",
      "Epoch 7733/10000\n",
      "2700/2700 [==============================] - 0s 31us/step - loss: 1.2506e-04 - mean_absolute_error: 0.0066\n",
      "\n",
      "Epoch 07733: loss did not improve\n",
      "Epoch 7734/10000\n",
      "2700/2700 [==============================] - 0s 30us/step - loss: 1.2706e-04 - mean_absolute_error: 0.0067\n",
      "\n",
      "Epoch 07734: loss did not improve\n",
      "Epoch 7735/10000\n",
      "2700/2700 [==============================] - 0s 32us/step - loss: 1.2138e-04 - mean_absolute_error: 0.0063\n",
      "\n",
      "Epoch 07735: loss did not improve\n",
      "Epoch 7736/10000\n",
      "2700/2700 [==============================] - 0s 30us/step - loss: 1.2300e-04 - mean_absolute_error: 0.0065\n",
      "\n",
      "Epoch 07736: loss did not improve\n",
      "Epoch 7737/10000\n",
      "2700/2700 [==============================] - 0s 33us/step - loss: 1.2201e-04 - mean_absolute_error: 0.0062\n",
      "\n",
      "Epoch 07737: loss did not improve\n",
      "Epoch 7738/10000\n",
      "2700/2700 [==============================] - 0s 26us/step - loss: 1.2179e-04 - mean_absolute_error: 0.0064\n",
      "\n",
      "Epoch 07738: loss did not improve\n",
      "Epoch 7739/10000\n",
      "2700/2700 [==============================] - 0s 28us/step - loss: 1.2481e-04 - mean_absolute_error: 0.0067\n",
      "\n",
      "Epoch 07739: loss did not improve\n",
      "Epoch 7740/10000\n",
      "2700/2700 [==============================] - 0s 29us/step - loss: 1.2519e-04 - mean_absolute_error: 0.0066\n",
      "\n",
      "Epoch 07740: loss did not improve\n",
      "Epoch 7741/10000\n",
      "2700/2700 [==============================] - 0s 30us/step - loss: 1.2239e-04 - mean_absolute_error: 0.0064\n",
      "\n",
      "Epoch 07741: loss did not improve\n",
      "Epoch 7742/10000\n",
      "2700/2700 [==============================] - 0s 26us/step - loss: 1.2159e-04 - mean_absolute_error: 0.0063\n",
      "\n",
      "Epoch 07742: loss did not improve\n",
      "Epoch 7743/10000\n",
      "2700/2700 [==============================] - 0s 31us/step - loss: 1.3086e-04 - mean_absolute_error: 0.0071\n",
      "\n",
      "Epoch 07743: loss did not improve\n",
      "Epoch 7744/10000\n",
      "2700/2700 [==============================] - 0s 30us/step - loss: 1.2172e-04 - mean_absolute_error: 0.0063\n",
      "\n",
      "Epoch 07744: loss did not improve\n",
      "Epoch 7745/10000\n",
      "2700/2700 [==============================] - 0s 28us/step - loss: 1.2418e-04 - mean_absolute_error: 0.0065\n",
      "\n",
      "Epoch 07745: loss did not improve\n",
      "Epoch 7746/10000\n",
      "2700/2700 [==============================] - 0s 29us/step - loss: 1.2262e-04 - mean_absolute_error: 0.0065\n",
      "\n",
      "Epoch 07746: loss did not improve\n",
      "Epoch 7747/10000\n",
      "2700/2700 [==============================] - 0s 28us/step - loss: 1.2267e-04 - mean_absolute_error: 0.0065\n",
      "\n",
      "Epoch 07747: loss did not improve\n",
      "Epoch 7748/10000\n",
      "2700/2700 [==============================] - 0s 29us/step - loss: 1.2102e-04 - mean_absolute_error: 0.0063\n",
      "\n",
      "Epoch 07748: loss did not improve\n",
      "Epoch 7749/10000\n",
      "2700/2700 [==============================] - 0s 28us/step - loss: 1.2209e-04 - mean_absolute_error: 0.0063\n",
      "\n",
      "Epoch 07749: loss did not improve\n",
      "Epoch 7750/10000\n",
      "2700/2700 [==============================] - 0s 30us/step - loss: 1.2561e-04 - mean_absolute_error: 0.0066\n",
      "\n",
      "Epoch 07750: loss did not improve\n",
      "Epoch 7751/10000\n",
      "2700/2700 [==============================] - 0s 28us/step - loss: 1.2306e-04 - mean_absolute_error: 0.0065\n",
      "\n",
      "Epoch 07751: loss did not improve\n",
      "Epoch 7752/10000\n",
      "2700/2700 [==============================] - 0s 26us/step - loss: 1.2363e-04 - mean_absolute_error: 0.0065\n",
      "\n",
      "Epoch 07752: loss did not improve\n",
      "Epoch 7753/10000\n",
      "2700/2700 [==============================] - 0s 31us/step - loss: 1.2143e-04 - mean_absolute_error: 0.0063\n",
      "\n",
      "Epoch 07753: loss did not improve\n",
      "Epoch 7754/10000\n",
      "2700/2700 [==============================] - 0s 31us/step - loss: 1.2271e-04 - mean_absolute_error: 0.0063\n",
      "\n",
      "Epoch 07754: loss did not improve\n",
      "Epoch 7755/10000\n",
      "2700/2700 [==============================] - 0s 30us/step - loss: 1.2044e-04 - mean_absolute_error: 0.0062\n",
      "\n",
      "Epoch 07755: loss did not improve\n",
      "Epoch 7756/10000\n",
      "2700/2700 [==============================] - 0s 30us/step - loss: 1.2173e-04 - mean_absolute_error: 0.0063\n",
      "\n",
      "Epoch 07756: loss did not improve\n",
      "Epoch 7757/10000\n",
      "2700/2700 [==============================] - 0s 30us/step - loss: 1.2354e-04 - mean_absolute_error: 0.0065\n",
      "\n",
      "Epoch 07757: loss did not improve\n",
      "Epoch 7758/10000\n",
      "2700/2700 [==============================] - 0s 31us/step - loss: 1.2265e-04 - mean_absolute_error: 0.0065\n",
      "\n",
      "Epoch 07758: loss did not improve\n",
      "Epoch 7759/10000\n",
      "2700/2700 [==============================] - 0s 31us/step - loss: 1.2256e-04 - mean_absolute_error: 0.0064\n",
      "\n",
      "Epoch 07759: loss did not improve\n",
      "Epoch 7760/10000\n",
      "2700/2700 [==============================] - 0s 31us/step - loss: 1.2241e-04 - mean_absolute_error: 0.0064\n",
      "\n",
      "Epoch 07760: loss did not improve\n",
      "Epoch 7761/10000\n",
      "2700/2700 [==============================] - 0s 29us/step - loss: 1.2298e-04 - mean_absolute_error: 0.0065\n",
      "\n",
      "Epoch 07761: loss did not improve\n",
      "Epoch 7762/10000\n",
      "2700/2700 [==============================] - 0s 27us/step - loss: 1.2240e-04 - mean_absolute_error: 0.0064\n",
      "\n",
      "Epoch 07762: loss did not improve\n",
      "Epoch 7763/10000\n",
      "2700/2700 [==============================] - 0s 28us/step - loss: 1.2235e-04 - mean_absolute_error: 0.0064\n",
      "\n",
      "Epoch 07763: loss did not improve\n",
      "Epoch 7764/10000\n",
      "2700/2700 [==============================] - 0s 28us/step - loss: 1.2581e-04 - mean_absolute_error: 0.0067\n",
      "\n",
      "Epoch 07764: loss did not improve\n",
      "Epoch 7765/10000\n",
      "2700/2700 [==============================] - 0s 32us/step - loss: 1.2204e-04 - mean_absolute_error: 0.0064\n",
      "\n",
      "Epoch 07765: loss did not improve\n",
      "Epoch 7766/10000\n",
      "2700/2700 [==============================] - 0s 28us/step - loss: 1.2552e-04 - mean_absolute_error: 0.0067\n",
      "\n",
      "Epoch 07766: loss did not improve\n",
      "Epoch 7767/10000\n",
      "2700/2700 [==============================] - 0s 26us/step - loss: 1.2170e-04 - mean_absolute_error: 0.0063\n",
      "\n",
      "Epoch 07767: loss did not improve\n",
      "Epoch 7768/10000\n",
      "2700/2700 [==============================] - 0s 30us/step - loss: 1.2570e-04 - mean_absolute_error: 0.0067\n",
      "\n",
      "Epoch 07768: loss did not improve\n",
      "Epoch 7769/10000\n",
      "2700/2700 [==============================] - 0s 30us/step - loss: 1.2219e-04 - mean_absolute_error: 0.0063\n",
      "\n",
      "Epoch 07769: loss did not improve\n",
      "Epoch 7770/10000\n",
      "2700/2700 [==============================] - 0s 30us/step - loss: 1.2237e-04 - mean_absolute_error: 0.0064\n",
      "\n",
      "Epoch 07770: loss did not improve\n",
      "Epoch 7771/10000\n",
      "2700/2700 [==============================] - 0s 26us/step - loss: 1.2398e-04 - mean_absolute_error: 0.0065\n",
      "\n",
      "Epoch 07771: loss did not improve\n",
      "Epoch 7772/10000\n",
      "2700/2700 [==============================] - 0s 33us/step - loss: 1.2271e-04 - mean_absolute_error: 0.0064\n",
      "\n",
      "Epoch 07772: loss did not improve\n",
      "Epoch 7773/10000\n",
      "2700/2700 [==============================] - 0s 29us/step - loss: 1.2437e-04 - mean_absolute_error: 0.0065\n",
      "\n",
      "Epoch 07773: loss did not improve\n",
      "Epoch 7774/10000\n",
      "2700/2700 [==============================] - 0s 29us/step - loss: 1.2226e-04 - mean_absolute_error: 0.0064\n",
      "\n",
      "Epoch 07774: loss did not improve\n",
      "Epoch 7775/10000\n",
      "2700/2700 [==============================] - 0s 35us/step - loss: 1.2298e-04 - mean_absolute_error: 0.0066\n",
      "\n",
      "Epoch 07775: loss did not improve\n",
      "Epoch 7776/10000\n",
      "2700/2700 [==============================] - 0s 31us/step - loss: 1.2206e-04 - mean_absolute_error: 0.0063\n",
      "\n",
      "Epoch 07776: loss did not improve\n",
      "Epoch 7777/10000\n",
      "2700/2700 [==============================] - 0s 26us/step - loss: 1.2677e-04 - mean_absolute_error: 0.0067\n",
      "\n",
      "Epoch 07777: loss did not improve\n",
      "Epoch 7778/10000\n",
      "2700/2700 [==============================] - 0s 31us/step - loss: 1.2329e-04 - mean_absolute_error: 0.0064\n",
      "\n",
      "Epoch 07778: loss did not improve\n",
      "Epoch 7779/10000\n",
      "2700/2700 [==============================] - 0s 28us/step - loss: 1.2329e-04 - mean_absolute_error: 0.0065\n",
      "\n",
      "Epoch 07779: loss did not improve\n",
      "Epoch 7780/10000\n",
      "2700/2700 [==============================] - 0s 29us/step - loss: 1.2131e-04 - mean_absolute_error: 0.0063\n",
      "\n",
      "Epoch 07780: loss did not improve\n",
      "Epoch 7781/10000\n",
      "2700/2700 [==============================] - 0s 32us/step - loss: 1.2253e-04 - mean_absolute_error: 0.0065\n",
      "\n",
      "Epoch 07781: loss did not improve\n",
      "Epoch 7782/10000\n",
      "2700/2700 [==============================] - 0s 29us/step - loss: 1.2660e-04 - mean_absolute_error: 0.0067\n",
      "\n",
      "Epoch 07782: loss did not improve\n",
      "Epoch 7783/10000\n",
      "2700/2700 [==============================] - 0s 30us/step - loss: 1.2748e-04 - mean_absolute_error: 0.0068\n",
      "\n",
      "Epoch 07783: loss did not improve\n",
      "Epoch 7784/10000\n",
      "2700/2700 [==============================] - 0s 29us/step - loss: 1.2758e-04 - mean_absolute_error: 0.0068\n",
      "\n",
      "Epoch 07784: loss did not improve\n",
      "Epoch 7785/10000\n",
      "2700/2700 [==============================] - 0s 30us/step - loss: 1.2303e-04 - mean_absolute_error: 0.0064\n",
      "\n",
      "Epoch 07785: loss did not improve\n",
      "Epoch 7786/10000\n",
      "2700/2700 [==============================] - 0s 29us/step - loss: 1.2275e-04 - mean_absolute_error: 0.0064\n",
      "\n",
      "Epoch 07786: loss did not improve\n",
      "Epoch 7787/10000\n",
      "2700/2700 [==============================] - 0s 27us/step - loss: 1.2226e-04 - mean_absolute_error: 0.0063\n",
      "\n",
      "Epoch 07787: loss did not improve\n",
      "Epoch 7788/10000\n",
      "2700/2700 [==============================] - 0s 30us/step - loss: 1.2206e-04 - mean_absolute_error: 0.0064\n",
      "\n",
      "Epoch 07788: loss did not improve\n",
      "Epoch 7789/10000\n",
      "2700/2700 [==============================] - 0s 29us/step - loss: 1.2467e-04 - mean_absolute_error: 0.0068\n",
      "\n",
      "Epoch 07789: loss did not improve\n",
      "Epoch 7790/10000\n",
      "2700/2700 [==============================] - 0s 28us/step - loss: 1.2383e-04 - mean_absolute_error: 0.0065\n",
      "\n",
      "Epoch 07790: loss did not improve\n",
      "Epoch 7791/10000\n",
      "2700/2700 [==============================] - 0s 27us/step - loss: 1.2527e-04 - mean_absolute_error: 0.0066\n",
      "\n",
      "Epoch 07791: loss did not improve\n",
      "Epoch 7792/10000\n",
      "2700/2700 [==============================] - 0s 29us/step - loss: 1.2626e-04 - mean_absolute_error: 0.0067\n",
      "\n",
      "Epoch 07792: loss did not improve\n",
      "Epoch 7793/10000\n",
      "2700/2700 [==============================] - 0s 29us/step - loss: 1.2382e-04 - mean_absolute_error: 0.0065\n",
      "\n",
      "Epoch 07793: loss did not improve\n",
      "Epoch 7794/10000\n",
      "2700/2700 [==============================] - 0s 30us/step - loss: 1.2412e-04 - mean_absolute_error: 0.0067\n",
      "\n",
      "Epoch 07794: loss did not improve\n",
      "Epoch 7795/10000\n",
      "2700/2700 [==============================] - 0s 30us/step - loss: 1.2264e-04 - mean_absolute_error: 0.0064\n",
      "\n",
      "Epoch 07795: loss did not improve\n",
      "Epoch 7796/10000\n",
      "2700/2700 [==============================] - 0s 26us/step - loss: 1.2115e-04 - mean_absolute_error: 0.0063\n",
      "\n",
      "Epoch 07796: loss did not improve\n",
      "Epoch 7797/10000\n",
      "2700/2700 [==============================] - 0s 28us/step - loss: 1.2103e-04 - mean_absolute_error: 0.0062\n",
      "\n",
      "Epoch 07797: loss did not improve\n",
      "Epoch 7798/10000\n",
      "2700/2700 [==============================] - 0s 29us/step - loss: 1.2405e-04 - mean_absolute_error: 0.0065\n",
      "\n",
      "Epoch 07798: loss did not improve\n",
      "Epoch 7799/10000\n",
      "2700/2700 [==============================] - 0s 27us/step - loss: 1.2296e-04 - mean_absolute_error: 0.0065\n",
      "\n",
      "Epoch 07799: loss did not improve\n",
      "Epoch 7800/10000\n",
      "2700/2700 [==============================] - 0s 30us/step - loss: 1.2327e-04 - mean_absolute_error: 0.0064\n",
      "\n",
      "Epoch 07800: loss did not improve\n",
      "Epoch 7801/10000\n",
      "2700/2700 [==============================] - 0s 31us/step - loss: 1.2106e-04 - mean_absolute_error: 0.0063\n",
      "\n",
      "Epoch 07801: loss did not improve\n",
      "Epoch 7802/10000\n",
      "2700/2700 [==============================] - 0s 29us/step - loss: 1.2324e-04 - mean_absolute_error: 0.0064\n",
      "\n",
      "Epoch 07802: loss did not improve\n",
      "Epoch 7803/10000\n",
      "2700/2700 [==============================] - 0s 33us/step - loss: 1.2229e-04 - mean_absolute_error: 0.0064\n",
      "\n",
      "Epoch 07803: loss did not improve\n",
      "Epoch 7804/10000\n",
      "2700/2700 [==============================] - 0s 30us/step - loss: 1.2490e-04 - mean_absolute_error: 0.0067\n",
      "\n",
      "Epoch 07804: loss did not improve\n",
      "Epoch 7805/10000\n",
      "2700/2700 [==============================] - 0s 30us/step - loss: 1.2457e-04 - mean_absolute_error: 0.0067\n",
      "\n",
      "Epoch 07805: loss did not improve\n",
      "Epoch 7806/10000\n",
      "2700/2700 [==============================] - 0s 29us/step - loss: 1.2393e-04 - mean_absolute_error: 0.0065\n",
      "\n",
      "Epoch 07806: loss did not improve\n",
      "Epoch 7807/10000\n",
      "2700/2700 [==============================] - 0s 30us/step - loss: 1.2182e-04 - mean_absolute_error: 0.0063\n",
      "\n",
      "Epoch 07807: loss did not improve\n",
      "Epoch 7808/10000\n",
      "2700/2700 [==============================] - 0s 29us/step - loss: 1.2195e-04 - mean_absolute_error: 0.0064\n",
      "\n",
      "Epoch 07808: loss did not improve\n",
      "Epoch 7809/10000\n",
      "2700/2700 [==============================] - 0s 29us/step - loss: 1.2059e-04 - mean_absolute_error: 0.0062\n",
      "\n",
      "Epoch 07809: loss did not improve\n",
      "Epoch 7810/10000\n",
      "2700/2700 [==============================] - 0s 27us/step - loss: 1.2282e-04 - mean_absolute_error: 0.0065\n",
      "\n",
      "Epoch 07810: loss did not improve\n",
      "Epoch 7811/10000\n",
      "2700/2700 [==============================] - 0s 31us/step - loss: 1.2365e-04 - mean_absolute_error: 0.0065\n",
      "\n",
      "Epoch 07811: loss did not improve\n",
      "Epoch 7812/10000\n",
      "2700/2700 [==============================] - 0s 31us/step - loss: 1.2161e-04 - mean_absolute_error: 0.0063\n",
      "\n",
      "Epoch 07812: loss did not improve\n",
      "Epoch 7813/10000\n",
      "2700/2700 [==============================] - 0s 30us/step - loss: 1.2309e-04 - mean_absolute_error: 0.0065\n",
      "\n",
      "Epoch 07813: loss did not improve\n",
      "Epoch 7814/10000\n",
      "2700/2700 [==============================] - 0s 28us/step - loss: 1.2089e-04 - mean_absolute_error: 0.0062\n",
      "\n",
      "Epoch 07814: loss did not improve\n",
      "Epoch 7815/10000\n",
      "2700/2700 [==============================] - 0s 30us/step - loss: 1.2952e-04 - mean_absolute_error: 0.0070\n",
      "\n",
      "Epoch 07815: loss did not improve\n",
      "Epoch 7816/10000\n",
      "2700/2700 [==============================] - 0s 28us/step - loss: 1.2334e-04 - mean_absolute_error: 0.0064\n",
      "\n",
      "Epoch 07816: loss did not improve\n",
      "Epoch 7817/10000\n",
      "2700/2700 [==============================] - 0s 29us/step - loss: 1.2278e-04 - mean_absolute_error: 0.0066\n",
      "\n",
      "Epoch 07817: loss did not improve\n",
      "Epoch 7818/10000\n",
      "2700/2700 [==============================] - 0s 29us/step - loss: 1.2251e-04 - mean_absolute_error: 0.0064\n",
      "\n",
      "Epoch 07818: loss did not improve\n",
      "Epoch 7819/10000\n",
      "2700/2700 [==============================] - 0s 31us/step - loss: 1.2375e-04 - mean_absolute_error: 0.0065\n",
      "\n",
      "Epoch 07819: loss did not improve\n",
      "Epoch 7820/10000\n",
      "2700/2700 [==============================] - 0s 26us/step - loss: 1.2106e-04 - mean_absolute_error: 0.0063\n",
      "\n",
      "Epoch 07820: loss did not improve\n",
      "Epoch 7821/10000\n",
      "2700/2700 [==============================] - 0s 32us/step - loss: 1.2286e-04 - mean_absolute_error: 0.0065\n",
      "\n",
      "Epoch 07821: loss did not improve\n",
      "Epoch 7822/10000\n",
      "2700/2700 [==============================] - 0s 30us/step - loss: 1.2669e-04 - mean_absolute_error: 0.0068\n",
      "\n",
      "Epoch 07822: loss did not improve\n",
      "Epoch 7823/10000\n",
      "2700/2700 [==============================] - 0s 30us/step - loss: 1.2287e-04 - mean_absolute_error: 0.0065\n",
      "\n",
      "Epoch 07823: loss did not improve\n",
      "Epoch 7824/10000\n",
      "2700/2700 [==============================] - 0s 28us/step - loss: 1.2119e-04 - mean_absolute_error: 0.0062\n",
      "\n",
      "Epoch 07824: loss did not improve\n",
      "Epoch 7825/10000\n",
      "2700/2700 [==============================] - 0s 26us/step - loss: 1.2404e-04 - mean_absolute_error: 0.0065\n",
      "\n",
      "Epoch 07825: loss did not improve\n",
      "Epoch 7826/10000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2700/2700 [==============================] - 0s 37us/step - loss: 1.2363e-04 - mean_absolute_error: 0.0066\n",
      "\n",
      "Epoch 07826: loss did not improve\n",
      "Epoch 7827/10000\n",
      "2700/2700 [==============================] - 0s 31us/step - loss: 1.2184e-04 - mean_absolute_error: 0.0064\n",
      "\n",
      "Epoch 07827: loss did not improve\n",
      "Epoch 7828/10000\n",
      "2700/2700 [==============================] - 0s 31us/step - loss: 1.2240e-04 - mean_absolute_error: 0.0063\n",
      "\n",
      "Epoch 07828: loss did not improve\n",
      "Epoch 7829/10000\n",
      "2700/2700 [==============================] - 0s 33us/step - loss: 1.2318e-04 - mean_absolute_error: 0.0065\n",
      "\n",
      "Epoch 07829: loss did not improve\n",
      "Epoch 7830/10000\n",
      "2700/2700 [==============================] - 0s 29us/step - loss: 1.2191e-04 - mean_absolute_error: 0.0063\n",
      "\n",
      "Epoch 07830: loss did not improve\n",
      "Epoch 7831/10000\n",
      "2700/2700 [==============================] - 0s 26us/step - loss: 1.2469e-04 - mean_absolute_error: 0.0066\n",
      "\n",
      "Epoch 07831: loss did not improve\n",
      "Epoch 7832/10000\n",
      "2700/2700 [==============================] - 0s 28us/step - loss: 1.2058e-04 - mean_absolute_error: 0.0063\n",
      "\n",
      "Epoch 07832: loss did not improve\n",
      "Epoch 7833/10000\n",
      "2700/2700 [==============================] - 0s 39us/step - loss: 1.2647e-04 - mean_absolute_error: 0.0067\n",
      "\n",
      "Epoch 07833: loss did not improve\n",
      "Epoch 7834/10000\n",
      "2700/2700 [==============================] - 0s 29us/step - loss: 1.2146e-04 - mean_absolute_error: 0.0063\n",
      "\n",
      "Epoch 07834: loss did not improve\n",
      "Epoch 7835/10000\n",
      "2700/2700 [==============================] - 0s 29us/step - loss: 1.2352e-04 - mean_absolute_error: 0.0065\n",
      "\n",
      "Epoch 07835: loss did not improve\n",
      "Epoch 7836/10000\n",
      "2700/2700 [==============================] - 0s 29us/step - loss: 1.2194e-04 - mean_absolute_error: 0.0063\n",
      "\n",
      "Epoch 07836: loss did not improve\n",
      "Epoch 7837/10000\n",
      "2700/2700 [==============================] - 0s 27us/step - loss: 1.2173e-04 - mean_absolute_error: 0.0065\n",
      "\n",
      "Epoch 07837: loss did not improve\n",
      "Epoch 7838/10000\n",
      "2700/2700 [==============================] - 0s 28us/step - loss: 1.2211e-04 - mean_absolute_error: 0.0064\n",
      "\n",
      "Epoch 07838: loss did not improve\n",
      "Epoch 7839/10000\n",
      "2700/2700 [==============================] - 0s 28us/step - loss: 1.2627e-04 - mean_absolute_error: 0.0067\n",
      "\n",
      "Epoch 07839: loss did not improve\n",
      "Epoch 7840/10000\n",
      "2700/2700 [==============================] - 0s 28us/step - loss: 1.2377e-04 - mean_absolute_error: 0.0066\n",
      "\n",
      "Epoch 07840: loss did not improve\n",
      "Epoch 7841/10000\n",
      "2700/2700 [==============================] - 0s 27us/step - loss: 1.2219e-04 - mean_absolute_error: 0.0064\n",
      "\n",
      "Epoch 07841: loss did not improve\n",
      "Epoch 7842/10000\n",
      "2700/2700 [==============================] - 0s 27us/step - loss: 1.2373e-04 - mean_absolute_error: 0.0065\n",
      "\n",
      "Epoch 07842: loss did not improve\n",
      "Epoch 7843/10000\n",
      "2700/2700 [==============================] - 0s 29us/step - loss: 1.2120e-04 - mean_absolute_error: 0.0063\n",
      "\n",
      "Epoch 07843: loss did not improve\n",
      "Epoch 7844/10000\n",
      "2700/2700 [==============================] - 0s 28us/step - loss: 1.2225e-04 - mean_absolute_error: 0.0064\n",
      "\n",
      "Epoch 07844: loss did not improve\n",
      "Epoch 7845/10000\n",
      "2700/2700 [==============================] - 0s 27us/step - loss: 1.2260e-04 - mean_absolute_error: 0.0065\n",
      "\n",
      "Epoch 07845: loss did not improve\n",
      "Epoch 7846/10000\n",
      "2700/2700 [==============================] - 0s 29us/step - loss: 1.2220e-04 - mean_absolute_error: 0.0066\n",
      "\n",
      "Epoch 07846: loss did not improve\n",
      "Epoch 7847/10000\n",
      "2700/2700 [==============================] - 0s 29us/step - loss: 1.2322e-04 - mean_absolute_error: 0.0064\n",
      "\n",
      "Epoch 07847: loss did not improve\n",
      "Epoch 7848/10000\n",
      "2700/2700 [==============================] - 0s 29us/step - loss: 1.2266e-04 - mean_absolute_error: 0.0064\n",
      "\n",
      "Epoch 07848: loss did not improve\n",
      "Epoch 7849/10000\n",
      "2700/2700 [==============================] - 0s 30us/step - loss: 1.2305e-04 - mean_absolute_error: 0.0064\n",
      "\n",
      "Epoch 07849: loss did not improve\n",
      "Epoch 7850/10000\n",
      "2700/2700 [==============================] - 0s 27us/step - loss: 1.2800e-04 - mean_absolute_error: 0.0068\n",
      "\n",
      "Epoch 07850: loss did not improve\n",
      "Epoch 7851/10000\n",
      "2700/2700 [==============================] - 0s 30us/step - loss: 1.2611e-04 - mean_absolute_error: 0.0067\n",
      "\n",
      "Epoch 07851: loss did not improve\n",
      "Epoch 7852/10000\n",
      "2700/2700 [==============================] - 0s 30us/step - loss: 1.2178e-04 - mean_absolute_error: 0.0064\n",
      "\n",
      "Epoch 07852: loss did not improve\n",
      "Epoch 7853/10000\n",
      "2700/2700 [==============================] - 0s 29us/step - loss: 1.2148e-04 - mean_absolute_error: 0.0063\n",
      "\n",
      "Epoch 07853: loss did not improve\n",
      "Epoch 7854/10000\n",
      "2700/2700 [==============================] - 0s 31us/step - loss: 1.2383e-04 - mean_absolute_error: 0.0065\n",
      "\n",
      "Epoch 07854: loss did not improve\n",
      "Epoch 7855/10000\n",
      "2700/2700 [==============================] - 0s 35us/step - loss: 1.2223e-04 - mean_absolute_error: 0.0063\n",
      "\n",
      "Epoch 07855: loss did not improve\n",
      "Epoch 7856/10000\n",
      "2700/2700 [==============================] - 0s 38us/step - loss: 1.2136e-04 - mean_absolute_error: 0.0064\n",
      "\n",
      "Epoch 07856: loss did not improve\n",
      "Epoch 7857/10000\n",
      "2700/2700 [==============================] - 0s 38us/step - loss: 1.2706e-04 - mean_absolute_error: 0.0068\n",
      "\n",
      "Epoch 07857: loss did not improve\n",
      "Epoch 7858/10000\n",
      "2700/2700 [==============================] - 0s 40us/step - loss: 1.2538e-04 - mean_absolute_error: 0.0067\n",
      "\n",
      "Epoch 07858: loss did not improve\n",
      "Epoch 7859/10000\n",
      "2700/2700 [==============================] - 0s 32us/step - loss: 1.2349e-04 - mean_absolute_error: 0.0064\n",
      "\n",
      "Epoch 07859: loss did not improve\n",
      "Epoch 7860/10000\n",
      "2700/2700 [==============================] - 0s 27us/step - loss: 1.2255e-04 - mean_absolute_error: 0.0065\n",
      "\n",
      "Epoch 07860: loss did not improve\n",
      "Epoch 7861/10000\n",
      "2700/2700 [==============================] - 0s 28us/step - loss: 1.2323e-04 - mean_absolute_error: 0.0064\n",
      "\n",
      "Epoch 07861: loss did not improve\n",
      "Epoch 7862/10000\n",
      "2700/2700 [==============================] - 0s 29us/step - loss: 1.2229e-04 - mean_absolute_error: 0.0065\n",
      "\n",
      "Epoch 07862: loss did not improve\n",
      "Epoch 7863/10000\n",
      "2700/2700 [==============================] - 0s 28us/step - loss: 1.2052e-04 - mean_absolute_error: 0.0063\n",
      "\n",
      "Epoch 07863: loss did not improve\n",
      "Epoch 7864/10000\n",
      "2700/2700 [==============================] - 0s 27us/step - loss: 1.2070e-04 - mean_absolute_error: 0.0062\n",
      "\n",
      "Epoch 07864: loss did not improve\n",
      "Epoch 7865/10000\n",
      "2700/2700 [==============================] - 0s 30us/step - loss: 1.2465e-04 - mean_absolute_error: 0.0066\n",
      "\n",
      "Epoch 07865: loss did not improve\n",
      "Epoch 7866/10000\n",
      "2700/2700 [==============================] - 0s 29us/step - loss: 1.2180e-04 - mean_absolute_error: 0.0064\n",
      "\n",
      "Epoch 07866: loss did not improve\n",
      "Epoch 7867/10000\n",
      "2700/2700 [==============================] - 0s 31us/step - loss: 1.2668e-04 - mean_absolute_error: 0.0068\n",
      "\n",
      "Epoch 07867: loss did not improve\n",
      "Epoch 7868/10000\n",
      "2700/2700 [==============================] - 0s 31us/step - loss: 1.2185e-04 - mean_absolute_error: 0.0063\n",
      "\n",
      "Epoch 07868: loss did not improve\n",
      "Epoch 7869/10000\n",
      "2700/2700 [==============================] - 0s 29us/step - loss: 1.2453e-04 - mean_absolute_error: 0.0066\n",
      "\n",
      "Epoch 07869: loss did not improve\n",
      "Epoch 7870/10000\n",
      "2700/2700 [==============================] - 0s 29us/step - loss: 1.2417e-04 - mean_absolute_error: 0.0066\n",
      "\n",
      "Epoch 07870: loss did not improve\n",
      "Epoch 7871/10000\n",
      "2700/2700 [==============================] - 0s 32us/step - loss: 1.2907e-04 - mean_absolute_error: 0.0069\n",
      "\n",
      "Epoch 07871: loss did not improve\n",
      "Epoch 7872/10000\n",
      "2700/2700 [==============================] - 0s 33us/step - loss: 1.2190e-04 - mean_absolute_error: 0.0064\n",
      "\n",
      "Epoch 07872: loss did not improve\n",
      "Epoch 7873/10000\n",
      "2700/2700 [==============================] - 0s 29us/step - loss: 1.2478e-04 - mean_absolute_error: 0.0065\n",
      "\n",
      "Epoch 07873: loss did not improve\n",
      "Epoch 7874/10000\n",
      "2700/2700 [==============================] - 0s 28us/step - loss: 1.2730e-04 - mean_absolute_error: 0.0069\n",
      "\n",
      "Epoch 07874: loss did not improve\n",
      "Epoch 7875/10000\n",
      "2700/2700 [==============================] - 0s 27us/step - loss: 1.2199e-04 - mean_absolute_error: 0.0064\n",
      "\n",
      "Epoch 07875: loss did not improve\n",
      "Epoch 7876/10000\n",
      "2700/2700 [==============================] - 0s 29us/step - loss: 1.2142e-04 - mean_absolute_error: 0.0063\n",
      "\n",
      "Epoch 07876: loss did not improve\n",
      "Epoch 7877/10000\n",
      "2700/2700 [==============================] - 0s 32us/step - loss: 1.2223e-04 - mean_absolute_error: 0.0064\n",
      "\n",
      "Epoch 07877: loss did not improve\n",
      "Epoch 7878/10000\n",
      "2700/2700 [==============================] - 0s 30us/step - loss: 1.2202e-04 - mean_absolute_error: 0.0064\n",
      "\n",
      "Epoch 07878: loss did not improve\n",
      "Epoch 7879/10000\n",
      "2700/2700 [==============================] - 0s 29us/step - loss: 1.2355e-04 - mean_absolute_error: 0.0064\n",
      "\n",
      "Epoch 07879: loss did not improve\n",
      "Epoch 7880/10000\n",
      "2700/2700 [==============================] - 0s 27us/step - loss: 1.2187e-04 - mean_absolute_error: 0.0064\n",
      "\n",
      "Epoch 07880: loss did not improve\n",
      "Epoch 7881/10000\n",
      "2700/2700 [==============================] - 0s 29us/step - loss: 1.2253e-04 - mean_absolute_error: 0.0065\n",
      "\n",
      "Epoch 07881: loss did not improve\n",
      "Epoch 7882/10000\n",
      "2700/2700 [==============================] - 0s 30us/step - loss: 1.2265e-04 - mean_absolute_error: 0.0066\n",
      "\n",
      "Epoch 07882: loss did not improve\n",
      "Epoch 7883/10000\n",
      "2700/2700 [==============================] - 0s 27us/step - loss: 1.2279e-04 - mean_absolute_error: 0.0065\n",
      "\n",
      "Epoch 07883: loss did not improve\n",
      "Epoch 7884/10000\n",
      "2700/2700 [==============================] - 0s 28us/step - loss: 1.2429e-04 - mean_absolute_error: 0.0067\n",
      "\n",
      "Epoch 07884: loss did not improve\n",
      "Epoch 7885/10000\n",
      "2700/2700 [==============================] - 0s 31us/step - loss: 1.2282e-04 - mean_absolute_error: 0.0064\n",
      "\n",
      "Epoch 07885: loss did not improve\n",
      "Epoch 7886/10000\n",
      "2700/2700 [==============================] - 0s 29us/step - loss: 1.2172e-04 - mean_absolute_error: 0.0064\n",
      "\n",
      "Epoch 07886: loss did not improve\n",
      "Epoch 7887/10000\n",
      "2700/2700 [==============================] - 0s 30us/step - loss: 1.2100e-04 - mean_absolute_error: 0.0064\n",
      "\n",
      "Epoch 07887: loss did not improve\n",
      "Epoch 7888/10000\n",
      "2700/2700 [==============================] - 0s 28us/step - loss: 1.3160e-04 - mean_absolute_error: 0.0071\n",
      "\n",
      "Epoch 07888: loss did not improve\n",
      "Epoch 7889/10000\n",
      "2700/2700 [==============================] - 0s 26us/step - loss: 1.2079e-04 - mean_absolute_error: 0.0063\n",
      "\n",
      "Epoch 07889: loss did not improve\n",
      "Epoch 7890/10000\n",
      "2700/2700 [==============================] - 0s 29us/step - loss: 1.2296e-04 - mean_absolute_error: 0.0064\n",
      "\n",
      "Epoch 07890: loss did not improve\n",
      "Epoch 7891/10000\n",
      "2700/2700 [==============================] - 0s 28us/step - loss: 1.2338e-04 - mean_absolute_error: 0.0065\n",
      "\n",
      "Epoch 07891: loss did not improve\n",
      "Epoch 7892/10000\n",
      "2700/2700 [==============================] - 0s 31us/step - loss: 1.2082e-04 - mean_absolute_error: 0.0063\n",
      "\n",
      "Epoch 07892: loss did not improve\n",
      "Epoch 7893/10000\n",
      "2700/2700 [==============================] - 0s 31us/step - loss: 1.2219e-04 - mean_absolute_error: 0.0064\n",
      "\n",
      "Epoch 07893: loss did not improve\n",
      "Epoch 7894/10000\n",
      "2700/2700 [==============================] - 0s 29us/step - loss: 1.2460e-04 - mean_absolute_error: 0.0067\n",
      "\n",
      "Epoch 07894: loss did not improve\n",
      "Epoch 7895/10000\n",
      "2700/2700 [==============================] - 0s 28us/step - loss: 1.2299e-04 - mean_absolute_error: 0.0064\n",
      "\n",
      "Epoch 07895: loss did not improve\n",
      "Epoch 7896/10000\n",
      "2700/2700 [==============================] - 0s 29us/step - loss: 1.2245e-04 - mean_absolute_error: 0.0063\n",
      "\n",
      "Epoch 07896: loss did not improve\n",
      "Epoch 7897/10000\n",
      "2700/2700 [==============================] - 0s 30us/step - loss: 1.2047e-04 - mean_absolute_error: 0.0063\n",
      "\n",
      "Epoch 07897: loss did not improve\n",
      "Epoch 7898/10000\n",
      "2700/2700 [==============================] - 0s 33us/step - loss: 1.2494e-04 - mean_absolute_error: 0.0067\n",
      "\n",
      "Epoch 07898: loss did not improve\n",
      "Epoch 7899/10000\n",
      "2700/2700 [==============================] - 0s 32us/step - loss: 1.2127e-04 - mean_absolute_error: 0.0063\n",
      "\n",
      "Epoch 07899: loss did not improve\n",
      "Epoch 7900/10000\n",
      "2700/2700 [==============================] - 0s 30us/step - loss: 1.2436e-04 - mean_absolute_error: 0.0066\n",
      "\n",
      "Epoch 07900: loss did not improve\n",
      "Epoch 7901/10000\n",
      "2700/2700 [==============================] - 0s 28us/step - loss: 1.2378e-04 - mean_absolute_error: 0.0065\n",
      "\n",
      "Epoch 07901: loss did not improve\n",
      "Epoch 7902/10000\n",
      "2700/2700 [==============================] - 0s 30us/step - loss: 1.2750e-04 - mean_absolute_error: 0.0068\n",
      "\n",
      "Epoch 07902: loss did not improve\n",
      "Epoch 7903/10000\n",
      "2700/2700 [==============================] - 0s 32us/step - loss: 1.2301e-04 - mean_absolute_error: 0.0064\n",
      "\n",
      "Epoch 07903: loss did not improve\n",
      "Epoch 7904/10000\n",
      "2700/2700 [==============================] - 0s 27us/step - loss: 1.2302e-04 - mean_absolute_error: 0.0064\n",
      "\n",
      "Epoch 07904: loss did not improve\n",
      "Epoch 7905/10000\n",
      "2700/2700 [==============================] - 0s 30us/step - loss: 1.2034e-04 - mean_absolute_error: 0.0062\n",
      "\n",
      "Epoch 07905: loss did not improve\n",
      "Epoch 7906/10000\n",
      "2700/2700 [==============================] - 0s 30us/step - loss: 1.2200e-04 - mean_absolute_error: 0.0064\n",
      "\n",
      "Epoch 07906: loss did not improve\n",
      "Epoch 7907/10000\n",
      "2700/2700 [==============================] - 0s 29us/step - loss: 1.2254e-04 - mean_absolute_error: 0.0064\n",
      "\n",
      "Epoch 07907: loss did not improve\n",
      "Epoch 7908/10000\n",
      "2700/2700 [==============================] - 0s 29us/step - loss: 1.2167e-04 - mean_absolute_error: 0.0064\n",
      "\n",
      "Epoch 07908: loss did not improve\n",
      "Epoch 7909/10000\n",
      "2700/2700 [==============================] - 0s 27us/step - loss: 1.2172e-04 - mean_absolute_error: 0.0064\n",
      "\n",
      "Epoch 07909: loss did not improve\n",
      "Epoch 7910/10000\n",
      "2700/2700 [==============================] - 0s 29us/step - loss: 1.2342e-04 - mean_absolute_error: 0.0064\n",
      "\n",
      "Epoch 07910: loss did not improve\n",
      "Epoch 7911/10000\n",
      "2700/2700 [==============================] - 0s 30us/step - loss: 1.2494e-04 - mean_absolute_error: 0.0066\n",
      "\n",
      "Epoch 07911: loss did not improve\n",
      "Epoch 7912/10000\n",
      "2700/2700 [==============================] - 0s 31us/step - loss: 1.2446e-04 - mean_absolute_error: 0.0066\n",
      "\n",
      "Epoch 07912: loss did not improve\n",
      "Epoch 7913/10000\n",
      "2700/2700 [==============================] - 0s 30us/step - loss: 1.2301e-04 - mean_absolute_error: 0.0065\n",
      "\n",
      "Epoch 07913: loss did not improve\n",
      "Epoch 7914/10000\n",
      "2700/2700 [==============================] - 0s 27us/step - loss: 1.2221e-04 - mean_absolute_error: 0.0063\n",
      "\n",
      "Epoch 07914: loss did not improve\n",
      "Epoch 7915/10000\n",
      "2700/2700 [==============================] - 0s 31us/step - loss: 1.2249e-04 - mean_absolute_error: 0.0065\n",
      "\n",
      "Epoch 07915: loss did not improve\n",
      "Epoch 7916/10000\n",
      "2700/2700 [==============================] - 0s 30us/step - loss: 1.2180e-04 - mean_absolute_error: 0.0064\n",
      "\n",
      "Epoch 07916: loss did not improve\n",
      "Epoch 7917/10000\n",
      "2700/2700 [==============================] - 0s 31us/step - loss: 1.2553e-04 - mean_absolute_error: 0.0066\n",
      "\n",
      "Epoch 07917: loss did not improve\n",
      "Epoch 7918/10000\n",
      "2700/2700 [==============================] - 0s 33us/step - loss: 1.2293e-04 - mean_absolute_error: 0.0066\n",
      "\n",
      "Epoch 07918: loss did not improve\n",
      "Epoch 7919/10000\n",
      "2700/2700 [==============================] - 0s 27us/step - loss: 1.2320e-04 - mean_absolute_error: 0.0064\n",
      "\n",
      "Epoch 07919: loss did not improve\n",
      "Epoch 7920/10000\n",
      "2700/2700 [==============================] - 0s 28us/step - loss: 1.2328e-04 - mean_absolute_error: 0.0065\n",
      "\n",
      "Epoch 07920: loss did not improve\n",
      "Epoch 7921/10000\n",
      "2700/2700 [==============================] - 0s 30us/step - loss: 1.2517e-04 - mean_absolute_error: 0.0067\n",
      "\n",
      "Epoch 07921: loss did not improve\n",
      "Epoch 7922/10000\n",
      "2700/2700 [==============================] - 0s 29us/step - loss: 1.2401e-04 - mean_absolute_error: 0.0067\n",
      "\n",
      "Epoch 07922: loss did not improve\n",
      "Epoch 7923/10000\n",
      "2700/2700 [==============================] - 0s 29us/step - loss: 1.2116e-04 - mean_absolute_error: 0.0063\n",
      "\n",
      "Epoch 07923: loss did not improve\n",
      "Epoch 7924/10000\n",
      "2700/2700 [==============================] - 0s 28us/step - loss: 1.2411e-04 - mean_absolute_error: 0.0067\n",
      "\n",
      "Epoch 07924: loss did not improve\n",
      "Epoch 7925/10000\n",
      "2700/2700 [==============================] - 0s 34us/step - loss: 1.2399e-04 - mean_absolute_error: 0.0065\n",
      "\n",
      "Epoch 07925: loss did not improve\n",
      "Epoch 7926/10000\n",
      "2700/2700 [==============================] - 0s 37us/step - loss: 1.2228e-04 - mean_absolute_error: 0.0063\n",
      "\n",
      "Epoch 07926: loss did not improve\n",
      "Epoch 7927/10000\n",
      "2700/2700 [==============================] - 0s 29us/step - loss: 1.2437e-04 - mean_absolute_error: 0.0066\n",
      "\n",
      "Epoch 07927: loss did not improve\n",
      "Epoch 7928/10000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2700/2700 [==============================] - 0s 31us/step - loss: 1.2319e-04 - mean_absolute_error: 0.0065\n",
      "\n",
      "Epoch 07928: loss did not improve\n",
      "Epoch 7929/10000\n",
      "2700/2700 [==============================] - 0s 29us/step - loss: 1.2400e-04 - mean_absolute_error: 0.0066\n",
      "\n",
      "Epoch 07929: loss did not improve\n",
      "Epoch 7930/10000\n",
      "2700/2700 [==============================] - 0s 32us/step - loss: 1.2273e-04 - mean_absolute_error: 0.0065\n",
      "\n",
      "Epoch 07930: loss did not improve\n",
      "Epoch 7931/10000\n",
      "2700/2700 [==============================] - 0s 27us/step - loss: 1.2205e-04 - mean_absolute_error: 0.0064\n",
      "\n",
      "Epoch 07931: loss did not improve\n",
      "Epoch 7932/10000\n",
      "2700/2700 [==============================] - 0s 29us/step - loss: 1.2475e-04 - mean_absolute_error: 0.0066\n",
      "\n",
      "Epoch 07932: loss did not improve\n",
      "Epoch 7933/10000\n",
      "2700/2700 [==============================] - 0s 29us/step - loss: 1.2761e-04 - mean_absolute_error: 0.0069\n",
      "\n",
      "Epoch 07933: loss did not improve\n",
      "Epoch 7934/10000\n",
      "2700/2700 [==============================] - 0s 29us/step - loss: 1.2310e-04 - mean_absolute_error: 0.0063\n",
      "\n",
      "Epoch 07934: loss did not improve\n",
      "Epoch 7935/10000\n",
      "2700/2700 [==============================] - 0s 27us/step - loss: 1.2088e-04 - mean_absolute_error: 0.0062\n",
      "\n",
      "Epoch 07935: loss did not improve\n",
      "Epoch 7936/10000\n",
      "2700/2700 [==============================] - 0s 29us/step - loss: 1.2163e-04 - mean_absolute_error: 0.0063\n",
      "\n",
      "Epoch 07936: loss did not improve\n",
      "Epoch 7937/10000\n",
      "2700/2700 [==============================] - 0s 29us/step - loss: 1.2095e-04 - mean_absolute_error: 0.0063\n",
      "\n",
      "Epoch 07937: loss did not improve\n",
      "Epoch 7938/10000\n",
      "2700/2700 [==============================] - 0s 28us/step - loss: 1.2342e-04 - mean_absolute_error: 0.0065\n",
      "\n",
      "Epoch 07938: loss did not improve\n",
      "Epoch 7939/10000\n",
      "2700/2700 [==============================] - 0s 28us/step - loss: 1.2217e-04 - mean_absolute_error: 0.0064\n",
      "\n",
      "Epoch 07939: loss did not improve\n",
      "Epoch 7940/10000\n",
      "2700/2700 [==============================] - 0s 28us/step - loss: 1.2153e-04 - mean_absolute_error: 0.0063\n",
      "\n",
      "Epoch 07940: loss did not improve\n",
      "Epoch 7941/10000\n",
      "2700/2700 [==============================] - 0s 28us/step - loss: 1.2162e-04 - mean_absolute_error: 0.0064\n",
      "\n",
      "Epoch 07941: loss did not improve\n",
      "Epoch 7942/10000\n",
      "2700/2700 [==============================] - 0s 29us/step - loss: 1.2375e-04 - mean_absolute_error: 0.0066\n",
      "\n",
      "Epoch 07942: loss did not improve\n",
      "Epoch 7943/10000\n",
      "2700/2700 [==============================] - 0s 35us/step - loss: 1.2100e-04 - mean_absolute_error: 0.0062\n",
      "\n",
      "Epoch 07943: loss did not improve\n",
      "Epoch 7944/10000\n",
      "2700/2700 [==============================] - 0s 31us/step - loss: 1.2242e-04 - mean_absolute_error: 0.0065\n",
      "\n",
      "Epoch 07944: loss did not improve\n",
      "Epoch 7945/10000\n",
      "2700/2700 [==============================] - 0s 30us/step - loss: 1.2334e-04 - mean_absolute_error: 0.0064\n",
      "\n",
      "Epoch 07945: loss did not improve\n",
      "Epoch 7946/10000\n",
      "2700/2700 [==============================] - 0s 27us/step - loss: 1.2457e-04 - mean_absolute_error: 0.0067\n",
      "\n",
      "Epoch 07946: loss did not improve\n",
      "Epoch 7947/10000\n",
      "2700/2700 [==============================] - 0s 29us/step - loss: 1.2361e-04 - mean_absolute_error: 0.0066\n",
      "\n",
      "Epoch 07947: loss did not improve\n",
      "Epoch 7948/10000\n",
      "2700/2700 [==============================] - 0s 31us/step - loss: 1.2283e-04 - mean_absolute_error: 0.0064\n",
      "\n",
      "Epoch 07948: loss did not improve\n",
      "Epoch 7949/10000\n",
      "2700/2700 [==============================] - 0s 28us/step - loss: 1.2132e-04 - mean_absolute_error: 0.0063\n",
      "\n",
      "Epoch 07949: loss did not improve\n",
      "Epoch 7950/10000\n",
      "2700/2700 [==============================] - 0s 29us/step - loss: 1.2294e-04 - mean_absolute_error: 0.0065\n",
      "\n",
      "Epoch 07950: loss did not improve\n",
      "Epoch 7951/10000\n",
      "2700/2700 [==============================] - 0s 30us/step - loss: 1.2559e-04 - mean_absolute_error: 0.0067\n",
      "\n",
      "Epoch 07951: loss did not improve\n",
      "Epoch 7952/10000\n",
      "2700/2700 [==============================] - 0s 29us/step - loss: 1.2268e-04 - mean_absolute_error: 0.0065\n",
      "\n",
      "Epoch 07952: loss did not improve\n",
      "Epoch 7953/10000\n",
      "2700/2700 [==============================] - 0s 28us/step - loss: 1.2265e-04 - mean_absolute_error: 0.0064\n",
      "\n",
      "Epoch 07953: loss did not improve\n",
      "Epoch 7954/10000\n",
      "2700/2700 [==============================] - 0s 26us/step - loss: 1.2085e-04 - mean_absolute_error: 0.0064\n",
      "\n",
      "Epoch 07954: loss did not improve\n",
      "Epoch 7955/10000\n",
      "2700/2700 [==============================] - 0s 28us/step - loss: 1.2387e-04 - mean_absolute_error: 0.0065\n",
      "\n",
      "Epoch 07955: loss did not improve\n",
      "Epoch 7956/10000\n",
      "2700/2700 [==============================] - 0s 29us/step - loss: 1.2059e-04 - mean_absolute_error: 0.0062\n",
      "\n",
      "Epoch 07956: loss did not improve\n",
      "Epoch 7957/10000\n",
      "2700/2700 [==============================] - 0s 32us/step - loss: 1.2350e-04 - mean_absolute_error: 0.0065\n",
      "\n",
      "Epoch 07957: loss did not improve\n",
      "Epoch 7958/10000\n",
      "2700/2700 [==============================] - 0s 28us/step - loss: 1.2122e-04 - mean_absolute_error: 0.0064\n",
      "\n",
      "Epoch 07958: loss did not improve\n",
      "Epoch 7959/10000\n",
      "2700/2700 [==============================] - 0s 26us/step - loss: 1.2131e-04 - mean_absolute_error: 0.0064\n",
      "\n",
      "Epoch 07959: loss did not improve\n",
      "Epoch 7960/10000\n",
      "2700/2700 [==============================] - 0s 29us/step - loss: 1.2176e-04 - mean_absolute_error: 0.0063\n",
      "\n",
      "Epoch 07960: loss did not improve\n",
      "Epoch 7961/10000\n",
      "2700/2700 [==============================] - 0s 29us/step - loss: 1.2128e-04 - mean_absolute_error: 0.0063\n",
      "\n",
      "Epoch 07961: loss did not improve\n",
      "Epoch 7962/10000\n",
      "2700/2700 [==============================] - 0s 27us/step - loss: 1.2264e-04 - mean_absolute_error: 0.0064\n",
      "\n",
      "Epoch 07962: loss did not improve\n",
      "Epoch 7963/10000\n",
      "2700/2700 [==============================] - 0s 27us/step - loss: 1.2296e-04 - mean_absolute_error: 0.0065\n",
      "\n",
      "Epoch 07963: loss did not improve\n",
      "Epoch 7964/10000\n",
      "2700/2700 [==============================] - 0s 31us/step - loss: 1.2363e-04 - mean_absolute_error: 0.0067\n",
      "\n",
      "Epoch 07964: loss did not improve\n",
      "Epoch 7965/10000\n",
      "2700/2700 [==============================] - 0s 30us/step - loss: 1.2819e-04 - mean_absolute_error: 0.0069\n",
      "\n",
      "Epoch 07965: loss did not improve\n",
      "Epoch 7966/10000\n",
      "2700/2700 [==============================] - 0s 28us/step - loss: 1.2090e-04 - mean_absolute_error: 0.0063\n",
      "\n",
      "Epoch 07966: loss did not improve\n",
      "Epoch 7967/10000\n",
      "2700/2700 [==============================] - 0s 30us/step - loss: 1.2304e-04 - mean_absolute_error: 0.0065\n",
      "\n",
      "Epoch 07967: loss did not improve\n",
      "Epoch 7968/10000\n",
      "2700/2700 [==============================] - 0s 30us/step - loss: 1.2140e-04 - mean_absolute_error: 0.0063\n",
      "\n",
      "Epoch 07968: loss did not improve\n",
      "Epoch 7969/10000\n",
      "2700/2700 [==============================] - 0s 28us/step - loss: 1.2290e-04 - mean_absolute_error: 0.0064\n",
      "\n",
      "Epoch 07969: loss did not improve\n",
      "Epoch 7970/10000\n",
      "2700/2700 [==============================] - 0s 29us/step - loss: 1.2254e-04 - mean_absolute_error: 0.0064\n",
      "\n",
      "Epoch 07970: loss did not improve\n",
      "Epoch 7971/10000\n",
      "2700/2700 [==============================] - 0s 27us/step - loss: 1.2145e-04 - mean_absolute_error: 0.0064\n",
      "\n",
      "Epoch 07971: loss did not improve\n",
      "Epoch 7972/10000\n",
      "2700/2700 [==============================] - 0s 25us/step - loss: 1.2853e-04 - mean_absolute_error: 0.0069\n",
      "\n",
      "Epoch 07972: loss did not improve\n",
      "Epoch 7973/10000\n",
      "2700/2700 [==============================] - 0s 30us/step - loss: 1.2249e-04 - mean_absolute_error: 0.0064\n",
      "\n",
      "Epoch 07973: loss did not improve\n",
      "Epoch 7974/10000\n",
      "2700/2700 [==============================] - 0s 30us/step - loss: 1.2195e-04 - mean_absolute_error: 0.0064\n",
      "\n",
      "Epoch 07974: loss did not improve\n",
      "Epoch 7975/10000\n",
      "2700/2700 [==============================] - 0s 30us/step - loss: 1.2430e-04 - mean_absolute_error: 0.0066\n",
      "\n",
      "Epoch 07975: loss did not improve\n",
      "Epoch 7976/10000\n",
      "2700/2700 [==============================] - 0s 29us/step - loss: 1.2174e-04 - mean_absolute_error: 0.0063\n",
      "\n",
      "Epoch 07976: loss did not improve\n",
      "Epoch 7977/10000\n",
      "2700/2700 [==============================] - 0s 26us/step - loss: 1.2310e-04 - mean_absolute_error: 0.0066\n",
      "\n",
      "Epoch 07977: loss did not improve\n",
      "Epoch 7978/10000\n",
      "2700/2700 [==============================] - 0s 29us/step - loss: 1.2374e-04 - mean_absolute_error: 0.0066\n",
      "\n",
      "Epoch 07978: loss did not improve\n",
      "Epoch 7979/10000\n",
      "2700/2700 [==============================] - 0s 29us/step - loss: 1.2121e-04 - mean_absolute_error: 0.0063\n",
      "\n",
      "Epoch 07979: loss did not improve\n",
      "Epoch 7980/10000\n",
      "2700/2700 [==============================] - 0s 28us/step - loss: 1.2196e-04 - mean_absolute_error: 0.0064\n",
      "\n",
      "Epoch 07980: loss did not improve\n",
      "Epoch 7981/10000\n",
      "2700/2700 [==============================] - 0s 25us/step - loss: 1.2529e-04 - mean_absolute_error: 0.0066\n",
      "\n",
      "Epoch 07981: loss did not improve\n",
      "Epoch 7982/10000\n",
      "2700/2700 [==============================] - 0s 30us/step - loss: 1.2120e-04 - mean_absolute_error: 0.0064\n",
      "\n",
      "Epoch 07982: loss did not improve\n",
      "Epoch 7983/10000\n",
      "2700/2700 [==============================] - 0s 27us/step - loss: 1.2231e-04 - mean_absolute_error: 0.0064\n",
      "\n",
      "Epoch 07983: loss did not improve\n",
      "Epoch 7984/10000\n",
      "2700/2700 [==============================] - 0s 29us/step - loss: 1.2331e-04 - mean_absolute_error: 0.0066\n",
      "\n",
      "Epoch 07984: loss did not improve\n",
      "Epoch 7985/10000\n",
      "2700/2700 [==============================] - 0s 31us/step - loss: 1.2089e-04 - mean_absolute_error: 0.0063\n",
      "\n",
      "Epoch 07985: loss did not improve\n",
      "Epoch 7986/10000\n",
      "2700/2700 [==============================] - 0s 26us/step - loss: 1.2365e-04 - mean_absolute_error: 0.0066\n",
      "\n",
      "Epoch 07986: loss did not improve\n",
      "Epoch 7987/10000\n",
      "2700/2700 [==============================] - 0s 31us/step - loss: 1.2117e-04 - mean_absolute_error: 0.0063\n",
      "\n",
      "Epoch 07987: loss did not improve\n",
      "Epoch 7988/10000\n",
      "2700/2700 [==============================] - 0s 28us/step - loss: 1.2058e-04 - mean_absolute_error: 0.0063\n",
      "\n",
      "Epoch 07988: loss did not improve\n",
      "Epoch 7989/10000\n",
      "2700/2700 [==============================] - 0s 30us/step - loss: 1.2368e-04 - mean_absolute_error: 0.0066\n",
      "\n",
      "Epoch 07989: loss did not improve\n",
      "Epoch 7990/10000\n",
      "2700/2700 [==============================] - 0s 29us/step - loss: 1.2426e-04 - mean_absolute_error: 0.0066\n",
      "\n",
      "Epoch 07990: loss did not improve\n",
      "Epoch 7991/10000\n",
      "2700/2700 [==============================] - 0s 29us/step - loss: 1.2156e-04 - mean_absolute_error: 0.0064\n",
      "\n",
      "Epoch 07991: loss did not improve\n",
      "Epoch 7992/10000\n",
      "2700/2700 [==============================] - 0s 29us/step - loss: 1.2263e-04 - mean_absolute_error: 0.0064\n",
      "\n",
      "Epoch 07992: loss did not improve\n",
      "Epoch 7993/10000\n",
      "2700/2700 [==============================] - 0s 28us/step - loss: 1.2176e-04 - mean_absolute_error: 0.0064\n",
      "\n",
      "Epoch 07993: loss did not improve\n",
      "Epoch 7994/10000\n",
      "2700/2700 [==============================] - 0s 32us/step - loss: 1.2392e-04 - mean_absolute_error: 0.0065\n",
      "\n",
      "Epoch 07994: loss did not improve\n",
      "Epoch 7995/10000\n",
      "2700/2700 [==============================] - 0s 32us/step - loss: 1.2199e-04 - mean_absolute_error: 0.0064\n",
      "\n",
      "Epoch 07995: loss did not improve\n",
      "Epoch 7996/10000\n",
      "2700/2700 [==============================] - 0s 27us/step - loss: 1.2230e-04 - mean_absolute_error: 0.0064\n",
      "\n",
      "Epoch 07996: loss did not improve\n",
      "Epoch 7997/10000\n",
      "2700/2700 [==============================] - 0s 30us/step - loss: 1.2542e-04 - mean_absolute_error: 0.0067\n",
      "\n",
      "Epoch 07997: loss did not improve\n",
      "Epoch 7998/10000\n",
      "2700/2700 [==============================] - 0s 29us/step - loss: 1.2196e-04 - mean_absolute_error: 0.0064\n",
      "\n",
      "Epoch 07998: loss did not improve\n",
      "Epoch 7999/10000\n",
      "2700/2700 [==============================] - 0s 28us/step - loss: 1.2042e-04 - mean_absolute_error: 0.0062\n",
      "\n",
      "Epoch 07999: loss did not improve\n",
      "Epoch 8000/10000\n",
      "2700/2700 [==============================] - 0s 29us/step - loss: 1.2340e-04 - mean_absolute_error: 0.0066\n",
      "\n",
      "Epoch 08000: loss did not improve\n",
      "Epoch 8001/10000\n",
      "2700/2700 [==============================] - 0s 29us/step - loss: 1.2118e-04 - mean_absolute_error: 0.0064\n",
      "\n",
      "Epoch 08001: loss did not improve\n",
      "Epoch 8002/10000\n",
      "2700/2700 [==============================] - 0s 29us/step - loss: 1.2176e-04 - mean_absolute_error: 0.0064\n",
      "\n",
      "Epoch 08002: loss did not improve\n",
      "Epoch 8003/10000\n",
      "2700/2700 [==============================] - 0s 28us/step - loss: 1.2441e-04 - mean_absolute_error: 0.0067\n",
      "\n",
      "Epoch 08003: loss did not improve\n",
      "Epoch 8004/10000\n",
      "2700/2700 [==============================] - 0s 28us/step - loss: 1.2295e-04 - mean_absolute_error: 0.0064\n",
      "\n",
      "Epoch 08004: loss did not improve\n",
      "Epoch 8005/10000\n",
      "2700/2700 [==============================] - 0s 28us/step - loss: 1.2203e-04 - mean_absolute_error: 0.0065\n",
      "\n",
      "Epoch 08005: loss did not improve\n",
      "Epoch 8006/10000\n",
      "2700/2700 [==============================] - 0s 28us/step - loss: 1.2215e-04 - mean_absolute_error: 0.0064\n",
      "\n",
      "Epoch 08006: loss did not improve\n",
      "Epoch 8007/10000\n",
      "2700/2700 [==============================] - 0s 30us/step - loss: 1.2195e-04 - mean_absolute_error: 0.0065\n",
      "\n",
      "Epoch 08007: loss did not improve\n",
      "Epoch 8008/10000\n",
      "2700/2700 [==============================] - 0s 29us/step - loss: 1.2333e-04 - mean_absolute_error: 0.0065\n",
      "\n",
      "Epoch 08008: loss did not improve\n",
      "Epoch 8009/10000\n",
      "2700/2700 [==============================] - 0s 30us/step - loss: 1.2322e-04 - mean_absolute_error: 0.0065\n",
      "\n",
      "Epoch 08009: loss did not improve\n",
      "Epoch 8010/10000\n",
      "2700/2700 [==============================] - 0s 31us/step - loss: 1.2300e-04 - mean_absolute_error: 0.0065\n",
      "\n",
      "Epoch 08010: loss did not improve\n",
      "Epoch 8011/10000\n",
      "2700/2700 [==============================] - 0s 30us/step - loss: 1.2283e-04 - mean_absolute_error: 0.0064\n",
      "\n",
      "Epoch 08011: loss did not improve\n",
      "Epoch 8012/10000\n",
      "2700/2700 [==============================] - 0s 31us/step - loss: 1.2097e-04 - mean_absolute_error: 0.0063\n",
      "\n",
      "Epoch 08012: loss did not improve\n",
      "Epoch 8013/10000\n",
      "2700/2700 [==============================] - 0s 29us/step - loss: 1.2226e-04 - mean_absolute_error: 0.0064\n",
      "\n",
      "Epoch 08013: loss did not improve\n",
      "Epoch 8014/10000\n",
      "2700/2700 [==============================] - 0s 29us/step - loss: 1.2190e-04 - mean_absolute_error: 0.0063\n",
      "\n",
      "Epoch 08014: loss did not improve\n",
      "Epoch 8015/10000\n",
      "2700/2700 [==============================] - 0s 29us/step - loss: 1.2316e-04 - mean_absolute_error: 0.0065\n",
      "\n",
      "Epoch 08015: loss did not improve\n",
      "Epoch 8016/10000\n",
      "2700/2700 [==============================] - 0s 31us/step - loss: 1.2250e-04 - mean_absolute_error: 0.0065\n",
      "\n",
      "Epoch 08016: loss did not improve\n",
      "Epoch 8017/10000\n",
      "2700/2700 [==============================] - 0s 29us/step - loss: 1.2178e-04 - mean_absolute_error: 0.0063\n",
      "\n",
      "Epoch 08017: loss did not improve\n",
      "Epoch 8018/10000\n",
      "2700/2700 [==============================] - 0s 31us/step - loss: 1.2108e-04 - mean_absolute_error: 0.0064\n",
      "\n",
      "Epoch 08018: loss did not improve\n",
      "Epoch 8019/10000\n",
      "2700/2700 [==============================] - 0s 28us/step - loss: 1.2544e-04 - mean_absolute_error: 0.0067\n",
      "\n",
      "Epoch 08019: loss did not improve\n",
      "Epoch 8020/10000\n",
      "2700/2700 [==============================] - 0s 28us/step - loss: 1.2303e-04 - mean_absolute_error: 0.0066\n",
      "\n",
      "Epoch 08020: loss did not improve\n",
      "Epoch 8021/10000\n",
      "2700/2700 [==============================] - 0s 28us/step - loss: 1.2345e-04 - mean_absolute_error: 0.0065\n",
      "\n",
      "Epoch 08021: loss did not improve\n",
      "Epoch 8022/10000\n",
      "2700/2700 [==============================] - 0s 31us/step - loss: 1.2220e-04 - mean_absolute_error: 0.0065\n",
      "\n",
      "Epoch 08022: loss did not improve\n",
      "Epoch 8023/10000\n",
      "2700/2700 [==============================] - 0s 31us/step - loss: 1.2076e-04 - mean_absolute_error: 0.0063\n",
      "\n",
      "Epoch 08023: loss did not improve\n",
      "Epoch 8024/10000\n",
      "2700/2700 [==============================] - 0s 30us/step - loss: 1.2450e-04 - mean_absolute_error: 0.0066\n",
      "\n",
      "Epoch 08024: loss did not improve\n",
      "Epoch 8025/10000\n",
      "2700/2700 [==============================] - 0s 31us/step - loss: 1.2234e-04 - mean_absolute_error: 0.0065\n",
      "\n",
      "Epoch 08025: loss did not improve\n",
      "Epoch 8026/10000\n",
      "2700/2700 [==============================] - 0s 30us/step - loss: 1.2221e-04 - mean_absolute_error: 0.0064\n",
      "\n",
      "Epoch 08026: loss did not improve\n",
      "Epoch 8027/10000\n",
      "2700/2700 [==============================] - 0s 32us/step - loss: 1.2364e-04 - mean_absolute_error: 0.0066\n",
      "\n",
      "Epoch 08027: loss did not improve\n",
      "Epoch 8028/10000\n",
      "2700/2700 [==============================] - 0s 31us/step - loss: 1.2721e-04 - mean_absolute_error: 0.0068\n",
      "\n",
      "Epoch 08028: loss did not improve\n",
      "Epoch 8029/10000\n",
      "2700/2700 [==============================] - 0s 31us/step - loss: 1.2218e-04 - mean_absolute_error: 0.0064\n",
      "\n",
      "Epoch 08029: loss did not improve\n",
      "Epoch 8030/10000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2700/2700 [==============================] - 0s 29us/step - loss: 1.2358e-04 - mean_absolute_error: 0.0066\n",
      "\n",
      "Epoch 08030: loss did not improve\n",
      "Epoch 8031/10000\n",
      "2700/2700 [==============================] - 0s 29us/step - loss: 1.2625e-04 - mean_absolute_error: 0.0068\n",
      "\n",
      "Epoch 08031: loss did not improve\n",
      "Epoch 8032/10000\n",
      "2700/2700 [==============================] - 0s 29us/step - loss: 1.2467e-04 - mean_absolute_error: 0.0067\n",
      "\n",
      "Epoch 08032: loss did not improve\n",
      "Epoch 8033/10000\n",
      "2700/2700 [==============================] - 0s 28us/step - loss: 1.2403e-04 - mean_absolute_error: 0.0065\n",
      "\n",
      "Epoch 08033: loss did not improve\n",
      "Epoch 8034/10000\n",
      "2700/2700 [==============================] - 0s 27us/step - loss: 1.2347e-04 - mean_absolute_error: 0.0065\n",
      "\n",
      "Epoch 08034: loss did not improve\n",
      "Epoch 8035/10000\n",
      "2700/2700 [==============================] - 0s 27us/step - loss: 1.2308e-04 - mean_absolute_error: 0.0066\n",
      "\n",
      "Epoch 08035: loss did not improve\n",
      "Epoch 8036/10000\n",
      "2700/2700 [==============================] - 0s 29us/step - loss: 1.2196e-04 - mean_absolute_error: 0.0064\n",
      "\n",
      "Epoch 08036: loss did not improve\n",
      "Epoch 8037/10000\n",
      "2700/2700 [==============================] - 0s 30us/step - loss: 1.2165e-04 - mean_absolute_error: 0.0064\n",
      "\n",
      "Epoch 08037: loss did not improve\n",
      "Epoch 8038/10000\n",
      "2700/2700 [==============================] - 0s 28us/step - loss: 1.2241e-04 - mean_absolute_error: 0.0065\n",
      "\n",
      "Epoch 08038: loss did not improve\n",
      "Epoch 8039/10000\n",
      "2700/2700 [==============================] - 0s 26us/step - loss: 1.2381e-04 - mean_absolute_error: 0.0065\n",
      "\n",
      "Epoch 08039: loss did not improve\n",
      "Epoch 8040/10000\n",
      "2700/2700 [==============================] - 0s 29us/step - loss: 1.2116e-04 - mean_absolute_error: 0.0063\n",
      "\n",
      "Epoch 08040: loss did not improve\n",
      "Epoch 8041/10000\n",
      "2700/2700 [==============================] - 0s 30us/step - loss: 1.2330e-04 - mean_absolute_error: 0.0065\n",
      "\n",
      "Epoch 08041: loss did not improve\n",
      "Epoch 8042/10000\n",
      "2700/2700 [==============================] - 0s 28us/step - loss: 1.2299e-04 - mean_absolute_error: 0.0065\n",
      "\n",
      "Epoch 08042: loss did not improve\n",
      "Epoch 8043/10000\n",
      "2700/2700 [==============================] - 0s 29us/step - loss: 1.2182e-04 - mean_absolute_error: 0.0063\n",
      "\n",
      "Epoch 08043: loss did not improve\n",
      "Epoch 8044/10000\n",
      "2700/2700 [==============================] - 0s 28us/step - loss: 1.2440e-04 - mean_absolute_error: 0.0065\n",
      "\n",
      "Epoch 08044: loss did not improve\n",
      "Epoch 8045/10000\n",
      "2700/2700 [==============================] - 0s 28us/step - loss: 1.2176e-04 - mean_absolute_error: 0.0064\n",
      "\n",
      "Epoch 08045: loss did not improve\n",
      "Epoch 8046/10000\n",
      "2700/2700 [==============================] - 0s 31us/step - loss: 1.2227e-04 - mean_absolute_error: 0.0064\n",
      "\n",
      "Epoch 08046: loss did not improve\n",
      "Epoch 8047/10000\n",
      "2700/2700 [==============================] - 0s 31us/step - loss: 1.2220e-04 - mean_absolute_error: 0.0064\n",
      "\n",
      "Epoch 08047: loss did not improve\n",
      "Epoch 8048/10000\n",
      "2700/2700 [==============================] - 0s 29us/step - loss: 1.2213e-04 - mean_absolute_error: 0.0064\n",
      "\n",
      "Epoch 08048: loss did not improve\n",
      "Epoch 8049/10000\n",
      "2700/2700 [==============================] - 0s 28us/step - loss: 1.2146e-04 - mean_absolute_error: 0.0064\n",
      "\n",
      "Epoch 08049: loss did not improve\n",
      "Epoch 8050/10000\n",
      "2700/2700 [==============================] - 0s 29us/step - loss: 1.2267e-04 - mean_absolute_error: 0.0065\n",
      "\n",
      "Epoch 08050: loss did not improve\n",
      "Epoch 8051/10000\n",
      "2700/2700 [==============================] - 0s 34us/step - loss: 1.2237e-04 - mean_absolute_error: 0.0064\n",
      "\n",
      "Epoch 08051: loss did not improve\n",
      "Epoch 8052/10000\n",
      "2700/2700 [==============================] - 0s 30us/step - loss: 1.2109e-04 - mean_absolute_error: 0.0062\n",
      "\n",
      "Epoch 08052: loss did not improve\n",
      "Epoch 8053/10000\n",
      "2700/2700 [==============================] - 0s 36us/step - loss: 1.2678e-04 - mean_absolute_error: 0.0068\n",
      "\n",
      "Epoch 08053: loss did not improve\n",
      "Epoch 8054/10000\n",
      "2700/2700 [==============================] - 0s 31us/step - loss: 1.2199e-04 - mean_absolute_error: 0.0064\n",
      "\n",
      "Epoch 08054: loss did not improve\n",
      "Epoch 8055/10000\n",
      "2700/2700 [==============================] - 0s 28us/step - loss: 1.2418e-04 - mean_absolute_error: 0.0067\n",
      "\n",
      "Epoch 08055: loss did not improve\n",
      "Epoch 8056/10000\n",
      "2700/2700 [==============================] - 0s 29us/step - loss: 1.2250e-04 - mean_absolute_error: 0.0064\n",
      "\n",
      "Epoch 08056: loss did not improve\n",
      "Epoch 8057/10000\n",
      "2700/2700 [==============================] - 0s 32us/step - loss: 1.2142e-04 - mean_absolute_error: 0.0064\n",
      "\n",
      "Epoch 08057: loss did not improve\n",
      "Epoch 8058/10000\n",
      "2700/2700 [==============================] - 0s 37us/step - loss: 1.2437e-04 - mean_absolute_error: 0.0067\n",
      "\n",
      "Epoch 08058: loss did not improve\n",
      "Epoch 8059/10000\n",
      "2700/2700 [==============================] - 0s 30us/step - loss: 1.2220e-04 - mean_absolute_error: 0.0064\n",
      "\n",
      "Epoch 08059: loss did not improve\n",
      "Epoch 8060/10000\n",
      "2700/2700 [==============================] - 0s 30us/step - loss: 1.2280e-04 - mean_absolute_error: 0.0066\n",
      "\n",
      "Epoch 08060: loss did not improve\n",
      "Epoch 8061/10000\n",
      "2700/2700 [==============================] - 0s 30us/step - loss: 1.2063e-04 - mean_absolute_error: 0.0064\n",
      "\n",
      "Epoch 08061: loss did not improve\n",
      "Epoch 8062/10000\n",
      "2700/2700 [==============================] - 0s 37us/step - loss: 1.2483e-04 - mean_absolute_error: 0.0067\n",
      "\n",
      "Epoch 08062: loss did not improve\n",
      "Epoch 8063/10000\n",
      "2700/2700 [==============================] - 0s 28us/step - loss: 1.2414e-04 - mean_absolute_error: 0.0066\n",
      "\n",
      "Epoch 08063: loss did not improve\n",
      "Epoch 8064/10000\n",
      "2700/2700 [==============================] - 0s 32us/step - loss: 1.2491e-04 - mean_absolute_error: 0.0066\n",
      "\n",
      "Epoch 08064: loss did not improve\n",
      "Epoch 8065/10000\n",
      "2700/2700 [==============================] - 0s 32us/step - loss: 1.2212e-04 - mean_absolute_error: 0.0064\n",
      "\n",
      "Epoch 08065: loss did not improve\n",
      "Epoch 8066/10000\n",
      "2700/2700 [==============================] - 0s 32us/step - loss: 1.2179e-04 - mean_absolute_error: 0.0063\n",
      "\n",
      "Epoch 08066: loss did not improve\n",
      "Epoch 8067/10000\n",
      "2700/2700 [==============================] - 0s 32us/step - loss: 1.2364e-04 - mean_absolute_error: 0.0066\n",
      "\n",
      "Epoch 08067: loss did not improve\n",
      "Epoch 8068/10000\n",
      "2700/2700 [==============================] - 0s 32us/step - loss: 1.2062e-04 - mean_absolute_error: 0.0062\n",
      "\n",
      "Epoch 08068: loss did not improve\n",
      "Epoch 8069/10000\n",
      "2700/2700 [==============================] - 0s 27us/step - loss: 1.2212e-04 - mean_absolute_error: 0.0064\n",
      "\n",
      "Epoch 08069: loss did not improve\n",
      "Epoch 8070/10000\n",
      "2700/2700 [==============================] - 0s 29us/step - loss: 1.2139e-04 - mean_absolute_error: 0.0063\n",
      "\n",
      "Epoch 08070: loss did not improve\n",
      "Epoch 8071/10000\n",
      "2700/2700 [==============================] - 0s 29us/step - loss: 1.2425e-04 - mean_absolute_error: 0.0067\n",
      "\n",
      "Epoch 08071: loss did not improve\n",
      "Epoch 8072/10000\n",
      "2700/2700 [==============================] - 0s 28us/step - loss: 1.2383e-04 - mean_absolute_error: 0.0066\n",
      "\n",
      "Epoch 08072: loss did not improve\n",
      "Epoch 8073/10000\n",
      "2700/2700 [==============================] - 0s 27us/step - loss: 1.2179e-04 - mean_absolute_error: 0.0065\n",
      "\n",
      "Epoch 08073: loss did not improve\n",
      "Epoch 8074/10000\n",
      "2700/2700 [==============================] - 0s 33us/step - loss: 1.2452e-04 - mean_absolute_error: 0.0065\n",
      "\n",
      "Epoch 08074: loss did not improve\n",
      "Epoch 8075/10000\n",
      "2700/2700 [==============================] - 0s 31us/step - loss: 1.2179e-04 - mean_absolute_error: 0.0065\n",
      "\n",
      "Epoch 08075: loss did not improve\n",
      "Epoch 8076/10000\n",
      "2700/2700 [==============================] - 0s 29us/step - loss: 1.2864e-04 - mean_absolute_error: 0.0069\n",
      "\n",
      "Epoch 08076: loss did not improve\n",
      "Epoch 8077/10000\n",
      "2700/2700 [==============================] - 0s 32us/step - loss: 1.2317e-04 - mean_absolute_error: 0.0064\n",
      "\n",
      "Epoch 08077: loss did not improve\n",
      "Epoch 8078/10000\n",
      "2700/2700 [==============================] - 0s 29us/step - loss: 1.2295e-04 - mean_absolute_error: 0.0065\n",
      "\n",
      "Epoch 08078: loss did not improve\n",
      "Epoch 8079/10000\n",
      "2700/2700 [==============================] - 0s 29us/step - loss: 1.2149e-04 - mean_absolute_error: 0.0063\n",
      "\n",
      "Epoch 08079: loss did not improve\n",
      "Epoch 8080/10000\n",
      "2700/2700 [==============================] - 0s 29us/step - loss: 1.2179e-04 - mean_absolute_error: 0.0066\n",
      "\n",
      "Epoch 08080: loss did not improve\n",
      "Epoch 8081/10000\n",
      "2700/2700 [==============================] - 0s 33us/step - loss: 1.2381e-04 - mean_absolute_error: 0.0065\n",
      "\n",
      "Epoch 08081: loss did not improve\n",
      "Epoch 8082/10000\n",
      "2700/2700 [==============================] - 0s 29us/step - loss: 1.2348e-04 - mean_absolute_error: 0.0065\n",
      "\n",
      "Epoch 08082: loss did not improve\n",
      "Epoch 8083/10000\n",
      "2700/2700 [==============================] - 0s 30us/step - loss: 1.2246e-04 - mean_absolute_error: 0.0065\n",
      "\n",
      "Epoch 08083: loss did not improve\n",
      "Epoch 8084/10000\n",
      "2700/2700 [==============================] - 0s 26us/step - loss: 1.2333e-04 - mean_absolute_error: 0.0064\n",
      "\n",
      "Epoch 08084: loss did not improve\n",
      "Epoch 8085/10000\n",
      "2700/2700 [==============================] - 0s 29us/step - loss: 1.2480e-04 - mean_absolute_error: 0.0067\n",
      "\n",
      "Epoch 08085: loss did not improve\n",
      "Epoch 8086/10000\n",
      "2700/2700 [==============================] - 0s 28us/step - loss: 1.2027e-04 - mean_absolute_error: 0.0063\n",
      "\n",
      "Epoch 08086: loss improved from 0.00012 to 0.00012, saving model to keras/best_baseline_weights.hdf5\n",
      "Epoch 8087/10000\n",
      "2700/2700 [==============================] - 0s 31us/step - loss: 1.2428e-04 - mean_absolute_error: 0.0066\n",
      "\n",
      "Epoch 08087: loss did not improve\n",
      "Epoch 8088/10000\n",
      "2700/2700 [==============================] - 0s 32us/step - loss: 1.2180e-04 - mean_absolute_error: 0.0063\n",
      "\n",
      "Epoch 08088: loss did not improve\n",
      "Epoch 8089/10000\n",
      "2700/2700 [==============================] - 0s 27us/step - loss: 1.2185e-04 - mean_absolute_error: 0.0064\n",
      "\n",
      "Epoch 08089: loss did not improve\n",
      "Epoch 8090/10000\n",
      "2700/2700 [==============================] - 0s 28us/step - loss: 1.2283e-04 - mean_absolute_error: 0.0065\n",
      "\n",
      "Epoch 08090: loss did not improve\n",
      "Epoch 8091/10000\n",
      "2700/2700 [==============================] - 0s 29us/step - loss: 1.2170e-04 - mean_absolute_error: 0.0063\n",
      "\n",
      "Epoch 08091: loss did not improve\n",
      "Epoch 8092/10000\n",
      "2700/2700 [==============================] - 0s 30us/step - loss: 1.2303e-04 - mean_absolute_error: 0.0065\n",
      "\n",
      "Epoch 08092: loss did not improve\n",
      "Epoch 8093/10000\n",
      "2700/2700 [==============================] - 0s 30us/step - loss: 1.2085e-04 - mean_absolute_error: 0.0064\n",
      "\n",
      "Epoch 08093: loss did not improve\n",
      "Epoch 8094/10000\n",
      "2700/2700 [==============================] - 0s 26us/step - loss: 1.2314e-04 - mean_absolute_error: 0.0065\n",
      "\n",
      "Epoch 08094: loss did not improve\n",
      "Epoch 8095/10000\n",
      "2700/2700 [==============================] - 0s 46us/step - loss: 1.2628e-04 - mean_absolute_error: 0.0067\n",
      "\n",
      "Epoch 08095: loss did not improve\n",
      "Epoch 8096/10000\n",
      "2700/2700 [==============================] - 0s 35us/step - loss: 1.2216e-04 - mean_absolute_error: 0.0064\n",
      "\n",
      "Epoch 08096: loss did not improve\n",
      "Epoch 8097/10000\n",
      "2700/2700 [==============================] - 0s 26us/step - loss: 1.2175e-04 - mean_absolute_error: 0.0064\n",
      "\n",
      "Epoch 08097: loss did not improve\n",
      "Epoch 8098/10000\n",
      "2700/2700 [==============================] - 0s 29us/step - loss: 1.2150e-04 - mean_absolute_error: 0.0064\n",
      "\n",
      "Epoch 08098: loss did not improve\n",
      "Epoch 8099/10000\n",
      "2700/2700 [==============================] - 0s 28us/step - loss: 1.2202e-04 - mean_absolute_error: 0.0064\n",
      "\n",
      "Epoch 08099: loss did not improve\n",
      "Epoch 8100/10000\n",
      "2700/2700 [==============================] - 0s 35us/step - loss: 1.2355e-04 - mean_absolute_error: 0.0066\n",
      "\n",
      "Epoch 08100: loss did not improve\n",
      "Epoch 8101/10000\n",
      "2700/2700 [==============================] - 0s 29us/step - loss: 1.2244e-04 - mean_absolute_error: 0.0065\n",
      "\n",
      "Epoch 08101: loss did not improve\n",
      "Epoch 8102/10000\n",
      "2700/2700 [==============================] - 0s 30us/step - loss: 1.2393e-04 - mean_absolute_error: 0.0065\n",
      "\n",
      "Epoch 08102: loss did not improve\n",
      "Epoch 8103/10000\n",
      "2700/2700 [==============================] - 0s 29us/step - loss: 1.2317e-04 - mean_absolute_error: 0.0066\n",
      "\n",
      "Epoch 08103: loss did not improve\n",
      "Epoch 8104/10000\n",
      "2700/2700 [==============================] - 0s 28us/step - loss: 1.2074e-04 - mean_absolute_error: 0.0062\n",
      "\n",
      "Epoch 08104: loss did not improve\n",
      "Epoch 8105/10000\n",
      "2700/2700 [==============================] - 0s 30us/step - loss: 1.2235e-04 - mean_absolute_error: 0.0065\n",
      "\n",
      "Epoch 08105: loss did not improve\n",
      "Epoch 8106/10000\n",
      "2700/2700 [==============================] - 0s 37us/step - loss: 1.2245e-04 - mean_absolute_error: 0.0064\n",
      "\n",
      "Epoch 08106: loss did not improve\n",
      "Epoch 8107/10000\n",
      "2700/2700 [==============================] - 0s 37us/step - loss: 1.2200e-04 - mean_absolute_error: 0.0064\n",
      "\n",
      "Epoch 08107: loss did not improve\n",
      "Epoch 8108/10000\n",
      "2700/2700 [==============================] - 0s 30us/step - loss: 1.2195e-04 - mean_absolute_error: 0.0064\n",
      "\n",
      "Epoch 08108: loss did not improve\n",
      "Epoch 8109/10000\n",
      "2700/2700 [==============================] - 0s 32us/step - loss: 1.2539e-04 - mean_absolute_error: 0.0068\n",
      "\n",
      "Epoch 08109: loss did not improve\n",
      "Epoch 8110/10000\n",
      "2700/2700 [==============================] - 0s 30us/step - loss: 1.2165e-04 - mean_absolute_error: 0.0064\n",
      "\n",
      "Epoch 08110: loss did not improve\n",
      "Epoch 8111/10000\n",
      "2700/2700 [==============================] - 0s 31us/step - loss: 1.2317e-04 - mean_absolute_error: 0.0064\n",
      "\n",
      "Epoch 08111: loss did not improve\n",
      "Epoch 8112/10000\n",
      "2700/2700 [==============================] - 0s 33us/step - loss: 1.2193e-04 - mean_absolute_error: 0.0065\n",
      "\n",
      "Epoch 08112: loss did not improve\n",
      "Epoch 8113/10000\n",
      "2700/2700 [==============================] - ETA: 0s - loss: 1.1776e-04 - mean_absolute_error: 0.006 - 0s 30us/step - loss: 1.2156e-04 - mean_absolute_error: 0.0063\n",
      "\n",
      "Epoch 08113: loss did not improve\n",
      "Epoch 8114/10000\n",
      "2700/2700 [==============================] - 0s 32us/step - loss: 1.2134e-04 - mean_absolute_error: 0.0063\n",
      "\n",
      "Epoch 08114: loss did not improve\n",
      "Epoch 8115/10000\n",
      "2700/2700 [==============================] - 0s 30us/step - loss: 1.2240e-04 - mean_absolute_error: 0.0064\n",
      "\n",
      "Epoch 08115: loss did not improve\n",
      "Epoch 8116/10000\n",
      "2700/2700 [==============================] - 0s 28us/step - loss: 1.2113e-04 - mean_absolute_error: 0.0064\n",
      "\n",
      "Epoch 08116: loss did not improve\n",
      "Epoch 8117/10000\n",
      "2700/2700 [==============================] - 0s 28us/step - loss: 1.2883e-04 - mean_absolute_error: 0.0070\n",
      "\n",
      "Epoch 08117: loss did not improve\n",
      "Epoch 8118/10000\n",
      "2700/2700 [==============================] - 0s 32us/step - loss: 1.2379e-04 - mean_absolute_error: 0.0066\n",
      "\n",
      "Epoch 08118: loss did not improve\n",
      "Epoch 8119/10000\n",
      "2700/2700 [==============================] - 0s 29us/step - loss: 1.2415e-04 - mean_absolute_error: 0.0066\n",
      "\n",
      "Epoch 08119: loss did not improve\n",
      "Epoch 8120/10000\n",
      "2700/2700 [==============================] - 0s 30us/step - loss: 1.2291e-04 - mean_absolute_error: 0.0066\n",
      "\n",
      "Epoch 08120: loss did not improve\n",
      "Epoch 8121/10000\n",
      "2700/2700 [==============================] - 0s 28us/step - loss: 1.2169e-04 - mean_absolute_error: 0.0064\n",
      "\n",
      "Epoch 08121: loss did not improve\n",
      "Epoch 8122/10000\n",
      "2700/2700 [==============================] - 0s 29us/step - loss: 1.2411e-04 - mean_absolute_error: 0.0066\n",
      "\n",
      "Epoch 08122: loss did not improve\n",
      "Epoch 8123/10000\n",
      "2700/2700 [==============================] - 0s 33us/step - loss: 1.2206e-04 - mean_absolute_error: 0.0065\n",
      "\n",
      "Epoch 08123: loss did not improve\n",
      "Epoch 8124/10000\n",
      "2700/2700 [==============================] - 0s 36us/step - loss: 1.2120e-04 - mean_absolute_error: 0.0062\n",
      "\n",
      "Epoch 08124: loss did not improve\n",
      "Epoch 8125/10000\n",
      "2700/2700 [==============================] - 0s 32us/step - loss: 1.2340e-04 - mean_absolute_error: 0.0066\n",
      "\n",
      "Epoch 08125: loss did not improve\n",
      "Epoch 8126/10000\n",
      "2700/2700 [==============================] - 0s 31us/step - loss: 1.2197e-04 - mean_absolute_error: 0.0064\n",
      "\n",
      "Epoch 08126: loss did not improve\n",
      "Epoch 8127/10000\n",
      "2700/2700 [==============================] - 0s 34us/step - loss: 1.2119e-04 - mean_absolute_error: 0.0064\n",
      "\n",
      "Epoch 08127: loss did not improve\n",
      "Epoch 8128/10000\n",
      "2700/2700 [==============================] - 0s 28us/step - loss: 1.2048e-04 - mean_absolute_error: 0.0063\n",
      "\n",
      "Epoch 08128: loss did not improve\n",
      "Epoch 8129/10000\n",
      "2700/2700 [==============================] - 0s 34us/step - loss: 1.2698e-04 - mean_absolute_error: 0.0068\n",
      "\n",
      "Epoch 08129: loss did not improve\n",
      "Epoch 8130/10000\n",
      "2700/2700 [==============================] - 0s 36us/step - loss: 1.2605e-04 - mean_absolute_error: 0.0069\n",
      "\n",
      "Epoch 08130: loss did not improve\n",
      "Epoch 8131/10000\n",
      "2700/2700 [==============================] - 0s 28us/step - loss: 1.2111e-04 - mean_absolute_error: 0.0063\n",
      "\n",
      "Epoch 08131: loss did not improve\n",
      "Epoch 8132/10000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2700/2700 [==============================] - 0s 34us/step - loss: 1.2401e-04 - mean_absolute_error: 0.0066\n",
      "\n",
      "Epoch 08132: loss did not improve\n",
      "Epoch 8133/10000\n",
      "2700/2700 [==============================] - 0s 31us/step - loss: 1.2314e-04 - mean_absolute_error: 0.0066\n",
      "\n",
      "Epoch 08133: loss did not improve\n",
      "Epoch 8134/10000\n",
      "2700/2700 [==============================] - 0s 30us/step - loss: 1.2201e-04 - mean_absolute_error: 0.0064\n",
      "\n",
      "Epoch 08134: loss did not improve\n",
      "Epoch 8135/10000\n",
      "2700/2700 [==============================] - 0s 28us/step - loss: 1.2153e-04 - mean_absolute_error: 0.0063\n",
      "\n",
      "Epoch 08135: loss did not improve\n",
      "Epoch 8136/10000\n",
      "2700/2700 [==============================] - 0s 28us/step - loss: 1.2564e-04 - mean_absolute_error: 0.0067\n",
      "\n",
      "Epoch 08136: loss did not improve\n",
      "Epoch 8137/10000\n",
      "2700/2700 [==============================] - 0s 28us/step - loss: 1.2286e-04 - mean_absolute_error: 0.0065\n",
      "\n",
      "Epoch 08137: loss did not improve\n",
      "Epoch 8138/10000\n",
      "2700/2700 [==============================] - 0s 34us/step - loss: 1.2675e-04 - mean_absolute_error: 0.0068\n",
      "\n",
      "Epoch 08138: loss did not improve\n",
      "Epoch 8139/10000\n",
      "2700/2700 [==============================] - 0s 28us/step - loss: 1.2300e-04 - mean_absolute_error: 0.0065\n",
      "\n",
      "Epoch 08139: loss did not improve\n",
      "Epoch 8140/10000\n",
      "2700/2700 [==============================] - 0s 28us/step - loss: 1.2185e-04 - mean_absolute_error: 0.0064\n",
      "\n",
      "Epoch 08140: loss did not improve\n",
      "Epoch 8141/10000\n",
      "2700/2700 [==============================] - 0s 28us/step - loss: 1.2317e-04 - mean_absolute_error: 0.0064\n",
      "\n",
      "Epoch 08141: loss did not improve\n",
      "Epoch 8142/10000\n",
      "2700/2700 [==============================] - 0s 29us/step - loss: 1.2249e-04 - mean_absolute_error: 0.0064\n",
      "\n",
      "Epoch 08142: loss did not improve\n",
      "Epoch 8143/10000\n",
      "2700/2700 [==============================] - 0s 51us/step - loss: 1.2103e-04 - mean_absolute_error: 0.0064\n",
      "\n",
      "Epoch 08143: loss did not improve\n",
      "Epoch 8144/10000\n",
      "2700/2700 [==============================] - 0s 33us/step - loss: 1.2157e-04 - mean_absolute_error: 0.0063\n",
      "\n",
      "Epoch 08144: loss did not improve\n",
      "Epoch 8145/10000\n",
      "2700/2700 [==============================] - 0s 36us/step - loss: 1.2177e-04 - mean_absolute_error: 0.0064\n",
      "\n",
      "Epoch 08145: loss did not improve\n",
      "Epoch 8146/10000\n",
      "2700/2700 [==============================] - 0s 34us/step - loss: 1.2530e-04 - mean_absolute_error: 0.0067\n",
      "\n",
      "Epoch 08146: loss did not improve\n",
      "Epoch 8147/10000\n",
      "2700/2700 [==============================] - 0s 33us/step - loss: 1.2469e-04 - mean_absolute_error: 0.0066\n",
      "\n",
      "Epoch 08147: loss did not improve\n",
      "Epoch 8148/10000\n",
      "2700/2700 [==============================] - 0s 31us/step - loss: 1.2208e-04 - mean_absolute_error: 0.0065\n",
      "\n",
      "Epoch 08148: loss did not improve\n",
      "Epoch 8149/10000\n",
      "2700/2700 [==============================] - 0s 31us/step - loss: 1.2204e-04 - mean_absolute_error: 0.0064\n",
      "\n",
      "Epoch 08149: loss did not improve\n",
      "Epoch 8150/10000\n",
      "2700/2700 [==============================] - 0s 29us/step - loss: 1.2107e-04 - mean_absolute_error: 0.0063\n",
      "\n",
      "Epoch 08150: loss did not improve\n",
      "Epoch 8151/10000\n",
      "2700/2700 [==============================] - 0s 32us/step - loss: 1.2364e-04 - mean_absolute_error: 0.0066\n",
      "\n",
      "Epoch 08151: loss did not improve\n",
      "Epoch 8152/10000\n",
      "2700/2700 [==============================] - 0s 30us/step - loss: 1.2097e-04 - mean_absolute_error: 0.0063\n",
      "\n",
      "Epoch 08152: loss did not improve\n",
      "Epoch 8153/10000\n",
      "2700/2700 [==============================] - 0s 32us/step - loss: 1.2153e-04 - mean_absolute_error: 0.0065\n",
      "\n",
      "Epoch 08153: loss did not improve\n",
      "Epoch 8154/10000\n",
      "2700/2700 [==============================] - 0s 32us/step - loss: 1.2070e-04 - mean_absolute_error: 0.0063\n",
      "\n",
      "Epoch 08154: loss did not improve\n",
      "Epoch 8155/10000\n",
      "2700/2700 [==============================] - 0s 31us/step - loss: 1.2204e-04 - mean_absolute_error: 0.0065\n",
      "\n",
      "Epoch 08155: loss did not improve\n",
      "Epoch 8156/10000\n",
      "2700/2700 [==============================] - 0s 30us/step - loss: 1.2217e-04 - mean_absolute_error: 0.0064\n",
      "\n",
      "Epoch 08156: loss did not improve\n",
      "Epoch 8157/10000\n",
      "2700/2700 [==============================] - 0s 28us/step - loss: 1.2507e-04 - mean_absolute_error: 0.0066\n",
      "\n",
      "Epoch 08157: loss did not improve\n",
      "Epoch 8158/10000\n",
      "2700/2700 [==============================] - 0s 29us/step - loss: 1.2062e-04 - mean_absolute_error: 0.0063\n",
      "\n",
      "Epoch 08158: loss did not improve\n",
      "Epoch 8159/10000\n",
      "2700/2700 [==============================] - 0s 30us/step - loss: 1.2351e-04 - mean_absolute_error: 0.0066\n",
      "\n",
      "Epoch 08159: loss did not improve\n",
      "Epoch 8160/10000\n",
      "2700/2700 [==============================] - 0s 30us/step - loss: 1.2573e-04 - mean_absolute_error: 0.0067\n",
      "\n",
      "Epoch 08160: loss did not improve\n",
      "Epoch 8161/10000\n",
      "2700/2700 [==============================] - 0s 30us/step - loss: 1.2293e-04 - mean_absolute_error: 0.0065\n",
      "\n",
      "Epoch 08161: loss did not improve\n",
      "Epoch 8162/10000\n",
      "2700/2700 [==============================] - 0s 29us/step - loss: 1.2191e-04 - mean_absolute_error: 0.0063\n",
      "\n",
      "Epoch 08162: loss did not improve\n",
      "Epoch 8163/10000\n",
      "2700/2700 [==============================] - 0s 31us/step - loss: 1.2364e-04 - mean_absolute_error: 0.0065\n",
      "\n",
      "Epoch 08163: loss did not improve\n",
      "Epoch 8164/10000\n",
      "2700/2700 [==============================] - 0s 30us/step - loss: 1.2258e-04 - mean_absolute_error: 0.0066\n",
      "\n",
      "Epoch 08164: loss did not improve\n",
      "Epoch 8165/10000\n",
      "2700/2700 [==============================] - 0s 30us/step - loss: 1.2364e-04 - mean_absolute_error: 0.0067\n",
      "\n",
      "Epoch 08165: loss did not improve\n",
      "Epoch 8166/10000\n",
      "2700/2700 [==============================] - 0s 31us/step - loss: 1.2453e-04 - mean_absolute_error: 0.0066\n",
      "\n",
      "Epoch 08166: loss did not improve\n",
      "Epoch 8167/10000\n",
      "2700/2700 [==============================] - 0s 32us/step - loss: 1.2398e-04 - mean_absolute_error: 0.0066\n",
      "\n",
      "Epoch 08167: loss did not improve\n",
      "Epoch 8168/10000\n",
      "2700/2700 [==============================] - 0s 28us/step - loss: 1.2479e-04 - mean_absolute_error: 0.0068\n",
      "\n",
      "Epoch 08168: loss did not improve\n",
      "Epoch 8169/10000\n",
      "2700/2700 [==============================] - 0s 30us/step - loss: 1.2174e-04 - mean_absolute_error: 0.0064\n",
      "\n",
      "Epoch 08169: loss did not improve\n",
      "Epoch 8170/10000\n",
      "2700/2700 [==============================] - 0s 29us/step - loss: 1.2205e-04 - mean_absolute_error: 0.0065\n",
      "\n",
      "Epoch 08170: loss did not improve\n",
      "Epoch 8171/10000\n",
      "2700/2700 [==============================] - 0s 32us/step - loss: 1.2206e-04 - mean_absolute_error: 0.0064\n",
      "\n",
      "Epoch 08171: loss did not improve\n",
      "Epoch 8172/10000\n",
      "2700/2700 [==============================] - 0s 33us/step - loss: 1.2118e-04 - mean_absolute_error: 0.0063\n",
      "\n",
      "Epoch 08172: loss did not improve\n",
      "Epoch 8173/10000\n",
      "2700/2700 [==============================] - 0s 44us/step - loss: 1.2206e-04 - mean_absolute_error: 0.0064\n",
      "\n",
      "Epoch 08173: loss did not improve\n",
      "Epoch 8174/10000\n",
      "2700/2700 [==============================] - 0s 32us/step - loss: 1.2469e-04 - mean_absolute_error: 0.0066\n",
      "\n",
      "Epoch 08174: loss did not improve\n",
      "Epoch 8175/10000\n",
      "2700/2700 [==============================] - 0s 29us/step - loss: 1.2128e-04 - mean_absolute_error: 0.0063\n",
      "\n",
      "Epoch 08175: loss did not improve\n",
      "Epoch 8176/10000\n",
      "2700/2700 [==============================] - 0s 28us/step - loss: 1.2319e-04 - mean_absolute_error: 0.0066\n",
      "\n",
      "Epoch 08176: loss did not improve\n",
      "Epoch 8177/10000\n",
      "2700/2700 [==============================] - 0s 30us/step - loss: 1.2255e-04 - mean_absolute_error: 0.0064\n",
      "\n",
      "Epoch 08177: loss did not improve\n",
      "Epoch 8178/10000\n",
      "2700/2700 [==============================] - 0s 29us/step - loss: 1.2322e-04 - mean_absolute_error: 0.0065\n",
      "\n",
      "Epoch 08178: loss did not improve\n",
      "Epoch 8179/10000\n",
      "2700/2700 [==============================] - 0s 45us/step - loss: 1.2076e-04 - mean_absolute_error: 0.0063\n",
      "\n",
      "Epoch 08179: loss did not improve\n",
      "Epoch 8180/10000\n",
      "2700/2700 [==============================] - 0s 34us/step - loss: 1.2149e-04 - mean_absolute_error: 0.0063\n",
      "\n",
      "Epoch 08180: loss did not improve\n",
      "Epoch 8181/10000\n",
      "2700/2700 [==============================] - 0s 33us/step - loss: 1.2210e-04 - mean_absolute_error: 0.0064\n",
      "\n",
      "Epoch 08181: loss did not improve\n",
      "Epoch 8182/10000\n",
      "2700/2700 [==============================] - 0s 31us/step - loss: 1.2388e-04 - mean_absolute_error: 0.0066\n",
      "\n",
      "Epoch 08182: loss did not improve\n",
      "Epoch 8183/10000\n",
      "2700/2700 [==============================] - 0s 30us/step - loss: 1.2204e-04 - mean_absolute_error: 0.0063\n",
      "\n",
      "Epoch 08183: loss did not improve\n",
      "Epoch 8184/10000\n",
      "2700/2700 [==============================] - 0s 29us/step - loss: 1.2136e-04 - mean_absolute_error: 0.0063\n",
      "\n",
      "Epoch 08184: loss did not improve\n",
      "Epoch 8185/10000\n",
      "2700/2700 [==============================] - 0s 29us/step - loss: 1.2162e-04 - mean_absolute_error: 0.0064\n",
      "\n",
      "Epoch 08185: loss did not improve\n",
      "Epoch 8186/10000\n",
      "2700/2700 [==============================] - 0s 27us/step - loss: 1.1994e-04 - mean_absolute_error: 0.0063\n",
      "\n",
      "Epoch 08186: loss improved from 0.00012 to 0.00012, saving model to keras/best_baseline_weights.hdf5\n",
      "Epoch 8187/10000\n",
      "2700/2700 [==============================] - 0s 28us/step - loss: 1.2121e-04 - mean_absolute_error: 0.0063\n",
      "\n",
      "Epoch 08187: loss did not improve\n",
      "Epoch 8188/10000\n",
      "2700/2700 [==============================] - 0s 28us/step - loss: 1.2524e-04 - mean_absolute_error: 0.0066\n",
      "\n",
      "Epoch 08188: loss did not improve\n",
      "Epoch 8189/10000\n",
      "2700/2700 [==============================] - 0s 28us/step - loss: 1.2245e-04 - mean_absolute_error: 0.0065\n",
      "\n",
      "Epoch 08189: loss did not improve\n",
      "Epoch 8190/10000\n",
      "2700/2700 [==============================] - 0s 30us/step - loss: 1.1998e-04 - mean_absolute_error: 0.0063\n",
      "\n",
      "Epoch 08190: loss did not improve\n",
      "Epoch 8191/10000\n",
      "2700/2700 [==============================] - 0s 28us/step - loss: 1.2160e-04 - mean_absolute_error: 0.0063\n",
      "\n",
      "Epoch 08191: loss did not improve\n",
      "Epoch 8192/10000\n",
      "2700/2700 [==============================] - 0s 29us/step - loss: 1.2277e-04 - mean_absolute_error: 0.0066\n",
      "\n",
      "Epoch 08192: loss did not improve\n",
      "Epoch 8193/10000\n",
      "2700/2700 [==============================] - 0s 28us/step - loss: 1.2169e-04 - mean_absolute_error: 0.0064\n",
      "\n",
      "Epoch 08193: loss did not improve\n",
      "Epoch 8194/10000\n",
      "2700/2700 [==============================] - 0s 27us/step - loss: 1.2182e-04 - mean_absolute_error: 0.0065\n",
      "\n",
      "Epoch 08194: loss did not improve\n",
      "Epoch 8195/10000\n",
      "2700/2700 [==============================] - 0s 29us/step - loss: 1.2061e-04 - mean_absolute_error: 0.0064\n",
      "\n",
      "Epoch 08195: loss did not improve\n",
      "Epoch 8196/10000\n",
      "2700/2700 [==============================] - 0s 27us/step - loss: 1.2530e-04 - mean_absolute_error: 0.0067\n",
      "\n",
      "Epoch 08196: loss did not improve\n",
      "Epoch 8197/10000\n",
      "2700/2700 [==============================] - 0s 31us/step - loss: 1.2371e-04 - mean_absolute_error: 0.0066\n",
      "\n",
      "Epoch 08197: loss did not improve\n",
      "Epoch 8198/10000\n",
      "2700/2700 [==============================] - 0s 26us/step - loss: 1.2160e-04 - mean_absolute_error: 0.0064\n",
      "\n",
      "Epoch 08198: loss did not improve\n",
      "Epoch 8199/10000\n",
      "2700/2700 [==============================] - 0s 28us/step - loss: 1.2392e-04 - mean_absolute_error: 0.0066\n",
      "\n",
      "Epoch 08199: loss did not improve\n",
      "Epoch 8200/10000\n",
      "2700/2700 [==============================] - 0s 29us/step - loss: 1.2378e-04 - mean_absolute_error: 0.0066\n",
      "\n",
      "Epoch 08200: loss did not improve\n",
      "Epoch 8201/10000\n",
      "2700/2700 [==============================] - 0s 29us/step - loss: 1.2239e-04 - mean_absolute_error: 0.0064\n",
      "\n",
      "Epoch 08201: loss did not improve\n",
      "Epoch 8202/10000\n",
      "2700/2700 [==============================] - 0s 26us/step - loss: 1.2222e-04 - mean_absolute_error: 0.0065\n",
      "\n",
      "Epoch 08202: loss did not improve\n",
      "Epoch 8203/10000\n",
      "2700/2700 [==============================] - 0s 30us/step - loss: 1.2198e-04 - mean_absolute_error: 0.0065\n",
      "\n",
      "Epoch 08203: loss did not improve\n",
      "Epoch 8204/10000\n",
      "2700/2700 [==============================] - 0s 28us/step - loss: 1.2285e-04 - mean_absolute_error: 0.0065\n",
      "\n",
      "Epoch 08204: loss did not improve\n",
      "Epoch 8205/10000\n",
      "2700/2700 [==============================] - 0s 28us/step - loss: 1.2096e-04 - mean_absolute_error: 0.0063\n",
      "\n",
      "Epoch 08205: loss did not improve\n",
      "Epoch 8206/10000\n",
      "2700/2700 [==============================] - 0s 26us/step - loss: 1.2214e-04 - mean_absolute_error: 0.0064\n",
      "\n",
      "Epoch 08206: loss did not improve\n",
      "Epoch 8207/10000\n",
      "2700/2700 [==============================] - 0s 29us/step - loss: 1.2352e-04 - mean_absolute_error: 0.0066\n",
      "\n",
      "Epoch 08207: loss did not improve\n",
      "Epoch 8208/10000\n",
      "2700/2700 [==============================] - 0s 28us/step - loss: 1.2202e-04 - mean_absolute_error: 0.0066\n",
      "\n",
      "Epoch 08208: loss did not improve\n",
      "Epoch 8209/10000\n",
      "2700/2700 [==============================] - 0s 29us/step - loss: 1.2354e-04 - mean_absolute_error: 0.0066\n",
      "\n",
      "Epoch 08209: loss did not improve\n",
      "Epoch 8210/10000\n",
      "2700/2700 [==============================] - 0s 25us/step - loss: 1.2428e-04 - mean_absolute_error: 0.0065\n",
      "\n",
      "Epoch 08210: loss did not improve\n",
      "Epoch 8211/10000\n",
      "2700/2700 [==============================] - 0s 29us/step - loss: 1.2259e-04 - mean_absolute_error: 0.0065\n",
      "\n",
      "Epoch 08211: loss did not improve\n",
      "Epoch 8212/10000\n",
      "2700/2700 [==============================] - 0s 29us/step - loss: 1.2163e-04 - mean_absolute_error: 0.0063\n",
      "\n",
      "Epoch 08212: loss did not improve\n",
      "Epoch 8213/10000\n",
      "2700/2700 [==============================] - 0s 28us/step - loss: 1.2222e-04 - mean_absolute_error: 0.0065\n",
      "\n",
      "Epoch 08213: loss did not improve\n",
      "Epoch 8214/10000\n",
      "2700/2700 [==============================] - 0s 29us/step - loss: 1.2072e-04 - mean_absolute_error: 0.0063\n",
      "\n",
      "Epoch 08214: loss did not improve\n",
      "Epoch 8215/10000\n",
      "2700/2700 [==============================] - 0s 28us/step - loss: 1.2449e-04 - mean_absolute_error: 0.0066\n",
      "\n",
      "Epoch 08215: loss did not improve\n",
      "Epoch 8216/10000\n",
      "2700/2700 [==============================] - 0s 29us/step - loss: 1.2391e-04 - mean_absolute_error: 0.0066\n",
      "\n",
      "Epoch 08216: loss did not improve\n",
      "Epoch 8217/10000\n",
      "2700/2700 [==============================] - 0s 29us/step - loss: 1.2211e-04 - mean_absolute_error: 0.0063\n",
      "\n",
      "Epoch 08217: loss did not improve\n",
      "Epoch 8218/10000\n",
      "2700/2700 [==============================] - 0s 28us/step - loss: 1.2153e-04 - mean_absolute_error: 0.0065\n",
      "\n",
      "Epoch 08218: loss did not improve\n",
      "Epoch 8219/10000\n",
      "2700/2700 [==============================] - 0s 29us/step - loss: 1.2182e-04 - mean_absolute_error: 0.0063\n",
      "\n",
      "Epoch 08219: loss did not improve\n",
      "Epoch 8220/10000\n",
      "2700/2700 [==============================] - 0s 29us/step - loss: 1.2271e-04 - mean_absolute_error: 0.0064\n",
      "\n",
      "Epoch 08220: loss did not improve\n",
      "Epoch 8221/10000\n",
      "2700/2700 [==============================] - 0s 28us/step - loss: 1.2164e-04 - mean_absolute_error: 0.0064\n",
      "\n",
      "Epoch 08221: loss did not improve\n",
      "Epoch 8222/10000\n",
      "2700/2700 [==============================] - 0s 31us/step - loss: 1.2252e-04 - mean_absolute_error: 0.0065\n",
      "\n",
      "Epoch 08222: loss did not improve\n",
      "Epoch 8223/10000\n",
      "2700/2700 [==============================] - 0s 30us/step - loss: 1.2370e-04 - mean_absolute_error: 0.0065\n",
      "\n",
      "Epoch 08223: loss did not improve\n",
      "Epoch 8224/10000\n",
      "2700/2700 [==============================] - 0s 30us/step - loss: 1.2197e-04 - mean_absolute_error: 0.0064\n",
      "\n",
      "Epoch 08224: loss did not improve\n",
      "Epoch 8225/10000\n",
      "2700/2700 [==============================] - 0s 37us/step - loss: 1.2329e-04 - mean_absolute_error: 0.0065\n",
      "\n",
      "Epoch 08225: loss did not improve\n",
      "Epoch 8226/10000\n",
      "2700/2700 [==============================] - 0s 47us/step - loss: 1.2227e-04 - mean_absolute_error: 0.0064\n",
      "\n",
      "Epoch 08226: loss did not improve\n",
      "Epoch 8227/10000\n",
      "2700/2700 [==============================] - 0s 31us/step - loss: 1.2580e-04 - mean_absolute_error: 0.0067\n",
      "\n",
      "Epoch 08227: loss did not improve\n",
      "Epoch 8228/10000\n",
      "2700/2700 [==============================] - 0s 27us/step - loss: 1.2176e-04 - mean_absolute_error: 0.0065\n",
      "\n",
      "Epoch 08228: loss did not improve\n",
      "Epoch 8229/10000\n",
      "2700/2700 [==============================] - 0s 29us/step - loss: 1.2503e-04 - mean_absolute_error: 0.0066\n",
      "\n",
      "Epoch 08229: loss did not improve\n",
      "Epoch 8230/10000\n",
      "2700/2700 [==============================] - 0s 28us/step - loss: 1.2452e-04 - mean_absolute_error: 0.0067\n",
      "\n",
      "Epoch 08230: loss did not improve\n",
      "Epoch 8231/10000\n",
      "2700/2700 [==============================] - 0s 28us/step - loss: 1.2478e-04 - mean_absolute_error: 0.0066\n",
      "\n",
      "Epoch 08231: loss did not improve\n",
      "Epoch 8232/10000\n",
      "2700/2700 [==============================] - 0s 29us/step - loss: 1.2154e-04 - mean_absolute_error: 0.0065\n",
      "\n",
      "Epoch 08232: loss did not improve\n",
      "Epoch 8233/10000\n",
      "2700/2700 [==============================] - 0s 30us/step - loss: 1.2526e-04 - mean_absolute_error: 0.0066\n",
      "\n",
      "Epoch 08233: loss did not improve\n",
      "Epoch 8234/10000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2700/2700 [==============================] - 0s 29us/step - loss: 1.2090e-04 - mean_absolute_error: 0.0064\n",
      "\n",
      "Epoch 08234: loss did not improve\n",
      "Epoch 8235/10000\n",
      "2700/2700 [==============================] - 0s 28us/step - loss: 1.2331e-04 - mean_absolute_error: 0.0065\n",
      "\n",
      "Epoch 08235: loss did not improve\n",
      "Epoch 8236/10000\n",
      "2700/2700 [==============================] - 0s 30us/step - loss: 1.2184e-04 - mean_absolute_error: 0.0063\n",
      "\n",
      "Epoch 08236: loss did not improve\n",
      "Epoch 8237/10000\n",
      "2700/2700 [==============================] - 0s 28us/step - loss: 1.2611e-04 - mean_absolute_error: 0.0067\n",
      "\n",
      "Epoch 08237: loss did not improve\n",
      "Epoch 8238/10000\n",
      "2700/2700 [==============================] - 0s 28us/step - loss: 1.2170e-04 - mean_absolute_error: 0.0064\n",
      "\n",
      "Epoch 08238: loss did not improve\n",
      "Epoch 8239/10000\n",
      "2700/2700 [==============================] - 0s 35us/step - loss: 1.2330e-04 - mean_absolute_error: 0.0066\n",
      "\n",
      "Epoch 08239: loss did not improve\n",
      "Epoch 8240/10000\n",
      "2700/2700 [==============================] - 0s 34us/step - loss: 1.2137e-04 - mean_absolute_error: 0.0064\n",
      "\n",
      "Epoch 08240: loss did not improve\n",
      "Epoch 8241/10000\n",
      "2700/2700 [==============================] - 0s 35us/step - loss: 1.2270e-04 - mean_absolute_error: 0.0064\n",
      "\n",
      "Epoch 08241: loss did not improve\n",
      "Epoch 8242/10000\n",
      "2700/2700 [==============================] - 0s 29us/step - loss: 1.2228e-04 - mean_absolute_error: 0.0065\n",
      "\n",
      "Epoch 08242: loss did not improve\n",
      "Epoch 8243/10000\n",
      "2700/2700 [==============================] - 0s 26us/step - loss: 1.2276e-04 - mean_absolute_error: 0.0065\n",
      "\n",
      "Epoch 08243: loss did not improve\n",
      "Epoch 8244/10000\n",
      "2700/2700 [==============================] - 0s 29us/step - loss: 1.2385e-04 - mean_absolute_error: 0.0065\n",
      "\n",
      "Epoch 08244: loss did not improve\n",
      "Epoch 8245/10000\n",
      "2700/2700 [==============================] - 0s 29us/step - loss: 1.2281e-04 - mean_absolute_error: 0.0065\n",
      "\n",
      "Epoch 08245: loss did not improve\n",
      "Epoch 8246/10000\n",
      "2700/2700 [==============================] - 0s 28us/step - loss: 1.2216e-04 - mean_absolute_error: 0.0064\n",
      "\n",
      "Epoch 08246: loss did not improve\n",
      "Epoch 8247/10000\n",
      "2700/2700 [==============================] - 0s 26us/step - loss: 1.2304e-04 - mean_absolute_error: 0.0064\n",
      "\n",
      "Epoch 08247: loss did not improve\n",
      "Epoch 8248/10000\n",
      "2700/2700 [==============================] - 0s 30us/step - loss: 1.2127e-04 - mean_absolute_error: 0.0064\n",
      "\n",
      "Epoch 08248: loss did not improve\n",
      "Epoch 8249/10000\n",
      "2700/2700 [==============================] - 0s 29us/step - loss: 1.2170e-04 - mean_absolute_error: 0.0064\n",
      "\n",
      "Epoch 08249: loss did not improve\n",
      "Epoch 8250/10000\n",
      "2700/2700 [==============================] - 0s 29us/step - loss: 1.2142e-04 - mean_absolute_error: 0.0064\n",
      "\n",
      "Epoch 08250: loss did not improve\n",
      "Epoch 8251/10000\n",
      "2700/2700 [==============================] - 0s 40us/step - loss: 1.2117e-04 - mean_absolute_error: 0.0063\n",
      "\n",
      "Epoch 08251: loss did not improve\n",
      "Epoch 8252/10000\n",
      "2700/2700 [==============================] - 0s 32us/step - loss: 1.2398e-04 - mean_absolute_error: 0.0066\n",
      "\n",
      "Epoch 08252: loss did not improve\n",
      "Epoch 8253/10000\n",
      "2700/2700 [==============================] - 0s 30us/step - loss: 1.2039e-04 - mean_absolute_error: 0.0063\n",
      "\n",
      "Epoch 08253: loss did not improve\n",
      "Epoch 8254/10000\n",
      "2700/2700 [==============================] - 0s 26us/step - loss: 1.2232e-04 - mean_absolute_error: 0.0065\n",
      "\n",
      "Epoch 08254: loss did not improve\n",
      "Epoch 8255/10000\n",
      "2700/2700 [==============================] - 0s 30us/step - loss: 1.2069e-04 - mean_absolute_error: 0.0063\n",
      "\n",
      "Epoch 08255: loss did not improve\n",
      "Epoch 8256/10000\n",
      "2700/2700 [==============================] - 0s 31us/step - loss: 1.2177e-04 - mean_absolute_error: 0.0064\n",
      "\n",
      "Epoch 08256: loss did not improve\n",
      "Epoch 8257/10000\n",
      "2700/2700 [==============================] - 0s 27us/step - loss: 1.2235e-04 - mean_absolute_error: 0.0064\n",
      "\n",
      "Epoch 08257: loss did not improve\n",
      "Epoch 8258/10000\n",
      "2700/2700 [==============================] - 0s 27us/step - loss: 1.2470e-04 - mean_absolute_error: 0.0066\n",
      "\n",
      "Epoch 08258: loss did not improve\n",
      "Epoch 8259/10000\n",
      "2700/2700 [==============================] - 0s 29us/step - loss: 1.2220e-04 - mean_absolute_error: 0.0064\n",
      "\n",
      "Epoch 08259: loss did not improve\n",
      "Epoch 8260/10000\n",
      "2700/2700 [==============================] - 0s 29us/step - loss: 1.2066e-04 - mean_absolute_error: 0.0062\n",
      "\n",
      "Epoch 08260: loss did not improve\n",
      "Epoch 8261/10000\n",
      "2700/2700 [==============================] - 0s 27us/step - loss: 1.2345e-04 - mean_absolute_error: 0.0065\n",
      "\n",
      "Epoch 08261: loss did not improve\n",
      "Epoch 8262/10000\n",
      "2700/2700 [==============================] - 0s 29us/step - loss: 1.2176e-04 - mean_absolute_error: 0.0063\n",
      "\n",
      "Epoch 08262: loss did not improve\n",
      "Epoch 8263/10000\n",
      "2700/2700 [==============================] - 0s 27us/step - loss: 1.2435e-04 - mean_absolute_error: 0.0066\n",
      "\n",
      "Epoch 08263: loss did not improve\n",
      "Epoch 8264/10000\n",
      "2700/2700 [==============================] - 0s 30us/step - loss: 1.2129e-04 - mean_absolute_error: 0.0064\n",
      "\n",
      "Epoch 08264: loss did not improve\n",
      "Epoch 8265/10000\n",
      "2700/2700 [==============================] - 0s 30us/step - loss: 1.2045e-04 - mean_absolute_error: 0.0063\n",
      "\n",
      "Epoch 08265: loss did not improve\n",
      "Epoch 8266/10000\n",
      "2700/2700 [==============================] - 0s 28us/step - loss: 1.2219e-04 - mean_absolute_error: 0.0065\n",
      "\n",
      "Epoch 08266: loss did not improve\n",
      "Epoch 8267/10000\n",
      "2700/2700 [==============================] - 0s 26us/step - loss: 1.2166e-04 - mean_absolute_error: 0.0064\n",
      "\n",
      "Epoch 08267: loss did not improve\n",
      "Epoch 8268/10000\n",
      "2700/2700 [==============================] - 0s 29us/step - loss: 1.2476e-04 - mean_absolute_error: 0.0069\n",
      "\n",
      "Epoch 08268: loss did not improve\n",
      "Epoch 8269/10000\n",
      "2700/2700 [==============================] - 0s 29us/step - loss: 1.2675e-04 - mean_absolute_error: 0.0067\n",
      "\n",
      "Epoch 08269: loss did not improve\n",
      "Epoch 8270/10000\n",
      "2700/2700 [==============================] - 0s 31us/step - loss: 1.2033e-04 - mean_absolute_error: 0.0062\n",
      "\n",
      "Epoch 08270: loss did not improve\n",
      "Epoch 8271/10000\n",
      "2700/2700 [==============================] - 0s 38us/step - loss: 1.2097e-04 - mean_absolute_error: 0.0063\n",
      "\n",
      "Epoch 08271: loss did not improve\n",
      "Epoch 8272/10000\n",
      "2700/2700 [==============================] - 0s 28us/step - loss: 1.2149e-04 - mean_absolute_error: 0.0063\n",
      "\n",
      "Epoch 08272: loss did not improve\n",
      "Epoch 8273/10000\n",
      "2700/2700 [==============================] - 0s 31us/step - loss: 1.2318e-04 - mean_absolute_error: 0.0066\n",
      "\n",
      "Epoch 08273: loss did not improve\n",
      "Epoch 8274/10000\n",
      "2700/2700 [==============================] - 0s 32us/step - loss: 1.2224e-04 - mean_absolute_error: 0.0064\n",
      "\n",
      "Epoch 08274: loss did not improve\n",
      "Epoch 8275/10000\n",
      "2700/2700 [==============================] - 0s 28us/step - loss: 1.2276e-04 - mean_absolute_error: 0.0065\n",
      "\n",
      "Epoch 08275: loss did not improve\n",
      "Epoch 8276/10000\n",
      "2700/2700 [==============================] - 0s 27us/step - loss: 1.2497e-04 - mean_absolute_error: 0.0067\n",
      "\n",
      "Epoch 08276: loss did not improve\n",
      "Epoch 8277/10000\n",
      "2700/2700 [==============================] - 0s 34us/step - loss: 1.2226e-04 - mean_absolute_error: 0.0064\n",
      "\n",
      "Epoch 08277: loss did not improve\n",
      "Epoch 8278/10000\n",
      "2700/2700 [==============================] - 0s 29us/step - loss: 1.2462e-04 - mean_absolute_error: 0.0066\n",
      "\n",
      "Epoch 08278: loss did not improve\n",
      "Epoch 8279/10000\n",
      "2700/2700 [==============================] - 0s 27us/step - loss: 1.2325e-04 - mean_absolute_error: 0.0065\n",
      "\n",
      "Epoch 08279: loss did not improve\n",
      "Epoch 8280/10000\n",
      "2700/2700 [==============================] - 0s 28us/step - loss: 1.2288e-04 - mean_absolute_error: 0.0065\n",
      "\n",
      "Epoch 08280: loss did not improve\n",
      "Epoch 8281/10000\n",
      "2700/2700 [==============================] - 0s 28us/step - loss: 1.1994e-04 - mean_absolute_error: 0.0063\n",
      "\n",
      "Epoch 08281: loss did not improve\n",
      "Epoch 8282/10000\n",
      "2700/2700 [==============================] - 0s 28us/step - loss: 1.2186e-04 - mean_absolute_error: 0.0065\n",
      "\n",
      "Epoch 08282: loss did not improve\n",
      "Epoch 8283/10000\n",
      "2700/2700 [==============================] - 0s 29us/step - loss: 1.2344e-04 - mean_absolute_error: 0.0066\n",
      "\n",
      "Epoch 08283: loss did not improve\n",
      "Epoch 8284/10000\n",
      "2700/2700 [==============================] - 0s 30us/step - loss: 1.2159e-04 - mean_absolute_error: 0.0064\n",
      "\n",
      "Epoch 08284: loss did not improve\n",
      "Epoch 8285/10000\n",
      "2700/2700 [==============================] - 0s 29us/step - loss: 1.2283e-04 - mean_absolute_error: 0.0064\n",
      "\n",
      "Epoch 08285: loss did not improve\n",
      "Epoch 8286/10000\n",
      "2700/2700 [==============================] - 0s 28us/step - loss: 1.2108e-04 - mean_absolute_error: 0.0064\n",
      "\n",
      "Epoch 08286: loss did not improve\n",
      "Epoch 8287/10000\n",
      "2700/2700 [==============================] - 0s 28us/step - loss: 1.2046e-04 - mean_absolute_error: 0.0063\n",
      "\n",
      "Epoch 08287: loss did not improve\n",
      "Epoch 8288/10000\n",
      "2700/2700 [==============================] - 0s 29us/step - loss: 1.2388e-04 - mean_absolute_error: 0.0067\n",
      "\n",
      "Epoch 08288: loss did not improve\n",
      "Epoch 8289/10000\n",
      "2700/2700 [==============================] - 0s 28us/step - loss: 1.2566e-04 - mean_absolute_error: 0.0067\n",
      "\n",
      "Epoch 08289: loss did not improve\n",
      "Epoch 8290/10000\n",
      "2700/2700 [==============================] - 0s 30us/step - loss: 1.2167e-04 - mean_absolute_error: 0.0064\n",
      "\n",
      "Epoch 08290: loss did not improve\n",
      "Epoch 8291/10000\n",
      "2700/2700 [==============================] - 0s 25us/step - loss: 1.2220e-04 - mean_absolute_error: 0.0064\n",
      "\n",
      "Epoch 08291: loss did not improve\n",
      "Epoch 8292/10000\n",
      "2700/2700 [==============================] - 0s 29us/step - loss: 1.2127e-04 - mean_absolute_error: 0.0063\n",
      "\n",
      "Epoch 08292: loss did not improve\n",
      "Epoch 8293/10000\n",
      "2700/2700 [==============================] - 0s 28us/step - loss: 1.2995e-04 - mean_absolute_error: 0.0069\n",
      "\n",
      "Epoch 08293: loss did not improve\n",
      "Epoch 8294/10000\n",
      "2700/2700 [==============================] - 0s 29us/step - loss: 1.2098e-04 - mean_absolute_error: 0.0063\n",
      "\n",
      "Epoch 08294: loss did not improve\n",
      "Epoch 8295/10000\n",
      "2700/2700 [==============================] - 0s 26us/step - loss: 1.2163e-04 - mean_absolute_error: 0.0066\n",
      "\n",
      "Epoch 08295: loss did not improve\n",
      "Epoch 8296/10000\n",
      "2700/2700 [==============================] - 0s 27us/step - loss: 1.2277e-04 - mean_absolute_error: 0.0064\n",
      "\n",
      "Epoch 08296: loss did not improve\n",
      "Epoch 8297/10000\n",
      "2700/2700 [==============================] - 0s 28us/step - loss: 1.2141e-04 - mean_absolute_error: 0.0064\n",
      "\n",
      "Epoch 08297: loss did not improve\n",
      "Epoch 8298/10000\n",
      "2700/2700 [==============================] - 0s 28us/step - loss: 1.2358e-04 - mean_absolute_error: 0.0066\n",
      "\n",
      "Epoch 08298: loss did not improve\n",
      "Epoch 8299/10000\n",
      "2700/2700 [==============================] - 0s 26us/step - loss: 1.2275e-04 - mean_absolute_error: 0.0065\n",
      "\n",
      "Epoch 08299: loss did not improve\n",
      "Epoch 8300/10000\n",
      "2700/2700 [==============================] - 0s 34us/step - loss: 1.2115e-04 - mean_absolute_error: 0.0063\n",
      "\n",
      "Epoch 08300: loss did not improve\n",
      "Epoch 8301/10000\n",
      "2700/2700 [==============================] - 0s 34us/step - loss: 1.2445e-04 - mean_absolute_error: 0.0066\n",
      "\n",
      "Epoch 08301: loss did not improve\n",
      "Epoch 8302/10000\n",
      "2700/2700 [==============================] - 0s 33us/step - loss: 1.2179e-04 - mean_absolute_error: 0.0064\n",
      "\n",
      "Epoch 08302: loss did not improve\n",
      "Epoch 8303/10000\n",
      "2700/2700 [==============================] - 0s 28us/step - loss: 1.2107e-04 - mean_absolute_error: 0.0063\n",
      "\n",
      "Epoch 08303: loss did not improve\n",
      "Epoch 8304/10000\n",
      "2700/2700 [==============================] - 0s 28us/step - loss: 1.2325e-04 - mean_absolute_error: 0.0065\n",
      "\n",
      "Epoch 08304: loss did not improve\n",
      "Epoch 8305/10000\n",
      "2700/2700 [==============================] - 0s 26us/step - loss: 1.2093e-04 - mean_absolute_error: 0.0063\n",
      "\n",
      "Epoch 08305: loss did not improve\n",
      "Epoch 8306/10000\n",
      "2700/2700 [==============================] - 0s 30us/step - loss: 1.2096e-04 - mean_absolute_error: 0.0064\n",
      "\n",
      "Epoch 08306: loss did not improve\n",
      "Epoch 8307/10000\n",
      "2700/2700 [==============================] - 0s 30us/step - loss: 1.2334e-04 - mean_absolute_error: 0.0066\n",
      "\n",
      "Epoch 08307: loss did not improve\n",
      "Epoch 8308/10000\n",
      "2700/2700 [==============================] - 0s 40us/step - loss: 1.2080e-04 - mean_absolute_error: 0.0063\n",
      "\n",
      "Epoch 08308: loss did not improve\n",
      "Epoch 8309/10000\n",
      "2700/2700 [==============================] - 0s 38us/step - loss: 1.2236e-04 - mean_absolute_error: 0.0065\n",
      "\n",
      "Epoch 08309: loss did not improve\n",
      "Epoch 8310/10000\n",
      "2700/2700 [==============================] - 0s 32us/step - loss: 1.2113e-04 - mean_absolute_error: 0.0063\n",
      "\n",
      "Epoch 08310: loss did not improve\n",
      "Epoch 8311/10000\n",
      "2700/2700 [==============================] - 0s 29us/step - loss: 1.2099e-04 - mean_absolute_error: 0.0064\n",
      "\n",
      "Epoch 08311: loss did not improve\n",
      "Epoch 8312/10000\n",
      "2700/2700 [==============================] - 0s 32us/step - loss: 1.2336e-04 - mean_absolute_error: 0.0065\n",
      "\n",
      "Epoch 08312: loss did not improve\n",
      "Epoch 8313/10000\n",
      "2700/2700 [==============================] - 0s 28us/step - loss: 1.2303e-04 - mean_absolute_error: 0.0065\n",
      "\n",
      "Epoch 08313: loss did not improve\n",
      "Epoch 8314/10000\n",
      "2700/2700 [==============================] - 0s 33us/step - loss: 1.2146e-04 - mean_absolute_error: 0.0064\n",
      "\n",
      "Epoch 08314: loss did not improve\n",
      "Epoch 8315/10000\n",
      "2700/2700 [==============================] - 0s 29us/step - loss: 1.2323e-04 - mean_absolute_error: 0.0066\n",
      "\n",
      "Epoch 08315: loss did not improve\n",
      "Epoch 8316/10000\n",
      "2700/2700 [==============================] - 0s 29us/step - loss: 1.2259e-04 - mean_absolute_error: 0.0064\n",
      "\n",
      "Epoch 08316: loss did not improve\n",
      "Epoch 8317/10000\n",
      "2700/2700 [==============================] - 0s 30us/step - loss: 1.1990e-04 - mean_absolute_error: 0.0062\n",
      "\n",
      "Epoch 08317: loss improved from 0.00012 to 0.00012, saving model to keras/best_baseline_weights.hdf5\n",
      "Epoch 8318/10000\n",
      "2700/2700 [==============================] - 0s 31us/step - loss: 1.2148e-04 - mean_absolute_error: 0.0064\n",
      "\n",
      "Epoch 08318: loss did not improve\n",
      "Epoch 8319/10000\n",
      "2700/2700 [==============================] - 0s 25us/step - loss: 1.2198e-04 - mean_absolute_error: 0.0064\n",
      "\n",
      "Epoch 08319: loss did not improve\n",
      "Epoch 8320/10000\n",
      "2700/2700 [==============================] - 0s 30us/step - loss: 1.2183e-04 - mean_absolute_error: 0.0064\n",
      "\n",
      "Epoch 08320: loss did not improve\n",
      "Epoch 8321/10000\n",
      "2700/2700 [==============================] - 0s 29us/step - loss: 1.2141e-04 - mean_absolute_error: 0.0064\n",
      "\n",
      "Epoch 08321: loss did not improve\n",
      "Epoch 8322/10000\n",
      "2700/2700 [==============================] - 0s 28us/step - loss: 1.2110e-04 - mean_absolute_error: 0.0064\n",
      "\n",
      "Epoch 08322: loss did not improve\n",
      "Epoch 8323/10000\n",
      "2700/2700 [==============================] - 0s 28us/step - loss: 1.2098e-04 - mean_absolute_error: 0.0064\n",
      "\n",
      "Epoch 08323: loss did not improve\n",
      "Epoch 8324/10000\n",
      "2700/2700 [==============================] - 0s 27us/step - loss: 1.2160e-04 - mean_absolute_error: 0.0064\n",
      "\n",
      "Epoch 08324: loss did not improve\n",
      "Epoch 8325/10000\n",
      "2700/2700 [==============================] - 0s 29us/step - loss: 1.2826e-04 - mean_absolute_error: 0.0068\n",
      "\n",
      "Epoch 08325: loss did not improve\n",
      "Epoch 8326/10000\n",
      "2700/2700 [==============================] - 0s 29us/step - loss: 1.2424e-04 - mean_absolute_error: 0.0066\n",
      "\n",
      "Epoch 08326: loss did not improve\n",
      "Epoch 8327/10000\n",
      "2700/2700 [==============================] - 0s 29us/step - loss: 1.2221e-04 - mean_absolute_error: 0.0064\n",
      "\n",
      "Epoch 08327: loss did not improve\n",
      "Epoch 8328/10000\n",
      "2700/2700 [==============================] - 0s 36us/step - loss: 1.2134e-04 - mean_absolute_error: 0.0063\n",
      "\n",
      "Epoch 08328: loss did not improve\n",
      "Epoch 8329/10000\n",
      "2700/2700 [==============================] - 0s 28us/step - loss: 1.2475e-04 - mean_absolute_error: 0.0068\n",
      "\n",
      "Epoch 08329: loss did not improve\n",
      "Epoch 8330/10000\n",
      "2700/2700 [==============================] - 0s 30us/step - loss: 1.2323e-04 - mean_absolute_error: 0.0065\n",
      "\n",
      "Epoch 08330: loss did not improve\n",
      "Epoch 8331/10000\n",
      "2700/2700 [==============================] - 0s 30us/step - loss: 1.2078e-04 - mean_absolute_error: 0.0063\n",
      "\n",
      "Epoch 08331: loss did not improve\n",
      "Epoch 8332/10000\n",
      "2700/2700 [==============================] - 0s 30us/step - loss: 1.2115e-04 - mean_absolute_error: 0.0063\n",
      "\n",
      "Epoch 08332: loss did not improve\n",
      "Epoch 8333/10000\n",
      "2700/2700 [==============================] - 0s 29us/step - loss: 1.2408e-04 - mean_absolute_error: 0.0066\n",
      "\n",
      "Epoch 08333: loss did not improve\n",
      "Epoch 8334/10000\n",
      "2700/2700 [==============================] - 0s 28us/step - loss: 1.2186e-04 - mean_absolute_error: 0.0064\n",
      "\n",
      "Epoch 08334: loss did not improve\n",
      "Epoch 8335/10000\n",
      "2700/2700 [==============================] - 0s 43us/step - loss: 1.2386e-04 - mean_absolute_error: 0.0066\n",
      "\n",
      "Epoch 08335: loss did not improve\n",
      "Epoch 8336/10000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2700/2700 [==============================] - 0s 39us/step - loss: 1.2114e-04 - mean_absolute_error: 0.0064\n",
      "\n",
      "Epoch 08336: loss did not improve\n",
      "Epoch 8337/10000\n",
      "2700/2700 [==============================] - 0s 36us/step - loss: 1.2552e-04 - mean_absolute_error: 0.0067\n",
      "\n",
      "Epoch 08337: loss did not improve\n",
      "Epoch 8338/10000\n",
      "2700/2700 [==============================] - 0s 27us/step - loss: 1.2126e-04 - mean_absolute_error: 0.0063\n",
      "\n",
      "Epoch 08338: loss did not improve\n",
      "Epoch 8339/10000\n",
      "2700/2700 [==============================] - 0s 30us/step - loss: 1.2216e-04 - mean_absolute_error: 0.0064\n",
      "\n",
      "Epoch 08339: loss did not improve\n",
      "Epoch 8340/10000\n",
      "2700/2700 [==============================] - 0s 29us/step - loss: 1.2319e-04 - mean_absolute_error: 0.0067\n",
      "\n",
      "Epoch 08340: loss did not improve\n",
      "Epoch 8341/10000\n",
      "2700/2700 [==============================] - 0s 29us/step - loss: 1.2230e-04 - mean_absolute_error: 0.0064\n",
      "\n",
      "Epoch 08341: loss did not improve\n",
      "Epoch 8342/10000\n",
      "2700/2700 [==============================] - 0s 26us/step - loss: 1.2342e-04 - mean_absolute_error: 0.0066\n",
      "\n",
      "Epoch 08342: loss did not improve\n",
      "Epoch 8343/10000\n",
      "2700/2700 [==============================] - 0s 29us/step - loss: 1.2407e-04 - mean_absolute_error: 0.0065\n",
      "\n",
      "Epoch 08343: loss did not improve\n",
      "Epoch 8344/10000\n",
      "2700/2700 [==============================] - 0s 34us/step - loss: 1.2776e-04 - mean_absolute_error: 0.0069\n",
      "\n",
      "Epoch 08344: loss did not improve\n",
      "Epoch 8345/10000\n",
      "2700/2700 [==============================] - 0s 32us/step - loss: 1.2103e-04 - mean_absolute_error: 0.0063\n",
      "\n",
      "Epoch 08345: loss did not improve\n",
      "Epoch 8346/10000\n",
      "2700/2700 [==============================] - 0s 34us/step - loss: 1.2326e-04 - mean_absolute_error: 0.0065\n",
      "\n",
      "Epoch 08346: loss did not improve\n",
      "Epoch 8347/10000\n",
      "2700/2700 [==============================] - 0s 31us/step - loss: 1.2469e-04 - mean_absolute_error: 0.0066\n",
      "\n",
      "Epoch 08347: loss did not improve\n",
      "Epoch 8348/10000\n",
      "2700/2700 [==============================] - 0s 33us/step - loss: 1.2234e-04 - mean_absolute_error: 0.0065\n",
      "\n",
      "Epoch 08348: loss did not improve\n",
      "Epoch 8349/10000\n",
      "2700/2700 [==============================] - 0s 32us/step - loss: 1.2383e-04 - mean_absolute_error: 0.0066\n",
      "\n",
      "Epoch 08349: loss did not improve\n",
      "Epoch 8350/10000\n",
      "2700/2700 [==============================] - 0s 34us/step - loss: 1.2238e-04 - mean_absolute_error: 0.0065\n",
      "\n",
      "Epoch 08350: loss did not improve\n",
      "Epoch 8351/10000\n",
      "2700/2700 [==============================] - 0s 30us/step - loss: 1.2181e-04 - mean_absolute_error: 0.0065\n",
      "\n",
      "Epoch 08351: loss did not improve\n",
      "Epoch 8352/10000\n",
      "2700/2700 [==============================] - 0s 43us/step - loss: 1.2127e-04 - mean_absolute_error: 0.0063\n",
      "\n",
      "Epoch 08352: loss did not improve\n",
      "Epoch 8353/10000\n",
      "2700/2700 [==============================] - 0s 33us/step - loss: 1.2257e-04 - mean_absolute_error: 0.0065\n",
      "\n",
      "Epoch 08353: loss did not improve\n",
      "Epoch 8354/10000\n",
      "2700/2700 [==============================] - 0s 28us/step - loss: 1.2224e-04 - mean_absolute_error: 0.0065\n",
      "\n",
      "Epoch 08354: loss did not improve\n",
      "Epoch 8355/10000\n",
      "2700/2700 [==============================] - 0s 30us/step - loss: 1.2099e-04 - mean_absolute_error: 0.0065\n",
      "\n",
      "Epoch 08355: loss did not improve\n",
      "Epoch 8356/10000\n",
      "2700/2700 [==============================] - 0s 34us/step - loss: 1.2287e-04 - mean_absolute_error: 0.0065\n",
      "\n",
      "Epoch 08356: loss did not improve\n",
      "Epoch 8357/10000\n",
      "2700/2700 [==============================] - 0s 28us/step - loss: 1.2618e-04 - mean_absolute_error: 0.0067\n",
      "\n",
      "Epoch 08357: loss did not improve\n",
      "Epoch 8358/10000\n",
      "2700/2700 [==============================] - 0s 29us/step - loss: 1.2213e-04 - mean_absolute_error: 0.0065\n",
      "\n",
      "Epoch 08358: loss did not improve\n",
      "Epoch 8359/10000\n",
      "2700/2700 [==============================] - 0s 29us/step - loss: 1.2303e-04 - mean_absolute_error: 0.0065\n",
      "\n",
      "Epoch 08359: loss did not improve\n",
      "Epoch 8360/10000\n",
      "2700/2700 [==============================] - 0s 29us/step - loss: 1.2177e-04 - mean_absolute_error: 0.0064\n",
      "\n",
      "Epoch 08360: loss did not improve\n",
      "Epoch 8361/10000\n",
      "2700/2700 [==============================] - 0s 28us/step - loss: 1.2278e-04 - mean_absolute_error: 0.0064\n",
      "\n",
      "Epoch 08361: loss did not improve\n",
      "Epoch 8362/10000\n",
      "2700/2700 [==============================] - 0s 29us/step - loss: 1.2145e-04 - mean_absolute_error: 0.0063\n",
      "\n",
      "Epoch 08362: loss did not improve\n",
      "Epoch 8363/10000\n",
      "2700/2700 [==============================] - 0s 27us/step - loss: 1.2127e-04 - mean_absolute_error: 0.0064\n",
      "\n",
      "Epoch 08363: loss did not improve\n",
      "Epoch 8364/10000\n",
      "2700/2700 [==============================] - 0s 35us/step - loss: 1.2101e-04 - mean_absolute_error: 0.0064\n",
      "\n",
      "Epoch 08364: loss did not improve\n",
      "Epoch 8365/10000\n",
      "2700/2700 [==============================] - 0s 29us/step - loss: 1.2141e-04 - mean_absolute_error: 0.0063\n",
      "\n",
      "Epoch 08365: loss did not improve\n",
      "Epoch 8366/10000\n",
      "2700/2700 [==============================] - 0s 28us/step - loss: 1.2044e-04 - mean_absolute_error: 0.0063\n",
      "\n",
      "Epoch 08366: loss did not improve\n",
      "Epoch 8367/10000\n",
      "2700/2700 [==============================] - 0s 29us/step - loss: 1.2390e-04 - mean_absolute_error: 0.0066\n",
      "\n",
      "Epoch 08367: loss did not improve\n",
      "Epoch 8368/10000\n",
      "2700/2700 [==============================] - 0s 28us/step - loss: 1.2256e-04 - mean_absolute_error: 0.0064\n",
      "\n",
      "Epoch 08368: loss did not improve\n",
      "Epoch 8369/10000\n",
      "2700/2700 [==============================] - 0s 29us/step - loss: 1.2234e-04 - mean_absolute_error: 0.0064\n",
      "\n",
      "Epoch 08369: loss did not improve\n",
      "Epoch 8370/10000\n",
      "2700/2700 [==============================] - 0s 28us/step - loss: 1.2060e-04 - mean_absolute_error: 0.0063\n",
      "\n",
      "Epoch 08370: loss did not improve\n",
      "Epoch 8371/10000\n",
      "2700/2700 [==============================] - 0s 33us/step - loss: 1.2325e-04 - mean_absolute_error: 0.0065\n",
      "\n",
      "Epoch 08371: loss did not improve\n",
      "Epoch 8372/10000\n",
      "2700/2700 [==============================] - 0s 27us/step - loss: 1.1928e-04 - mean_absolute_error: 0.0062\n",
      "\n",
      "Epoch 08372: loss improved from 0.00012 to 0.00012, saving model to keras/best_baseline_weights.hdf5\n",
      "Epoch 8373/10000\n",
      "2700/2700 [==============================] - 0s 29us/step - loss: 1.2268e-04 - mean_absolute_error: 0.0064\n",
      "\n",
      "Epoch 08373: loss did not improve\n",
      "Epoch 8374/10000\n",
      "2700/2700 [==============================] - 0s 28us/step - loss: 1.2204e-04 - mean_absolute_error: 0.0064\n",
      "\n",
      "Epoch 08374: loss did not improve\n",
      "Epoch 8375/10000\n",
      "2700/2700 [==============================] - 0s 29us/step - loss: 1.2276e-04 - mean_absolute_error: 0.0065\n",
      "\n",
      "Epoch 08375: loss did not improve\n",
      "Epoch 8376/10000\n",
      "2700/2700 [==============================] - 0s 34us/step - loss: 1.2451e-04 - mean_absolute_error: 0.0066\n",
      "\n",
      "Epoch 08376: loss did not improve\n",
      "Epoch 8377/10000\n",
      "2700/2700 [==============================] - 0s 27us/step - loss: 1.2302e-04 - mean_absolute_error: 0.0065\n",
      "\n",
      "Epoch 08377: loss did not improve\n",
      "Epoch 8378/10000\n",
      "2700/2700 [==============================] - 0s 29us/step - loss: 1.2169e-04 - mean_absolute_error: 0.0064\n",
      "\n",
      "Epoch 08378: loss did not improve\n",
      "Epoch 8379/10000\n",
      "2700/2700 [==============================] - 0s 31us/step - loss: 1.2172e-04 - mean_absolute_error: 0.0064\n",
      "\n",
      "Epoch 08379: loss did not improve\n",
      "Epoch 8380/10000\n",
      "2700/2700 [==============================] - 0s 29us/step - loss: 1.2203e-04 - mean_absolute_error: 0.0064\n",
      "\n",
      "Epoch 08380: loss did not improve\n",
      "Epoch 8381/10000\n",
      "2700/2700 [==============================] - 0s 30us/step - loss: 1.2338e-04 - mean_absolute_error: 0.0065\n",
      "\n",
      "Epoch 08381: loss did not improve\n",
      "Epoch 8382/10000\n",
      "2700/2700 [==============================] - 0s 28us/step - loss: 1.2114e-04 - mean_absolute_error: 0.0063\n",
      "\n",
      "Epoch 08382: loss did not improve\n",
      "Epoch 8383/10000\n",
      "2700/2700 [==============================] - 0s 27us/step - loss: 1.2483e-04 - mean_absolute_error: 0.0066\n",
      "\n",
      "Epoch 08383: loss did not improve\n",
      "Epoch 8384/10000\n",
      "2700/2700 [==============================] - 0s 30us/step - loss: 1.2415e-04 - mean_absolute_error: 0.0066\n",
      "\n",
      "Epoch 08384: loss did not improve\n",
      "Epoch 8385/10000\n",
      "2700/2700 [==============================] - 0s 28us/step - loss: 1.2492e-04 - mean_absolute_error: 0.0068\n",
      "\n",
      "Epoch 08385: loss did not improve\n",
      "Epoch 8386/10000\n",
      "2700/2700 [==============================] - 0s 30us/step - loss: 1.2074e-04 - mean_absolute_error: 0.0063\n",
      "\n",
      "Epoch 08386: loss did not improve\n",
      "Epoch 8387/10000\n",
      "2700/2700 [==============================] - 0s 31us/step - loss: 1.2190e-04 - mean_absolute_error: 0.0064\n",
      "\n",
      "Epoch 08387: loss did not improve\n",
      "Epoch 8388/10000\n",
      "2700/2700 [==============================] - 0s 26us/step - loss: 1.2088e-04 - mean_absolute_error: 0.0063\n",
      "\n",
      "Epoch 08388: loss did not improve\n",
      "Epoch 8389/10000\n",
      "2700/2700 [==============================] - 0s 30us/step - loss: 1.2075e-04 - mean_absolute_error: 0.0063\n",
      "\n",
      "Epoch 08389: loss did not improve\n",
      "Epoch 8390/10000\n",
      "2700/2700 [==============================] - 0s 27us/step - loss: 1.2181e-04 - mean_absolute_error: 0.0064\n",
      "\n",
      "Epoch 08390: loss did not improve\n",
      "Epoch 8391/10000\n",
      "2700/2700 [==============================] - 0s 31us/step - loss: 1.2619e-04 - mean_absolute_error: 0.0069\n",
      "\n",
      "Epoch 08391: loss did not improve\n",
      "Epoch 8392/10000\n",
      "2700/2700 [==============================] - 0s 27us/step - loss: 1.2301e-04 - mean_absolute_error: 0.0066\n",
      "\n",
      "Epoch 08392: loss did not improve\n",
      "Epoch 8393/10000\n",
      "2700/2700 [==============================] - 0s 28us/step - loss: 1.2205e-04 - mean_absolute_error: 0.0064\n",
      "\n",
      "Epoch 08393: loss did not improve\n",
      "Epoch 8394/10000\n",
      "2700/2700 [==============================] - 0s 30us/step - loss: 1.2028e-04 - mean_absolute_error: 0.0063\n",
      "\n",
      "Epoch 08394: loss did not improve\n",
      "Epoch 8395/10000\n",
      "2700/2700 [==============================] - 0s 30us/step - loss: 1.2051e-04 - mean_absolute_error: 0.0063\n",
      "\n",
      "Epoch 08395: loss did not improve\n",
      "Epoch 8396/10000\n",
      "2700/2700 [==============================] - 0s 29us/step - loss: 1.2202e-04 - mean_absolute_error: 0.0064\n",
      "\n",
      "Epoch 08396: loss did not improve\n",
      "Epoch 8397/10000\n",
      "2700/2700 [==============================] - 0s 26us/step - loss: 1.2134e-04 - mean_absolute_error: 0.0064\n",
      "\n",
      "Epoch 08397: loss did not improve\n",
      "Epoch 8398/10000\n",
      "2700/2700 [==============================] - 0s 31us/step - loss: 1.2376e-04 - mean_absolute_error: 0.0066\n",
      "\n",
      "Epoch 08398: loss did not improve\n",
      "Epoch 8399/10000\n",
      "2700/2700 [==============================] - 0s 28us/step - loss: 1.2064e-04 - mean_absolute_error: 0.0064\n",
      "\n",
      "Epoch 08399: loss did not improve\n",
      "Epoch 8400/10000\n",
      "2700/2700 [==============================] - 0s 28us/step - loss: 1.2438e-04 - mean_absolute_error: 0.0068\n",
      "\n",
      "Epoch 08400: loss did not improve\n",
      "Epoch 8401/10000\n",
      "2700/2700 [==============================] - 0s 27us/step - loss: 1.2259e-04 - mean_absolute_error: 0.0065\n",
      "\n",
      "Epoch 08401: loss did not improve\n",
      "Epoch 8402/10000\n",
      "2700/2700 [==============================] - 0s 29us/step - loss: 1.2225e-04 - mean_absolute_error: 0.0064\n",
      "\n",
      "Epoch 08402: loss did not improve\n",
      "Epoch 8403/10000\n",
      "2700/2700 [==============================] - 0s 27us/step - loss: 1.2436e-04 - mean_absolute_error: 0.0065\n",
      "\n",
      "Epoch 08403: loss did not improve\n",
      "Epoch 8404/10000\n",
      "2700/2700 [==============================] - 0s 29us/step - loss: 1.2446e-04 - mean_absolute_error: 0.0067\n",
      "\n",
      "Epoch 08404: loss did not improve\n",
      "Epoch 8405/10000\n",
      "2700/2700 [==============================] - 0s 36us/step - loss: 1.2334e-04 - mean_absolute_error: 0.0065\n",
      "\n",
      "Epoch 08405: loss did not improve\n",
      "Epoch 8406/10000\n",
      "2700/2700 [==============================] - 0s 29us/step - loss: 1.2106e-04 - mean_absolute_error: 0.0063\n",
      "\n",
      "Epoch 08406: loss did not improve\n",
      "Epoch 8407/10000\n",
      "2700/2700 [==============================] - 0s 27us/step - loss: 1.2495e-04 - mean_absolute_error: 0.0066\n",
      "\n",
      "Epoch 08407: loss did not improve\n",
      "Epoch 8408/10000\n",
      "2700/2700 [==============================] - 0s 35us/step - loss: 1.2278e-04 - mean_absolute_error: 0.0065\n",
      "\n",
      "Epoch 08408: loss did not improve\n",
      "Epoch 8409/10000\n",
      "2700/2700 [==============================] - 0s 36us/step - loss: 1.2193e-04 - mean_absolute_error: 0.0064\n",
      "\n",
      "Epoch 08409: loss did not improve\n",
      "Epoch 8410/10000\n",
      "2700/2700 [==============================] - 0s 40us/step - loss: 1.2224e-04 - mean_absolute_error: 0.0065\n",
      "\n",
      "Epoch 08410: loss did not improve\n",
      "Epoch 8411/10000\n",
      "2700/2700 [==============================] - 0s 36us/step - loss: 1.2049e-04 - mean_absolute_error: 0.0063\n",
      "\n",
      "Epoch 08411: loss did not improve\n",
      "Epoch 8412/10000\n",
      "2700/2700 [==============================] - 0s 36us/step - loss: 1.2125e-04 - mean_absolute_error: 0.0063\n",
      "\n",
      "Epoch 08412: loss did not improve\n",
      "Epoch 8413/10000\n",
      "2700/2700 [==============================] - 0s 36us/step - loss: 1.2099e-04 - mean_absolute_error: 0.0063\n",
      "\n",
      "Epoch 08413: loss did not improve\n",
      "Epoch 8414/10000\n",
      "2700/2700 [==============================] - 0s 29us/step - loss: 1.2109e-04 - mean_absolute_error: 0.0063\n",
      "\n",
      "Epoch 08414: loss did not improve\n",
      "Epoch 8415/10000\n",
      "2700/2700 [==============================] - 0s 31us/step - loss: 1.2130e-04 - mean_absolute_error: 0.0064\n",
      "\n",
      "Epoch 08415: loss did not improve\n",
      "Epoch 8416/10000\n",
      "2700/2700 [==============================] - 0s 29us/step - loss: 1.2094e-04 - mean_absolute_error: 0.0063\n",
      "\n",
      "Epoch 08416: loss did not improve\n",
      "Epoch 8417/10000\n",
      "2700/2700 [==============================] - 0s 26us/step - loss: 1.2248e-04 - mean_absolute_error: 0.0065\n",
      "\n",
      "Epoch 08417: loss did not improve\n",
      "Epoch 8418/10000\n",
      "2700/2700 [==============================] - 0s 29us/step - loss: 1.2020e-04 - mean_absolute_error: 0.0062\n",
      "\n",
      "Epoch 08418: loss did not improve\n",
      "Epoch 8419/10000\n",
      "2700/2700 [==============================] - 0s 30us/step - loss: 1.2665e-04 - mean_absolute_error: 0.0068\n",
      "\n",
      "Epoch 08419: loss did not improve\n",
      "Epoch 8420/10000\n",
      "2700/2700 [==============================] - 0s 28us/step - loss: 1.2211e-04 - mean_absolute_error: 0.0066\n",
      "\n",
      "Epoch 08420: loss did not improve\n",
      "Epoch 8421/10000\n",
      "2700/2700 [==============================] - 0s 34us/step - loss: 1.2485e-04 - mean_absolute_error: 0.0066\n",
      "\n",
      "Epoch 08421: loss did not improve\n",
      "Epoch 8422/10000\n",
      "2700/2700 [==============================] - 0s 31us/step - loss: 1.2184e-04 - mean_absolute_error: 0.0066\n",
      "\n",
      "Epoch 08422: loss did not improve\n",
      "Epoch 8423/10000\n",
      "2700/2700 [==============================] - 0s 31us/step - loss: 1.2192e-04 - mean_absolute_error: 0.0064\n",
      "\n",
      "Epoch 08423: loss did not improve\n",
      "Epoch 8424/10000\n",
      "2700/2700 [==============================] - 0s 30us/step - loss: 1.2246e-04 - mean_absolute_error: 0.0064\n",
      "\n",
      "Epoch 08424: loss did not improve\n",
      "Epoch 8425/10000\n",
      "2700/2700 [==============================] - 0s 29us/step - loss: 1.2087e-04 - mean_absolute_error: 0.0064\n",
      "\n",
      "Epoch 08425: loss did not improve\n",
      "Epoch 8426/10000\n",
      "2700/2700 [==============================] - 0s 28us/step - loss: 1.2487e-04 - mean_absolute_error: 0.0067\n",
      "\n",
      "Epoch 08426: loss did not improve\n",
      "Epoch 8427/10000\n",
      "2700/2700 [==============================] - 0s 27us/step - loss: 1.2199e-04 - mean_absolute_error: 0.0065\n",
      "\n",
      "Epoch 08427: loss did not improve\n",
      "Epoch 8428/10000\n",
      "2700/2700 [==============================] - 0s 28us/step - loss: 1.2272e-04 - mean_absolute_error: 0.0065\n",
      "\n",
      "Epoch 08428: loss did not improve\n",
      "Epoch 8429/10000\n",
      "2700/2700 [==============================] - 0s 28us/step - loss: 1.2504e-04 - mean_absolute_error: 0.0066\n",
      "\n",
      "Epoch 08429: loss did not improve\n",
      "Epoch 8430/10000\n",
      "2700/2700 [==============================] - 0s 30us/step - loss: 1.2232e-04 - mean_absolute_error: 0.0064\n",
      "\n",
      "Epoch 08430: loss did not improve\n",
      "Epoch 8431/10000\n",
      "2700/2700 [==============================] - 0s 27us/step - loss: 1.2232e-04 - mean_absolute_error: 0.0065\n",
      "\n",
      "Epoch 08431: loss did not improve\n",
      "Epoch 8432/10000\n",
      "2700/2700 [==============================] - 0s 28us/step - loss: 1.2283e-04 - mean_absolute_error: 0.0065\n",
      "\n",
      "Epoch 08432: loss did not improve\n",
      "Epoch 8433/10000\n",
      "2700/2700 [==============================] - 0s 30us/step - loss: 1.2392e-04 - mean_absolute_error: 0.0067\n",
      "\n",
      "Epoch 08433: loss did not improve\n",
      "Epoch 8434/10000\n",
      "2700/2700 [==============================] - 0s 28us/step - loss: 1.2404e-04 - mean_absolute_error: 0.0066\n",
      "\n",
      "Epoch 08434: loss did not improve\n",
      "Epoch 8435/10000\n",
      "2700/2700 [==============================] - 0s 28us/step - loss: 1.2217e-04 - mean_absolute_error: 0.0065\n",
      "\n",
      "Epoch 08435: loss did not improve\n",
      "Epoch 8436/10000\n",
      "2700/2700 [==============================] - 0s 27us/step - loss: 1.2179e-04 - mean_absolute_error: 0.0065\n",
      "\n",
      "Epoch 08436: loss did not improve\n",
      "Epoch 8437/10000\n",
      "2700/2700 [==============================] - 0s 29us/step - loss: 1.2163e-04 - mean_absolute_error: 0.0064\n",
      "\n",
      "Epoch 08437: loss did not improve\n",
      "Epoch 8438/10000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2700/2700 [==============================] - 0s 31us/step - loss: 1.2269e-04 - mean_absolute_error: 0.0065\n",
      "\n",
      "Epoch 08438: loss did not improve\n",
      "Epoch 8439/10000\n",
      "2700/2700 [==============================] - 0s 26us/step - loss: 1.2138e-04 - mean_absolute_error: 0.0064\n",
      "\n",
      "Epoch 08439: loss did not improve\n",
      "Epoch 8440/10000\n",
      "2700/2700 [==============================] - 0s 29us/step - loss: 1.2516e-04 - mean_absolute_error: 0.0068\n",
      "\n",
      "Epoch 08440: loss did not improve\n",
      "Epoch 8441/10000\n",
      "2700/2700 [==============================] - 0s 31us/step - loss: 1.2034e-04 - mean_absolute_error: 0.0062\n",
      "\n",
      "Epoch 08441: loss did not improve\n",
      "Epoch 8442/10000\n",
      "2700/2700 [==============================] - 0s 30us/step - loss: 1.2644e-04 - mean_absolute_error: 0.0067\n",
      "\n",
      "Epoch 08442: loss did not improve\n",
      "Epoch 8443/10000\n",
      "2700/2700 [==============================] - 0s 30us/step - loss: 1.2212e-04 - mean_absolute_error: 0.0065\n",
      "\n",
      "Epoch 08443: loss did not improve\n",
      "Epoch 8444/10000\n",
      "2700/2700 [==============================] - 0s 29us/step - loss: 1.2538e-04 - mean_absolute_error: 0.0067\n",
      "\n",
      "Epoch 08444: loss did not improve\n",
      "Epoch 8445/10000\n",
      "2700/2700 [==============================] - 0s 27us/step - loss: 1.2051e-04 - mean_absolute_error: 0.0063\n",
      "\n",
      "Epoch 08445: loss did not improve\n",
      "Epoch 8446/10000\n",
      "2700/2700 [==============================] - 0s 29us/step - loss: 1.2145e-04 - mean_absolute_error: 0.0064\n",
      "\n",
      "Epoch 08446: loss did not improve\n",
      "Epoch 8447/10000\n",
      "2700/2700 [==============================] - 0s 27us/step - loss: 1.2068e-04 - mean_absolute_error: 0.0063\n",
      "\n",
      "Epoch 08447: loss did not improve\n",
      "Epoch 8448/10000\n",
      "2700/2700 [==============================] - 0s 30us/step - loss: 1.2161e-04 - mean_absolute_error: 0.0064\n",
      "\n",
      "Epoch 08448: loss did not improve\n",
      "Epoch 8449/10000\n",
      "2700/2700 [==============================] - 0s 26us/step - loss: 1.2064e-04 - mean_absolute_error: 0.0063\n",
      "\n",
      "Epoch 08449: loss did not improve\n",
      "Epoch 8450/10000\n",
      "2700/2700 [==============================] - 0s 32us/step - loss: 1.2218e-04 - mean_absolute_error: 0.0065\n",
      "\n",
      "Epoch 08450: loss did not improve\n",
      "Epoch 8451/10000\n",
      "2700/2700 [==============================] - 0s 28us/step - loss: 1.2675e-04 - mean_absolute_error: 0.0067\n",
      "\n",
      "Epoch 08451: loss did not improve\n",
      "Epoch 8452/10000\n",
      "2700/2700 [==============================] - 0s 28us/step - loss: 1.2291e-04 - mean_absolute_error: 0.0066\n",
      "\n",
      "Epoch 08452: loss did not improve\n",
      "Epoch 8453/10000\n",
      "2700/2700 [==============================] - 0s 30us/step - loss: 1.2464e-04 - mean_absolute_error: 0.0067\n",
      "\n",
      "Epoch 08453: loss did not improve\n",
      "Epoch 8454/10000\n",
      "2700/2700 [==============================] - 0s 27us/step - loss: 1.2579e-04 - mean_absolute_error: 0.0067\n",
      "\n",
      "Epoch 08454: loss did not improve\n",
      "Epoch 8455/10000\n",
      "2700/2700 [==============================] - 0s 29us/step - loss: 1.2459e-04 - mean_absolute_error: 0.0066\n",
      "\n",
      "Epoch 08455: loss did not improve\n",
      "Epoch 8456/10000\n",
      "2700/2700 [==============================] - 0s 28us/step - loss: 1.2090e-04 - mean_absolute_error: 0.0064\n",
      "\n",
      "Epoch 08456: loss did not improve\n",
      "Epoch 8457/10000\n",
      "2700/2700 [==============================] - 0s 32us/step - loss: 1.2199e-04 - mean_absolute_error: 0.0064\n",
      "\n",
      "Epoch 08457: loss did not improve\n",
      "Epoch 8458/10000\n",
      "2700/2700 [==============================] - 0s 27us/step - loss: 1.2082e-04 - mean_absolute_error: 0.0063\n",
      "\n",
      "Epoch 08458: loss did not improve\n",
      "Epoch 8459/10000\n",
      "2700/2700 [==============================] - 0s 30us/step - loss: 1.2296e-04 - mean_absolute_error: 0.0066\n",
      "\n",
      "Epoch 08459: loss did not improve\n",
      "Epoch 8460/10000\n",
      "2700/2700 [==============================] - 0s 29us/step - loss: 1.2159e-04 - mean_absolute_error: 0.0064\n",
      "\n",
      "Epoch 08460: loss did not improve\n",
      "Epoch 8461/10000\n",
      "2700/2700 [==============================] - 0s 28us/step - loss: 1.2293e-04 - mean_absolute_error: 0.0064\n",
      "\n",
      "Epoch 08461: loss did not improve\n",
      "Epoch 8462/10000\n",
      "2700/2700 [==============================] - 0s 31us/step - loss: 1.2537e-04 - mean_absolute_error: 0.0067\n",
      "\n",
      "Epoch 08462: loss did not improve\n",
      "Epoch 8463/10000\n",
      "2700/2700 [==============================] - 0s 29us/step - loss: 1.2098e-04 - mean_absolute_error: 0.0064\n",
      "\n",
      "Epoch 08463: loss did not improve\n",
      "Epoch 8464/10000\n",
      "2700/2700 [==============================] - 0s 30us/step - loss: 1.2104e-04 - mean_absolute_error: 0.0063\n",
      "\n",
      "Epoch 08464: loss did not improve\n",
      "Epoch 8465/10000\n",
      "2700/2700 [==============================] - 0s 29us/step - loss: 1.2258e-04 - mean_absolute_error: 0.0065\n",
      "\n",
      "Epoch 08465: loss did not improve\n",
      "Epoch 8466/10000\n",
      "2700/2700 [==============================] - 0s 28us/step - loss: 1.2389e-04 - mean_absolute_error: 0.0066\n",
      "\n",
      "Epoch 08466: loss did not improve\n",
      "Epoch 8467/10000\n",
      "2700/2700 [==============================] - 0s 30us/step - loss: 1.2147e-04 - mean_absolute_error: 0.0063\n",
      "\n",
      "Epoch 08467: loss did not improve\n",
      "Epoch 8468/10000\n",
      "2700/2700 [==============================] - 0s 27us/step - loss: 1.2153e-04 - mean_absolute_error: 0.0064\n",
      "\n",
      "Epoch 08468: loss did not improve\n",
      "Epoch 8469/10000\n",
      "2700/2700 [==============================] - 0s 30us/step - loss: 1.2253e-04 - mean_absolute_error: 0.0065\n",
      "\n",
      "Epoch 08469: loss did not improve\n",
      "Epoch 8470/10000\n",
      "2700/2700 [==============================] - 0s 29us/step - loss: 1.2131e-04 - mean_absolute_error: 0.0064\n",
      "\n",
      "Epoch 08470: loss did not improve\n",
      "Epoch 8471/10000\n",
      "2700/2700 [==============================] - 0s 30us/step - loss: 1.2433e-04 - mean_absolute_error: 0.0066\n",
      "\n",
      "Epoch 08471: loss did not improve\n",
      "Epoch 8472/10000\n",
      "2700/2700 [==============================] - 0s 29us/step - loss: 1.2322e-04 - mean_absolute_error: 0.0066\n",
      "\n",
      "Epoch 08472: loss did not improve\n",
      "Epoch 8473/10000\n",
      "2700/2700 [==============================] - 0s 31us/step - loss: 1.2228e-04 - mean_absolute_error: 0.0065\n",
      "\n",
      "Epoch 08473: loss did not improve\n",
      "Epoch 8474/10000\n",
      "2700/2700 [==============================] - 0s 30us/step - loss: 1.2274e-04 - mean_absolute_error: 0.0065\n",
      "\n",
      "Epoch 08474: loss did not improve\n",
      "Epoch 8475/10000\n",
      "2700/2700 [==============================] - 0s 30us/step - loss: 1.2013e-04 - mean_absolute_error: 0.0064\n",
      "\n",
      "Epoch 08475: loss did not improve\n",
      "Epoch 8476/10000\n",
      "2700/2700 [==============================] - 0s 29us/step - loss: 1.2498e-04 - mean_absolute_error: 0.0067\n",
      "\n",
      "Epoch 08476: loss did not improve\n",
      "Epoch 8477/10000\n",
      "2700/2700 [==============================] - 0s 29us/step - loss: 1.2319e-04 - mean_absolute_error: 0.0065\n",
      "\n",
      "Epoch 08477: loss did not improve\n",
      "Epoch 8478/10000\n",
      "2700/2700 [==============================] - 0s 27us/step - loss: 1.2187e-04 - mean_absolute_error: 0.0064\n",
      "\n",
      "Epoch 08478: loss did not improve\n",
      "Epoch 8479/10000\n",
      "2700/2700 [==============================] - 0s 32us/step - loss: 1.2152e-04 - mean_absolute_error: 0.0064\n",
      "\n",
      "Epoch 08479: loss did not improve\n",
      "Epoch 8480/10000\n",
      "2700/2700 [==============================] - 0s 32us/step - loss: 1.2237e-04 - mean_absolute_error: 0.0065\n",
      "\n",
      "Epoch 08480: loss did not improve\n",
      "Epoch 8481/10000\n",
      "2700/2700 [==============================] - 0s 37us/step - loss: 1.2173e-04 - mean_absolute_error: 0.0064\n",
      "\n",
      "Epoch 08481: loss did not improve\n",
      "Epoch 8482/10000\n",
      "2700/2700 [==============================] - 0s 29us/step - loss: 1.2355e-04 - mean_absolute_error: 0.0067\n",
      "\n",
      "Epoch 08482: loss did not improve\n",
      "Epoch 8483/10000\n",
      "2700/2700 [==============================] - 0s 30us/step - loss: 1.2098e-04 - mean_absolute_error: 0.0063\n",
      "\n",
      "Epoch 08483: loss did not improve\n",
      "Epoch 8484/10000\n",
      "2700/2700 [==============================] - 0s 26us/step - loss: 1.2095e-04 - mean_absolute_error: 0.0064\n",
      "\n",
      "Epoch 08484: loss did not improve\n",
      "Epoch 8485/10000\n",
      "2700/2700 [==============================] - 0s 30us/step - loss: 1.2099e-04 - mean_absolute_error: 0.0063\n",
      "\n",
      "Epoch 08485: loss did not improve\n",
      "Epoch 8486/10000\n",
      "2700/2700 [==============================] - 0s 33us/step - loss: 1.2568e-04 - mean_absolute_error: 0.0068\n",
      "\n",
      "Epoch 08486: loss did not improve\n",
      "Epoch 8487/10000\n",
      "2700/2700 [==============================] - 0s 30us/step - loss: 1.2362e-04 - mean_absolute_error: 0.0065\n",
      "\n",
      "Epoch 08487: loss did not improve\n",
      "Epoch 8488/10000\n",
      "2700/2700 [==============================] - 0s 28us/step - loss: 1.2221e-04 - mean_absolute_error: 0.0065\n",
      "\n",
      "Epoch 08488: loss did not improve\n",
      "Epoch 8489/10000\n",
      "2700/2700 [==============================] - 0s 27us/step - loss: 1.2161e-04 - mean_absolute_error: 0.0064\n",
      "\n",
      "Epoch 08489: loss did not improve\n",
      "Epoch 8490/10000\n",
      "2700/2700 [==============================] - 0s 28us/step - loss: 1.2337e-04 - mean_absolute_error: 0.0065\n",
      "\n",
      "Epoch 08490: loss did not improve\n",
      "Epoch 8491/10000\n",
      "2700/2700 [==============================] - 0s 29us/step - loss: 1.2236e-04 - mean_absolute_error: 0.0065\n",
      "\n",
      "Epoch 08491: loss did not improve\n",
      "Epoch 8492/10000\n",
      "2700/2700 [==============================] - 0s 30us/step - loss: 1.2224e-04 - mean_absolute_error: 0.0065\n",
      "\n",
      "Epoch 08492: loss did not improve\n",
      "Epoch 8493/10000\n",
      "2700/2700 [==============================] - 0s 26us/step - loss: 1.2310e-04 - mean_absolute_error: 0.0066\n",
      "\n",
      "Epoch 08493: loss did not improve\n",
      "Epoch 8494/10000\n",
      "2700/2700 [==============================] - 0s 29us/step - loss: 1.2009e-04 - mean_absolute_error: 0.0063\n",
      "\n",
      "Epoch 08494: loss did not improve\n",
      "Epoch 8495/10000\n",
      "2700/2700 [==============================] - 0s 30us/step - loss: 1.2068e-04 - mean_absolute_error: 0.0062\n",
      "\n",
      "Epoch 08495: loss did not improve\n",
      "Epoch 8496/10000\n",
      "2700/2700 [==============================] - 0s 29us/step - loss: 1.2183e-04 - mean_absolute_error: 0.0065\n",
      "\n",
      "Epoch 08496: loss did not improve\n",
      "Epoch 8497/10000\n",
      "2700/2700 [==============================] - 0s 30us/step - loss: 1.2366e-04 - mean_absolute_error: 0.0065\n",
      "\n",
      "Epoch 08497: loss did not improve\n",
      "Epoch 8498/10000\n",
      "2700/2700 [==============================] - 0s 27us/step - loss: 1.2243e-04 - mean_absolute_error: 0.0066\n",
      "\n",
      "Epoch 08498: loss did not improve\n",
      "Epoch 8499/10000\n",
      "2700/2700 [==============================] - 0s 28us/step - loss: 1.2195e-04 - mean_absolute_error: 0.0064\n",
      "\n",
      "Epoch 08499: loss did not improve\n",
      "Epoch 8500/10000\n",
      "2700/2700 [==============================] - 0s 31us/step - loss: 1.2202e-04 - mean_absolute_error: 0.0064\n",
      "\n",
      "Epoch 08500: loss did not improve\n",
      "Epoch 8501/10000\n",
      "2700/2700 [==============================] - 0s 28us/step - loss: 1.2487e-04 - mean_absolute_error: 0.0067\n",
      "\n",
      "Epoch 08501: loss did not improve\n",
      "Epoch 8502/10000\n",
      "2700/2700 [==============================] - 0s 26us/step - loss: 1.2139e-04 - mean_absolute_error: 0.0064\n",
      "\n",
      "Epoch 08502: loss did not improve\n",
      "Epoch 8503/10000\n",
      "2700/2700 [==============================] - 0s 29us/step - loss: 1.2189e-04 - mean_absolute_error: 0.0064\n",
      "\n",
      "Epoch 08503: loss did not improve\n",
      "Epoch 8504/10000\n",
      "2700/2700 [==============================] - 0s 30us/step - loss: 1.2323e-04 - mean_absolute_error: 0.0067\n",
      "\n",
      "Epoch 08504: loss did not improve\n",
      "Epoch 8505/10000\n",
      "2700/2700 [==============================] - 0s 28us/step - loss: 1.2018e-04 - mean_absolute_error: 0.0064\n",
      "\n",
      "Epoch 08505: loss did not improve\n",
      "Epoch 8506/10000\n",
      "2700/2700 [==============================] - 0s 28us/step - loss: 1.2191e-04 - mean_absolute_error: 0.0063\n",
      "\n",
      "Epoch 08506: loss did not improve\n",
      "Epoch 8507/10000\n",
      "2700/2700 [==============================] - 0s 27us/step - loss: 1.2440e-04 - mean_absolute_error: 0.0067\n",
      "\n",
      "Epoch 08507: loss did not improve\n",
      "Epoch 8508/10000\n",
      "2700/2700 [==============================] - 0s 30us/step - loss: 1.2269e-04 - mean_absolute_error: 0.0065\n",
      "\n",
      "Epoch 08508: loss did not improve\n",
      "Epoch 8509/10000\n",
      "2700/2700 [==============================] - 0s 32us/step - loss: 1.2584e-04 - mean_absolute_error: 0.0068\n",
      "\n",
      "Epoch 08509: loss did not improve\n",
      "Epoch 8510/10000\n",
      "2700/2700 [==============================] - 0s 30us/step - loss: 1.2240e-04 - mean_absolute_error: 0.0065\n",
      "\n",
      "Epoch 08510: loss did not improve\n",
      "Epoch 8511/10000\n",
      "2700/2700 [==============================] - 0s 32us/step - loss: 1.2516e-04 - mean_absolute_error: 0.0067\n",
      "\n",
      "Epoch 08511: loss did not improve\n",
      "Epoch 8512/10000\n",
      "2700/2700 [==============================] - 0s 26us/step - loss: 1.2247e-04 - mean_absolute_error: 0.0065\n",
      "\n",
      "Epoch 08512: loss did not improve\n",
      "Epoch 8513/10000\n",
      "2700/2700 [==============================] - 0s 30us/step - loss: 1.2144e-04 - mean_absolute_error: 0.0064\n",
      "\n",
      "Epoch 08513: loss did not improve\n",
      "Epoch 8514/10000\n",
      "2700/2700 [==============================] - 0s 28us/step - loss: 1.2123e-04 - mean_absolute_error: 0.0063\n",
      "\n",
      "Epoch 08514: loss did not improve\n",
      "Epoch 8515/10000\n",
      "2700/2700 [==============================] - 0s 28us/step - loss: 1.2445e-04 - mean_absolute_error: 0.0067\n",
      "\n",
      "Epoch 08515: loss did not improve\n",
      "Epoch 8516/10000\n",
      "2700/2700 [==============================] - 0s 33us/step - loss: 1.2114e-04 - mean_absolute_error: 0.0063\n",
      "\n",
      "Epoch 08516: loss did not improve\n",
      "Epoch 8517/10000\n",
      "2700/2700 [==============================] - 0s 28us/step - loss: 1.2308e-04 - mean_absolute_error: 0.0065\n",
      "\n",
      "Epoch 08517: loss did not improve\n",
      "Epoch 8518/10000\n",
      "2700/2700 [==============================] - 0s 31us/step - loss: 1.2215e-04 - mean_absolute_error: 0.0064\n",
      "\n",
      "Epoch 08518: loss did not improve\n",
      "Epoch 8519/10000\n",
      "2700/2700 [==============================] - 0s 31us/step - loss: 1.2053e-04 - mean_absolute_error: 0.0063\n",
      "\n",
      "Epoch 08519: loss did not improve\n",
      "Epoch 8520/10000\n",
      "2700/2700 [==============================] - 0s 30us/step - loss: 1.2377e-04 - mean_absolute_error: 0.0066\n",
      "\n",
      "Epoch 08520: loss did not improve\n",
      "Epoch 8521/10000\n",
      "2700/2700 [==============================] - 0s 31us/step - loss: 1.2050e-04 - mean_absolute_error: 0.0063\n",
      "\n",
      "Epoch 08521: loss did not improve\n",
      "Epoch 8522/10000\n",
      "2700/2700 [==============================] - 0s 30us/step - loss: 1.2212e-04 - mean_absolute_error: 0.0064\n",
      "\n",
      "Epoch 08522: loss did not improve\n",
      "Epoch 8523/10000\n",
      "2700/2700 [==============================] - 0s 26us/step - loss: 1.2140e-04 - mean_absolute_error: 0.0064\n",
      "\n",
      "Epoch 08523: loss did not improve\n",
      "Epoch 8524/10000\n",
      "2700/2700 [==============================] - 0s 29us/step - loss: 1.2104e-04 - mean_absolute_error: 0.0063\n",
      "\n",
      "Epoch 08524: loss did not improve\n",
      "Epoch 8525/10000\n",
      "2700/2700 [==============================] - 0s 32us/step - loss: 1.2270e-04 - mean_absolute_error: 0.0066\n",
      "\n",
      "Epoch 08525: loss did not improve\n",
      "Epoch 8526/10000\n",
      "2700/2700 [==============================] - 0s 40us/step - loss: 1.2271e-04 - mean_absolute_error: 0.0064\n",
      "\n",
      "Epoch 08526: loss did not improve\n",
      "Epoch 8527/10000\n",
      "2700/2700 [==============================] - 0s 31us/step - loss: 1.2164e-04 - mean_absolute_error: 0.0063\n",
      "\n",
      "Epoch 08527: loss did not improve\n",
      "Epoch 8528/10000\n",
      "2700/2700 [==============================] - 0s 28us/step - loss: 1.2970e-04 - mean_absolute_error: 0.0071\n",
      "\n",
      "Epoch 08528: loss did not improve\n",
      "Epoch 8529/10000\n",
      "2700/2700 [==============================] - 0s 28us/step - loss: 1.2324e-04 - mean_absolute_error: 0.0065\n",
      "\n",
      "Epoch 08529: loss did not improve\n",
      "Epoch 8530/10000\n",
      "2700/2700 [==============================] - 0s 36us/step - loss: 1.2415e-04 - mean_absolute_error: 0.0066\n",
      "\n",
      "Epoch 08530: loss did not improve\n",
      "Epoch 8531/10000\n",
      "2700/2700 [==============================] - 0s 30us/step - loss: 1.2212e-04 - mean_absolute_error: 0.0065\n",
      "\n",
      "Epoch 08531: loss did not improve\n",
      "Epoch 8532/10000\n",
      "2700/2700 [==============================] - 0s 32us/step - loss: 1.2070e-04 - mean_absolute_error: 0.0063\n",
      "\n",
      "Epoch 08532: loss did not improve\n",
      "Epoch 8533/10000\n",
      "2700/2700 [==============================] - 0s 36us/step - loss: 1.2155e-04 - mean_absolute_error: 0.0065\n",
      "\n",
      "Epoch 08533: loss did not improve\n",
      "Epoch 8534/10000\n",
      "2700/2700 [==============================] - 0s 29us/step - loss: 1.2069e-04 - mean_absolute_error: 0.0063\n",
      "\n",
      "Epoch 08534: loss did not improve\n",
      "Epoch 8535/10000\n",
      "2700/2700 [==============================] - 0s 31us/step - loss: 1.2383e-04 - mean_absolute_error: 0.0066\n",
      "\n",
      "Epoch 08535: loss did not improve\n",
      "Epoch 8536/10000\n",
      "2700/2700 [==============================] - 0s 30us/step - loss: 1.2355e-04 - mean_absolute_error: 0.0066\n",
      "\n",
      "Epoch 08536: loss did not improve\n",
      "Epoch 8537/10000\n",
      "2700/2700 [==============================] - 0s 30us/step - loss: 1.2109e-04 - mean_absolute_error: 0.0064\n",
      "\n",
      "Epoch 08537: loss did not improve\n",
      "Epoch 8538/10000\n",
      "2700/2700 [==============================] - 0s 30us/step - loss: 1.2177e-04 - mean_absolute_error: 0.0063\n",
      "\n",
      "Epoch 08538: loss did not improve\n",
      "Epoch 8539/10000\n",
      "2700/2700 [==============================] - 0s 29us/step - loss: 1.2307e-04 - mean_absolute_error: 0.0065\n",
      "\n",
      "Epoch 08539: loss did not improve\n",
      "Epoch 8540/10000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2700/2700 [==============================] - 0s 40us/step - loss: 1.2379e-04 - mean_absolute_error: 0.0067\n",
      "\n",
      "Epoch 08540: loss did not improve\n",
      "Epoch 8541/10000\n",
      "2700/2700 [==============================] - 0s 33us/step - loss: 1.2141e-04 - mean_absolute_error: 0.0064\n",
      "\n",
      "Epoch 08541: loss did not improve\n",
      "Epoch 8542/10000\n",
      "2700/2700 [==============================] - 0s 31us/step - loss: 1.2408e-04 - mean_absolute_error: 0.0066\n",
      "\n",
      "Epoch 08542: loss did not improve\n",
      "Epoch 8543/10000\n",
      "2700/2700 [==============================] - 0s 27us/step - loss: 1.2138e-04 - mean_absolute_error: 0.0064\n",
      "\n",
      "Epoch 08543: loss did not improve\n",
      "Epoch 8544/10000\n",
      "2700/2700 [==============================] - 0s 30us/step - loss: 1.2117e-04 - mean_absolute_error: 0.0063\n",
      "\n",
      "Epoch 08544: loss did not improve\n",
      "Epoch 8545/10000\n",
      "2700/2700 [==============================] - 0s 27us/step - loss: 1.2216e-04 - mean_absolute_error: 0.0064\n",
      "\n",
      "Epoch 08545: loss did not improve\n",
      "Epoch 8546/10000\n",
      "2700/2700 [==============================] - 0s 29us/step - loss: 1.2320e-04 - mean_absolute_error: 0.0065\n",
      "\n",
      "Epoch 08546: loss did not improve\n",
      "Epoch 8547/10000\n",
      "2700/2700 [==============================] - 0s 29us/step - loss: 1.2327e-04 - mean_absolute_error: 0.0067\n",
      "\n",
      "Epoch 08547: loss did not improve\n",
      "Epoch 8548/10000\n",
      "2700/2700 [==============================] - 0s 29us/step - loss: 1.2228e-04 - mean_absolute_error: 0.0065\n",
      "\n",
      "Epoch 08548: loss did not improve\n",
      "Epoch 8549/10000\n",
      "2700/2700 [==============================] - 0s 28us/step - loss: 1.2347e-04 - mean_absolute_error: 0.0065\n",
      "\n",
      "Epoch 08549: loss did not improve\n",
      "Epoch 8550/10000\n",
      "2700/2700 [==============================] - 0s 29us/step - loss: 1.2105e-04 - mean_absolute_error: 0.0064\n",
      "\n",
      "Epoch 08550: loss did not improve\n",
      "Epoch 8551/10000\n",
      "2700/2700 [==============================] - 0s 27us/step - loss: 1.2508e-04 - mean_absolute_error: 0.0066\n",
      "\n",
      "Epoch 08551: loss did not improve\n",
      "Epoch 8552/10000\n",
      "2700/2700 [==============================] - 0s 29us/step - loss: 1.2209e-04 - mean_absolute_error: 0.0065\n",
      "\n",
      "Epoch 08552: loss did not improve\n",
      "Epoch 8553/10000\n",
      "2700/2700 [==============================] - 0s 28us/step - loss: 1.2377e-04 - mean_absolute_error: 0.0066\n",
      "\n",
      "Epoch 08553: loss did not improve\n",
      "Epoch 8554/10000\n",
      "2700/2700 [==============================] - 0s 28us/step - loss: 1.2203e-04 - mean_absolute_error: 0.0064\n",
      "\n",
      "Epoch 08554: loss did not improve\n",
      "Epoch 8555/10000\n",
      "2700/2700 [==============================] - 0s 27us/step - loss: 1.2170e-04 - mean_absolute_error: 0.0063\n",
      "\n",
      "Epoch 08555: loss did not improve\n",
      "Epoch 8556/10000\n",
      "2700/2700 [==============================] - 0s 29us/step - loss: 1.2151e-04 - mean_absolute_error: 0.0064\n",
      "\n",
      "Epoch 08556: loss did not improve\n",
      "Epoch 8557/10000\n",
      "2700/2700 [==============================] - 0s 30us/step - loss: 1.2143e-04 - mean_absolute_error: 0.0064\n",
      "\n",
      "Epoch 08557: loss did not improve\n",
      "Epoch 8558/10000\n",
      "2700/2700 [==============================] - 0s 27us/step - loss: 1.2623e-04 - mean_absolute_error: 0.0067\n",
      "\n",
      "Epoch 08558: loss did not improve\n",
      "Epoch 8559/10000\n",
      "2700/2700 [==============================] - 0s 29us/step - loss: 1.2232e-04 - mean_absolute_error: 0.0064\n",
      "\n",
      "Epoch 08559: loss did not improve\n",
      "Epoch 8560/10000\n",
      "2700/2700 [==============================] - 0s 26us/step - loss: 1.2057e-04 - mean_absolute_error: 0.0063\n",
      "\n",
      "Epoch 08560: loss did not improve\n",
      "Epoch 8561/10000\n",
      "2700/2700 [==============================] - 0s 29us/step - loss: 1.2191e-04 - mean_absolute_error: 0.0063\n",
      "\n",
      "Epoch 08561: loss did not improve\n",
      "Epoch 8562/10000\n",
      "2700/2700 [==============================] - 0s 28us/step - loss: 1.2132e-04 - mean_absolute_error: 0.0064\n",
      "\n",
      "Epoch 08562: loss did not improve\n",
      "Epoch 8563/10000\n",
      "2700/2700 [==============================] - 0s 29us/step - loss: 1.2201e-04 - mean_absolute_error: 0.0065\n",
      "\n",
      "Epoch 08563: loss did not improve\n",
      "Epoch 8564/10000\n",
      "2700/2700 [==============================] - 0s 26us/step - loss: 1.2155e-04 - mean_absolute_error: 0.0064\n",
      "\n",
      "Epoch 08564: loss did not improve\n",
      "Epoch 8565/10000\n",
      "2700/2700 [==============================] - 0s 31us/step - loss: 1.2144e-04 - mean_absolute_error: 0.0064\n",
      "\n",
      "Epoch 08565: loss did not improve\n",
      "Epoch 8566/10000\n",
      "2700/2700 [==============================] - 0s 29us/step - loss: 1.2405e-04 - mean_absolute_error: 0.0067\n",
      "\n",
      "Epoch 08566: loss did not improve\n",
      "Epoch 8567/10000\n",
      "2700/2700 [==============================] - 0s 28us/step - loss: 1.2105e-04 - mean_absolute_error: 0.0064\n",
      "\n",
      "Epoch 08567: loss did not improve\n",
      "Epoch 8568/10000\n",
      "2700/2700 [==============================] - 0s 26us/step - loss: 1.2210e-04 - mean_absolute_error: 0.0065\n",
      "\n",
      "Epoch 08568: loss did not improve\n",
      "Epoch 8569/10000\n",
      "2700/2700 [==============================] - 0s 32us/step - loss: 1.2378e-04 - mean_absolute_error: 0.0066\n",
      "\n",
      "Epoch 08569: loss did not improve\n",
      "Epoch 8570/10000\n",
      "2700/2700 [==============================] - 0s 29us/step - loss: 1.2025e-04 - mean_absolute_error: 0.0063\n",
      "\n",
      "Epoch 08570: loss did not improve\n",
      "Epoch 8571/10000\n",
      "2700/2700 [==============================] - 0s 28us/step - loss: 1.2126e-04 - mean_absolute_error: 0.0064\n",
      "\n",
      "Epoch 08571: loss did not improve\n",
      "Epoch 8572/10000\n",
      "2700/2700 [==============================] - 0s 31us/step - loss: 1.2202e-04 - mean_absolute_error: 0.0064\n",
      "\n",
      "Epoch 08572: loss did not improve\n",
      "Epoch 8573/10000\n",
      "2700/2700 [==============================] - 0s 26us/step - loss: 1.2195e-04 - mean_absolute_error: 0.0064\n",
      "\n",
      "Epoch 08573: loss did not improve\n",
      "Epoch 8574/10000\n",
      "2700/2700 [==============================] - 0s 29us/step - loss: 1.2148e-04 - mean_absolute_error: 0.0065\n",
      "\n",
      "Epoch 08574: loss did not improve\n",
      "Epoch 8575/10000\n",
      "2700/2700 [==============================] - 0s 29us/step - loss: 1.2111e-04 - mean_absolute_error: 0.0064\n",
      "\n",
      "Epoch 08575: loss did not improve\n",
      "Epoch 8576/10000\n",
      "2700/2700 [==============================] - 0s 29us/step - loss: 1.2051e-04 - mean_absolute_error: 0.0064\n",
      "\n",
      "Epoch 08576: loss did not improve\n",
      "Epoch 8577/10000\n",
      "2700/2700 [==============================] - 0s 28us/step - loss: 1.2247e-04 - mean_absolute_error: 0.0064\n",
      "\n",
      "Epoch 08577: loss did not improve\n",
      "Epoch 8578/10000\n",
      "2700/2700 [==============================] - 0s 27us/step - loss: 1.2078e-04 - mean_absolute_error: 0.0063\n",
      "\n",
      "Epoch 08578: loss did not improve\n",
      "Epoch 8579/10000\n",
      "2700/2700 [==============================] - 0s 29us/step - loss: 1.2227e-04 - mean_absolute_error: 0.0064\n",
      "\n",
      "Epoch 08579: loss did not improve\n",
      "Epoch 8580/10000\n",
      "2700/2700 [==============================] - 0s 27us/step - loss: 1.2406e-04 - mean_absolute_error: 0.0065\n",
      "\n",
      "Epoch 08580: loss did not improve\n",
      "Epoch 8581/10000\n",
      "2700/2700 [==============================] - 0s 27us/step - loss: 1.2061e-04 - mean_absolute_error: 0.0063\n",
      "\n",
      "Epoch 08581: loss did not improve\n",
      "Epoch 8582/10000\n",
      "2700/2700 [==============================] - 0s 28us/step - loss: 1.2567e-04 - mean_absolute_error: 0.0069\n",
      "\n",
      "Epoch 08582: loss did not improve\n",
      "Epoch 8583/10000\n",
      "2700/2700 [==============================] - 0s 30us/step - loss: 1.2294e-04 - mean_absolute_error: 0.0065\n",
      "\n",
      "Epoch 08583: loss did not improve\n",
      "Epoch 8584/10000\n",
      "2700/2700 [==============================] - 0s 28us/step - loss: 1.2311e-04 - mean_absolute_error: 0.0065\n",
      "\n",
      "Epoch 08584: loss did not improve\n",
      "Epoch 8585/10000\n",
      "2700/2700 [==============================] - 0s 29us/step - loss: 1.2266e-04 - mean_absolute_error: 0.0065\n",
      "\n",
      "Epoch 08585: loss did not improve\n",
      "Epoch 8586/10000\n",
      "2700/2700 [==============================] - 0s 40us/step - loss: 1.2390e-04 - mean_absolute_error: 0.0066\n",
      "\n",
      "Epoch 08586: loss did not improve\n",
      "Epoch 8587/10000\n",
      "2700/2700 [==============================] - 0s 36us/step - loss: 1.2094e-04 - mean_absolute_error: 0.0063\n",
      "\n",
      "Epoch 08587: loss did not improve\n",
      "Epoch 8588/10000\n",
      "2700/2700 [==============================] - 0s 27us/step - loss: 1.2617e-04 - mean_absolute_error: 0.0068\n",
      "\n",
      "Epoch 08588: loss did not improve\n",
      "Epoch 8589/10000\n",
      "2700/2700 [==============================] - 0s 32us/step - loss: 1.2103e-04 - mean_absolute_error: 0.0063\n",
      "\n",
      "Epoch 08589: loss did not improve\n",
      "Epoch 8590/10000\n",
      "2700/2700 [==============================] - 0s 28us/step - loss: 1.2143e-04 - mean_absolute_error: 0.0064\n",
      "\n",
      "Epoch 08590: loss did not improve\n",
      "Epoch 8591/10000\n",
      "2700/2700 [==============================] - 0s 29us/step - loss: 1.2213e-04 - mean_absolute_error: 0.0063\n",
      "\n",
      "Epoch 08591: loss did not improve\n",
      "Epoch 8592/10000\n",
      "2700/2700 [==============================] - 0s 29us/step - loss: 1.2059e-04 - mean_absolute_error: 0.0063\n",
      "\n",
      "Epoch 08592: loss did not improve\n",
      "Epoch 8593/10000\n",
      "2700/2700 [==============================] - 0s 27us/step - loss: 1.2146e-04 - mean_absolute_error: 0.0064\n",
      "\n",
      "Epoch 08593: loss did not improve\n",
      "Epoch 8594/10000\n",
      "2700/2700 [==============================] - 0s 29us/step - loss: 1.2238e-04 - mean_absolute_error: 0.0065\n",
      "\n",
      "Epoch 08594: loss did not improve\n",
      "Epoch 8595/10000\n",
      "2700/2700 [==============================] - 0s 29us/step - loss: 1.2463e-04 - mean_absolute_error: 0.0066\n",
      "\n",
      "Epoch 08595: loss did not improve\n",
      "Epoch 8596/10000\n",
      "2700/2700 [==============================] - 0s 28us/step - loss: 1.2162e-04 - mean_absolute_error: 0.0065\n",
      "\n",
      "Epoch 08596: loss did not improve\n",
      "Epoch 8597/10000\n",
      "2700/2700 [==============================] - 0s 29us/step - loss: 1.2085e-04 - mean_absolute_error: 0.0064\n",
      "\n",
      "Epoch 08597: loss did not improve\n",
      "Epoch 8598/10000\n",
      "2700/2700 [==============================] - 0s 28us/step - loss: 1.2182e-04 - mean_absolute_error: 0.0064\n",
      "\n",
      "Epoch 08598: loss did not improve\n",
      "Epoch 8599/10000\n",
      "2700/2700 [==============================] - 0s 29us/step - loss: 1.2206e-04 - mean_absolute_error: 0.0065\n",
      "\n",
      "Epoch 08599: loss did not improve\n",
      "Epoch 8600/10000\n",
      "2700/2700 [==============================] - 0s 30us/step - loss: 1.2208e-04 - mean_absolute_error: 0.0065\n",
      "\n",
      "Epoch 08600: loss did not improve\n",
      "Epoch 8601/10000\n",
      "2700/2700 [==============================] - 0s 30us/step - loss: 1.2221e-04 - mean_absolute_error: 0.0065\n",
      "\n",
      "Epoch 08601: loss did not improve\n",
      "Epoch 8602/10000\n",
      "2700/2700 [==============================] - 0s 27us/step - loss: 1.2159e-04 - mean_absolute_error: 0.0064\n",
      "\n",
      "Epoch 08602: loss did not improve\n",
      "Epoch 8603/10000\n",
      "2700/2700 [==============================] - 0s 28us/step - loss: 1.2457e-04 - mean_absolute_error: 0.0067\n",
      "\n",
      "Epoch 08603: loss did not improve\n",
      "Epoch 8604/10000\n",
      "2700/2700 [==============================] - 0s 29us/step - loss: 1.2382e-04 - mean_absolute_error: 0.0067\n",
      "\n",
      "Epoch 08604: loss did not improve\n",
      "Epoch 8605/10000\n",
      "2700/2700 [==============================] - 0s 29us/step - loss: 1.2096e-04 - mean_absolute_error: 0.0064\n",
      "\n",
      "Epoch 08605: loss did not improve\n",
      "Epoch 8606/10000\n",
      "2700/2700 [==============================] - 0s 27us/step - loss: 1.2688e-04 - mean_absolute_error: 0.0069\n",
      "\n",
      "Epoch 08606: loss did not improve\n",
      "Epoch 8607/10000\n",
      "2700/2700 [==============================] - 0s 29us/step - loss: 1.2351e-04 - mean_absolute_error: 0.0066\n",
      "\n",
      "Epoch 08607: loss did not improve\n",
      "Epoch 8608/10000\n",
      "2700/2700 [==============================] - 0s 28us/step - loss: 1.1903e-04 - mean_absolute_error: 0.0062\n",
      "\n",
      "Epoch 08608: loss improved from 0.00012 to 0.00012, saving model to keras/best_baseline_weights.hdf5\n",
      "Epoch 8609/10000\n",
      "2700/2700 [==============================] - 0s 29us/step - loss: 1.2200e-04 - mean_absolute_error: 0.0065\n",
      "\n",
      "Epoch 08609: loss did not improve\n",
      "Epoch 8610/10000\n",
      "2700/2700 [==============================] - 0s 30us/step - loss: 1.2216e-04 - mean_absolute_error: 0.0065\n",
      "\n",
      "Epoch 08610: loss did not improve\n",
      "Epoch 8611/10000\n",
      "2700/2700 [==============================] - 0s 29us/step - loss: 1.2379e-04 - mean_absolute_error: 0.0068\n",
      "\n",
      "Epoch 08611: loss did not improve\n",
      "Epoch 8612/10000\n",
      "2700/2700 [==============================] - 0s 28us/step - loss: 1.2380e-04 - mean_absolute_error: 0.0066\n",
      "\n",
      "Epoch 08612: loss did not improve\n",
      "Epoch 8613/10000\n",
      "2700/2700 [==============================] - 0s 27us/step - loss: 1.2055e-04 - mean_absolute_error: 0.0063\n",
      "\n",
      "Epoch 08613: loss did not improve\n",
      "Epoch 8614/10000\n",
      "2700/2700 [==============================] - 0s 29us/step - loss: 1.2156e-04 - mean_absolute_error: 0.0064\n",
      "\n",
      "Epoch 08614: loss did not improve\n",
      "Epoch 8615/10000\n",
      "2700/2700 [==============================] - 0s 28us/step - loss: 1.2029e-04 - mean_absolute_error: 0.0062\n",
      "\n",
      "Epoch 08615: loss did not improve\n",
      "Epoch 8616/10000\n",
      "2700/2700 [==============================] - 0s 29us/step - loss: 1.2217e-04 - mean_absolute_error: 0.0065\n",
      "\n",
      "Epoch 08616: loss did not improve\n",
      "Epoch 8617/10000\n",
      "2700/2700 [==============================] - 0s 31us/step - loss: 1.2167e-04 - mean_absolute_error: 0.0064\n",
      "\n",
      "Epoch 08617: loss did not improve\n",
      "Epoch 8618/10000\n",
      "2700/2700 [==============================] - 0s 29us/step - loss: 1.2577e-04 - mean_absolute_error: 0.0067\n",
      "\n",
      "Epoch 08618: loss did not improve\n",
      "Epoch 8619/10000\n",
      "2700/2700 [==============================] - 0s 30us/step - loss: 1.2135e-04 - mean_absolute_error: 0.0064\n",
      "\n",
      "Epoch 08619: loss did not improve\n",
      "Epoch 8620/10000\n",
      "2700/2700 [==============================] - 0s 29us/step - loss: 1.2168e-04 - mean_absolute_error: 0.0064\n",
      "\n",
      "Epoch 08620: loss did not improve\n",
      "Epoch 8621/10000\n",
      "2700/2700 [==============================] - 0s 27us/step - loss: 1.2344e-04 - mean_absolute_error: 0.0066\n",
      "\n",
      "Epoch 08621: loss did not improve\n",
      "Epoch 8622/10000\n",
      "2700/2700 [==============================] - 0s 30us/step - loss: 1.2262e-04 - mean_absolute_error: 0.0065\n",
      "\n",
      "Epoch 08622: loss did not improve\n",
      "Epoch 8623/10000\n",
      "2700/2700 [==============================] - 0s 29us/step - loss: 1.2226e-04 - mean_absolute_error: 0.0064\n",
      "\n",
      "Epoch 08623: loss did not improve\n",
      "Epoch 8624/10000\n",
      "2700/2700 [==============================] - 0s 30us/step - loss: 1.2456e-04 - mean_absolute_error: 0.0067\n",
      "\n",
      "Epoch 08624: loss did not improve\n",
      "Epoch 8625/10000\n",
      "2700/2700 [==============================] - 0s 31us/step - loss: 1.2223e-04 - mean_absolute_error: 0.0065\n",
      "\n",
      "Epoch 08625: loss did not improve\n",
      "Epoch 8626/10000\n",
      "2700/2700 [==============================] - 0s 29us/step - loss: 1.2379e-04 - mean_absolute_error: 0.0066\n",
      "\n",
      "Epoch 08626: loss did not improve\n",
      "Epoch 8627/10000\n",
      "2700/2700 [==============================] - 0s 28us/step - loss: 1.2258e-04 - mean_absolute_error: 0.0064\n",
      "\n",
      "Epoch 08627: loss did not improve\n",
      "Epoch 8628/10000\n",
      "2700/2700 [==============================] - 0s 28us/step - loss: 1.2171e-04 - mean_absolute_error: 0.0064\n",
      "\n",
      "Epoch 08628: loss did not improve\n",
      "Epoch 8629/10000\n",
      "2700/2700 [==============================] - 0s 31us/step - loss: 1.2456e-04 - mean_absolute_error: 0.0067\n",
      "\n",
      "Epoch 08629: loss did not improve\n",
      "Epoch 8630/10000\n",
      "2700/2700 [==============================] - 0s 31us/step - loss: 1.2187e-04 - mean_absolute_error: 0.0063\n",
      "\n",
      "Epoch 08630: loss did not improve\n",
      "Epoch 8631/10000\n",
      "2700/2700 [==============================] - 0s 29us/step - loss: 1.2013e-04 - mean_absolute_error: 0.0063\n",
      "\n",
      "Epoch 08631: loss did not improve\n",
      "Epoch 8632/10000\n",
      "2700/2700 [==============================] - 0s 32us/step - loss: 1.2044e-04 - mean_absolute_error: 0.0064\n",
      "\n",
      "Epoch 08632: loss did not improve\n",
      "Epoch 8633/10000\n",
      "2700/2700 [==============================] - 0s 29us/step - loss: 1.2485e-04 - mean_absolute_error: 0.0067\n",
      "\n",
      "Epoch 08633: loss did not improve\n",
      "Epoch 8634/10000\n",
      "2700/2700 [==============================] - 0s 30us/step - loss: 1.2382e-04 - mean_absolute_error: 0.0066\n",
      "\n",
      "Epoch 08634: loss did not improve\n",
      "Epoch 8635/10000\n",
      "2700/2700 [==============================] - 0s 32us/step - loss: 1.2075e-04 - mean_absolute_error: 0.0063\n",
      "\n",
      "Epoch 08635: loss did not improve\n",
      "Epoch 8636/10000\n",
      "2700/2700 [==============================] - 0s 32us/step - loss: 1.2077e-04 - mean_absolute_error: 0.0063\n",
      "\n",
      "Epoch 08636: loss did not improve\n",
      "Epoch 8637/10000\n",
      "2700/2700 [==============================] - 0s 28us/step - loss: 1.2058e-04 - mean_absolute_error: 0.0063\n",
      "\n",
      "Epoch 08637: loss did not improve\n",
      "Epoch 8638/10000\n",
      "2700/2700 [==============================] - 0s 31us/step - loss: 1.2045e-04 - mean_absolute_error: 0.0063\n",
      "\n",
      "Epoch 08638: loss did not improve\n",
      "Epoch 8639/10000\n",
      "2700/2700 [==============================] - 0s 32us/step - loss: 1.1990e-04 - mean_absolute_error: 0.0063\n",
      "\n",
      "Epoch 08639: loss did not improve\n",
      "Epoch 8640/10000\n",
      "2700/2700 [==============================] - 0s 35us/step - loss: 1.2143e-04 - mean_absolute_error: 0.0066\n",
      "\n",
      "Epoch 08640: loss did not improve\n",
      "Epoch 8641/10000\n",
      "2700/2700 [==============================] - 0s 37us/step - loss: 1.2042e-04 - mean_absolute_error: 0.0063\n",
      "\n",
      "Epoch 08641: loss did not improve\n",
      "Epoch 8642/10000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2700/2700 [==============================] - 0s 31us/step - loss: 1.2878e-04 - mean_absolute_error: 0.0070\n",
      "\n",
      "Epoch 08642: loss did not improve\n",
      "Epoch 8643/10000\n",
      "2700/2700 [==============================] - 0s 34us/step - loss: 1.2169e-04 - mean_absolute_error: 0.0064\n",
      "\n",
      "Epoch 08643: loss did not improve\n",
      "Epoch 8644/10000\n",
      "2700/2700 [==============================] - 0s 30us/step - loss: 1.2301e-04 - mean_absolute_error: 0.0065\n",
      "\n",
      "Epoch 08644: loss did not improve\n",
      "Epoch 8645/10000\n",
      "2700/2700 [==============================] - 0s 33us/step - loss: 1.2293e-04 - mean_absolute_error: 0.0065\n",
      "\n",
      "Epoch 08645: loss did not improve\n",
      "Epoch 8646/10000\n",
      "2700/2700 [==============================] - 0s 36us/step - loss: 1.2194e-04 - mean_absolute_error: 0.0064\n",
      "\n",
      "Epoch 08646: loss did not improve\n",
      "Epoch 8647/10000\n",
      "2700/2700 [==============================] - 0s 29us/step - loss: 1.2069e-04 - mean_absolute_error: 0.0063\n",
      "\n",
      "Epoch 08647: loss did not improve\n",
      "Epoch 8648/10000\n",
      "2700/2700 [==============================] - 0s 29us/step - loss: 1.2266e-04 - mean_absolute_error: 0.0065\n",
      "\n",
      "Epoch 08648: loss did not improve\n",
      "Epoch 8649/10000\n",
      "2700/2700 [==============================] - 0s 31us/step - loss: 1.2158e-04 - mean_absolute_error: 0.0065\n",
      "\n",
      "Epoch 08649: loss did not improve\n",
      "Epoch 8650/10000\n",
      "2700/2700 [==============================] - 0s 31us/step - loss: 1.2184e-04 - mean_absolute_error: 0.0064\n",
      "\n",
      "Epoch 08650: loss did not improve\n",
      "Epoch 8651/10000\n",
      "2700/2700 [==============================] - 0s 29us/step - loss: 1.2461e-04 - mean_absolute_error: 0.0066\n",
      "\n",
      "Epoch 08651: loss did not improve\n",
      "Epoch 8652/10000\n",
      "2700/2700 [==============================] - 0s 29us/step - loss: 1.2097e-04 - mean_absolute_error: 0.0063\n",
      "\n",
      "Epoch 08652: loss did not improve\n",
      "Epoch 8653/10000\n",
      "2700/2700 [==============================] - 0s 28us/step - loss: 1.2018e-04 - mean_absolute_error: 0.0063\n",
      "\n",
      "Epoch 08653: loss did not improve\n",
      "Epoch 8654/10000\n",
      "2700/2700 [==============================] - 0s 29us/step - loss: 1.2353e-04 - mean_absolute_error: 0.0065\n",
      "\n",
      "Epoch 08654: loss did not improve\n",
      "Epoch 8655/10000\n",
      "2700/2700 [==============================] - 0s 27us/step - loss: 1.2299e-04 - mean_absolute_error: 0.0066\n",
      "\n",
      "Epoch 08655: loss did not improve\n",
      "Epoch 8656/10000\n",
      "2700/2700 [==============================] - 0s 31us/step - loss: 1.2521e-04 - mean_absolute_error: 0.0067\n",
      "\n",
      "Epoch 08656: loss did not improve\n",
      "Epoch 8657/10000\n",
      "2700/2700 [==============================] - 0s 29us/step - loss: 1.2360e-04 - mean_absolute_error: 0.0067\n",
      "\n",
      "Epoch 08657: loss did not improve\n",
      "Epoch 8658/10000\n",
      "2700/2700 [==============================] - 0s 30us/step - loss: 1.2067e-04 - mean_absolute_error: 0.0063\n",
      "\n",
      "Epoch 08658: loss did not improve\n",
      "Epoch 8659/10000\n",
      "2700/2700 [==============================] - 0s 28us/step - loss: 1.2339e-04 - mean_absolute_error: 0.0066\n",
      "\n",
      "Epoch 08659: loss did not improve\n",
      "Epoch 8660/10000\n",
      "2700/2700 [==============================] - 0s 32us/step - loss: 1.2080e-04 - mean_absolute_error: 0.0064\n",
      "\n",
      "Epoch 08660: loss did not improve\n",
      "Epoch 8661/10000\n",
      "2700/2700 [==============================] - 0s 30us/step - loss: 1.2025e-04 - mean_absolute_error: 0.0063\n",
      "\n",
      "Epoch 08661: loss did not improve\n",
      "Epoch 8662/10000\n",
      "2700/2700 [==============================] - 0s 28us/step - loss: 1.2376e-04 - mean_absolute_error: 0.0067\n",
      "\n",
      "Epoch 08662: loss did not improve\n",
      "Epoch 8663/10000\n",
      "2700/2700 [==============================] - 0s 29us/step - loss: 1.2329e-04 - mean_absolute_error: 0.0066\n",
      "\n",
      "Epoch 08663: loss did not improve\n",
      "Epoch 8664/10000\n",
      "2700/2700 [==============================] - 0s 31us/step - loss: 1.2216e-04 - mean_absolute_error: 0.0066\n",
      "\n",
      "Epoch 08664: loss did not improve\n",
      "Epoch 8665/10000\n",
      "2700/2700 [==============================] - 0s 28us/step - loss: 1.2098e-04 - mean_absolute_error: 0.0064\n",
      "\n",
      "Epoch 08665: loss did not improve\n",
      "Epoch 8666/10000\n",
      "2700/2700 [==============================] - 0s 28us/step - loss: 1.1946e-04 - mean_absolute_error: 0.0062\n",
      "\n",
      "Epoch 08666: loss did not improve\n",
      "Epoch 8667/10000\n",
      "2700/2700 [==============================] - 0s 30us/step - loss: 1.2163e-04 - mean_absolute_error: 0.0063\n",
      "\n",
      "Epoch 08667: loss did not improve\n",
      "Epoch 8668/10000\n",
      "2700/2700 [==============================] - 0s 30us/step - loss: 1.2229e-04 - mean_absolute_error: 0.0064\n",
      "\n",
      "Epoch 08668: loss did not improve\n",
      "Epoch 8669/10000\n",
      "2700/2700 [==============================] - 0s 29us/step - loss: 1.2257e-04 - mean_absolute_error: 0.0065\n",
      "\n",
      "Epoch 08669: loss did not improve\n",
      "Epoch 8670/10000\n",
      "2700/2700 [==============================] - 0s 26us/step - loss: 1.2001e-04 - mean_absolute_error: 0.0063\n",
      "\n",
      "Epoch 08670: loss did not improve\n",
      "Epoch 8671/10000\n",
      "2700/2700 [==============================] - 0s 29us/step - loss: 1.2244e-04 - mean_absolute_error: 0.0065\n",
      "\n",
      "Epoch 08671: loss did not improve\n",
      "Epoch 8672/10000\n",
      "2700/2700 [==============================] - 0s 28us/step - loss: 1.2540e-04 - mean_absolute_error: 0.0068\n",
      "\n",
      "Epoch 08672: loss did not improve\n",
      "Epoch 8673/10000\n",
      "2700/2700 [==============================] - 0s 28us/step - loss: 1.2282e-04 - mean_absolute_error: 0.0065\n",
      "\n",
      "Epoch 08673: loss did not improve\n",
      "Epoch 8674/10000\n",
      "2700/2700 [==============================] - 0s 28us/step - loss: 1.1968e-04 - mean_absolute_error: 0.0063\n",
      "\n",
      "Epoch 08674: loss did not improve\n",
      "Epoch 8675/10000\n",
      "2700/2700 [==============================] - 0s 28us/step - loss: 1.2160e-04 - mean_absolute_error: 0.0063\n",
      "\n",
      "Epoch 08675: loss did not improve\n",
      "Epoch 8676/10000\n",
      "2700/2700 [==============================] - 0s 31us/step - loss: 1.2180e-04 - mean_absolute_error: 0.0063\n",
      "\n",
      "Epoch 08676: loss did not improve\n",
      "Epoch 8677/10000\n",
      "2700/2700 [==============================] - 0s 30us/step - loss: 1.2366e-04 - mean_absolute_error: 0.0067\n",
      "\n",
      "Epoch 08677: loss did not improve\n",
      "Epoch 8678/10000\n",
      "2700/2700 [==============================] - 0s 29us/step - loss: 1.2667e-04 - mean_absolute_error: 0.0070\n",
      "\n",
      "Epoch 08678: loss did not improve\n",
      "Epoch 8679/10000\n",
      "2700/2700 [==============================] - 0s 35us/step - loss: 1.2239e-04 - mean_absolute_error: 0.0066\n",
      "\n",
      "Epoch 08679: loss did not improve\n",
      "Epoch 8680/10000\n",
      "2700/2700 [==============================] - 0s 28us/step - loss: 1.2022e-04 - mean_absolute_error: 0.0064\n",
      "\n",
      "Epoch 08680: loss did not improve\n",
      "Epoch 8681/10000\n",
      "2700/2700 [==============================] - 0s 32us/step - loss: 1.2066e-04 - mean_absolute_error: 0.0063\n",
      "\n",
      "Epoch 08681: loss did not improve\n",
      "Epoch 8682/10000\n",
      "2700/2700 [==============================] - 0s 28us/step - loss: 1.2559e-04 - mean_absolute_error: 0.0067\n",
      "\n",
      "Epoch 08682: loss did not improve\n",
      "Epoch 8683/10000\n",
      "2700/2700 [==============================] - 0s 29us/step - loss: 1.2322e-04 - mean_absolute_error: 0.0065\n",
      "\n",
      "Epoch 08683: loss did not improve\n",
      "Epoch 8684/10000\n",
      "2700/2700 [==============================] - 0s 27us/step - loss: 1.2662e-04 - mean_absolute_error: 0.0068\n",
      "\n",
      "Epoch 08684: loss did not improve\n",
      "Epoch 8685/10000\n",
      "2700/2700 [==============================] - 0s 30us/step - loss: 1.2066e-04 - mean_absolute_error: 0.0064\n",
      "\n",
      "Epoch 08685: loss did not improve\n",
      "Epoch 8686/10000\n",
      "2700/2700 [==============================] - 0s 30us/step - loss: 1.2149e-04 - mean_absolute_error: 0.0064\n",
      "\n",
      "Epoch 08686: loss did not improve\n",
      "Epoch 8687/10000\n",
      "2700/2700 [==============================] - 0s 28us/step - loss: 1.1965e-04 - mean_absolute_error: 0.0063\n",
      "\n",
      "Epoch 08687: loss did not improve\n",
      "Epoch 8688/10000\n",
      "2700/2700 [==============================] - 0s 28us/step - loss: 1.2086e-04 - mean_absolute_error: 0.0063\n",
      "\n",
      "Epoch 08688: loss did not improve\n",
      "Epoch 8689/10000\n",
      "2700/2700 [==============================] - 0s 26us/step - loss: 1.2177e-04 - mean_absolute_error: 0.0064\n",
      "\n",
      "Epoch 08689: loss did not improve\n",
      "Epoch 8690/10000\n",
      "2700/2700 [==============================] - 0s 30us/step - loss: 1.2116e-04 - mean_absolute_error: 0.0064\n",
      "\n",
      "Epoch 08690: loss did not improve\n",
      "Epoch 8691/10000\n",
      "2700/2700 [==============================] - 0s 28us/step - loss: 1.2475e-04 - mean_absolute_error: 0.0067\n",
      "\n",
      "Epoch 08691: loss did not improve\n",
      "Epoch 8692/10000\n",
      "2700/2700 [==============================] - 0s 28us/step - loss: 1.2297e-04 - mean_absolute_error: 0.0066\n",
      "\n",
      "Epoch 08692: loss did not improve\n",
      "Epoch 8693/10000\n",
      "2700/2700 [==============================] - 0s 26us/step - loss: 1.2089e-04 - mean_absolute_error: 0.0063\n",
      "\n",
      "Epoch 08693: loss did not improve\n",
      "Epoch 8694/10000\n",
      "2700/2700 [==============================] - 0s 29us/step - loss: 1.2202e-04 - mean_absolute_error: 0.0065\n",
      "\n",
      "Epoch 08694: loss did not improve\n",
      "Epoch 8695/10000\n",
      "2700/2700 [==============================] - 0s 29us/step - loss: 1.2416e-04 - mean_absolute_error: 0.0067\n",
      "\n",
      "Epoch 08695: loss did not improve\n",
      "Epoch 8696/10000\n",
      "2700/2700 [==============================] - 0s 29us/step - loss: 1.2237e-04 - mean_absolute_error: 0.0064\n",
      "\n",
      "Epoch 08696: loss did not improve\n",
      "Epoch 8697/10000\n",
      "2700/2700 [==============================] - 0s 32us/step - loss: 1.2152e-04 - mean_absolute_error: 0.0064\n",
      "\n",
      "Epoch 08697: loss did not improve\n",
      "Epoch 8698/10000\n",
      "2700/2700 [==============================] - 0s 25us/step - loss: 1.2156e-04 - mean_absolute_error: 0.0065\n",
      "\n",
      "Epoch 08698: loss did not improve\n",
      "Epoch 8699/10000\n",
      "2700/2700 [==============================] - 0s 31us/step - loss: 1.2094e-04 - mean_absolute_error: 0.0063\n",
      "\n",
      "Epoch 08699: loss did not improve\n",
      "Epoch 8700/10000\n",
      "2700/2700 [==============================] - 0s 31us/step - loss: 1.2263e-04 - mean_absolute_error: 0.0065\n",
      "\n",
      "Epoch 08700: loss did not improve\n",
      "Epoch 8701/10000\n",
      "2700/2700 [==============================] - 0s 27us/step - loss: 1.2218e-04 - mean_absolute_error: 0.0065\n",
      "\n",
      "Epoch 08701: loss did not improve\n",
      "Epoch 8702/10000\n",
      "2700/2700 [==============================] - 0s 27us/step - loss: 1.2177e-04 - mean_absolute_error: 0.0064\n",
      "\n",
      "Epoch 08702: loss did not improve\n",
      "Epoch 8703/10000\n",
      "2700/2700 [==============================] - 0s 30us/step - loss: 1.2162e-04 - mean_absolute_error: 0.0063\n",
      "\n",
      "Epoch 08703: loss did not improve\n",
      "Epoch 8704/10000\n",
      "2700/2700 [==============================] - 0s 28us/step - loss: 1.2075e-04 - mean_absolute_error: 0.0063\n",
      "\n",
      "Epoch 08704: loss did not improve\n",
      "Epoch 8705/10000\n",
      "2700/2700 [==============================] - 0s 28us/step - loss: 1.2470e-04 - mean_absolute_error: 0.0067\n",
      "\n",
      "Epoch 08705: loss did not improve\n",
      "Epoch 8706/10000\n",
      "2700/2700 [==============================] - 0s 29us/step - loss: 1.2161e-04 - mean_absolute_error: 0.0064\n",
      "\n",
      "Epoch 08706: loss did not improve\n",
      "Epoch 8707/10000\n",
      "2700/2700 [==============================] - 0s 28us/step - loss: 1.2032e-04 - mean_absolute_error: 0.0064\n",
      "\n",
      "Epoch 08707: loss did not improve\n",
      "Epoch 8708/10000\n",
      "2700/2700 [==============================] - 0s 29us/step - loss: 1.2154e-04 - mean_absolute_error: 0.0065\n",
      "\n",
      "Epoch 08708: loss did not improve\n",
      "Epoch 8709/10000\n",
      "2700/2700 [==============================] - 0s 30us/step - loss: 1.2439e-04 - mean_absolute_error: 0.0067\n",
      "\n",
      "Epoch 08709: loss did not improve\n",
      "Epoch 8710/10000\n",
      "2700/2700 [==============================] - 0s 28us/step - loss: 1.2554e-04 - mean_absolute_error: 0.0067\n",
      "\n",
      "Epoch 08710: loss did not improve\n",
      "Epoch 8711/10000\n",
      "2700/2700 [==============================] - 0s 27us/step - loss: 1.2589e-04 - mean_absolute_error: 0.0066\n",
      "\n",
      "Epoch 08711: loss did not improve\n",
      "Epoch 8712/10000\n",
      "2700/2700 [==============================] - 0s 28us/step - loss: 1.2146e-04 - mean_absolute_error: 0.0064\n",
      "\n",
      "Epoch 08712: loss did not improve\n",
      "Epoch 8713/10000\n",
      "2700/2700 [==============================] - 0s 30us/step - loss: 1.2023e-04 - mean_absolute_error: 0.0063\n",
      "\n",
      "Epoch 08713: loss did not improve\n",
      "Epoch 8714/10000\n",
      "2700/2700 [==============================] - 0s 28us/step - loss: 1.2110e-04 - mean_absolute_error: 0.0064\n",
      "\n",
      "Epoch 08714: loss did not improve\n",
      "Epoch 8715/10000\n",
      "2700/2700 [==============================] - 0s 29us/step - loss: 1.2356e-04 - mean_absolute_error: 0.0065\n",
      "\n",
      "Epoch 08715: loss did not improve\n",
      "Epoch 8716/10000\n",
      "2700/2700 [==============================] - 0s 27us/step - loss: 1.2218e-04 - mean_absolute_error: 0.0064\n",
      "\n",
      "Epoch 08716: loss did not improve\n",
      "Epoch 8717/10000\n",
      "2700/2700 [==============================] - 0s 29us/step - loss: 1.2248e-04 - mean_absolute_error: 0.0064\n",
      "\n",
      "Epoch 08717: loss did not improve\n",
      "Epoch 8718/10000\n",
      "2700/2700 [==============================] - 0s 28us/step - loss: 1.2248e-04 - mean_absolute_error: 0.0065\n",
      "\n",
      "Epoch 08718: loss did not improve\n",
      "Epoch 8719/10000\n",
      "2700/2700 [==============================] - 0s 30us/step - loss: 1.2283e-04 - mean_absolute_error: 0.0065\n",
      "\n",
      "Epoch 08719: loss did not improve\n",
      "Epoch 8720/10000\n",
      "2700/2700 [==============================] - 0s 26us/step - loss: 1.2377e-04 - mean_absolute_error: 0.0066\n",
      "\n",
      "Epoch 08720: loss did not improve\n",
      "Epoch 8721/10000\n",
      "2700/2700 [==============================] - 0s 29us/step - loss: 1.2150e-04 - mean_absolute_error: 0.0064\n",
      "\n",
      "Epoch 08721: loss did not improve\n",
      "Epoch 8722/10000\n",
      "2700/2700 [==============================] - 0s 28us/step - loss: 1.2454e-04 - mean_absolute_error: 0.0067\n",
      "\n",
      "Epoch 08722: loss did not improve\n",
      "Epoch 8723/10000\n",
      "2700/2700 [==============================] - 0s 30us/step - loss: 1.2239e-04 - mean_absolute_error: 0.0065\n",
      "\n",
      "Epoch 08723: loss did not improve\n",
      "Epoch 8724/10000\n",
      "2700/2700 [==============================] - 0s 30us/step - loss: 1.2363e-04 - mean_absolute_error: 0.0066\n",
      "\n",
      "Epoch 08724: loss did not improve\n",
      "Epoch 8725/10000\n",
      "2700/2700 [==============================] - 0s 28us/step - loss: 1.2357e-04 - mean_absolute_error: 0.0065\n",
      "\n",
      "Epoch 08725: loss did not improve\n",
      "Epoch 8726/10000\n",
      "2700/2700 [==============================] - 0s 29us/step - loss: 1.2066e-04 - mean_absolute_error: 0.0064\n",
      "\n",
      "Epoch 08726: loss did not improve\n",
      "Epoch 8727/10000\n",
      "2700/2700 [==============================] - 0s 28us/step - loss: 1.2165e-04 - mean_absolute_error: 0.0064\n",
      "\n",
      "Epoch 08727: loss did not improve\n",
      "Epoch 8728/10000\n",
      "2700/2700 [==============================] - 0s 29us/step - loss: 1.2202e-04 - mean_absolute_error: 0.0065\n",
      "\n",
      "Epoch 08728: loss did not improve\n",
      "Epoch 8729/10000\n",
      "2700/2700 [==============================] - 0s 26us/step - loss: 1.2209e-04 - mean_absolute_error: 0.0065\n",
      "\n",
      "Epoch 08729: loss did not improve\n",
      "Epoch 8730/10000\n",
      "2700/2700 [==============================] - 0s 31us/step - loss: 1.2302e-04 - mean_absolute_error: 0.0067\n",
      "\n",
      "Epoch 08730: loss did not improve\n",
      "Epoch 8731/10000\n",
      "2700/2700 [==============================] - 0s 28us/step - loss: 1.2458e-04 - mean_absolute_error: 0.0067\n",
      "\n",
      "Epoch 08731: loss did not improve\n",
      "Epoch 8732/10000\n",
      "2700/2700 [==============================] - 0s 29us/step - loss: 1.2245e-04 - mean_absolute_error: 0.0064\n",
      "\n",
      "Epoch 08732: loss did not improve\n",
      "Epoch 8733/10000\n",
      "2700/2700 [==============================] - 0s 29us/step - loss: 1.2616e-04 - mean_absolute_error: 0.0068\n",
      "\n",
      "Epoch 08733: loss did not improve\n",
      "Epoch 8734/10000\n",
      "2700/2700 [==============================] - 0s 28us/step - loss: 1.1994e-04 - mean_absolute_error: 0.0062\n",
      "\n",
      "Epoch 08734: loss did not improve\n",
      "Epoch 8735/10000\n",
      "2700/2700 [==============================] - 0s 29us/step - loss: 1.2168e-04 - mean_absolute_error: 0.0065\n",
      "\n",
      "Epoch 08735: loss did not improve\n",
      "Epoch 8736/10000\n",
      "2700/2700 [==============================] - 0s 27us/step - loss: 1.2618e-04 - mean_absolute_error: 0.0067\n",
      "\n",
      "Epoch 08736: loss did not improve\n",
      "Epoch 8737/10000\n",
      "2700/2700 [==============================] - 0s 30us/step - loss: 1.2068e-04 - mean_absolute_error: 0.0063\n",
      "\n",
      "Epoch 08737: loss did not improve\n",
      "Epoch 8738/10000\n",
      "2700/2700 [==============================] - 0s 27us/step - loss: 1.2188e-04 - mean_absolute_error: 0.0065\n",
      "\n",
      "Epoch 08738: loss did not improve\n",
      "Epoch 8739/10000\n",
      "2700/2700 [==============================] - 0s 30us/step - loss: 1.2116e-04 - mean_absolute_error: 0.0064\n",
      "\n",
      "Epoch 08739: loss did not improve\n",
      "Epoch 8740/10000\n",
      "2700/2700 [==============================] - 0s 28us/step - loss: 1.2283e-04 - mean_absolute_error: 0.0065\n",
      "\n",
      "Epoch 08740: loss did not improve\n",
      "Epoch 8741/10000\n",
      "2700/2700 [==============================] - 0s 30us/step - loss: 1.2099e-04 - mean_absolute_error: 0.0063\n",
      "\n",
      "Epoch 08741: loss did not improve\n",
      "Epoch 8742/10000\n",
      "2700/2700 [==============================] - 0s 31us/step - loss: 1.2136e-04 - mean_absolute_error: 0.0064\n",
      "\n",
      "Epoch 08742: loss did not improve\n",
      "Epoch 8743/10000\n",
      "2700/2700 [==============================] - 0s 26us/step - loss: 1.2020e-04 - mean_absolute_error: 0.0064\n",
      "\n",
      "Epoch 08743: loss did not improve\n",
      "Epoch 8744/10000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2700/2700 [==============================] - 0s 30us/step - loss: 1.2428e-04 - mean_absolute_error: 0.0067\n",
      "\n",
      "Epoch 08744: loss did not improve\n",
      "Epoch 8745/10000\n",
      "2700/2700 [==============================] - 0s 29us/step - loss: 1.2584e-04 - mean_absolute_error: 0.0067\n",
      "\n",
      "Epoch 08745: loss did not improve\n",
      "Epoch 8746/10000\n",
      "2700/2700 [==============================] - 0s 30us/step - loss: 1.2339e-04 - mean_absolute_error: 0.0066\n",
      "\n",
      "Epoch 08746: loss did not improve\n",
      "Epoch 8747/10000\n",
      "2700/2700 [==============================] - 0s 31us/step - loss: 1.2246e-04 - mean_absolute_error: 0.0065\n",
      "\n",
      "Epoch 08747: loss did not improve\n",
      "Epoch 8748/10000\n",
      "2700/2700 [==============================] - 0s 28us/step - loss: 1.2103e-04 - mean_absolute_error: 0.0064\n",
      "\n",
      "Epoch 08748: loss did not improve\n",
      "Epoch 8749/10000\n",
      "2700/2700 [==============================] - 0s 29us/step - loss: 1.2062e-04 - mean_absolute_error: 0.0064\n",
      "\n",
      "Epoch 08749: loss did not improve\n",
      "Epoch 8750/10000\n",
      "2700/2700 [==============================] - 0s 28us/step - loss: 1.2547e-04 - mean_absolute_error: 0.0066\n",
      "\n",
      "Epoch 08750: loss did not improve\n",
      "Epoch 8751/10000\n",
      "2700/2700 [==============================] - 0s 29us/step - loss: 1.2016e-04 - mean_absolute_error: 0.0064\n",
      "\n",
      "Epoch 08751: loss did not improve\n",
      "Epoch 8752/10000\n",
      "2700/2700 [==============================] - 0s 33us/step - loss: 1.2188e-04 - mean_absolute_error: 0.0065\n",
      "\n",
      "Epoch 08752: loss did not improve\n",
      "Epoch 8753/10000\n",
      "2700/2700 [==============================] - 0s 30us/step - loss: 1.2341e-04 - mean_absolute_error: 0.0066\n",
      "\n",
      "Epoch 08753: loss did not improve\n",
      "Epoch 8754/10000\n",
      "2700/2700 [==============================] - 0s 31us/step - loss: 1.2184e-04 - mean_absolute_error: 0.0064\n",
      "\n",
      "Epoch 08754: loss did not improve\n",
      "Epoch 8755/10000\n",
      "2700/2700 [==============================] - 0s 29us/step - loss: 1.2149e-04 - mean_absolute_error: 0.0064\n",
      "\n",
      "Epoch 08755: loss did not improve\n",
      "Epoch 8756/10000\n",
      "2700/2700 [==============================] - 0s 29us/step - loss: 1.2086e-04 - mean_absolute_error: 0.0063\n",
      "\n",
      "Epoch 08756: loss did not improve\n",
      "Epoch 8757/10000\n",
      "2700/2700 [==============================] - 0s 31us/step - loss: 1.2112e-04 - mean_absolute_error: 0.0064\n",
      "\n",
      "Epoch 08757: loss did not improve\n",
      "Epoch 8758/10000\n",
      "2700/2700 [==============================] - 0s 28us/step - loss: 1.2632e-04 - mean_absolute_error: 0.0069\n",
      "\n",
      "Epoch 08758: loss did not improve\n",
      "Epoch 8759/10000\n",
      "2700/2700 [==============================] - 0s 30us/step - loss: 1.2840e-04 - mean_absolute_error: 0.0069\n",
      "\n",
      "Epoch 08759: loss did not improve\n",
      "Epoch 8760/10000\n",
      "2700/2700 [==============================] - 0s 30us/step - loss: 1.2143e-04 - mean_absolute_error: 0.0064\n",
      "\n",
      "Epoch 08760: loss did not improve\n",
      "Epoch 8761/10000\n",
      "2700/2700 [==============================] - 0s 30us/step - loss: 1.2118e-04 - mean_absolute_error: 0.0063\n",
      "\n",
      "Epoch 08761: loss did not improve\n",
      "Epoch 8762/10000\n",
      "2700/2700 [==============================] - 0s 30us/step - loss: 1.2195e-04 - mean_absolute_error: 0.0065\n",
      "\n",
      "Epoch 08762: loss did not improve\n",
      "Epoch 8763/10000\n",
      "2700/2700 [==============================] - 0s 28us/step - loss: 1.2084e-04 - mean_absolute_error: 0.0063\n",
      "\n",
      "Epoch 08763: loss did not improve\n",
      "Epoch 8764/10000\n",
      "2700/2700 [==============================] - 0s 31us/step - loss: 1.2279e-04 - mean_absolute_error: 0.0065\n",
      "\n",
      "Epoch 08764: loss did not improve\n",
      "Epoch 8765/10000\n",
      "2700/2700 [==============================] - 0s 30us/step - loss: 1.2121e-04 - mean_absolute_error: 0.0064\n",
      "\n",
      "Epoch 08765: loss did not improve\n",
      "Epoch 8766/10000\n",
      "2700/2700 [==============================] - 0s 30us/step - loss: 1.2273e-04 - mean_absolute_error: 0.0065\n",
      "\n",
      "Epoch 08766: loss did not improve\n",
      "Epoch 8767/10000\n",
      "2700/2700 [==============================] - 0s 31us/step - loss: 1.2798e-04 - mean_absolute_error: 0.0069\n",
      "\n",
      "Epoch 08767: loss did not improve\n",
      "Epoch 8768/10000\n",
      "2700/2700 [==============================] - 0s 25us/step - loss: 1.2266e-04 - mean_absolute_error: 0.0066\n",
      "\n",
      "Epoch 08768: loss did not improve\n",
      "Epoch 8769/10000\n",
      "2700/2700 [==============================] - 0s 30us/step - loss: 1.2092e-04 - mean_absolute_error: 0.0064\n",
      "\n",
      "Epoch 08769: loss did not improve\n",
      "Epoch 8770/10000\n",
      "2700/2700 [==============================] - 0s 30us/step - loss: 1.2118e-04 - mean_absolute_error: 0.0064\n",
      "\n",
      "Epoch 08770: loss did not improve\n",
      "Epoch 8771/10000\n",
      "2700/2700 [==============================] - 0s 29us/step - loss: 1.2171e-04 - mean_absolute_error: 0.0064\n",
      "\n",
      "Epoch 08771: loss did not improve\n",
      "Epoch 8772/10000\n",
      "2700/2700 [==============================] - 0s 28us/step - loss: 1.1977e-04 - mean_absolute_error: 0.0062\n",
      "\n",
      "Epoch 08772: loss did not improve\n",
      "Epoch 8773/10000\n",
      "2700/2700 [==============================] - 0s 27us/step - loss: 1.2173e-04 - mean_absolute_error: 0.0064\n",
      "\n",
      "Epoch 08773: loss did not improve\n",
      "Epoch 8774/10000\n",
      "2700/2700 [==============================] - 0s 32us/step - loss: 1.2209e-04 - mean_absolute_error: 0.0065\n",
      "\n",
      "Epoch 08774: loss did not improve\n",
      "Epoch 8775/10000\n",
      "2700/2700 [==============================] - 0s 29us/step - loss: 1.2069e-04 - mean_absolute_error: 0.0063\n",
      "\n",
      "Epoch 08775: loss did not improve\n",
      "Epoch 8776/10000\n",
      "2700/2700 [==============================] - 0s 29us/step - loss: 1.2160e-04 - mean_absolute_error: 0.0064\n",
      "\n",
      "Epoch 08776: loss did not improve\n",
      "Epoch 8777/10000\n",
      "2700/2700 [==============================] - 0s 29us/step - loss: 1.2447e-04 - mean_absolute_error: 0.0067\n",
      "\n",
      "Epoch 08777: loss did not improve\n",
      "Epoch 8778/10000\n",
      "2700/2700 [==============================] - 0s 30us/step - loss: 1.1974e-04 - mean_absolute_error: 0.0063\n",
      "\n",
      "Epoch 08778: loss did not improve\n",
      "Epoch 8779/10000\n",
      "2700/2700 [==============================] - 0s 34us/step - loss: 1.2167e-04 - mean_absolute_error: 0.0065\n",
      "\n",
      "Epoch 08779: loss did not improve\n",
      "Epoch 8780/10000\n",
      "2700/2700 [==============================] - 0s 31us/step - loss: 1.2122e-04 - mean_absolute_error: 0.0064\n",
      "\n",
      "Epoch 08780: loss did not improve\n",
      "Epoch 8781/10000\n",
      "2700/2700 [==============================] - 0s 35us/step - loss: 1.2038e-04 - mean_absolute_error: 0.0063\n",
      "\n",
      "Epoch 08781: loss did not improve\n",
      "Epoch 8782/10000\n",
      "2700/2700 [==============================] - 0s 29us/step - loss: 1.2270e-04 - mean_absolute_error: 0.0065\n",
      "\n",
      "Epoch 08782: loss did not improve\n",
      "Epoch 8783/10000\n",
      "2700/2700 [==============================] - 0s 30us/step - loss: 1.2027e-04 - mean_absolute_error: 0.0062\n",
      "\n",
      "Epoch 08783: loss did not improve\n",
      "Epoch 8784/10000\n",
      "2700/2700 [==============================] - 0s 29us/step - loss: 1.2093e-04 - mean_absolute_error: 0.0064\n",
      "\n",
      "Epoch 08784: loss did not improve\n",
      "Epoch 8785/10000\n",
      "2700/2700 [==============================] - 0s 33us/step - loss: 1.2029e-04 - mean_absolute_error: 0.0064\n",
      "\n",
      "Epoch 08785: loss did not improve\n",
      "Epoch 8786/10000\n",
      "2700/2700 [==============================] - 0s 42us/step - loss: 1.2180e-04 - mean_absolute_error: 0.0065\n",
      "\n",
      "Epoch 08786: loss did not improve\n",
      "Epoch 8787/10000\n",
      "2700/2700 [==============================] - 0s 40us/step - loss: 1.2130e-04 - mean_absolute_error: 0.0063\n",
      "\n",
      "Epoch 08787: loss did not improve\n",
      "Epoch 8788/10000\n",
      "2700/2700 [==============================] - 0s 43us/step - loss: 1.2308e-04 - mean_absolute_error: 0.0065\n",
      "\n",
      "Epoch 08788: loss did not improve\n",
      "Epoch 8789/10000\n",
      "2700/2700 [==============================] - 0s 32us/step - loss: 1.2416e-04 - mean_absolute_error: 0.0067\n",
      "\n",
      "Epoch 08789: loss did not improve\n",
      "Epoch 8790/10000\n",
      "2700/2700 [==============================] - 0s 30us/step - loss: 1.2204e-04 - mean_absolute_error: 0.0065\n",
      "\n",
      "Epoch 08790: loss did not improve\n",
      "Epoch 8791/10000\n",
      "2700/2700 [==============================] - 0s 42us/step - loss: 1.2077e-04 - mean_absolute_error: 0.0063\n",
      "\n",
      "Epoch 08791: loss did not improve\n",
      "Epoch 8792/10000\n",
      "2700/2700 [==============================] - 0s 38us/step - loss: 1.2074e-04 - mean_absolute_error: 0.0064\n",
      "\n",
      "Epoch 08792: loss did not improve\n",
      "Epoch 8793/10000\n",
      "2700/2700 [==============================] - 0s 40us/step - loss: 1.2256e-04 - mean_absolute_error: 0.0066\n",
      "\n",
      "Epoch 08793: loss did not improve\n",
      "Epoch 8794/10000\n",
      "2700/2700 [==============================] - 0s 35us/step - loss: 1.2165e-04 - mean_absolute_error: 0.0064\n",
      "\n",
      "Epoch 08794: loss did not improve\n",
      "Epoch 8795/10000\n",
      "2700/2700 [==============================] - 0s 28us/step - loss: 1.2472e-04 - mean_absolute_error: 0.0067\n",
      "\n",
      "Epoch 08795: loss did not improve\n",
      "Epoch 8796/10000\n",
      "2700/2700 [==============================] - 0s 30us/step - loss: 1.2549e-04 - mean_absolute_error: 0.0067\n",
      "\n",
      "Epoch 08796: loss did not improve\n",
      "Epoch 8797/10000\n",
      "2700/2700 [==============================] - 0s 29us/step - loss: 1.2237e-04 - mean_absolute_error: 0.0065\n",
      "\n",
      "Epoch 08797: loss did not improve\n",
      "Epoch 8798/10000\n",
      "2700/2700 [==============================] - 0s 27us/step - loss: 1.1997e-04 - mean_absolute_error: 0.0062\n",
      "\n",
      "Epoch 08798: loss did not improve\n",
      "Epoch 8799/10000\n",
      "2700/2700 [==============================] - 0s 26us/step - loss: 1.2281e-04 - mean_absolute_error: 0.0064\n",
      "\n",
      "Epoch 08799: loss did not improve\n",
      "Epoch 8800/10000\n",
      "2700/2700 [==============================] - 0s 29us/step - loss: 1.2112e-04 - mean_absolute_error: 0.0064\n",
      "\n",
      "Epoch 08800: loss did not improve\n",
      "Epoch 8801/10000\n",
      "2700/2700 [==============================] - 0s 37us/step - loss: 1.2047e-04 - mean_absolute_error: 0.0063\n",
      "\n",
      "Epoch 08801: loss did not improve\n",
      "Epoch 8802/10000\n",
      "2700/2700 [==============================] - 0s 55us/step - loss: 1.2113e-04 - mean_absolute_error: 0.0063\n",
      "\n",
      "Epoch 08802: loss did not improve\n",
      "Epoch 8803/10000\n",
      "2700/2700 [==============================] - 0s 37us/step - loss: 1.2523e-04 - mean_absolute_error: 0.0066\n",
      "\n",
      "Epoch 08803: loss did not improve\n",
      "Epoch 8804/10000\n",
      "2700/2700 [==============================] - 0s 30us/step - loss: 1.2020e-04 - mean_absolute_error: 0.0062\n",
      "\n",
      "Epoch 08804: loss did not improve\n",
      "Epoch 8805/10000\n",
      "2700/2700 [==============================] - 0s 30us/step - loss: 1.1991e-04 - mean_absolute_error: 0.0062\n",
      "\n",
      "Epoch 08805: loss did not improve\n",
      "Epoch 8806/10000\n",
      "2700/2700 [==============================] - 0s 29us/step - loss: 1.1995e-04 - mean_absolute_error: 0.0063\n",
      "\n",
      "Epoch 08806: loss did not improve\n",
      "Epoch 8807/10000\n",
      "2700/2700 [==============================] - 0s 28us/step - loss: 1.2318e-04 - mean_absolute_error: 0.0065\n",
      "\n",
      "Epoch 08807: loss did not improve\n",
      "Epoch 8808/10000\n",
      "2700/2700 [==============================] - 0s 27us/step - loss: 1.2575e-04 - mean_absolute_error: 0.0068\n",
      "\n",
      "Epoch 08808: loss did not improve\n",
      "Epoch 8809/10000\n",
      "2700/2700 [==============================] - 0s 26us/step - loss: 1.1985e-04 - mean_absolute_error: 0.0062\n",
      "\n",
      "Epoch 08809: loss did not improve\n",
      "Epoch 8810/10000\n",
      "2700/2700 [==============================] - 0s 30us/step - loss: 1.2211e-04 - mean_absolute_error: 0.0066\n",
      "\n",
      "Epoch 08810: loss did not improve\n",
      "Epoch 8811/10000\n",
      "2700/2700 [==============================] - 0s 31us/step - loss: 1.2459e-04 - mean_absolute_error: 0.0067\n",
      "\n",
      "Epoch 08811: loss did not improve\n",
      "Epoch 8812/10000\n",
      "2700/2700 [==============================] - 0s 28us/step - loss: 1.2417e-04 - mean_absolute_error: 0.0066\n",
      "\n",
      "Epoch 08812: loss did not improve\n",
      "Epoch 8813/10000\n",
      "2700/2700 [==============================] - 0s 26us/step - loss: 1.2052e-04 - mean_absolute_error: 0.0065\n",
      "\n",
      "Epoch 08813: loss did not improve\n",
      "Epoch 8814/10000\n",
      "2700/2700 [==============================] - 0s 28us/step - loss: 1.2288e-04 - mean_absolute_error: 0.0066\n",
      "\n",
      "Epoch 08814: loss did not improve\n",
      "Epoch 8815/10000\n",
      "2700/2700 [==============================] - 0s 29us/step - loss: 1.2314e-04 - mean_absolute_error: 0.0065\n",
      "\n",
      "Epoch 08815: loss did not improve\n",
      "Epoch 8816/10000\n",
      "2700/2700 [==============================] - 0s 33us/step - loss: 1.2087e-04 - mean_absolute_error: 0.0065\n",
      "\n",
      "Epoch 08816: loss did not improve\n",
      "Epoch 8817/10000\n",
      "2700/2700 [==============================] - 0s 30us/step - loss: 1.2466e-04 - mean_absolute_error: 0.0066\n",
      "\n",
      "Epoch 08817: loss did not improve\n",
      "Epoch 8818/10000\n",
      "2700/2700 [==============================] - 0s 29us/step - loss: 1.2160e-04 - mean_absolute_error: 0.0063\n",
      "\n",
      "Epoch 08818: loss did not improve\n",
      "Epoch 8819/10000\n",
      "2700/2700 [==============================] - 0s 30us/step - loss: 1.2052e-04 - mean_absolute_error: 0.0063\n",
      "\n",
      "Epoch 08819: loss did not improve\n",
      "Epoch 8820/10000\n",
      "2700/2700 [==============================] - 0s 32us/step - loss: 1.2282e-04 - mean_absolute_error: 0.0067\n",
      "\n",
      "Epoch 08820: loss did not improve\n",
      "Epoch 8821/10000\n",
      "2700/2700 [==============================] - 0s 29us/step - loss: 1.2289e-04 - mean_absolute_error: 0.0066\n",
      "\n",
      "Epoch 08821: loss did not improve\n",
      "Epoch 8822/10000\n",
      "2700/2700 [==============================] - 0s 31us/step - loss: 1.2365e-04 - mean_absolute_error: 0.0066\n",
      "\n",
      "Epoch 08822: loss did not improve\n",
      "Epoch 8823/10000\n",
      "2700/2700 [==============================] - 0s 31us/step - loss: 1.2100e-04 - mean_absolute_error: 0.0064\n",
      "\n",
      "Epoch 08823: loss did not improve\n",
      "Epoch 8824/10000\n",
      "2700/2700 [==============================] - 0s 29us/step - loss: 1.2014e-04 - mean_absolute_error: 0.0063\n",
      "\n",
      "Epoch 08824: loss did not improve\n",
      "Epoch 8825/10000\n",
      "2700/2700 [==============================] - 0s 28us/step - loss: 1.2076e-04 - mean_absolute_error: 0.0063\n",
      "\n",
      "Epoch 08825: loss did not improve\n",
      "Epoch 8826/10000\n",
      "2700/2700 [==============================] - 0s 29us/step - loss: 1.2223e-04 - mean_absolute_error: 0.0065\n",
      "\n",
      "Epoch 08826: loss did not improve\n",
      "Epoch 8827/10000\n",
      "2700/2700 [==============================] - 0s 29us/step - loss: 1.2240e-04 - mean_absolute_error: 0.0065\n",
      "\n",
      "Epoch 08827: loss did not improve\n",
      "Epoch 8828/10000\n",
      "2700/2700 [==============================] - 0s 39us/step - loss: 1.2155e-04 - mean_absolute_error: 0.0064\n",
      "\n",
      "Epoch 08828: loss did not improve\n",
      "Epoch 8829/10000\n",
      "2700/2700 [==============================] - 0s 42us/step - loss: 1.1998e-04 - mean_absolute_error: 0.0063\n",
      "\n",
      "Epoch 08829: loss did not improve\n",
      "Epoch 8830/10000\n",
      "2700/2700 [==============================] - 0s 28us/step - loss: 1.2327e-04 - mean_absolute_error: 0.0066\n",
      "\n",
      "Epoch 08830: loss did not improve\n",
      "Epoch 8831/10000\n",
      "2700/2700 [==============================] - 0s 31us/step - loss: 1.2072e-04 - mean_absolute_error: 0.0064\n",
      "\n",
      "Epoch 08831: loss did not improve\n",
      "Epoch 8832/10000\n",
      "2700/2700 [==============================] - 0s 27us/step - loss: 1.2256e-04 - mean_absolute_error: 0.0064\n",
      "\n",
      "Epoch 08832: loss did not improve\n",
      "Epoch 8833/10000\n",
      "2700/2700 [==============================] - 0s 30us/step - loss: 1.2067e-04 - mean_absolute_error: 0.0063\n",
      "\n",
      "Epoch 08833: loss did not improve\n",
      "Epoch 8834/10000\n",
      "2700/2700 [==============================] - 0s 30us/step - loss: 1.2394e-04 - mean_absolute_error: 0.0065\n",
      "\n",
      "Epoch 08834: loss did not improve\n",
      "Epoch 8835/10000\n",
      "2700/2700 [==============================] - 0s 31us/step - loss: 1.2047e-04 - mean_absolute_error: 0.0062\n",
      "\n",
      "Epoch 08835: loss did not improve\n",
      "Epoch 8836/10000\n",
      "2700/2700 [==============================] - 0s 29us/step - loss: 1.2250e-04 - mean_absolute_error: 0.0065\n",
      "\n",
      "Epoch 08836: loss did not improve\n",
      "Epoch 8837/10000\n",
      "2700/2700 [==============================] - 0s 27us/step - loss: 1.2235e-04 - mean_absolute_error: 0.0065\n",
      "\n",
      "Epoch 08837: loss did not improve\n",
      "Epoch 8838/10000\n",
      "2700/2700 [==============================] - 0s 30us/step - loss: 1.2097e-04 - mean_absolute_error: 0.0064\n",
      "\n",
      "Epoch 08838: loss did not improve\n",
      "Epoch 8839/10000\n",
      "2700/2700 [==============================] - 0s 36us/step - loss: 1.2152e-04 - mean_absolute_error: 0.0064\n",
      "\n",
      "Epoch 08839: loss did not improve\n",
      "Epoch 8840/10000\n",
      "2700/2700 [==============================] - 0s 33us/step - loss: 1.2509e-04 - mean_absolute_error: 0.0067\n",
      "\n",
      "Epoch 08840: loss did not improve\n",
      "Epoch 8841/10000\n",
      "2700/2700 [==============================] - 0s 29us/step - loss: 1.2065e-04 - mean_absolute_error: 0.0064\n",
      "\n",
      "Epoch 08841: loss did not improve\n",
      "Epoch 8842/10000\n",
      "2700/2700 [==============================] - 0s 28us/step - loss: 1.2218e-04 - mean_absolute_error: 0.0066\n",
      "\n",
      "Epoch 08842: loss did not improve\n",
      "Epoch 8843/10000\n",
      "2700/2700 [==============================] - 0s 27us/step - loss: 1.2088e-04 - mean_absolute_error: 0.0064\n",
      "\n",
      "Epoch 08843: loss did not improve\n",
      "Epoch 8844/10000\n",
      "2700/2700 [==============================] - 0s 37us/step - loss: 1.2232e-04 - mean_absolute_error: 0.0065\n",
      "\n",
      "Epoch 08844: loss did not improve\n",
      "Epoch 8845/10000\n",
      "2700/2700 [==============================] - 0s 31us/step - loss: 1.2054e-04 - mean_absolute_error: 0.0063\n",
      "\n",
      "Epoch 08845: loss did not improve\n",
      "Epoch 8846/10000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2700/2700 [==============================] - 0s 28us/step - loss: 1.2101e-04 - mean_absolute_error: 0.0064\n",
      "\n",
      "Epoch 08846: loss did not improve\n",
      "Epoch 8847/10000\n",
      "2700/2700 [==============================] - 0s 30us/step - loss: 1.2122e-04 - mean_absolute_error: 0.0065\n",
      "\n",
      "Epoch 08847: loss did not improve\n",
      "Epoch 8848/10000\n",
      "2700/2700 [==============================] - 0s 29us/step - loss: 1.2330e-04 - mean_absolute_error: 0.0065\n",
      "\n",
      "Epoch 08848: loss did not improve\n",
      "Epoch 8849/10000\n",
      "2700/2700 [==============================] - 0s 32us/step - loss: 1.2429e-04 - mean_absolute_error: 0.0067\n",
      "\n",
      "Epoch 08849: loss did not improve\n",
      "Epoch 8850/10000\n",
      "2700/2700 [==============================] - 0s 37us/step - loss: 1.2279e-04 - mean_absolute_error: 0.0066\n",
      "\n",
      "Epoch 08850: loss did not improve\n",
      "Epoch 8851/10000\n",
      "2700/2700 [==============================] - 0s 28us/step - loss: 1.2273e-04 - mean_absolute_error: 0.0064\n",
      "\n",
      "Epoch 08851: loss did not improve\n",
      "Epoch 8852/10000\n",
      "2700/2700 [==============================] - 0s 29us/step - loss: 1.2111e-04 - mean_absolute_error: 0.0064\n",
      "\n",
      "Epoch 08852: loss did not improve\n",
      "Epoch 8853/10000\n",
      "2700/2700 [==============================] - 0s 28us/step - loss: 1.2124e-04 - mean_absolute_error: 0.0063\n",
      "\n",
      "Epoch 08853: loss did not improve\n",
      "Epoch 8854/10000\n",
      "2700/2700 [==============================] - 0s 25us/step - loss: 1.2181e-04 - mean_absolute_error: 0.0065\n",
      "\n",
      "Epoch 08854: loss did not improve\n",
      "Epoch 8855/10000\n",
      "2700/2700 [==============================] - 0s 30us/step - loss: 1.2225e-04 - mean_absolute_error: 0.0066\n",
      "\n",
      "Epoch 08855: loss did not improve\n",
      "Epoch 8856/10000\n",
      "2700/2700 [==============================] - 0s 28us/step - loss: 1.2294e-04 - mean_absolute_error: 0.0064\n",
      "\n",
      "Epoch 08856: loss did not improve\n",
      "Epoch 8857/10000\n",
      "2700/2700 [==============================] - 0s 32us/step - loss: 1.2146e-04 - mean_absolute_error: 0.0064\n",
      "\n",
      "Epoch 08857: loss did not improve\n",
      "Epoch 8858/10000\n",
      "2700/2700 [==============================] - 0s 29us/step - loss: 1.2040e-04 - mean_absolute_error: 0.0063\n",
      "\n",
      "Epoch 08858: loss did not improve\n",
      "Epoch 8859/10000\n",
      "2700/2700 [==============================] - 0s 30us/step - loss: 1.2112e-04 - mean_absolute_error: 0.0063\n",
      "\n",
      "Epoch 08859: loss did not improve\n",
      "Epoch 8860/10000\n",
      "2700/2700 [==============================] - 0s 31us/step - loss: 1.2742e-04 - mean_absolute_error: 0.0069\n",
      "\n",
      "Epoch 08860: loss did not improve\n",
      "Epoch 8861/10000\n",
      "2700/2700 [==============================] - 0s 28us/step - loss: 1.2202e-04 - mean_absolute_error: 0.0065\n",
      "\n",
      "Epoch 08861: loss did not improve\n",
      "Epoch 8862/10000\n",
      "2700/2700 [==============================] - 0s 30us/step - loss: 1.1969e-04 - mean_absolute_error: 0.0063\n",
      "\n",
      "Epoch 08862: loss did not improve\n",
      "Epoch 8863/10000\n",
      "2700/2700 [==============================] - 0s 28us/step - loss: 1.2047e-04 - mean_absolute_error: 0.0064\n",
      "\n",
      "Epoch 08863: loss did not improve\n",
      "Epoch 8864/10000\n",
      "2700/2700 [==============================] - 0s 27us/step - loss: 1.2151e-04 - mean_absolute_error: 0.0063\n",
      "\n",
      "Epoch 08864: loss did not improve\n",
      "Epoch 8865/10000\n",
      "2700/2700 [==============================] - 0s 30us/step - loss: 1.2207e-04 - mean_absolute_error: 0.0064\n",
      "\n",
      "Epoch 08865: loss did not improve\n",
      "Epoch 8866/10000\n",
      "2700/2700 [==============================] - 0s 28us/step - loss: 1.2460e-04 - mean_absolute_error: 0.0067\n",
      "\n",
      "Epoch 08866: loss did not improve\n",
      "Epoch 8867/10000\n",
      "2700/2700 [==============================] - 0s 30us/step - loss: 1.2104e-04 - mean_absolute_error: 0.0064\n",
      "\n",
      "Epoch 08867: loss did not improve\n",
      "Epoch 8868/10000\n",
      "2700/2700 [==============================] - 0s 28us/step - loss: 1.2024e-04 - mean_absolute_error: 0.0063\n",
      "\n",
      "Epoch 08868: loss did not improve\n",
      "Epoch 8869/10000\n",
      "2700/2700 [==============================] - 0s 29us/step - loss: 1.2069e-04 - mean_absolute_error: 0.0063\n",
      "\n",
      "Epoch 08869: loss did not improve\n",
      "Epoch 8870/10000\n",
      "2700/2700 [==============================] - 0s 28us/step - loss: 1.2262e-04 - mean_absolute_error: 0.0065\n",
      "\n",
      "Epoch 08870: loss did not improve\n",
      "Epoch 8871/10000\n",
      "2700/2700 [==============================] - 0s 30us/step - loss: 1.2264e-04 - mean_absolute_error: 0.0065\n",
      "\n",
      "Epoch 08871: loss did not improve\n",
      "Epoch 8872/10000\n",
      "2700/2700 [==============================] - 0s 28us/step - loss: 1.2042e-04 - mean_absolute_error: 0.0063\n",
      "\n",
      "Epoch 08872: loss did not improve\n",
      "Epoch 8873/10000\n",
      "2700/2700 [==============================] - 0s 28us/step - loss: 1.2388e-04 - mean_absolute_error: 0.0066\n",
      "\n",
      "Epoch 08873: loss did not improve\n",
      "Epoch 8874/10000\n",
      "2700/2700 [==============================] - 0s 30us/step - loss: 1.2202e-04 - mean_absolute_error: 0.0066\n",
      "\n",
      "Epoch 08874: loss did not improve\n",
      "Epoch 8875/10000\n",
      "2700/2700 [==============================] - 0s 28us/step - loss: 1.2263e-04 - mean_absolute_error: 0.0066\n",
      "\n",
      "Epoch 08875: loss did not improve\n",
      "Epoch 8876/10000\n",
      "2700/2700 [==============================] - 0s 29us/step - loss: 1.2673e-04 - mean_absolute_error: 0.0068\n",
      "\n",
      "Epoch 08876: loss did not improve\n",
      "Epoch 8877/10000\n",
      "2700/2700 [==============================] - 0s 26us/step - loss: 1.2154e-04 - mean_absolute_error: 0.0063\n",
      "\n",
      "Epoch 08877: loss did not improve\n",
      "Epoch 8878/10000\n",
      "2700/2700 [==============================] - 0s 29us/step - loss: 1.2064e-04 - mean_absolute_error: 0.0063\n",
      "\n",
      "Epoch 08878: loss did not improve\n",
      "Epoch 8879/10000\n",
      "2700/2700 [==============================] - 0s 29us/step - loss: 1.2510e-04 - mean_absolute_error: 0.0067\n",
      "\n",
      "Epoch 08879: loss did not improve\n",
      "Epoch 8880/10000\n",
      "2700/2700 [==============================] - 0s 30us/step - loss: 1.2297e-04 - mean_absolute_error: 0.0065\n",
      "\n",
      "Epoch 08880: loss did not improve\n",
      "Epoch 8881/10000\n",
      "2700/2700 [==============================] - 0s 32us/step - loss: 1.2077e-04 - mean_absolute_error: 0.0064\n",
      "\n",
      "Epoch 08881: loss did not improve\n",
      "Epoch 8882/10000\n",
      "2700/2700 [==============================] - 0s 27us/step - loss: 1.2289e-04 - mean_absolute_error: 0.0065\n",
      "\n",
      "Epoch 08882: loss did not improve\n",
      "Epoch 8883/10000\n",
      "2700/2700 [==============================] - 0s 29us/step - loss: 1.2325e-04 - mean_absolute_error: 0.0066\n",
      "\n",
      "Epoch 08883: loss did not improve\n",
      "Epoch 8884/10000\n",
      "2700/2700 [==============================] - 0s 27us/step - loss: 1.2125e-04 - mean_absolute_error: 0.0065\n",
      "\n",
      "Epoch 08884: loss did not improve\n",
      "Epoch 8885/10000\n",
      "2700/2700 [==============================] - 0s 30us/step - loss: 1.2185e-04 - mean_absolute_error: 0.0065\n",
      "\n",
      "Epoch 08885: loss did not improve\n",
      "Epoch 8886/10000\n",
      "2700/2700 [==============================] - 0s 29us/step - loss: 1.2119e-04 - mean_absolute_error: 0.0065\n",
      "\n",
      "Epoch 08886: loss did not improve\n",
      "Epoch 8887/10000\n",
      "2700/2700 [==============================] - 0s 27us/step - loss: 1.2135e-04 - mean_absolute_error: 0.0064\n",
      "\n",
      "Epoch 08887: loss did not improve\n",
      "Epoch 8888/10000\n",
      "2700/2700 [==============================] - 0s 29us/step - loss: 1.2194e-04 - mean_absolute_error: 0.0065\n",
      "\n",
      "Epoch 08888: loss did not improve\n",
      "Epoch 8889/10000\n",
      "2700/2700 [==============================] - 0s 32us/step - loss: 1.2351e-04 - mean_absolute_error: 0.0066\n",
      "\n",
      "Epoch 08889: loss did not improve\n",
      "Epoch 8890/10000\n",
      "2700/2700 [==============================] - 0s 29us/step - loss: 1.2325e-04 - mean_absolute_error: 0.0065\n",
      "\n",
      "Epoch 08890: loss did not improve\n",
      "Epoch 8891/10000\n",
      "2700/2700 [==============================] - 0s 31us/step - loss: 1.1989e-04 - mean_absolute_error: 0.0063\n",
      "\n",
      "Epoch 08891: loss did not improve\n",
      "Epoch 8892/10000\n",
      "2700/2700 [==============================] - 0s 28us/step - loss: 1.1960e-04 - mean_absolute_error: 0.0062\n",
      "\n",
      "Epoch 08892: loss did not improve\n",
      "Epoch 8893/10000\n",
      "2700/2700 [==============================] - 0s 29us/step - loss: 1.2227e-04 - mean_absolute_error: 0.0065\n",
      "\n",
      "Epoch 08893: loss did not improve\n",
      "Epoch 8894/10000\n",
      "2700/2700 [==============================] - 0s 27us/step - loss: 1.2149e-04 - mean_absolute_error: 0.0064\n",
      "\n",
      "Epoch 08894: loss did not improve\n",
      "Epoch 8895/10000\n",
      "2700/2700 [==============================] - 0s 30us/step - loss: 1.1975e-04 - mean_absolute_error: 0.0063\n",
      "\n",
      "Epoch 08895: loss did not improve\n",
      "Epoch 8896/10000\n",
      "2700/2700 [==============================] - 0s 29us/step - loss: 1.2105e-04 - mean_absolute_error: 0.0064\n",
      "\n",
      "Epoch 08896: loss did not improve\n",
      "Epoch 8897/10000\n",
      "2700/2700 [==============================] - 0s 32us/step - loss: 1.2214e-04 - mean_absolute_error: 0.0065\n",
      "\n",
      "Epoch 08897: loss did not improve\n",
      "Epoch 8898/10000\n",
      "2700/2700 [==============================] - 0s 37us/step - loss: 1.2423e-04 - mean_absolute_error: 0.0067\n",
      "\n",
      "Epoch 08898: loss did not improve\n",
      "Epoch 8899/10000\n",
      "2700/2700 [==============================] - 0s 37us/step - loss: 1.1970e-04 - mean_absolute_error: 0.0063\n",
      "\n",
      "Epoch 08899: loss did not improve\n",
      "Epoch 8900/10000\n",
      "2700/2700 [==============================] - 0s 36us/step - loss: 1.2057e-04 - mean_absolute_error: 0.0062\n",
      "\n",
      "Epoch 08900: loss did not improve\n",
      "Epoch 8901/10000\n",
      "2700/2700 [==============================] - 0s 32us/step - loss: 1.2097e-04 - mean_absolute_error: 0.0065\n",
      "\n",
      "Epoch 08901: loss did not improve\n",
      "Epoch 8902/10000\n",
      "2700/2700 [==============================] - 0s 37us/step - loss: 1.2365e-04 - mean_absolute_error: 0.0066\n",
      "\n",
      "Epoch 08902: loss did not improve\n",
      "Epoch 8903/10000\n",
      "2700/2700 [==============================] - 0s 37us/step - loss: 1.2363e-04 - mean_absolute_error: 0.0066\n",
      "\n",
      "Epoch 08903: loss did not improve\n",
      "Epoch 8904/10000\n",
      "2700/2700 [==============================] - 0s 37us/step - loss: 1.2235e-04 - mean_absolute_error: 0.0066\n",
      "\n",
      "Epoch 08904: loss did not improve\n",
      "Epoch 8905/10000\n",
      "2700/2700 [==============================] - 0s 31us/step - loss: 1.2084e-04 - mean_absolute_error: 0.0064\n",
      "\n",
      "Epoch 08905: loss did not improve\n",
      "Epoch 8906/10000\n",
      "2700/2700 [==============================] - 0s 28us/step - loss: 1.2458e-04 - mean_absolute_error: 0.0067\n",
      "\n",
      "Epoch 08906: loss did not improve\n",
      "Epoch 8907/10000\n",
      "2700/2700 [==============================] - 0s 29us/step - loss: 1.2125e-04 - mean_absolute_error: 0.0064\n",
      "\n",
      "Epoch 08907: loss did not improve\n",
      "Epoch 8908/10000\n",
      "2700/2700 [==============================] - 0s 27us/step - loss: 1.2413e-04 - mean_absolute_error: 0.0067\n",
      "\n",
      "Epoch 08908: loss did not improve\n",
      "Epoch 8909/10000\n",
      "2700/2700 [==============================] - 0s 31us/step - loss: 1.2414e-04 - mean_absolute_error: 0.0065\n",
      "\n",
      "Epoch 08909: loss did not improve\n",
      "Epoch 8910/10000\n",
      "2700/2700 [==============================] - 0s 35us/step - loss: 1.2523e-04 - mean_absolute_error: 0.0066\n",
      "\n",
      "Epoch 08910: loss did not improve\n",
      "Epoch 8911/10000\n",
      "2700/2700 [==============================] - 0s 36us/step - loss: 1.2302e-04 - mean_absolute_error: 0.0066\n",
      "\n",
      "Epoch 08911: loss did not improve\n",
      "Epoch 8912/10000\n",
      "2700/2700 [==============================] - 0s 34us/step - loss: 1.2119e-04 - mean_absolute_error: 0.0064\n",
      "\n",
      "Epoch 08912: loss did not improve\n",
      "Epoch 8913/10000\n",
      "2700/2700 [==============================] - 0s 36us/step - loss: 1.1996e-04 - mean_absolute_error: 0.0063\n",
      "\n",
      "Epoch 08913: loss did not improve\n",
      "Epoch 8914/10000\n",
      "2700/2700 [==============================] - 0s 31us/step - loss: 1.2054e-04 - mean_absolute_error: 0.0063\n",
      "\n",
      "Epoch 08914: loss did not improve\n",
      "Epoch 8915/10000\n",
      "2700/2700 [==============================] - 0s 31us/step - loss: 1.2153e-04 - mean_absolute_error: 0.0065\n",
      "\n",
      "Epoch 08915: loss did not improve\n",
      "Epoch 8916/10000\n",
      "2700/2700 [==============================] - 0s 28us/step - loss: 1.2024e-04 - mean_absolute_error: 0.0063\n",
      "\n",
      "Epoch 08916: loss did not improve\n",
      "Epoch 8917/10000\n",
      "2700/2700 [==============================] - 0s 28us/step - loss: 1.2214e-04 - mean_absolute_error: 0.0065\n",
      "\n",
      "Epoch 08917: loss did not improve\n",
      "Epoch 8918/10000\n",
      "2700/2700 [==============================] - 0s 27us/step - loss: 1.2060e-04 - mean_absolute_error: 0.0063\n",
      "\n",
      "Epoch 08918: loss did not improve\n",
      "Epoch 8919/10000\n",
      "2700/2700 [==============================] - 0s 28us/step - loss: 1.2573e-04 - mean_absolute_error: 0.0069\n",
      "\n",
      "Epoch 08919: loss did not improve\n",
      "Epoch 8920/10000\n",
      "2700/2700 [==============================] - 0s 27us/step - loss: 1.2175e-04 - mean_absolute_error: 0.0064\n",
      "\n",
      "Epoch 08920: loss did not improve\n",
      "Epoch 8921/10000\n",
      "2700/2700 [==============================] - 0s 29us/step - loss: 1.2055e-04 - mean_absolute_error: 0.0063\n",
      "\n",
      "Epoch 08921: loss did not improve\n",
      "Epoch 8922/10000\n",
      "2700/2700 [==============================] - 0s 29us/step - loss: 1.2069e-04 - mean_absolute_error: 0.0064\n",
      "\n",
      "Epoch 08922: loss did not improve\n",
      "Epoch 8923/10000\n",
      "2700/2700 [==============================] - 0s 29us/step - loss: 1.2054e-04 - mean_absolute_error: 0.0064\n",
      "\n",
      "Epoch 08923: loss did not improve\n",
      "Epoch 8924/10000\n",
      "2700/2700 [==============================] - 0s 26us/step - loss: 1.2161e-04 - mean_absolute_error: 0.0064\n",
      "\n",
      "Epoch 08924: loss did not improve\n",
      "Epoch 8925/10000\n",
      "2700/2700 [==============================] - 0s 29us/step - loss: 1.2141e-04 - mean_absolute_error: 0.0063\n",
      "\n",
      "Epoch 08925: loss did not improve\n",
      "Epoch 8926/10000\n",
      "2700/2700 [==============================] - 0s 30us/step - loss: 1.2095e-04 - mean_absolute_error: 0.0064\n",
      "\n",
      "Epoch 08926: loss did not improve\n",
      "Epoch 8927/10000\n",
      "2700/2700 [==============================] - 0s 30us/step - loss: 1.2171e-04 - mean_absolute_error: 0.0065\n",
      "\n",
      "Epoch 08927: loss did not improve\n",
      "Epoch 8928/10000\n",
      "2700/2700 [==============================] - 0s 29us/step - loss: 1.2202e-04 - mean_absolute_error: 0.0064\n",
      "\n",
      "Epoch 08928: loss did not improve\n",
      "Epoch 8929/10000\n",
      "2700/2700 [==============================] - 0s 27us/step - loss: 1.2190e-04 - mean_absolute_error: 0.0065\n",
      "\n",
      "Epoch 08929: loss did not improve\n",
      "Epoch 8930/10000\n",
      "2700/2700 [==============================] - 0s 28us/step - loss: 1.2168e-04 - mean_absolute_error: 0.0064\n",
      "\n",
      "Epoch 08930: loss did not improve\n",
      "Epoch 8931/10000\n",
      "2700/2700 [==============================] - 0s 27us/step - loss: 1.2159e-04 - mean_absolute_error: 0.0064\n",
      "\n",
      "Epoch 08931: loss did not improve\n",
      "Epoch 8932/10000\n",
      "2700/2700 [==============================] - 0s 27us/step - loss: 1.1990e-04 - mean_absolute_error: 0.0063\n",
      "\n",
      "Epoch 08932: loss did not improve\n",
      "Epoch 8933/10000\n",
      "2700/2700 [==============================] - 0s 27us/step - loss: 1.2755e-04 - mean_absolute_error: 0.0069\n",
      "\n",
      "Epoch 08933: loss did not improve\n",
      "Epoch 8934/10000\n",
      "2700/2700 [==============================] - 0s 30us/step - loss: 1.2313e-04 - mean_absolute_error: 0.0065\n",
      "\n",
      "Epoch 08934: loss did not improve\n",
      "Epoch 8935/10000\n",
      "2700/2700 [==============================] - 0s 31us/step - loss: 1.2273e-04 - mean_absolute_error: 0.0065\n",
      "\n",
      "Epoch 08935: loss did not improve\n",
      "Epoch 8936/10000\n",
      "2700/2700 [==============================] - 0s 29us/step - loss: 1.2050e-04 - mean_absolute_error: 0.0063\n",
      "\n",
      "Epoch 08936: loss did not improve\n",
      "Epoch 8937/10000\n",
      "2700/2700 [==============================] - 0s 37us/step - loss: 1.2052e-04 - mean_absolute_error: 0.0063\n",
      "\n",
      "Epoch 08937: loss did not improve\n",
      "Epoch 8938/10000\n",
      "2700/2700 [==============================] - 0s 41us/step - loss: 1.2478e-04 - mean_absolute_error: 0.0068\n",
      "\n",
      "Epoch 08938: loss did not improve\n",
      "Epoch 8939/10000\n",
      "2700/2700 [==============================] - 0s 43us/step - loss: 1.2251e-04 - mean_absolute_error: 0.0065\n",
      "\n",
      "Epoch 08939: loss did not improve\n",
      "Epoch 8940/10000\n",
      "2700/2700 [==============================] - 0s 46us/step - loss: 1.2429e-04 - mean_absolute_error: 0.0066\n",
      "\n",
      "Epoch 08940: loss did not improve\n",
      "Epoch 8941/10000\n",
      "2700/2700 [==============================] - 0s 30us/step - loss: 1.2160e-04 - mean_absolute_error: 0.0064\n",
      "\n",
      "Epoch 08941: loss did not improve\n",
      "Epoch 8942/10000\n",
      "2700/2700 [==============================] - 0s 31us/step - loss: 1.2144e-04 - mean_absolute_error: 0.0064\n",
      "\n",
      "Epoch 08942: loss did not improve\n",
      "Epoch 8943/10000\n",
      "2700/2700 [==============================] - 0s 31us/step - loss: 1.2124e-04 - mean_absolute_error: 0.0065\n",
      "\n",
      "Epoch 08943: loss did not improve\n",
      "Epoch 8944/10000\n",
      "2700/2700 [==============================] - 0s 26us/step - loss: 1.2075e-04 - mean_absolute_error: 0.0063\n",
      "\n",
      "Epoch 08944: loss did not improve\n",
      "Epoch 8945/10000\n",
      "2700/2700 [==============================] - 0s 35us/step - loss: 1.2163e-04 - mean_absolute_error: 0.0065\n",
      "\n",
      "Epoch 08945: loss did not improve\n",
      "Epoch 8946/10000\n",
      "2700/2700 [==============================] - 0s 46us/step - loss: 1.2052e-04 - mean_absolute_error: 0.0063\n",
      "\n",
      "Epoch 08946: loss did not improve\n",
      "Epoch 8947/10000\n",
      "2700/2700 [==============================] - 0s 40us/step - loss: 1.1955e-04 - mean_absolute_error: 0.0063\n",
      "\n",
      "Epoch 08947: loss did not improve\n",
      "Epoch 8948/10000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2700/2700 [==============================] - 0s 28us/step - loss: 1.2297e-04 - mean_absolute_error: 0.0065\n",
      "\n",
      "Epoch 08948: loss did not improve\n",
      "Epoch 8949/10000\n",
      "2700/2700 [==============================] - 0s 31us/step - loss: 1.2029e-04 - mean_absolute_error: 0.0063\n",
      "\n",
      "Epoch 08949: loss did not improve\n",
      "Epoch 8950/10000\n",
      "2700/2700 [==============================] - 0s 30us/step - loss: 1.2227e-04 - mean_absolute_error: 0.0065\n",
      "\n",
      "Epoch 08950: loss did not improve\n",
      "Epoch 8951/10000\n",
      "2700/2700 [==============================] - 0s 31us/step - loss: 1.2079e-04 - mean_absolute_error: 0.0064\n",
      "\n",
      "Epoch 08951: loss did not improve\n",
      "Epoch 8952/10000\n",
      "2700/2700 [==============================] - 0s 29us/step - loss: 1.2143e-04 - mean_absolute_error: 0.0064\n",
      "\n",
      "Epoch 08952: loss did not improve\n",
      "Epoch 8953/10000\n",
      "2700/2700 [==============================] - 0s 28us/step - loss: 1.2066e-04 - mean_absolute_error: 0.0063\n",
      "\n",
      "Epoch 08953: loss did not improve\n",
      "Epoch 8954/10000\n",
      "2700/2700 [==============================] - 0s 27us/step - loss: 1.2182e-04 - mean_absolute_error: 0.0064\n",
      "\n",
      "Epoch 08954: loss did not improve\n",
      "Epoch 8955/10000\n",
      "2700/2700 [==============================] - 0s 29us/step - loss: 1.1959e-04 - mean_absolute_error: 0.0062\n",
      "\n",
      "Epoch 08955: loss did not improve\n",
      "Epoch 8956/10000\n",
      "2700/2700 [==============================] - 0s 44us/step - loss: 1.2021e-04 - mean_absolute_error: 0.0063\n",
      "\n",
      "Epoch 08956: loss did not improve\n",
      "Epoch 8957/10000\n",
      "2700/2700 [==============================] - 0s 52us/step - loss: 1.2104e-04 - mean_absolute_error: 0.0064\n",
      "\n",
      "Epoch 08957: loss did not improve\n",
      "Epoch 8958/10000\n",
      "2700/2700 [==============================] - 0s 35us/step - loss: 1.2186e-04 - mean_absolute_error: 0.0066\n",
      "\n",
      "Epoch 08958: loss did not improve\n",
      "Epoch 8959/10000\n",
      "2700/2700 [==============================] - 0s 27us/step - loss: 1.2234e-04 - mean_absolute_error: 0.0065\n",
      "\n",
      "Epoch 08959: loss did not improve\n",
      "Epoch 8960/10000\n",
      "2700/2700 [==============================] - 0s 30us/step - loss: 1.2136e-04 - mean_absolute_error: 0.0063\n",
      "\n",
      "Epoch 08960: loss did not improve\n",
      "Epoch 8961/10000\n",
      "2700/2700 [==============================] - 0s 31us/step - loss: 1.2577e-04 - mean_absolute_error: 0.0067\n",
      "\n",
      "Epoch 08961: loss did not improve\n",
      "Epoch 8962/10000\n",
      "2700/2700 [==============================] - 0s 30us/step - loss: 1.2345e-04 - mean_absolute_error: 0.0065\n",
      "\n",
      "Epoch 08962: loss did not improve\n",
      "Epoch 8963/10000\n",
      "2700/2700 [==============================] - 0s 30us/step - loss: 1.2198e-04 - mean_absolute_error: 0.0066\n",
      "\n",
      "Epoch 08963: loss did not improve\n",
      "Epoch 8964/10000\n",
      "2700/2700 [==============================] - 0s 29us/step - loss: 1.2391e-04 - mean_absolute_error: 0.0066\n",
      "\n",
      "Epoch 08964: loss did not improve\n",
      "Epoch 8965/10000\n",
      "2700/2700 [==============================] - 0s 28us/step - loss: 1.2113e-04 - mean_absolute_error: 0.0063\n",
      "\n",
      "Epoch 08965: loss did not improve\n",
      "Epoch 8966/10000\n",
      "2700/2700 [==============================] - 0s 29us/step - loss: 1.2269e-04 - mean_absolute_error: 0.0065\n",
      "\n",
      "Epoch 08966: loss did not improve\n",
      "Epoch 8967/10000\n",
      "2700/2700 [==============================] - 0s 29us/step - loss: 1.2134e-04 - mean_absolute_error: 0.0064\n",
      "\n",
      "Epoch 08967: loss did not improve\n",
      "Epoch 8968/10000\n",
      "2700/2700 [==============================] - 0s 30us/step - loss: 1.2175e-04 - mean_absolute_error: 0.0064\n",
      "\n",
      "Epoch 08968: loss did not improve\n",
      "Epoch 8969/10000\n",
      "2700/2700 [==============================] - 0s 29us/step - loss: 1.1988e-04 - mean_absolute_error: 0.0063\n",
      "\n",
      "Epoch 08969: loss did not improve\n",
      "Epoch 8970/10000\n",
      "2700/2700 [==============================] - 0s 29us/step - loss: 1.1973e-04 - mean_absolute_error: 0.0062\n",
      "\n",
      "Epoch 08970: loss did not improve\n",
      "Epoch 8971/10000\n",
      "2700/2700 [==============================] - 0s 31us/step - loss: 1.2338e-04 - mean_absolute_error: 0.0067\n",
      "\n",
      "Epoch 08971: loss did not improve\n",
      "Epoch 8972/10000\n",
      "2700/2700 [==============================] - 0s 34us/step - loss: 1.2094e-04 - mean_absolute_error: 0.0064\n",
      "\n",
      "Epoch 08972: loss did not improve\n",
      "Epoch 8973/10000\n",
      "2700/2700 [==============================] - 0s 30us/step - loss: 1.2344e-04 - mean_absolute_error: 0.0066\n",
      "\n",
      "Epoch 08973: loss did not improve\n",
      "Epoch 8974/10000\n",
      "2700/2700 [==============================] - 0s 30us/step - loss: 1.2125e-04 - mean_absolute_error: 0.0064\n",
      "\n",
      "Epoch 08974: loss did not improve\n",
      "Epoch 8975/10000\n",
      "2700/2700 [==============================] - 0s 27us/step - loss: 1.2148e-04 - mean_absolute_error: 0.0065\n",
      "\n",
      "Epoch 08975: loss did not improve\n",
      "Epoch 8976/10000\n",
      "2700/2700 [==============================] - 0s 28us/step - loss: 1.2154e-04 - mean_absolute_error: 0.0065\n",
      "\n",
      "Epoch 08976: loss did not improve\n",
      "Epoch 8977/10000\n",
      "2700/2700 [==============================] - 0s 31us/step - loss: 1.2104e-04 - mean_absolute_error: 0.0064\n",
      "\n",
      "Epoch 08977: loss did not improve\n",
      "Epoch 8978/10000\n",
      "2700/2700 [==============================] - 0s 27us/step - loss: 1.2354e-04 - mean_absolute_error: 0.0067\n",
      "\n",
      "Epoch 08978: loss did not improve\n",
      "Epoch 8979/10000\n",
      "2700/2700 [==============================] - 0s 26us/step - loss: 1.2153e-04 - mean_absolute_error: 0.0065\n",
      "\n",
      "Epoch 08979: loss did not improve\n",
      "Epoch 8980/10000\n",
      "2700/2700 [==============================] - 0s 31us/step - loss: 1.2161e-04 - mean_absolute_error: 0.0065\n",
      "\n",
      "Epoch 08980: loss did not improve\n",
      "Epoch 8981/10000\n",
      "2700/2700 [==============================] - 0s 28us/step - loss: 1.2319e-04 - mean_absolute_error: 0.0066\n",
      "\n",
      "Epoch 08981: loss did not improve\n",
      "Epoch 8982/10000\n",
      "2700/2700 [==============================] - 0s 29us/step - loss: 1.2082e-04 - mean_absolute_error: 0.0064\n",
      "\n",
      "Epoch 08982: loss did not improve\n",
      "Epoch 8983/10000\n",
      "2700/2700 [==============================] - 0s 32us/step - loss: 1.2353e-04 - mean_absolute_error: 0.0065\n",
      "\n",
      "Epoch 08983: loss did not improve\n",
      "Epoch 8984/10000\n",
      "2700/2700 [==============================] - 0s 30us/step - loss: 1.2284e-04 - mean_absolute_error: 0.0067\n",
      "\n",
      "Epoch 08984: loss did not improve\n",
      "Epoch 8985/10000\n",
      "2700/2700 [==============================] - 0s 31us/step - loss: 1.2135e-04 - mean_absolute_error: 0.0064\n",
      "\n",
      "Epoch 08985: loss did not improve\n",
      "Epoch 8986/10000\n",
      "2700/2700 [==============================] - 0s 30us/step - loss: 1.2123e-04 - mean_absolute_error: 0.0064\n",
      "\n",
      "Epoch 08986: loss did not improve\n",
      "Epoch 8987/10000\n",
      "2700/2700 [==============================] - 0s 30us/step - loss: 1.2178e-04 - mean_absolute_error: 0.0065\n",
      "\n",
      "Epoch 08987: loss did not improve\n",
      "Epoch 8988/10000\n",
      "2700/2700 [==============================] - 0s 32us/step - loss: 1.1944e-04 - mean_absolute_error: 0.0063\n",
      "\n",
      "Epoch 08988: loss did not improve\n",
      "Epoch 8989/10000\n",
      "2700/2700 [==============================] - 0s 30us/step - loss: 1.2273e-04 - mean_absolute_error: 0.0065\n",
      "\n",
      "Epoch 08989: loss did not improve\n",
      "Epoch 8990/10000\n",
      "2700/2700 [==============================] - 0s 31us/step - loss: 1.2336e-04 - mean_absolute_error: 0.0065\n",
      "\n",
      "Epoch 08990: loss did not improve\n",
      "Epoch 8991/10000\n",
      "2700/2700 [==============================] - 0s 30us/step - loss: 1.2010e-04 - mean_absolute_error: 0.0063\n",
      "\n",
      "Epoch 08991: loss did not improve\n",
      "Epoch 8992/10000\n",
      "2700/2700 [==============================] - 0s 31us/step - loss: 1.2301e-04 - mean_absolute_error: 0.0065\n",
      "\n",
      "Epoch 08992: loss did not improve\n",
      "Epoch 8993/10000\n",
      "2700/2700 [==============================] - 0s 32us/step - loss: 1.2301e-04 - mean_absolute_error: 0.0066\n",
      "\n",
      "Epoch 08993: loss did not improve\n",
      "Epoch 8994/10000\n",
      "2700/2700 [==============================] - 0s 30us/step - loss: 1.2090e-04 - mean_absolute_error: 0.0065\n",
      "\n",
      "Epoch 08994: loss did not improve\n",
      "Epoch 8995/10000\n",
      "2700/2700 [==============================] - ETA: 0s - loss: 1.2880e-04 - mean_absolute_error: 0.006 - 0s 31us/step - loss: 1.2205e-04 - mean_absolute_error: 0.0064\n",
      "\n",
      "Epoch 08995: loss did not improve\n",
      "Epoch 8996/10000\n",
      "2700/2700 [==============================] - 0s 27us/step - loss: 1.2344e-04 - mean_absolute_error: 0.0066\n",
      "\n",
      "Epoch 08996: loss did not improve\n",
      "Epoch 8997/10000\n",
      "2700/2700 [==============================] - 0s 30us/step - loss: 1.2167e-04 - mean_absolute_error: 0.0064\n",
      "\n",
      "Epoch 08997: loss did not improve\n",
      "Epoch 8998/10000\n",
      "2700/2700 [==============================] - 0s 30us/step - loss: 1.2192e-04 - mean_absolute_error: 0.0065\n",
      "\n",
      "Epoch 08998: loss did not improve\n",
      "Epoch 8999/10000\n",
      "2700/2700 [==============================] - 0s 31us/step - loss: 1.2118e-04 - mean_absolute_error: 0.0065\n",
      "\n",
      "Epoch 08999: loss did not improve\n",
      "Epoch 9000/10000\n",
      "2700/2700 [==============================] - 0s 30us/step - loss: 1.2290e-04 - mean_absolute_error: 0.0065\n",
      "\n",
      "Epoch 09000: loss did not improve\n",
      "Epoch 9001/10000\n",
      "2700/2700 [==============================] - 0s 28us/step - loss: 1.2034e-04 - mean_absolute_error: 0.0063\n",
      "\n",
      "Epoch 09001: loss did not improve\n",
      "Epoch 9002/10000\n",
      "2700/2700 [==============================] - 0s 29us/step - loss: 1.2008e-04 - mean_absolute_error: 0.0063\n",
      "\n",
      "Epoch 09002: loss did not improve\n",
      "Epoch 9003/10000\n",
      "2700/2700 [==============================] - 0s 30us/step - loss: 1.1947e-04 - mean_absolute_error: 0.0062\n",
      "\n",
      "Epoch 09003: loss did not improve\n",
      "Epoch 9004/10000\n",
      "2700/2700 [==============================] - 0s 31us/step - loss: 1.2309e-04 - mean_absolute_error: 0.0066\n",
      "\n",
      "Epoch 09004: loss did not improve\n",
      "Epoch 9005/10000\n",
      "2700/2700 [==============================] - 0s 30us/step - loss: 1.2300e-04 - mean_absolute_error: 0.0065\n",
      "\n",
      "Epoch 09005: loss did not improve\n",
      "Epoch 9006/10000\n",
      "2700/2700 [==============================] - 0s 30us/step - loss: 1.2052e-04 - mean_absolute_error: 0.0064\n",
      "\n",
      "Epoch 09006: loss did not improve\n",
      "Epoch 9007/10000\n",
      "2700/2700 [==============================] - 0s 29us/step - loss: 1.2285e-04 - mean_absolute_error: 0.0066\n",
      "\n",
      "Epoch 09007: loss did not improve\n",
      "Epoch 9008/10000\n",
      "2700/2700 [==============================] - 0s 28us/step - loss: 1.2156e-04 - mean_absolute_error: 0.0064\n",
      "\n",
      "Epoch 09008: loss did not improve\n",
      "Epoch 9009/10000\n",
      "2700/2700 [==============================] - 0s 30us/step - loss: 1.2077e-04 - mean_absolute_error: 0.0063\n",
      "\n",
      "Epoch 09009: loss did not improve\n",
      "Epoch 9010/10000\n",
      "2700/2700 [==============================] - 0s 31us/step - loss: 1.2381e-04 - mean_absolute_error: 0.0067\n",
      "\n",
      "Epoch 09010: loss did not improve\n",
      "Epoch 9011/10000\n",
      "2700/2700 [==============================] - 0s 28us/step - loss: 1.2274e-04 - mean_absolute_error: 0.0065\n",
      "\n",
      "Epoch 09011: loss did not improve\n",
      "Epoch 9012/10000\n",
      "2700/2700 [==============================] - 0s 31us/step - loss: 1.2068e-04 - mean_absolute_error: 0.0063\n",
      "\n",
      "Epoch 09012: loss did not improve\n",
      "Epoch 9013/10000\n",
      "2700/2700 [==============================] - 0s 30us/step - loss: 1.2462e-04 - mean_absolute_error: 0.0067\n",
      "\n",
      "Epoch 09013: loss did not improve\n",
      "Epoch 9014/10000\n",
      "2700/2700 [==============================] - 0s 29us/step - loss: 1.2294e-04 - mean_absolute_error: 0.0066\n",
      "\n",
      "Epoch 09014: loss did not improve\n",
      "Epoch 9015/10000\n",
      "2700/2700 [==============================] - 0s 27us/step - loss: 1.2192e-04 - mean_absolute_error: 0.0064\n",
      "\n",
      "Epoch 09015: loss did not improve\n",
      "Epoch 9016/10000\n",
      "2700/2700 [==============================] - 0s 44us/step - loss: 1.2292e-04 - mean_absolute_error: 0.0066\n",
      "\n",
      "Epoch 09016: loss did not improve\n",
      "Epoch 9017/10000\n",
      "2700/2700 [==============================] - 0s 27us/step - loss: 1.2146e-04 - mean_absolute_error: 0.0064\n",
      "\n",
      "Epoch 09017: loss did not improve\n",
      "Epoch 9018/10000\n",
      "2700/2700 [==============================] - 0s 43us/step - loss: 1.2052e-04 - mean_absolute_error: 0.0063\n",
      "\n",
      "Epoch 09018: loss did not improve\n",
      "Epoch 9019/10000\n",
      "2700/2700 [==============================] - 0s 33us/step - loss: 1.2082e-04 - mean_absolute_error: 0.0063\n",
      "\n",
      "Epoch 09019: loss did not improve\n",
      "Epoch 9020/10000\n",
      "2700/2700 [==============================] - 0s 29us/step - loss: 1.2245e-04 - mean_absolute_error: 0.0065\n",
      "\n",
      "Epoch 09020: loss did not improve\n",
      "Epoch 9021/10000\n",
      "2700/2700 [==============================] - 0s 32us/step - loss: 1.2118e-04 - mean_absolute_error: 0.0065\n",
      "\n",
      "Epoch 09021: loss did not improve\n",
      "Epoch 9022/10000\n",
      "2700/2700 [==============================] - 0s 29us/step - loss: 1.2076e-04 - mean_absolute_error: 0.0064\n",
      "\n",
      "Epoch 09022: loss did not improve\n",
      "Epoch 9023/10000\n",
      "2700/2700 [==============================] - 0s 29us/step - loss: 1.2202e-04 - mean_absolute_error: 0.0064\n",
      "\n",
      "Epoch 09023: loss did not improve\n",
      "Epoch 9024/10000\n",
      "2700/2700 [==============================] - 0s 27us/step - loss: 1.2060e-04 - mean_absolute_error: 0.0064\n",
      "\n",
      "Epoch 09024: loss did not improve\n",
      "Epoch 9025/10000\n",
      "2700/2700 [==============================] - 0s 31us/step - loss: 1.2017e-04 - mean_absolute_error: 0.0063\n",
      "\n",
      "Epoch 09025: loss did not improve\n",
      "Epoch 9026/10000\n",
      "2700/2700 [==============================] - 0s 30us/step - loss: 1.2221e-04 - mean_absolute_error: 0.0065\n",
      "\n",
      "Epoch 09026: loss did not improve\n",
      "Epoch 9027/10000\n",
      "2700/2700 [==============================] - 0s 32us/step - loss: 1.2045e-04 - mean_absolute_error: 0.0063\n",
      "\n",
      "Epoch 09027: loss did not improve\n",
      "Epoch 9028/10000\n",
      "2700/2700 [==============================] - 0s 29us/step - loss: 1.2407e-04 - mean_absolute_error: 0.0066\n",
      "\n",
      "Epoch 09028: loss did not improve\n",
      "Epoch 9029/10000\n",
      "2700/2700 [==============================] - 0s 27us/step - loss: 1.1966e-04 - mean_absolute_error: 0.0063\n",
      "\n",
      "Epoch 09029: loss did not improve\n",
      "Epoch 9030/10000\n",
      "2700/2700 [==============================] - 0s 32us/step - loss: 1.2032e-04 - mean_absolute_error: 0.0064\n",
      "\n",
      "Epoch 09030: loss did not improve\n",
      "Epoch 9031/10000\n",
      "2700/2700 [==============================] - 0s 30us/step - loss: 1.2168e-04 - mean_absolute_error: 0.0064\n",
      "\n",
      "Epoch 09031: loss did not improve\n",
      "Epoch 9032/10000\n",
      "2700/2700 [==============================] - 0s 29us/step - loss: 1.2047e-04 - mean_absolute_error: 0.0064\n",
      "\n",
      "Epoch 09032: loss did not improve\n",
      "Epoch 9033/10000\n",
      "2700/2700 [==============================] - 0s 36us/step - loss: 1.2138e-04 - mean_absolute_error: 0.0064\n",
      "\n",
      "Epoch 09033: loss did not improve\n",
      "Epoch 9034/10000\n",
      "2700/2700 [==============================] - 0s 28us/step - loss: 1.2188e-04 - mean_absolute_error: 0.0065\n",
      "\n",
      "Epoch 09034: loss did not improve\n",
      "Epoch 9035/10000\n",
      "2700/2700 [==============================] - 0s 27us/step - loss: 1.2117e-04 - mean_absolute_error: 0.0064\n",
      "\n",
      "Epoch 09035: loss did not improve\n",
      "Epoch 9036/10000\n",
      "2700/2700 [==============================] - 0s 29us/step - loss: 1.2388e-04 - mean_absolute_error: 0.0068\n",
      "\n",
      "Epoch 09036: loss did not improve\n",
      "Epoch 9037/10000\n",
      "2700/2700 [==============================] - 0s 32us/step - loss: 1.1924e-04 - mean_absolute_error: 0.0063\n",
      "\n",
      "Epoch 09037: loss did not improve\n",
      "Epoch 9038/10000\n",
      "2700/2700 [==============================] - 0s 29us/step - loss: 1.2115e-04 - mean_absolute_error: 0.0065\n",
      "\n",
      "Epoch 09038: loss did not improve\n",
      "Epoch 9039/10000\n",
      "2700/2700 [==============================] - 0s 31us/step - loss: 1.2250e-04 - mean_absolute_error: 0.0064\n",
      "\n",
      "Epoch 09039: loss did not improve\n",
      "Epoch 9040/10000\n",
      "2700/2700 [==============================] - 0s 32us/step - loss: 1.2242e-04 - mean_absolute_error: 0.0065\n",
      "\n",
      "Epoch 09040: loss did not improve\n",
      "Epoch 9041/10000\n",
      "2700/2700 [==============================] - 0s 28us/step - loss: 1.2058e-04 - mean_absolute_error: 0.0065\n",
      "\n",
      "Epoch 09041: loss did not improve\n",
      "Epoch 9042/10000\n",
      "2700/2700 [==============================] - 0s 30us/step - loss: 1.2149e-04 - mean_absolute_error: 0.0064\n",
      "\n",
      "Epoch 09042: loss did not improve\n",
      "Epoch 9043/10000\n",
      "2700/2700 [==============================] - 0s 29us/step - loss: 1.2308e-04 - mean_absolute_error: 0.0066\n",
      "\n",
      "Epoch 09043: loss did not improve\n",
      "Epoch 9044/10000\n",
      "2700/2700 [==============================] - 0s 31us/step - loss: 1.2236e-04 - mean_absolute_error: 0.0065\n",
      "\n",
      "Epoch 09044: loss did not improve\n",
      "Epoch 9045/10000\n",
      "2700/2700 [==============================] - 0s 31us/step - loss: 1.2125e-04 - mean_absolute_error: 0.0063\n",
      "\n",
      "Epoch 09045: loss did not improve\n",
      "Epoch 9046/10000\n",
      "2700/2700 [==============================] - 0s 31us/step - loss: 1.2468e-04 - mean_absolute_error: 0.0067\n",
      "\n",
      "Epoch 09046: loss did not improve\n",
      "Epoch 9047/10000\n",
      "2700/2700 [==============================] - 0s 32us/step - loss: 1.2065e-04 - mean_absolute_error: 0.0065\n",
      "\n",
      "Epoch 09047: loss did not improve\n",
      "Epoch 9048/10000\n",
      "2700/2700 [==============================] - 0s 32us/step - loss: 1.2517e-04 - mean_absolute_error: 0.0066\n",
      "\n",
      "Epoch 09048: loss did not improve\n",
      "Epoch 9049/10000\n",
      "2700/2700 [==============================] - 0s 30us/step - loss: 1.2149e-04 - mean_absolute_error: 0.0064\n",
      "\n",
      "Epoch 09049: loss did not improve\n",
      "Epoch 9050/10000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2700/2700 [==============================] - 0s 29us/step - loss: 1.2096e-04 - mean_absolute_error: 0.0065\n",
      "\n",
      "Epoch 09050: loss did not improve\n",
      "Epoch 9051/10000\n",
      "2700/2700 [==============================] - 0s 32us/step - loss: 1.2236e-04 - mean_absolute_error: 0.0065\n",
      "\n",
      "Epoch 09051: loss did not improve\n",
      "Epoch 9052/10000\n",
      "2700/2700 [==============================] - 0s 28us/step - loss: 1.2156e-04 - mean_absolute_error: 0.0063\n",
      "\n",
      "Epoch 09052: loss did not improve\n",
      "Epoch 9053/10000\n",
      "2700/2700 [==============================] - 0s 30us/step - loss: 1.1985e-04 - mean_absolute_error: 0.0064\n",
      "\n",
      "Epoch 09053: loss did not improve\n",
      "Epoch 9054/10000\n",
      "2700/2700 [==============================] - 0s 30us/step - loss: 1.2360e-04 - mean_absolute_error: 0.0067\n",
      "\n",
      "Epoch 09054: loss did not improve\n",
      "Epoch 9055/10000\n",
      "2700/2700 [==============================] - 0s 29us/step - loss: 1.2049e-04 - mean_absolute_error: 0.0064\n",
      "\n",
      "Epoch 09055: loss did not improve\n",
      "Epoch 9056/10000\n",
      "2700/2700 [==============================] - 0s 29us/step - loss: 1.2218e-04 - mean_absolute_error: 0.0065\n",
      "\n",
      "Epoch 09056: loss did not improve\n",
      "Epoch 9057/10000\n",
      "2700/2700 [==============================] - 0s 27us/step - loss: 1.2178e-04 - mean_absolute_error: 0.0066\n",
      "\n",
      "Epoch 09057: loss did not improve\n",
      "Epoch 9058/10000\n",
      "2700/2700 [==============================] - 0s 31us/step - loss: 1.2189e-04 - mean_absolute_error: 0.0065\n",
      "\n",
      "Epoch 09058: loss did not improve\n",
      "Epoch 9059/10000\n",
      "2700/2700 [==============================] - 0s 32us/step - loss: 1.2261e-04 - mean_absolute_error: 0.0065\n",
      "\n",
      "Epoch 09059: loss did not improve\n",
      "Epoch 9060/10000\n",
      "2700/2700 [==============================] - 0s 29us/step - loss: 1.2068e-04 - mean_absolute_error: 0.0064\n",
      "\n",
      "Epoch 09060: loss did not improve\n",
      "Epoch 9061/10000\n",
      "2700/2700 [==============================] - 0s 30us/step - loss: 1.2358e-04 - mean_absolute_error: 0.0066\n",
      "\n",
      "Epoch 09061: loss did not improve\n",
      "Epoch 9062/10000\n",
      "2700/2700 [==============================] - 0s 28us/step - loss: 1.2298e-04 - mean_absolute_error: 0.0066\n",
      "\n",
      "Epoch 09062: loss did not improve\n",
      "Epoch 9063/10000\n",
      "2700/2700 [==============================] - 0s 28us/step - loss: 1.2032e-04 - mean_absolute_error: 0.0063\n",
      "\n",
      "Epoch 09063: loss did not improve\n",
      "Epoch 9064/10000\n",
      "2700/2700 [==============================] - 0s 30us/step - loss: 1.2042e-04 - mean_absolute_error: 0.0063\n",
      "\n",
      "Epoch 09064: loss did not improve\n",
      "Epoch 9065/10000\n",
      "2700/2700 [==============================] - 0s 31us/step - loss: 1.2203e-04 - mean_absolute_error: 0.0065\n",
      "\n",
      "Epoch 09065: loss did not improve\n",
      "Epoch 9066/10000\n",
      "2700/2700 [==============================] - 0s 31us/step - loss: 1.2219e-04 - mean_absolute_error: 0.0065\n",
      "\n",
      "Epoch 09066: loss did not improve\n",
      "Epoch 9067/10000\n",
      "2700/2700 [==============================] - 0s 29us/step - loss: 1.2070e-04 - mean_absolute_error: 0.0064\n",
      "\n",
      "Epoch 09067: loss did not improve\n",
      "Epoch 9068/10000\n",
      "2700/2700 [==============================] - 0s 29us/step - loss: 1.2034e-04 - mean_absolute_error: 0.0064\n",
      "\n",
      "Epoch 09068: loss did not improve\n",
      "Epoch 9069/10000\n",
      "2700/2700 [==============================] - 0s 31us/step - loss: 1.2310e-04 - mean_absolute_error: 0.0067\n",
      "\n",
      "Epoch 09069: loss did not improve\n",
      "Epoch 9070/10000\n",
      "2700/2700 [==============================] - 0s 30us/step - loss: 1.2131e-04 - mean_absolute_error: 0.0064\n",
      "\n",
      "Epoch 09070: loss did not improve\n",
      "Epoch 9071/10000\n",
      "2700/2700 [==============================] - 0s 33us/step - loss: 1.2020e-04 - mean_absolute_error: 0.0064\n",
      "\n",
      "Epoch 09071: loss did not improve\n",
      "Epoch 9072/10000\n",
      "2700/2700 [==============================] - 0s 39us/step - loss: 1.2154e-04 - mean_absolute_error: 0.0065\n",
      "\n",
      "Epoch 09072: loss did not improve\n",
      "Epoch 9073/10000\n",
      "2700/2700 [==============================] - 0s 31us/step - loss: 1.2374e-04 - mean_absolute_error: 0.0067\n",
      "\n",
      "Epoch 09073: loss did not improve\n",
      "Epoch 9074/10000\n",
      "2700/2700 [==============================] - 0s 28us/step - loss: 1.2094e-04 - mean_absolute_error: 0.0064\n",
      "\n",
      "Epoch 09074: loss did not improve\n",
      "Epoch 9075/10000\n",
      "2700/2700 [==============================] - 0s 27us/step - loss: 1.2022e-04 - mean_absolute_error: 0.0064\n",
      "\n",
      "Epoch 09075: loss did not improve\n",
      "Epoch 9076/10000\n",
      "2700/2700 [==============================] - 0s 30us/step - loss: 1.2076e-04 - mean_absolute_error: 0.0064\n",
      "\n",
      "Epoch 09076: loss did not improve\n",
      "Epoch 9077/10000\n",
      "2700/2700 [==============================] - 0s 29us/step - loss: 1.2105e-04 - mean_absolute_error: 0.0064\n",
      "\n",
      "Epoch 09077: loss did not improve\n",
      "Epoch 9078/10000\n",
      "2700/2700 [==============================] - 0s 29us/step - loss: 1.2103e-04 - mean_absolute_error: 0.0064\n",
      "\n",
      "Epoch 09078: loss did not improve\n",
      "Epoch 9079/10000\n",
      "2700/2700 [==============================] - 0s 27us/step - loss: 1.2108e-04 - mean_absolute_error: 0.0064\n",
      "\n",
      "Epoch 09079: loss did not improve\n",
      "Epoch 9080/10000\n",
      "2700/2700 [==============================] - 0s 28us/step - loss: 1.2053e-04 - mean_absolute_error: 0.0064\n",
      "\n",
      "Epoch 09080: loss did not improve\n",
      "Epoch 9081/10000\n",
      "2700/2700 [==============================] - 0s 33us/step - loss: 1.2299e-04 - mean_absolute_error: 0.0066\n",
      "\n",
      "Epoch 09081: loss did not improve\n",
      "Epoch 9082/10000\n",
      "2700/2700 [==============================] - 0s 33us/step - loss: 1.2028e-04 - mean_absolute_error: 0.0063\n",
      "\n",
      "Epoch 09082: loss did not improve\n",
      "Epoch 9083/10000\n",
      "2700/2700 [==============================] - 0s 32us/step - loss: 1.2129e-04 - mean_absolute_error: 0.0064\n",
      "\n",
      "Epoch 09083: loss did not improve\n",
      "Epoch 9084/10000\n",
      "2700/2700 [==============================] - 0s 29us/step - loss: 1.2044e-04 - mean_absolute_error: 0.0064\n",
      "\n",
      "Epoch 09084: loss did not improve\n",
      "Epoch 9085/10000\n",
      "2700/2700 [==============================] - 0s 26us/step - loss: 1.2282e-04 - mean_absolute_error: 0.0067\n",
      "\n",
      "Epoch 09085: loss did not improve\n",
      "Epoch 9086/10000\n",
      "2700/2700 [==============================] - 0s 31us/step - loss: 1.2110e-04 - mean_absolute_error: 0.0064\n",
      "\n",
      "Epoch 09086: loss did not improve\n",
      "Epoch 9087/10000\n",
      "2700/2700 [==============================] - 0s 30us/step - loss: 1.1981e-04 - mean_absolute_error: 0.0063\n",
      "\n",
      "Epoch 09087: loss did not improve\n",
      "Epoch 9088/10000\n",
      "2700/2700 [==============================] - 0s 29us/step - loss: 1.2577e-04 - mean_absolute_error: 0.0068\n",
      "\n",
      "Epoch 09088: loss did not improve\n",
      "Epoch 9089/10000\n",
      "2700/2700 [==============================] - 0s 34us/step - loss: 1.2056e-04 - mean_absolute_error: 0.0063\n",
      "\n",
      "Epoch 09089: loss did not improve\n",
      "Epoch 9090/10000\n",
      "2700/2700 [==============================] - 0s 27us/step - loss: 1.2257e-04 - mean_absolute_error: 0.0066\n",
      "\n",
      "Epoch 09090: loss did not improve\n",
      "Epoch 9091/10000\n",
      "2700/2700 [==============================] - 0s 29us/step - loss: 1.2441e-04 - mean_absolute_error: 0.0066\n",
      "\n",
      "Epoch 09091: loss did not improve\n",
      "Epoch 9092/10000\n",
      "2700/2700 [==============================] - 0s 31us/step - loss: 1.2153e-04 - mean_absolute_error: 0.0065\n",
      "\n",
      "Epoch 09092: loss did not improve\n",
      "Epoch 9093/10000\n",
      "2700/2700 [==============================] - 0s 31us/step - loss: 1.2303e-04 - mean_absolute_error: 0.0066\n",
      "\n",
      "Epoch 09093: loss did not improve\n",
      "Epoch 9094/10000\n",
      "2700/2700 [==============================] - 0s 30us/step - loss: 1.2008e-04 - mean_absolute_error: 0.0063\n",
      "\n",
      "Epoch 09094: loss did not improve\n",
      "Epoch 9095/10000\n",
      "2700/2700 [==============================] - 0s 31us/step - loss: 1.2052e-04 - mean_absolute_error: 0.0064\n",
      "\n",
      "Epoch 09095: loss did not improve\n",
      "Epoch 9096/10000\n",
      "2700/2700 [==============================] - 0s 27us/step - loss: 1.2503e-04 - mean_absolute_error: 0.0067\n",
      "\n",
      "Epoch 09096: loss did not improve\n",
      "Epoch 9097/10000\n",
      "2700/2700 [==============================] - 0s 30us/step - loss: 1.2149e-04 - mean_absolute_error: 0.0065\n",
      "\n",
      "Epoch 09097: loss did not improve\n",
      "Epoch 9098/10000\n",
      "2700/2700 [==============================] - 0s 28us/step - loss: 1.2052e-04 - mean_absolute_error: 0.0063\n",
      "\n",
      "Epoch 09098: loss did not improve\n",
      "Epoch 9099/10000\n",
      "2700/2700 [==============================] - 0s 29us/step - loss: 1.2072e-04 - mean_absolute_error: 0.0065\n",
      "\n",
      "Epoch 09099: loss did not improve\n",
      "Epoch 9100/10000\n",
      "2700/2700 [==============================] - 0s 30us/step - loss: 1.2210e-04 - mean_absolute_error: 0.0065\n",
      "\n",
      "Epoch 09100: loss did not improve\n",
      "Epoch 9101/10000\n",
      "2700/2700 [==============================] - 0s 31us/step - loss: 1.1994e-04 - mean_absolute_error: 0.0063\n",
      "\n",
      "Epoch 09101: loss did not improve\n",
      "Epoch 9102/10000\n",
      "2700/2700 [==============================] - 0s 31us/step - loss: 1.2134e-04 - mean_absolute_error: 0.0064\n",
      "\n",
      "Epoch 09102: loss did not improve\n",
      "Epoch 9103/10000\n",
      "2700/2700 [==============================] - 0s 30us/step - loss: 1.1956e-04 - mean_absolute_error: 0.0064\n",
      "\n",
      "Epoch 09103: loss did not improve\n",
      "Epoch 9104/10000\n",
      "2700/2700 [==============================] - 0s 31us/step - loss: 1.2160e-04 - mean_absolute_error: 0.0064\n",
      "\n",
      "Epoch 09104: loss did not improve\n",
      "Epoch 9105/10000\n",
      "2700/2700 [==============================] - 0s 30us/step - loss: 1.2072e-04 - mean_absolute_error: 0.0064\n",
      "\n",
      "Epoch 09105: loss did not improve\n",
      "Epoch 9106/10000\n",
      "2700/2700 [==============================] - 0s 27us/step - loss: 1.2160e-04 - mean_absolute_error: 0.0065\n",
      "\n",
      "Epoch 09106: loss did not improve\n",
      "Epoch 9107/10000\n",
      "2700/2700 [==============================] - 0s 30us/step - loss: 1.2801e-04 - mean_absolute_error: 0.0069\n",
      "\n",
      "Epoch 09107: loss did not improve\n",
      "Epoch 9108/10000\n",
      "2700/2700 [==============================] - 0s 28us/step - loss: 1.2084e-04 - mean_absolute_error: 0.0064\n",
      "\n",
      "Epoch 09108: loss did not improve\n",
      "Epoch 9109/10000\n",
      "2700/2700 [==============================] - 0s 29us/step - loss: 1.2363e-04 - mean_absolute_error: 0.0066\n",
      "\n",
      "Epoch 09109: loss did not improve\n",
      "Epoch 9110/10000\n",
      "2700/2700 [==============================] - 0s 26us/step - loss: 1.2074e-04 - mean_absolute_error: 0.0064\n",
      "\n",
      "Epoch 09110: loss did not improve\n",
      "Epoch 9111/10000\n",
      "2700/2700 [==============================] - 0s 30us/step - loss: 1.2257e-04 - mean_absolute_error: 0.0064\n",
      "\n",
      "Epoch 09111: loss did not improve\n",
      "Epoch 9112/10000\n",
      "2700/2700 [==============================] - 0s 31us/step - loss: 1.2039e-04 - mean_absolute_error: 0.0063\n",
      "\n",
      "Epoch 09112: loss did not improve\n",
      "Epoch 9113/10000\n",
      "2700/2700 [==============================] - 0s 29us/step - loss: 1.2252e-04 - mean_absolute_error: 0.0066\n",
      "\n",
      "Epoch 09113: loss did not improve\n",
      "Epoch 9114/10000\n",
      "2700/2700 [==============================] - 0s 28us/step - loss: 1.2361e-04 - mean_absolute_error: 0.0067\n",
      "\n",
      "Epoch 09114: loss did not improve\n",
      "Epoch 9115/10000\n",
      "2700/2700 [==============================] - 0s 28us/step - loss: 1.2031e-04 - mean_absolute_error: 0.0063\n",
      "\n",
      "Epoch 09115: loss did not improve\n",
      "Epoch 9116/10000\n",
      "2700/2700 [==============================] - 0s 30us/step - loss: 1.2140e-04 - mean_absolute_error: 0.0064\n",
      "\n",
      "Epoch 09116: loss did not improve\n",
      "Epoch 9117/10000\n",
      "2700/2700 [==============================] - 0s 29us/step - loss: 1.2016e-04 - mean_absolute_error: 0.0062\n",
      "\n",
      "Epoch 09117: loss did not improve\n",
      "Epoch 9118/10000\n",
      "2700/2700 [==============================] - 0s 31us/step - loss: 1.2149e-04 - mean_absolute_error: 0.0065\n",
      "\n",
      "Epoch 09118: loss did not improve\n",
      "Epoch 9119/10000\n",
      "2700/2700 [==============================] - 0s 33us/step - loss: 1.2444e-04 - mean_absolute_error: 0.0067\n",
      "\n",
      "Epoch 09119: loss did not improve\n",
      "Epoch 9120/10000\n",
      "2700/2700 [==============================] - 0s 30us/step - loss: 1.2284e-04 - mean_absolute_error: 0.0067\n",
      "\n",
      "Epoch 09120: loss did not improve\n",
      "Epoch 9121/10000\n",
      "2700/2700 [==============================] - 0s 30us/step - loss: 1.2253e-04 - mean_absolute_error: 0.0066\n",
      "\n",
      "Epoch 09121: loss did not improve\n",
      "Epoch 9122/10000\n",
      "2700/2700 [==============================] - 0s 30us/step - loss: 1.2123e-04 - mean_absolute_error: 0.0065\n",
      "\n",
      "Epoch 09122: loss did not improve\n",
      "Epoch 9123/10000\n",
      "2700/2700 [==============================] - 0s 29us/step - loss: 1.2113e-04 - mean_absolute_error: 0.0064\n",
      "\n",
      "Epoch 09123: loss did not improve\n",
      "Epoch 9124/10000\n",
      "2700/2700 [==============================] - 0s 28us/step - loss: 1.2352e-04 - mean_absolute_error: 0.0066\n",
      "\n",
      "Epoch 09124: loss did not improve\n",
      "Epoch 9125/10000\n",
      "2700/2700 [==============================] - 0s 30us/step - loss: 1.2563e-04 - mean_absolute_error: 0.0068\n",
      "\n",
      "Epoch 09125: loss did not improve\n",
      "Epoch 9126/10000\n",
      "2700/2700 [==============================] - 0s 27us/step - loss: 1.2227e-04 - mean_absolute_error: 0.0065\n",
      "\n",
      "Epoch 09126: loss did not improve\n",
      "Epoch 9127/10000\n",
      "2700/2700 [==============================] - 0s 29us/step - loss: 1.2552e-04 - mean_absolute_error: 0.0067\n",
      "\n",
      "Epoch 09127: loss did not improve\n",
      "Epoch 9128/10000\n",
      "2700/2700 [==============================] - 0s 31us/step - loss: 1.2025e-04 - mean_absolute_error: 0.0064\n",
      "\n",
      "Epoch 09128: loss did not improve\n",
      "Epoch 9129/10000\n",
      "2700/2700 [==============================] - 0s 32us/step - loss: 1.2270e-04 - mean_absolute_error: 0.0065\n",
      "\n",
      "Epoch 09129: loss did not improve\n",
      "Epoch 9130/10000\n",
      "2700/2700 [==============================] - 0s 31us/step - loss: 1.2116e-04 - mean_absolute_error: 0.0065\n",
      "\n",
      "Epoch 09130: loss did not improve\n",
      "Epoch 9131/10000\n",
      "2700/2700 [==============================] - 0s 27us/step - loss: 1.2331e-04 - mean_absolute_error: 0.0065\n",
      "\n",
      "Epoch 09131: loss did not improve\n",
      "Epoch 9132/10000\n",
      "2700/2700 [==============================] - 0s 31us/step - loss: 1.1956e-04 - mean_absolute_error: 0.0063\n",
      "\n",
      "Epoch 09132: loss did not improve\n",
      "Epoch 9133/10000\n",
      "2700/2700 [==============================] - 0s 34us/step - loss: 1.2338e-04 - mean_absolute_error: 0.0065\n",
      "\n",
      "Epoch 09133: loss did not improve\n",
      "Epoch 9134/10000\n",
      "2700/2700 [==============================] - 0s 35us/step - loss: 1.2257e-04 - mean_absolute_error: 0.0066\n",
      "\n",
      "Epoch 09134: loss did not improve\n",
      "Epoch 9135/10000\n",
      "2700/2700 [==============================] - 0s 32us/step - loss: 1.2155e-04 - mean_absolute_error: 0.0066\n",
      "\n",
      "Epoch 09135: loss did not improve\n",
      "Epoch 9136/10000\n",
      "2700/2700 [==============================] - 0s 29us/step - loss: 1.2059e-04 - mean_absolute_error: 0.0064\n",
      "\n",
      "Epoch 09136: loss did not improve\n",
      "Epoch 9137/10000\n",
      "2700/2700 [==============================] - 0s 26us/step - loss: 1.2188e-04 - mean_absolute_error: 0.0064\n",
      "\n",
      "Epoch 09137: loss did not improve\n",
      "Epoch 9138/10000\n",
      "2700/2700 [==============================] - 0s 30us/step - loss: 1.2154e-04 - mean_absolute_error: 0.0064\n",
      "\n",
      "Epoch 09138: loss did not improve\n",
      "Epoch 9139/10000\n",
      "2700/2700 [==============================] - 0s 38us/step - loss: 1.1974e-04 - mean_absolute_error: 0.0063\n",
      "\n",
      "Epoch 09139: loss did not improve\n",
      "Epoch 9140/10000\n",
      "2700/2700 [==============================] - 0s 34us/step - loss: 1.2141e-04 - mean_absolute_error: 0.0065\n",
      "\n",
      "Epoch 09140: loss did not improve\n",
      "Epoch 9141/10000\n",
      "2700/2700 [==============================] - 0s 32us/step - loss: 1.2235e-04 - mean_absolute_error: 0.0065\n",
      "\n",
      "Epoch 09141: loss did not improve\n",
      "Epoch 9142/10000\n",
      "2700/2700 [==============================] - 0s 30us/step - loss: 1.2101e-04 - mean_absolute_error: 0.0065\n",
      "\n",
      "Epoch 09142: loss did not improve\n",
      "Epoch 9143/10000\n",
      "2700/2700 [==============================] - 0s 30us/step - loss: 1.2012e-04 - mean_absolute_error: 0.0063\n",
      "\n",
      "Epoch 09143: loss did not improve\n",
      "Epoch 9144/10000\n",
      "2700/2700 [==============================] - 0s 31us/step - loss: 1.2556e-04 - mean_absolute_error: 0.0068\n",
      "\n",
      "Epoch 09144: loss did not improve\n",
      "Epoch 9145/10000\n",
      "2700/2700 [==============================] - 0s 34us/step - loss: 1.1986e-04 - mean_absolute_error: 0.0063\n",
      "\n",
      "Epoch 09145: loss did not improve\n",
      "Epoch 9146/10000\n",
      "2700/2700 [==============================] - 0s 31us/step - loss: 1.2087e-04 - mean_absolute_error: 0.0064\n",
      "\n",
      "Epoch 09146: loss did not improve\n",
      "Epoch 9147/10000\n",
      "2700/2700 [==============================] - 0s 28us/step - loss: 1.2010e-04 - mean_absolute_error: 0.0063\n",
      "\n",
      "Epoch 09147: loss did not improve\n",
      "Epoch 9148/10000\n",
      "2700/2700 [==============================] - 0s 30us/step - loss: 1.1946e-04 - mean_absolute_error: 0.0063\n",
      "\n",
      "Epoch 09148: loss did not improve\n",
      "Epoch 9149/10000\n",
      "2700/2700 [==============================] - 0s 29us/step - loss: 1.2318e-04 - mean_absolute_error: 0.0065\n",
      "\n",
      "Epoch 09149: loss did not improve\n",
      "Epoch 9150/10000\n",
      "2700/2700 [==============================] - 0s 26us/step - loss: 1.2345e-04 - mean_absolute_error: 0.0067\n",
      "\n",
      "Epoch 09150: loss did not improve\n",
      "Epoch 9151/10000\n",
      "2700/2700 [==============================] - 0s 33us/step - loss: 1.2104e-04 - mean_absolute_error: 0.0064\n",
      "\n",
      "Epoch 09151: loss did not improve\n",
      "Epoch 9152/10000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2700/2700 [==============================] - 0s 30us/step - loss: 1.2285e-04 - mean_absolute_error: 0.0065\n",
      "\n",
      "Epoch 09152: loss did not improve\n",
      "Epoch 9153/10000\n",
      "2700/2700 [==============================] - 0s 28us/step - loss: 1.2250e-04 - mean_absolute_error: 0.0066\n",
      "\n",
      "Epoch 09153: loss did not improve\n",
      "Epoch 9154/10000\n",
      "2700/2700 [==============================] - 0s 30us/step - loss: 1.2177e-04 - mean_absolute_error: 0.0065\n",
      "\n",
      "Epoch 09154: loss did not improve\n",
      "Epoch 9155/10000\n",
      "2700/2700 [==============================] - 0s 26us/step - loss: 1.2407e-04 - mean_absolute_error: 0.0067\n",
      "\n",
      "Epoch 09155: loss did not improve\n",
      "Epoch 9156/10000\n",
      "2700/2700 [==============================] - 0s 30us/step - loss: 1.2288e-04 - mean_absolute_error: 0.0066\n",
      "\n",
      "Epoch 09156: loss did not improve\n",
      "Epoch 9157/10000\n",
      "2700/2700 [==============================] - 0s 28us/step - loss: 1.2228e-04 - mean_absolute_error: 0.0064\n",
      "\n",
      "Epoch 09157: loss did not improve\n",
      "Epoch 9158/10000\n",
      "2700/2700 [==============================] - 0s 27us/step - loss: 1.2073e-04 - mean_absolute_error: 0.0064\n",
      "\n",
      "Epoch 09158: loss did not improve\n",
      "Epoch 9159/10000\n",
      "2700/2700 [==============================] - 0s 32us/step - loss: 1.2152e-04 - mean_absolute_error: 0.0064\n",
      "\n",
      "Epoch 09159: loss did not improve\n",
      "Epoch 9160/10000\n",
      "2700/2700 [==============================] - 0s 28us/step - loss: 1.2033e-04 - mean_absolute_error: 0.0064\n",
      "\n",
      "Epoch 09160: loss did not improve\n",
      "Epoch 9161/10000\n",
      "2700/2700 [==============================] - 0s 30us/step - loss: 1.2196e-04 - mean_absolute_error: 0.0064\n",
      "\n",
      "Epoch 09161: loss did not improve\n",
      "Epoch 9162/10000\n",
      "2700/2700 [==============================] - 0s 30us/step - loss: 1.1977e-04 - mean_absolute_error: 0.0062\n",
      "\n",
      "Epoch 09162: loss did not improve\n",
      "Epoch 9163/10000\n",
      "2700/2700 [==============================] - 0s 28us/step - loss: 1.2116e-04 - mean_absolute_error: 0.0064\n",
      "\n",
      "Epoch 09163: loss did not improve\n",
      "Epoch 9164/10000\n",
      "2700/2700 [==============================] - 0s 32us/step - loss: 1.2074e-04 - mean_absolute_error: 0.0064\n",
      "\n",
      "Epoch 09164: loss did not improve\n",
      "Epoch 9165/10000\n",
      "2700/2700 [==============================] - 0s 29us/step - loss: 1.2065e-04 - mean_absolute_error: 0.0063\n",
      "\n",
      "Epoch 09165: loss did not improve\n",
      "Epoch 9166/10000\n",
      "2700/2700 [==============================] - 0s 30us/step - loss: 1.2433e-04 - mean_absolute_error: 0.0066\n",
      "\n",
      "Epoch 09166: loss did not improve\n",
      "Epoch 9167/10000\n",
      "2700/2700 [==============================] - 0s 30us/step - loss: 1.2583e-04 - mean_absolute_error: 0.0069\n",
      "\n",
      "Epoch 09167: loss did not improve\n",
      "Epoch 9168/10000\n",
      "2700/2700 [==============================] - 0s 30us/step - loss: 1.2388e-04 - mean_absolute_error: 0.0066\n",
      "\n",
      "Epoch 09168: loss did not improve\n",
      "Epoch 9169/10000\n",
      "2700/2700 [==============================] - 0s 30us/step - loss: 1.2277e-04 - mean_absolute_error: 0.0066\n",
      "\n",
      "Epoch 09169: loss did not improve\n",
      "Epoch 9170/10000\n",
      "2700/2700 [==============================] - 0s 29us/step - loss: 1.2248e-04 - mean_absolute_error: 0.0065\n",
      "\n",
      "Epoch 09170: loss did not improve\n",
      "Epoch 9171/10000\n",
      "2700/2700 [==============================] - 0s 31us/step - loss: 1.2136e-04 - mean_absolute_error: 0.0064\n",
      "\n",
      "Epoch 09171: loss did not improve\n",
      "Epoch 9172/10000\n",
      "2700/2700 [==============================] - 0s 29us/step - loss: 1.2097e-04 - mean_absolute_error: 0.0063\n",
      "\n",
      "Epoch 09172: loss did not improve\n",
      "Epoch 9173/10000\n",
      "2700/2700 [==============================] - 0s 30us/step - loss: 1.2229e-04 - mean_absolute_error: 0.0066\n",
      "\n",
      "Epoch 09173: loss did not improve\n",
      "Epoch 9174/10000\n",
      "2700/2700 [==============================] - 0s 29us/step - loss: 1.2059e-04 - mean_absolute_error: 0.0064\n",
      "\n",
      "Epoch 09174: loss did not improve\n",
      "Epoch 9175/10000\n",
      "2700/2700 [==============================] - 0s 28us/step - loss: 1.2122e-04 - mean_absolute_error: 0.0064\n",
      "\n",
      "Epoch 09175: loss did not improve\n",
      "Epoch 9176/10000\n",
      "2700/2700 [==============================] - 0s 28us/step - loss: 1.1989e-04 - mean_absolute_error: 0.0063\n",
      "\n",
      "Epoch 09176: loss did not improve\n",
      "Epoch 9177/10000\n",
      "2700/2700 [==============================] - 0s 33us/step - loss: 1.2214e-04 - mean_absolute_error: 0.0066\n",
      "\n",
      "Epoch 09177: loss did not improve\n",
      "Epoch 9178/10000\n",
      "2700/2700 [==============================] - 0s 36us/step - loss: 1.2239e-04 - mean_absolute_error: 0.0065\n",
      "\n",
      "Epoch 09178: loss did not improve\n",
      "Epoch 9179/10000\n",
      "2700/2700 [==============================] - 0s 41us/step - loss: 1.2098e-04 - mean_absolute_error: 0.0064\n",
      "\n",
      "Epoch 09179: loss did not improve\n",
      "Epoch 9180/10000\n",
      "2700/2700 [==============================] - 0s 32us/step - loss: 1.2174e-04 - mean_absolute_error: 0.0065\n",
      "\n",
      "Epoch 09180: loss did not improve\n",
      "Epoch 9181/10000\n",
      "2700/2700 [==============================] - 0s 29us/step - loss: 1.2350e-04 - mean_absolute_error: 0.0066\n",
      "\n",
      "Epoch 09181: loss did not improve\n",
      "Epoch 9182/10000\n",
      "2700/2700 [==============================] - 0s 29us/step - loss: 1.2286e-04 - mean_absolute_error: 0.0065\n",
      "\n",
      "Epoch 09182: loss did not improve\n",
      "Epoch 9183/10000\n",
      "2700/2700 [==============================] - 0s 28us/step - loss: 1.2057e-04 - mean_absolute_error: 0.0064\n",
      "\n",
      "Epoch 09183: loss did not improve\n",
      "Epoch 9184/10000\n",
      "2700/2700 [==============================] - 0s 37us/step - loss: 1.1982e-04 - mean_absolute_error: 0.0063\n",
      "\n",
      "Epoch 09184: loss did not improve\n",
      "Epoch 9185/10000\n",
      "2700/2700 [==============================] - 0s 30us/step - loss: 1.2887e-04 - mean_absolute_error: 0.0070\n",
      "\n",
      "Epoch 09185: loss did not improve\n",
      "Epoch 9186/10000\n",
      "2700/2700 [==============================] - 0s 32us/step - loss: 1.2116e-04 - mean_absolute_error: 0.0064\n",
      "\n",
      "Epoch 09186: loss did not improve\n",
      "Epoch 9187/10000\n",
      "2700/2700 [==============================] - 0s 28us/step - loss: 1.2226e-04 - mean_absolute_error: 0.0065\n",
      "\n",
      "Epoch 09187: loss did not improve\n",
      "Epoch 9188/10000\n",
      "2700/2700 [==============================] - 0s 29us/step - loss: 1.2016e-04 - mean_absolute_error: 0.0063\n",
      "\n",
      "Epoch 09188: loss did not improve\n",
      "Epoch 9189/10000\n",
      "2700/2700 [==============================] - 0s 28us/step - loss: 1.2249e-04 - mean_absolute_error: 0.0065\n",
      "\n",
      "Epoch 09189: loss did not improve\n",
      "Epoch 9190/10000\n",
      "2700/2700 [==============================] - 0s 29us/step - loss: 1.2130e-04 - mean_absolute_error: 0.0065\n",
      "\n",
      "Epoch 09190: loss did not improve\n",
      "Epoch 9191/10000\n",
      "2700/2700 [==============================] - 0s 31us/step - loss: 1.2126e-04 - mean_absolute_error: 0.0064\n",
      "\n",
      "Epoch 09191: loss did not improve\n",
      "Epoch 9192/10000\n",
      "2700/2700 [==============================] - 0s 30us/step - loss: 1.2590e-04 - mean_absolute_error: 0.0069\n",
      "\n",
      "Epoch 09192: loss did not improve\n",
      "Epoch 9193/10000\n",
      "2700/2700 [==============================] - 0s 28us/step - loss: 1.2162e-04 - mean_absolute_error: 0.0065\n",
      "\n",
      "Epoch 09193: loss did not improve\n",
      "Epoch 9194/10000\n",
      "2700/2700 [==============================] - 0s 28us/step - loss: 1.2128e-04 - mean_absolute_error: 0.0063\n",
      "\n",
      "Epoch 09194: loss did not improve\n",
      "Epoch 9195/10000\n",
      "2700/2700 [==============================] - 0s 31us/step - loss: 1.2455e-04 - mean_absolute_error: 0.0068\n",
      "\n",
      "Epoch 09195: loss did not improve\n",
      "Epoch 9196/10000\n",
      "2700/2700 [==============================] - 0s 29us/step - loss: 1.1955e-04 - mean_absolute_error: 0.0063\n",
      "\n",
      "Epoch 09196: loss did not improve\n",
      "Epoch 9197/10000\n",
      "2700/2700 [==============================] - 0s 34us/step - loss: 1.2040e-04 - mean_absolute_error: 0.0064\n",
      "\n",
      "Epoch 09197: loss did not improve\n",
      "Epoch 9198/10000\n",
      "2700/2700 [==============================] - 0s 30us/step - loss: 1.2185e-04 - mean_absolute_error: 0.0064\n",
      "\n",
      "Epoch 09198: loss did not improve\n",
      "Epoch 9199/10000\n",
      "2700/2700 [==============================] - 0s 28us/step - loss: 1.2225e-04 - mean_absolute_error: 0.0065\n",
      "\n",
      "Epoch 09199: loss did not improve\n",
      "Epoch 9200/10000\n",
      "2700/2700 [==============================] - 0s 30us/step - loss: 1.2058e-04 - mean_absolute_error: 0.0064\n",
      "\n",
      "Epoch 09200: loss did not improve\n",
      "Epoch 9201/10000\n",
      "2700/2700 [==============================] - 0s 29us/step - loss: 1.2168e-04 - mean_absolute_error: 0.0064\n",
      "\n",
      "Epoch 09201: loss did not improve\n",
      "Epoch 9202/10000\n",
      "2700/2700 [==============================] - 0s 29us/step - loss: 1.2217e-04 - mean_absolute_error: 0.0065\n",
      "\n",
      "Epoch 09202: loss did not improve\n",
      "Epoch 9203/10000\n",
      "2700/2700 [==============================] - 0s 29us/step - loss: 1.2024e-04 - mean_absolute_error: 0.0063\n",
      "\n",
      "Epoch 09203: loss did not improve\n",
      "Epoch 9204/10000\n",
      "2700/2700 [==============================] - 0s 30us/step - loss: 1.2202e-04 - mean_absolute_error: 0.0065\n",
      "\n",
      "Epoch 09204: loss did not improve\n",
      "Epoch 9205/10000\n",
      "2700/2700 [==============================] - 0s 31us/step - loss: 1.2019e-04 - mean_absolute_error: 0.0064\n",
      "\n",
      "Epoch 09205: loss did not improve\n",
      "Epoch 9206/10000\n",
      "2700/2700 [==============================] - 0s 30us/step - loss: 1.2082e-04 - mean_absolute_error: 0.0064\n",
      "\n",
      "Epoch 09206: loss did not improve\n",
      "Epoch 9207/10000\n",
      "2700/2700 [==============================] - 0s 30us/step - loss: 1.2128e-04 - mean_absolute_error: 0.0064\n",
      "\n",
      "Epoch 09207: loss did not improve\n",
      "Epoch 9208/10000\n",
      "2700/2700 [==============================] - 0s 32us/step - loss: 1.2097e-04 - mean_absolute_error: 0.0064\n",
      "\n",
      "Epoch 09208: loss did not improve\n",
      "Epoch 9209/10000\n",
      "2700/2700 [==============================] - 0s 27us/step - loss: 1.2134e-04 - mean_absolute_error: 0.0065\n",
      "\n",
      "Epoch 09209: loss did not improve\n",
      "Epoch 9210/10000\n",
      "2700/2700 [==============================] - 0s 29us/step - loss: 1.1981e-04 - mean_absolute_error: 0.0063\n",
      "\n",
      "Epoch 09210: loss did not improve\n",
      "Epoch 9211/10000\n",
      "2700/2700 [==============================] - 0s 28us/step - loss: 1.2032e-04 - mean_absolute_error: 0.0063\n",
      "\n",
      "Epoch 09211: loss did not improve\n",
      "Epoch 9212/10000\n",
      "2700/2700 [==============================] - 0s 28us/step - loss: 1.2250e-04 - mean_absolute_error: 0.0067\n",
      "\n",
      "Epoch 09212: loss did not improve\n",
      "Epoch 9213/10000\n",
      "2700/2700 [==============================] - 0s 27us/step - loss: 1.2446e-04 - mean_absolute_error: 0.0068\n",
      "\n",
      "Epoch 09213: loss did not improve\n",
      "Epoch 9214/10000\n",
      "2700/2700 [==============================] - 0s 28us/step - loss: 1.2126e-04 - mean_absolute_error: 0.0064\n",
      "\n",
      "Epoch 09214: loss did not improve\n",
      "Epoch 9215/10000\n",
      "2700/2700 [==============================] - 0s 29us/step - loss: 1.2080e-04 - mean_absolute_error: 0.0065\n",
      "\n",
      "Epoch 09215: loss did not improve\n",
      "Epoch 9216/10000\n",
      "2700/2700 [==============================] - 0s 29us/step - loss: 1.2082e-04 - mean_absolute_error: 0.0064\n",
      "\n",
      "Epoch 09216: loss did not improve\n",
      "Epoch 9217/10000\n",
      "2700/2700 [==============================] - 0s 29us/step - loss: 1.2598e-04 - mean_absolute_error: 0.0068\n",
      "\n",
      "Epoch 09217: loss did not improve\n",
      "Epoch 9218/10000\n",
      "2700/2700 [==============================] - 0s 29us/step - loss: 1.2161e-04 - mean_absolute_error: 0.0065\n",
      "\n",
      "Epoch 09218: loss did not improve\n",
      "Epoch 9219/10000\n",
      "2700/2700 [==============================] - 0s 30us/step - loss: 1.2047e-04 - mean_absolute_error: 0.0063\n",
      "\n",
      "Epoch 09219: loss did not improve\n",
      "Epoch 9220/10000\n",
      "2700/2700 [==============================] - 0s 30us/step - loss: 1.2148e-04 - mean_absolute_error: 0.0065\n",
      "\n",
      "Epoch 09220: loss did not improve\n",
      "Epoch 9221/10000\n",
      "2700/2700 [==============================] - 0s 27us/step - loss: 1.2057e-04 - mean_absolute_error: 0.0063\n",
      "\n",
      "Epoch 09221: loss did not improve\n",
      "Epoch 9222/10000\n",
      "2700/2700 [==============================] - 0s 31us/step - loss: 1.2123e-04 - mean_absolute_error: 0.0065\n",
      "\n",
      "Epoch 09222: loss did not improve\n",
      "Epoch 9223/10000\n",
      "2700/2700 [==============================] - 0s 26us/step - loss: 1.2305e-04 - mean_absolute_error: 0.0065\n",
      "\n",
      "Epoch 09223: loss did not improve\n",
      "Epoch 9224/10000\n",
      "2700/2700 [==============================] - 0s 30us/step - loss: 1.2096e-04 - mean_absolute_error: 0.0064\n",
      "\n",
      "Epoch 09224: loss did not improve\n",
      "Epoch 9225/10000\n",
      "2700/2700 [==============================] - 0s 29us/step - loss: 1.2243e-04 - mean_absolute_error: 0.0066\n",
      "\n",
      "Epoch 09225: loss did not improve\n",
      "Epoch 9226/10000\n",
      "2700/2700 [==============================] - 0s 29us/step - loss: 1.2119e-04 - mean_absolute_error: 0.0064\n",
      "\n",
      "Epoch 09226: loss did not improve\n",
      "Epoch 9227/10000\n",
      "2700/2700 [==============================] - 0s 26us/step - loss: 1.2181e-04 - mean_absolute_error: 0.0063\n",
      "\n",
      "Epoch 09227: loss did not improve\n",
      "Epoch 9228/10000\n",
      "2700/2700 [==============================] - 0s 28us/step - loss: 1.2130e-04 - mean_absolute_error: 0.0065\n",
      "\n",
      "Epoch 09228: loss did not improve\n",
      "Epoch 9229/10000\n",
      "2700/2700 [==============================] - 0s 28us/step - loss: 1.2216e-04 - mean_absolute_error: 0.0066\n",
      "\n",
      "Epoch 09229: loss did not improve\n",
      "Epoch 9230/10000\n",
      "2700/2700 [==============================] - 0s 29us/step - loss: 1.1967e-04 - mean_absolute_error: 0.0063\n",
      "\n",
      "Epoch 09230: loss did not improve\n",
      "Epoch 9231/10000\n",
      "2700/2700 [==============================] - 0s 33us/step - loss: 1.1994e-04 - mean_absolute_error: 0.0063\n",
      "\n",
      "Epoch 09231: loss did not improve\n",
      "Epoch 9232/10000\n",
      "2700/2700 [==============================] - 0s 29us/step - loss: 1.2211e-04 - mean_absolute_error: 0.0065\n",
      "\n",
      "Epoch 09232: loss did not improve\n",
      "Epoch 9233/10000\n",
      "2700/2700 [==============================] - 0s 33us/step - loss: 1.2062e-04 - mean_absolute_error: 0.0064\n",
      "\n",
      "Epoch 09233: loss did not improve\n",
      "Epoch 9234/10000\n",
      "2700/2700 [==============================] - 0s 29us/step - loss: 1.2131e-04 - mean_absolute_error: 0.0064\n",
      "\n",
      "Epoch 09234: loss did not improve\n",
      "Epoch 9235/10000\n",
      "2700/2700 [==============================] - 0s 29us/step - loss: 1.1998e-04 - mean_absolute_error: 0.0063\n",
      "\n",
      "Epoch 09235: loss did not improve\n",
      "Epoch 9236/10000\n",
      "2700/2700 [==============================] - 0s 30us/step - loss: 1.2182e-04 - mean_absolute_error: 0.0066\n",
      "\n",
      "Epoch 09236: loss did not improve\n",
      "Epoch 9237/10000\n",
      "2700/2700 [==============================] - 0s 26us/step - loss: 1.2296e-04 - mean_absolute_error: 0.0066\n",
      "\n",
      "Epoch 09237: loss did not improve\n",
      "Epoch 9238/10000\n",
      "2700/2700 [==============================] - 0s 30us/step - loss: 1.2021e-04 - mean_absolute_error: 0.0063\n",
      "\n",
      "Epoch 09238: loss did not improve\n",
      "Epoch 9239/10000\n",
      "2700/2700 [==============================] - 0s 31us/step - loss: 1.2154e-04 - mean_absolute_error: 0.0065\n",
      "\n",
      "Epoch 09239: loss did not improve\n",
      "Epoch 9240/10000\n",
      "2700/2700 [==============================] - 0s 30us/step - loss: 1.2175e-04 - mean_absolute_error: 0.0065\n",
      "\n",
      "Epoch 09240: loss did not improve\n",
      "Epoch 9241/10000\n",
      "2700/2700 [==============================] - 0s 31us/step - loss: 1.2941e-04 - mean_absolute_error: 0.0070\n",
      "\n",
      "Epoch 09241: loss did not improve\n",
      "Epoch 9242/10000\n",
      "2700/2700 [==============================] - 0s 30us/step - loss: 1.2034e-04 - mean_absolute_error: 0.0064\n",
      "\n",
      "Epoch 09242: loss did not improve\n",
      "Epoch 9243/10000\n",
      "2700/2700 [==============================] - 0s 31us/step - loss: 1.2226e-04 - mean_absolute_error: 0.0065\n",
      "\n",
      "Epoch 09243: loss did not improve\n",
      "Epoch 9244/10000\n",
      "2700/2700 [==============================] - 0s 33us/step - loss: 1.2186e-04 - mean_absolute_error: 0.0065\n",
      "\n",
      "Epoch 09244: loss did not improve\n",
      "Epoch 9245/10000\n",
      "2700/2700 [==============================] - 0s 29us/step - loss: 1.2235e-04 - mean_absolute_error: 0.0066\n",
      "\n",
      "Epoch 09245: loss did not improve\n",
      "Epoch 9246/10000\n",
      "2700/2700 [==============================] - 0s 30us/step - loss: 1.2036e-04 - mean_absolute_error: 0.0065\n",
      "\n",
      "Epoch 09246: loss did not improve\n",
      "Epoch 9247/10000\n",
      "2700/2700 [==============================] - 0s 29us/step - loss: 1.2234e-04 - mean_absolute_error: 0.0065\n",
      "\n",
      "Epoch 09247: loss did not improve\n",
      "Epoch 9248/10000\n",
      "2700/2700 [==============================] - 0s 26us/step - loss: 1.2199e-04 - mean_absolute_error: 0.0065\n",
      "\n",
      "Epoch 09248: loss did not improve\n",
      "Epoch 9249/10000\n",
      "2700/2700 [==============================] - 0s 31us/step - loss: 1.2084e-04 - mean_absolute_error: 0.0064\n",
      "\n",
      "Epoch 09249: loss did not improve\n",
      "Epoch 9250/10000\n",
      "2700/2700 [==============================] - 0s 29us/step - loss: 1.2013e-04 - mean_absolute_error: 0.0063\n",
      "\n",
      "Epoch 09250: loss did not improve\n",
      "Epoch 9251/10000\n",
      "2700/2700 [==============================] - 0s 36us/step - loss: 1.2084e-04 - mean_absolute_error: 0.0064\n",
      "\n",
      "Epoch 09251: loss did not improve\n",
      "Epoch 9252/10000\n",
      "2700/2700 [==============================] - 0s 31us/step - loss: 1.2106e-04 - mean_absolute_error: 0.0064\n",
      "\n",
      "Epoch 09252: loss did not improve\n",
      "Epoch 9253/10000\n",
      "2700/2700 [==============================] - 0s 26us/step - loss: 1.2422e-04 - mean_absolute_error: 0.0068\n",
      "\n",
      "Epoch 09253: loss did not improve\n",
      "Epoch 9254/10000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2700/2700 [==============================] - 0s 34us/step - loss: 1.2299e-04 - mean_absolute_error: 0.0065\n",
      "\n",
      "Epoch 09254: loss did not improve\n",
      "Epoch 9255/10000\n",
      "2700/2700 [==============================] - 0s 39us/step - loss: 1.2333e-04 - mean_absolute_error: 0.0067\n",
      "\n",
      "Epoch 09255: loss did not improve\n",
      "Epoch 9256/10000\n",
      "2700/2700 [==============================] - 0s 29us/step - loss: 1.2056e-04 - mean_absolute_error: 0.0063\n",
      "\n",
      "Epoch 09256: loss did not improve\n",
      "Epoch 9257/10000\n",
      "2700/2700 [==============================] - 0s 29us/step - loss: 1.2077e-04 - mean_absolute_error: 0.0064\n",
      "\n",
      "Epoch 09257: loss did not improve\n",
      "Epoch 9258/10000\n",
      "2700/2700 [==============================] - 0s 30us/step - loss: 1.2276e-04 - mean_absolute_error: 0.0066\n",
      "\n",
      "Epoch 09258: loss did not improve\n",
      "Epoch 9259/10000\n",
      "2700/2700 [==============================] - 0s 31us/step - loss: 1.2172e-04 - mean_absolute_error: 0.0063\n",
      "\n",
      "Epoch 09259: loss did not improve\n",
      "Epoch 9260/10000\n",
      "2700/2700 [==============================] - 0s 29us/step - loss: 1.2215e-04 - mean_absolute_error: 0.0065\n",
      "\n",
      "Epoch 09260: loss did not improve\n",
      "Epoch 9261/10000\n",
      "2700/2700 [==============================] - 0s 30us/step - loss: 1.2148e-04 - mean_absolute_error: 0.0065\n",
      "\n",
      "Epoch 09261: loss did not improve\n",
      "Epoch 9262/10000\n",
      "2700/2700 [==============================] - 0s 28us/step - loss: 1.1996e-04 - mean_absolute_error: 0.0063\n",
      "\n",
      "Epoch 09262: loss did not improve\n",
      "Epoch 9263/10000\n",
      "2700/2700 [==============================] - 0s 29us/step - loss: 1.2497e-04 - mean_absolute_error: 0.0067\n",
      "\n",
      "Epoch 09263: loss did not improve\n",
      "Epoch 9264/10000\n",
      "2700/2700 [==============================] - 0s 28us/step - loss: 1.2094e-04 - mean_absolute_error: 0.0064\n",
      "\n",
      "Epoch 09264: loss did not improve\n",
      "Epoch 9265/10000\n",
      "2700/2700 [==============================] - 0s 29us/step - loss: 1.2334e-04 - mean_absolute_error: 0.0067\n",
      "\n",
      "Epoch 09265: loss did not improve\n",
      "Epoch 9266/10000\n",
      "2700/2700 [==============================] - 0s 29us/step - loss: 1.2105e-04 - mean_absolute_error: 0.0064\n",
      "\n",
      "Epoch 09266: loss did not improve\n",
      "Epoch 9267/10000\n",
      "2700/2700 [==============================] - 0s 34us/step - loss: 1.2672e-04 - mean_absolute_error: 0.0070\n",
      "\n",
      "Epoch 09267: loss did not improve\n",
      "Epoch 9268/10000\n",
      "2700/2700 [==============================] - 0s 32us/step - loss: 1.2263e-04 - mean_absolute_error: 0.0064\n",
      "\n",
      "Epoch 09268: loss did not improve\n",
      "Epoch 9269/10000\n",
      "2700/2700 [==============================] - 0s 32us/step - loss: 1.2114e-04 - mean_absolute_error: 0.0064\n",
      "\n",
      "Epoch 09269: loss did not improve\n",
      "Epoch 9270/10000\n",
      "2700/2700 [==============================] - 0s 26us/step - loss: 1.2195e-04 - mean_absolute_error: 0.0065\n",
      "\n",
      "Epoch 09270: loss did not improve\n",
      "Epoch 9271/10000\n",
      "2700/2700 [==============================] - 0s 31us/step - loss: 1.2013e-04 - mean_absolute_error: 0.0064\n",
      "\n",
      "Epoch 09271: loss did not improve\n",
      "Epoch 9272/10000\n",
      "2700/2700 [==============================] - 0s 27us/step - loss: 1.2187e-04 - mean_absolute_error: 0.0066\n",
      "\n",
      "Epoch 09272: loss did not improve\n",
      "Epoch 9273/10000\n",
      "2700/2700 [==============================] - 0s 29us/step - loss: 1.2102e-04 - mean_absolute_error: 0.0064\n",
      "\n",
      "Epoch 09273: loss did not improve\n",
      "Epoch 9274/10000\n",
      "2700/2700 [==============================] - 0s 33us/step - loss: 1.2120e-04 - mean_absolute_error: 0.0065\n",
      "\n",
      "Epoch 09274: loss did not improve\n",
      "Epoch 9275/10000\n",
      "2700/2700 [==============================] - 0s 32us/step - loss: 1.2207e-04 - mean_absolute_error: 0.0064\n",
      "\n",
      "Epoch 09275: loss did not improve\n",
      "Epoch 9276/10000\n",
      "2700/2700 [==============================] - 0s 45us/step - loss: 1.2253e-04 - mean_absolute_error: 0.0065\n",
      "\n",
      "Epoch 09276: loss did not improve\n",
      "Epoch 9277/10000\n",
      "2700/2700 [==============================] - 0s 37us/step - loss: 1.2217e-04 - mean_absolute_error: 0.0066\n",
      "\n",
      "Epoch 09277: loss did not improve\n",
      "Epoch 9278/10000\n",
      "2700/2700 [==============================] - 0s 32us/step - loss: 1.2063e-04 - mean_absolute_error: 0.0064\n",
      "\n",
      "Epoch 09278: loss did not improve\n",
      "Epoch 9279/10000\n",
      "2700/2700 [==============================] - 0s 33us/step - loss: 1.2456e-04 - mean_absolute_error: 0.0067\n",
      "\n",
      "Epoch 09279: loss did not improve\n",
      "Epoch 9280/10000\n",
      "2700/2700 [==============================] - 0s 27us/step - loss: 1.2115e-04 - mean_absolute_error: 0.0065\n",
      "\n",
      "Epoch 09280: loss did not improve\n",
      "Epoch 9281/10000\n",
      "2700/2700 [==============================] - 0s 29us/step - loss: 1.2253e-04 - mean_absolute_error: 0.0065\n",
      "\n",
      "Epoch 09281: loss did not improve\n",
      "Epoch 9282/10000\n",
      "2700/2700 [==============================] - 0s 29us/step - loss: 1.2287e-04 - mean_absolute_error: 0.0065\n",
      "\n",
      "Epoch 09282: loss did not improve\n",
      "Epoch 9283/10000\n",
      "2700/2700 [==============================] - 0s 30us/step - loss: 1.2296e-04 - mean_absolute_error: 0.0065\n",
      "\n",
      "Epoch 09283: loss did not improve\n",
      "Epoch 9284/10000\n",
      "2700/2700 [==============================] - 0s 27us/step - loss: 1.2192e-04 - mean_absolute_error: 0.0065\n",
      "\n",
      "Epoch 09284: loss did not improve\n",
      "Epoch 9285/10000\n",
      "2700/2700 [==============================] - 0s 27us/step - loss: 1.1959e-04 - mean_absolute_error: 0.0063\n",
      "\n",
      "Epoch 09285: loss did not improve\n",
      "Epoch 9286/10000\n",
      "2700/2700 [==============================] - 0s 31us/step - loss: 1.2115e-04 - mean_absolute_error: 0.0064\n",
      "\n",
      "Epoch 09286: loss did not improve\n",
      "Epoch 9287/10000\n",
      "2700/2700 [==============================] - 0s 30us/step - loss: 1.2008e-04 - mean_absolute_error: 0.0063\n",
      "\n",
      "Epoch 09287: loss did not improve\n",
      "Epoch 9288/10000\n",
      "2700/2700 [==============================] - 0s 28us/step - loss: 1.1909e-04 - mean_absolute_error: 0.0062\n",
      "\n",
      "Epoch 09288: loss did not improve\n",
      "Epoch 9289/10000\n",
      "2700/2700 [==============================] - 0s 26us/step - loss: 1.2019e-04 - mean_absolute_error: 0.0063\n",
      "\n",
      "Epoch 09289: loss did not improve\n",
      "Epoch 9290/10000\n",
      "2700/2700 [==============================] - 0s 29us/step - loss: 1.2083e-04 - mean_absolute_error: 0.0064\n",
      "\n",
      "Epoch 09290: loss did not improve\n",
      "Epoch 9291/10000\n",
      "2700/2700 [==============================] - 0s 29us/step - loss: 1.2157e-04 - mean_absolute_error: 0.0065\n",
      "\n",
      "Epoch 09291: loss did not improve\n",
      "Epoch 9292/10000\n",
      "2700/2700 [==============================] - 0s 31us/step - loss: 1.2138e-04 - mean_absolute_error: 0.0063\n",
      "\n",
      "Epoch 09292: loss did not improve\n",
      "Epoch 9293/10000\n",
      "2700/2700 [==============================] - 0s 30us/step - loss: 1.3015e-04 - mean_absolute_error: 0.0071\n",
      "\n",
      "Epoch 09293: loss did not improve\n",
      "Epoch 9294/10000\n",
      "2700/2700 [==============================] - 0s 29us/step - loss: 1.2118e-04 - mean_absolute_error: 0.0065\n",
      "\n",
      "Epoch 09294: loss did not improve\n",
      "Epoch 9295/10000\n",
      "2700/2700 [==============================] - 0s 29us/step - loss: 1.2482e-04 - mean_absolute_error: 0.0068\n",
      "\n",
      "Epoch 09295: loss did not improve\n",
      "Epoch 9296/10000\n",
      "2700/2700 [==============================] - 0s 30us/step - loss: 1.2044e-04 - mean_absolute_error: 0.0063\n",
      "\n",
      "Epoch 09296: loss did not improve\n",
      "Epoch 9297/10000\n",
      "2700/2700 [==============================] - 0s 28us/step - loss: 1.2293e-04 - mean_absolute_error: 0.0065\n",
      "\n",
      "Epoch 09297: loss did not improve\n",
      "Epoch 9298/10000\n",
      "2700/2700 [==============================] - 0s 26us/step - loss: 1.2058e-04 - mean_absolute_error: 0.0064\n",
      "\n",
      "Epoch 09298: loss did not improve\n",
      "Epoch 9299/10000\n",
      "2700/2700 [==============================] - 0s 30us/step - loss: 1.2010e-04 - mean_absolute_error: 0.0063\n",
      "\n",
      "Epoch 09299: loss did not improve\n",
      "Epoch 9300/10000\n",
      "2700/2700 [==============================] - 0s 30us/step - loss: 1.1939e-04 - mean_absolute_error: 0.0062\n",
      "\n",
      "Epoch 09300: loss did not improve\n",
      "Epoch 9301/10000\n",
      "2700/2700 [==============================] - 0s 29us/step - loss: 1.2086e-04 - mean_absolute_error: 0.0063\n",
      "\n",
      "Epoch 09301: loss did not improve\n",
      "Epoch 9302/10000\n",
      "2700/2700 [==============================] - 0s 29us/step - loss: 1.2377e-04 - mean_absolute_error: 0.0067\n",
      "\n",
      "Epoch 09302: loss did not improve\n",
      "Epoch 9303/10000\n",
      "2700/2700 [==============================] - 0s 29us/step - loss: 1.2096e-04 - mean_absolute_error: 0.0064\n",
      "\n",
      "Epoch 09303: loss did not improve\n",
      "Epoch 9304/10000\n",
      "2700/2700 [==============================] - 0s 36us/step - loss: 1.2076e-04 - mean_absolute_error: 0.0064\n",
      "\n",
      "Epoch 09304: loss did not improve\n",
      "Epoch 9305/10000\n",
      "2700/2700 [==============================] - 0s 33us/step - loss: 1.2015e-04 - mean_absolute_error: 0.0063\n",
      "\n",
      "Epoch 09305: loss did not improve\n",
      "Epoch 9306/10000\n",
      "2700/2700 [==============================] - 0s 34us/step - loss: 1.2202e-04 - mean_absolute_error: 0.0065\n",
      "\n",
      "Epoch 09306: loss did not improve\n",
      "Epoch 9307/10000\n",
      "2700/2700 [==============================] - 0s 32us/step - loss: 1.1915e-04 - mean_absolute_error: 0.0063\n",
      "\n",
      "Epoch 09307: loss did not improve\n",
      "Epoch 9308/10000\n",
      "2700/2700 [==============================] - 0s 31us/step - loss: 1.2260e-04 - mean_absolute_error: 0.0065\n",
      "\n",
      "Epoch 09308: loss did not improve\n",
      "Epoch 9309/10000\n",
      "2700/2700 [==============================] - 0s 27us/step - loss: 1.2113e-04 - mean_absolute_error: 0.0064\n",
      "\n",
      "Epoch 09309: loss did not improve\n",
      "Epoch 9310/10000\n",
      "2700/2700 [==============================] - 0s 29us/step - loss: 1.2254e-04 - mean_absolute_error: 0.0065\n",
      "\n",
      "Epoch 09310: loss did not improve\n",
      "Epoch 9311/10000\n",
      "2700/2700 [==============================] - 0s 30us/step - loss: 1.2270e-04 - mean_absolute_error: 0.0065\n",
      "\n",
      "Epoch 09311: loss did not improve\n",
      "Epoch 9312/10000\n",
      "2700/2700 [==============================] - 0s 30us/step - loss: 1.2000e-04 - mean_absolute_error: 0.0064\n",
      "\n",
      "Epoch 09312: loss did not improve\n",
      "Epoch 9313/10000\n",
      "2700/2700 [==============================] - 0s 31us/step - loss: 1.2108e-04 - mean_absolute_error: 0.0064\n",
      "\n",
      "Epoch 09313: loss did not improve\n",
      "Epoch 9314/10000\n",
      "2700/2700 [==============================] - 0s 31us/step - loss: 1.2301e-04 - mean_absolute_error: 0.0066\n",
      "\n",
      "Epoch 09314: loss did not improve\n",
      "Epoch 9315/10000\n",
      "2700/2700 [==============================] - 0s 40us/step - loss: 1.2109e-04 - mean_absolute_error: 0.0064\n",
      "\n",
      "Epoch 09315: loss did not improve\n",
      "Epoch 9316/10000\n",
      "2700/2700 [==============================] - 0s 33us/step - loss: 1.2016e-04 - mean_absolute_error: 0.0063\n",
      "\n",
      "Epoch 09316: loss did not improve\n",
      "Epoch 9317/10000\n",
      "2700/2700 [==============================] - 0s 30us/step - loss: 1.2276e-04 - mean_absolute_error: 0.0065\n",
      "\n",
      "Epoch 09317: loss did not improve\n",
      "Epoch 9318/10000\n",
      "2700/2700 [==============================] - 0s 29us/step - loss: 1.2010e-04 - mean_absolute_error: 0.0064\n",
      "\n",
      "Epoch 09318: loss did not improve\n",
      "Epoch 9319/10000\n",
      "2700/2700 [==============================] - 0s 30us/step - loss: 1.2082e-04 - mean_absolute_error: 0.0064\n",
      "\n",
      "Epoch 09319: loss did not improve\n",
      "Epoch 9320/10000\n",
      "2700/2700 [==============================] - 0s 29us/step - loss: 1.2167e-04 - mean_absolute_error: 0.0065\n",
      "\n",
      "Epoch 09320: loss did not improve\n",
      "Epoch 9321/10000\n",
      "2700/2700 [==============================] - 0s 29us/step - loss: 1.2129e-04 - mean_absolute_error: 0.0064\n",
      "\n",
      "Epoch 09321: loss did not improve\n",
      "Epoch 9322/10000\n",
      "2700/2700 [==============================] - 0s 32us/step - loss: 1.2157e-04 - mean_absolute_error: 0.0065\n",
      "\n",
      "Epoch 09322: loss did not improve\n",
      "Epoch 9323/10000\n",
      "2700/2700 [==============================] - 0s 36us/step - loss: 1.2233e-04 - mean_absolute_error: 0.0067\n",
      "\n",
      "Epoch 09323: loss did not improve\n",
      "Epoch 9324/10000\n",
      "2700/2700 [==============================] - 0s 30us/step - loss: 1.2850e-04 - mean_absolute_error: 0.0070\n",
      "\n",
      "Epoch 09324: loss did not improve\n",
      "Epoch 9325/10000\n",
      "2700/2700 [==============================] - 0s 30us/step - loss: 1.2102e-04 - mean_absolute_error: 0.0065\n",
      "\n",
      "Epoch 09325: loss did not improve\n",
      "Epoch 9326/10000\n",
      "2700/2700 [==============================] - 0s 31us/step - loss: 1.2124e-04 - mean_absolute_error: 0.0064\n",
      "\n",
      "Epoch 09326: loss did not improve\n",
      "Epoch 9327/10000\n",
      "2700/2700 [==============================] - 0s 35us/step - loss: 1.2007e-04 - mean_absolute_error: 0.0063\n",
      "\n",
      "Epoch 09327: loss did not improve\n",
      "Epoch 9328/10000\n",
      "2700/2700 [==============================] - 0s 32us/step - loss: 1.2086e-04 - mean_absolute_error: 0.0065\n",
      "\n",
      "Epoch 09328: loss did not improve\n",
      "Epoch 9329/10000\n",
      "2700/2700 [==============================] - 0s 33us/step - loss: 1.2205e-04 - mean_absolute_error: 0.0065\n",
      "\n",
      "Epoch 09329: loss did not improve\n",
      "Epoch 9330/10000\n",
      "2700/2700 [==============================] - 0s 30us/step - loss: 1.2123e-04 - mean_absolute_error: 0.0065\n",
      "\n",
      "Epoch 09330: loss did not improve\n",
      "Epoch 9331/10000\n",
      "2700/2700 [==============================] - 0s 26us/step - loss: 1.2165e-04 - mean_absolute_error: 0.0065\n",
      "\n",
      "Epoch 09331: loss did not improve\n",
      "Epoch 9332/10000\n",
      "2700/2700 [==============================] - 0s 29us/step - loss: 1.2058e-04 - mean_absolute_error: 0.0064\n",
      "\n",
      "Epoch 09332: loss did not improve\n",
      "Epoch 9333/10000\n",
      "2700/2700 [==============================] - 0s 28us/step - loss: 1.2072e-04 - mean_absolute_error: 0.0064\n",
      "\n",
      "Epoch 09333: loss did not improve\n",
      "Epoch 9334/10000\n",
      "2700/2700 [==============================] - 0s 31us/step - loss: 1.2287e-04 - mean_absolute_error: 0.0066\n",
      "\n",
      "Epoch 09334: loss did not improve\n",
      "Epoch 9335/10000\n",
      "2700/2700 [==============================] - 0s 28us/step - loss: 1.2089e-04 - mean_absolute_error: 0.0063\n",
      "\n",
      "Epoch 09335: loss did not improve\n",
      "Epoch 9336/10000\n",
      "2700/2700 [==============================] - 0s 30us/step - loss: 1.2186e-04 - mean_absolute_error: 0.0064\n",
      "\n",
      "Epoch 09336: loss did not improve\n",
      "Epoch 9337/10000\n",
      "2700/2700 [==============================] - 0s 29us/step - loss: 1.2054e-04 - mean_absolute_error: 0.0064\n",
      "\n",
      "Epoch 09337: loss did not improve\n",
      "Epoch 9338/10000\n",
      "2700/2700 [==============================] - 0s 29us/step - loss: 1.2432e-04 - mean_absolute_error: 0.0067\n",
      "\n",
      "Epoch 09338: loss did not improve\n",
      "Epoch 9339/10000\n",
      "2700/2700 [==============================] - 0s 31us/step - loss: 1.2068e-04 - mean_absolute_error: 0.0064\n",
      "\n",
      "Epoch 09339: loss did not improve\n",
      "Epoch 9340/10000\n",
      "2700/2700 [==============================] - 0s 27us/step - loss: 1.2091e-04 - mean_absolute_error: 0.0064\n",
      "\n",
      "Epoch 09340: loss did not improve\n",
      "Epoch 9341/10000\n",
      "2700/2700 [==============================] - 0s 29us/step - loss: 1.2141e-04 - mean_absolute_error: 0.0064\n",
      "\n",
      "Epoch 09341: loss did not improve\n",
      "Epoch 9342/10000\n",
      "2700/2700 [==============================] - 0s 29us/step - loss: 1.1932e-04 - mean_absolute_error: 0.0063\n",
      "\n",
      "Epoch 09342: loss did not improve\n",
      "Epoch 9343/10000\n",
      "2700/2700 [==============================] - 0s 32us/step - loss: 1.2214e-04 - mean_absolute_error: 0.0064\n",
      "\n",
      "Epoch 09343: loss did not improve\n",
      "Epoch 9344/10000\n",
      "2700/2700 [==============================] - 0s 28us/step - loss: 1.2274e-04 - mean_absolute_error: 0.0066\n",
      "\n",
      "Epoch 09344: loss did not improve\n",
      "Epoch 9345/10000\n",
      "2700/2700 [==============================] - 0s 27us/step - loss: 1.2132e-04 - mean_absolute_error: 0.0064\n",
      "\n",
      "Epoch 09345: loss did not improve\n",
      "Epoch 9346/10000\n",
      "2700/2700 [==============================] - 0s 29us/step - loss: 1.2205e-04 - mean_absolute_error: 0.0065\n",
      "\n",
      "Epoch 09346: loss did not improve\n",
      "Epoch 9347/10000\n",
      "2700/2700 [==============================] - 0s 30us/step - loss: 1.2414e-04 - mean_absolute_error: 0.0066\n",
      "\n",
      "Epoch 09347: loss did not improve\n",
      "Epoch 9348/10000\n",
      "2700/2700 [==============================] - 0s 30us/step - loss: 1.2194e-04 - mean_absolute_error: 0.0064\n",
      "\n",
      "Epoch 09348: loss did not improve\n",
      "Epoch 9349/10000\n",
      "2700/2700 [==============================] - 0s 28us/step - loss: 1.2250e-04 - mean_absolute_error: 0.0065\n",
      "\n",
      "Epoch 09349: loss did not improve\n",
      "Epoch 9350/10000\n",
      "2700/2700 [==============================] - 0s 29us/step - loss: 1.2012e-04 - mean_absolute_error: 0.0062\n",
      "\n",
      "Epoch 09350: loss did not improve\n",
      "Epoch 9351/10000\n",
      "2700/2700 [==============================] - 0s 33us/step - loss: 1.2590e-04 - mean_absolute_error: 0.0067\n",
      "\n",
      "Epoch 09351: loss did not improve\n",
      "Epoch 9352/10000\n",
      "2700/2700 [==============================] - 0s 30us/step - loss: 1.2128e-04 - mean_absolute_error: 0.0064\n",
      "\n",
      "Epoch 09352: loss did not improve\n",
      "Epoch 9353/10000\n",
      "2700/2700 [==============================] - 0s 30us/step - loss: 1.1873e-04 - mean_absolute_error: 0.0062\n",
      "\n",
      "Epoch 09353: loss improved from 0.00012 to 0.00012, saving model to keras/best_baseline_weights.hdf5\n",
      "Epoch 9354/10000\n",
      "2700/2700 [==============================] - 0s 29us/step - loss: 1.2079e-04 - mean_absolute_error: 0.0064\n",
      "\n",
      "Epoch 09354: loss did not improve\n",
      "Epoch 9355/10000\n",
      "2700/2700 [==============================] - 0s 29us/step - loss: 1.2223e-04 - mean_absolute_error: 0.0065\n",
      "\n",
      "Epoch 09355: loss did not improve\n",
      "Epoch 9356/10000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2700/2700 [==============================] - 0s 28us/step - loss: 1.2228e-04 - mean_absolute_error: 0.0065\n",
      "\n",
      "Epoch 09356: loss did not improve\n",
      "Epoch 9357/10000\n",
      "2700/2700 [==============================] - 0s 28us/step - loss: 1.2207e-04 - mean_absolute_error: 0.0065\n",
      "\n",
      "Epoch 09357: loss did not improve\n",
      "Epoch 9358/10000\n",
      "2700/2700 [==============================] - 0s 27us/step - loss: 1.2272e-04 - mean_absolute_error: 0.0065\n",
      "\n",
      "Epoch 09358: loss did not improve\n",
      "Epoch 9359/10000\n",
      "2700/2700 [==============================] - 0s 28us/step - loss: 1.2068e-04 - mean_absolute_error: 0.0064\n",
      "\n",
      "Epoch 09359: loss did not improve\n",
      "Epoch 9360/10000\n",
      "2700/2700 [==============================] - 0s 30us/step - loss: 1.1939e-04 - mean_absolute_error: 0.0064\n",
      "\n",
      "Epoch 09360: loss did not improve\n",
      "Epoch 9361/10000\n",
      "2700/2700 [==============================] - 0s 31us/step - loss: 1.2403e-04 - mean_absolute_error: 0.0066\n",
      "\n",
      "Epoch 09361: loss did not improve\n",
      "Epoch 9362/10000\n",
      "2700/2700 [==============================] - 0s 30us/step - loss: 1.2234e-04 - mean_absolute_error: 0.0066\n",
      "\n",
      "Epoch 09362: loss did not improve\n",
      "Epoch 9363/10000\n",
      "2700/2700 [==============================] - 0s 28us/step - loss: 1.2194e-04 - mean_absolute_error: 0.0065\n",
      "\n",
      "Epoch 09363: loss did not improve\n",
      "Epoch 9364/10000\n",
      "2700/2700 [==============================] - 0s 28us/step - loss: 1.2265e-04 - mean_absolute_error: 0.0066\n",
      "\n",
      "Epoch 09364: loss did not improve\n",
      "Epoch 9365/10000\n",
      "2700/2700 [==============================] - 0s 30us/step - loss: 1.2144e-04 - mean_absolute_error: 0.0064\n",
      "\n",
      "Epoch 09365: loss did not improve\n",
      "Epoch 9366/10000\n",
      "2700/2700 [==============================] - 0s 28us/step - loss: 1.1994e-04 - mean_absolute_error: 0.0063\n",
      "\n",
      "Epoch 09366: loss did not improve\n",
      "Epoch 9367/10000\n",
      "2700/2700 [==============================] - 0s 30us/step - loss: 1.2011e-04 - mean_absolute_error: 0.0064\n",
      "\n",
      "Epoch 09367: loss did not improve\n",
      "Epoch 9368/10000\n",
      "2700/2700 [==============================] - 0s 29us/step - loss: 1.2086e-04 - mean_absolute_error: 0.0062\n",
      "\n",
      "Epoch 09368: loss did not improve\n",
      "Epoch 9369/10000\n",
      "2700/2700 [==============================] - 0s 27us/step - loss: 1.2048e-04 - mean_absolute_error: 0.0063\n",
      "\n",
      "Epoch 09369: loss did not improve\n",
      "Epoch 9370/10000\n",
      "2700/2700 [==============================] - 0s 29us/step - loss: 1.2090e-04 - mean_absolute_error: 0.0064\n",
      "\n",
      "Epoch 09370: loss did not improve\n",
      "Epoch 9371/10000\n",
      "2700/2700 [==============================] - 0s 28us/step - loss: 1.2091e-04 - mean_absolute_error: 0.0064\n",
      "\n",
      "Epoch 09371: loss did not improve\n",
      "Epoch 9372/10000\n",
      "2700/2700 [==============================] - 0s 29us/step - loss: 1.2147e-04 - mean_absolute_error: 0.0065\n",
      "\n",
      "Epoch 09372: loss did not improve\n",
      "Epoch 9373/10000\n",
      "2700/2700 [==============================] - 0s 30us/step - loss: 1.2142e-04 - mean_absolute_error: 0.0065\n",
      "\n",
      "Epoch 09373: loss did not improve\n",
      "Epoch 9374/10000\n",
      "2700/2700 [==============================] - 0s 27us/step - loss: 1.1900e-04 - mean_absolute_error: 0.0063\n",
      "\n",
      "Epoch 09374: loss did not improve\n",
      "Epoch 9375/10000\n",
      "2700/2700 [==============================] - 0s 33us/step - loss: 1.2253e-04 - mean_absolute_error: 0.0066\n",
      "\n",
      "Epoch 09375: loss did not improve\n",
      "Epoch 9376/10000\n",
      "2700/2700 [==============================] - 0s 30us/step - loss: 1.2128e-04 - mean_absolute_error: 0.0064\n",
      "\n",
      "Epoch 09376: loss did not improve\n",
      "Epoch 9377/10000\n",
      "2700/2700 [==============================] - 0s 32us/step - loss: 1.2204e-04 - mean_absolute_error: 0.0065\n",
      "\n",
      "Epoch 09377: loss did not improve\n",
      "Epoch 9378/10000\n",
      "2700/2700 [==============================] - 0s 30us/step - loss: 1.2234e-04 - mean_absolute_error: 0.0066\n",
      "\n",
      "Epoch 09378: loss did not improve\n",
      "Epoch 9379/10000\n",
      "2700/2700 [==============================] - 0s 30us/step - loss: 1.1969e-04 - mean_absolute_error: 0.0063\n",
      "\n",
      "Epoch 09379: loss did not improve\n",
      "Epoch 9380/10000\n",
      "2700/2700 [==============================] - 0s 27us/step - loss: 1.2035e-04 - mean_absolute_error: 0.0064\n",
      "\n",
      "Epoch 09380: loss did not improve\n",
      "Epoch 9381/10000\n",
      "2700/2700 [==============================] - 0s 30us/step - loss: 1.2115e-04 - mean_absolute_error: 0.0065\n",
      "\n",
      "Epoch 09381: loss did not improve\n",
      "Epoch 9382/10000\n",
      "2700/2700 [==============================] - 0s 29us/step - loss: 1.2149e-04 - mean_absolute_error: 0.0065\n",
      "\n",
      "Epoch 09382: loss did not improve\n",
      "Epoch 9383/10000\n",
      "2700/2700 [==============================] - 0s 30us/step - loss: 1.2088e-04 - mean_absolute_error: 0.0064\n",
      "\n",
      "Epoch 09383: loss did not improve\n",
      "Epoch 9384/10000\n",
      "2700/2700 [==============================] - 0s 27us/step - loss: 1.2193e-04 - mean_absolute_error: 0.0065\n",
      "\n",
      "Epoch 09384: loss did not improve\n",
      "Epoch 9385/10000\n",
      "2700/2700 [==============================] - 0s 32us/step - loss: 1.2043e-04 - mean_absolute_error: 0.0063\n",
      "\n",
      "Epoch 09385: loss did not improve\n",
      "Epoch 9386/10000\n",
      "2700/2700 [==============================] - 0s 29us/step - loss: 1.2364e-04 - mean_absolute_error: 0.0066\n",
      "\n",
      "Epoch 09386: loss did not improve\n",
      "Epoch 9387/10000\n",
      "2700/2700 [==============================] - 0s 33us/step - loss: 1.2161e-04 - mean_absolute_error: 0.0065\n",
      "\n",
      "Epoch 09387: loss did not improve\n",
      "Epoch 9388/10000\n",
      "2700/2700 [==============================] - 0s 29us/step - loss: 1.2487e-04 - mean_absolute_error: 0.0067\n",
      "\n",
      "Epoch 09388: loss did not improve\n",
      "Epoch 9389/10000\n",
      "2700/2700 [==============================] - 0s 29us/step - loss: 1.2144e-04 - mean_absolute_error: 0.0064\n",
      "\n",
      "Epoch 09389: loss did not improve\n",
      "Epoch 9390/10000\n",
      "2700/2700 [==============================] - 0s 28us/step - loss: 1.2042e-04 - mean_absolute_error: 0.0064\n",
      "\n",
      "Epoch 09390: loss did not improve\n",
      "Epoch 9391/10000\n",
      "2700/2700 [==============================] - 0s 29us/step - loss: 1.2104e-04 - mean_absolute_error: 0.0064\n",
      "\n",
      "Epoch 09391: loss did not improve\n",
      "Epoch 9392/10000\n",
      "2700/2700 [==============================] - 0s 28us/step - loss: 1.2233e-04 - mean_absolute_error: 0.0066\n",
      "\n",
      "Epoch 09392: loss did not improve\n",
      "Epoch 9393/10000\n",
      "2700/2700 [==============================] - 0s 34us/step - loss: 1.2204e-04 - mean_absolute_error: 0.0065\n",
      "\n",
      "Epoch 09393: loss did not improve\n",
      "Epoch 9394/10000\n",
      "2700/2700 [==============================] - 0s 32us/step - loss: 1.2088e-04 - mean_absolute_error: 0.0064\n",
      "\n",
      "Epoch 09394: loss did not improve\n",
      "Epoch 9395/10000\n",
      "2700/2700 [==============================] - 0s 32us/step - loss: 1.1914e-04 - mean_absolute_error: 0.0062\n",
      "\n",
      "Epoch 09395: loss did not improve\n",
      "Epoch 9396/10000\n",
      "2700/2700 [==============================] - 0s 29us/step - loss: 1.2205e-04 - mean_absolute_error: 0.0066\n",
      "\n",
      "Epoch 09396: loss did not improve\n",
      "Epoch 9397/10000\n",
      "2700/2700 [==============================] - 0s 29us/step - loss: 1.2140e-04 - mean_absolute_error: 0.0064\n",
      "\n",
      "Epoch 09397: loss did not improve\n",
      "Epoch 9398/10000\n",
      "2700/2700 [==============================] - 0s 30us/step - loss: 1.2063e-04 - mean_absolute_error: 0.0064\n",
      "\n",
      "Epoch 09398: loss did not improve\n",
      "Epoch 9399/10000\n",
      "2700/2700 [==============================] - 0s 29us/step - loss: 1.1934e-04 - mean_absolute_error: 0.0062\n",
      "\n",
      "Epoch 09399: loss did not improve\n",
      "Epoch 9400/10000\n",
      "2700/2700 [==============================] - 0s 28us/step - loss: 1.2324e-04 - mean_absolute_error: 0.0065\n",
      "\n",
      "Epoch 09400: loss did not improve\n",
      "Epoch 9401/10000\n",
      "2700/2700 [==============================] - 0s 32us/step - loss: 1.2259e-04 - mean_absolute_error: 0.0065\n",
      "\n",
      "Epoch 09401: loss did not improve\n",
      "Epoch 9402/10000\n",
      "2700/2700 [==============================] - 0s 29us/step - loss: 1.2020e-04 - mean_absolute_error: 0.0064\n",
      "\n",
      "Epoch 09402: loss did not improve\n",
      "Epoch 9403/10000\n",
      "2700/2700 [==============================] - 0s 30us/step - loss: 1.2331e-04 - mean_absolute_error: 0.0067\n",
      "\n",
      "Epoch 09403: loss did not improve\n",
      "Epoch 9404/10000\n",
      "2700/2700 [==============================] - 0s 28us/step - loss: 1.1998e-04 - mean_absolute_error: 0.0064\n",
      "\n",
      "Epoch 09404: loss did not improve\n",
      "Epoch 9405/10000\n",
      "2700/2700 [==============================] - 0s 31us/step - loss: 1.2182e-04 - mean_absolute_error: 0.0064\n",
      "\n",
      "Epoch 09405: loss did not improve\n",
      "Epoch 9406/10000\n",
      "2700/2700 [==============================] - 0s 37us/step - loss: 1.2390e-04 - mean_absolute_error: 0.0066\n",
      "\n",
      "Epoch 09406: loss did not improve\n",
      "Epoch 9407/10000\n",
      "2700/2700 [==============================] - 0s 27us/step - loss: 1.2060e-04 - mean_absolute_error: 0.0065\n",
      "\n",
      "Epoch 09407: loss did not improve\n",
      "Epoch 9408/10000\n",
      "2700/2700 [==============================] - 0s 29us/step - loss: 1.2372e-04 - mean_absolute_error: 0.0067\n",
      "\n",
      "Epoch 09408: loss did not improve\n",
      "Epoch 9409/10000\n",
      "2700/2700 [==============================] - 0s 31us/step - loss: 1.2222e-04 - mean_absolute_error: 0.0064\n",
      "\n",
      "Epoch 09409: loss did not improve\n",
      "Epoch 9410/10000\n",
      "2700/2700 [==============================] - 0s 27us/step - loss: 1.2056e-04 - mean_absolute_error: 0.0064\n",
      "\n",
      "Epoch 09410: loss did not improve\n",
      "Epoch 9411/10000\n",
      "2700/2700 [==============================] - 0s 28us/step - loss: 1.2080e-04 - mean_absolute_error: 0.0065\n",
      "\n",
      "Epoch 09411: loss did not improve\n",
      "Epoch 9412/10000\n",
      "2700/2700 [==============================] - 0s 29us/step - loss: 1.2244e-04 - mean_absolute_error: 0.0065\n",
      "\n",
      "Epoch 09412: loss did not improve\n",
      "Epoch 9413/10000\n",
      "2700/2700 [==============================] - 0s 28us/step - loss: 1.2292e-04 - mean_absolute_error: 0.0066\n",
      "\n",
      "Epoch 09413: loss did not improve\n",
      "Epoch 9414/10000\n",
      "2700/2700 [==============================] - 0s 29us/step - loss: 1.2227e-04 - mean_absolute_error: 0.0065\n",
      "\n",
      "Epoch 09414: loss did not improve\n",
      "Epoch 9415/10000\n",
      "2700/2700 [==============================] - 0s 29us/step - loss: 1.1927e-04 - mean_absolute_error: 0.0062\n",
      "\n",
      "Epoch 09415: loss did not improve\n",
      "Epoch 9416/10000\n",
      "2700/2700 [==============================] - 0s 31us/step - loss: 1.2007e-04 - mean_absolute_error: 0.0062\n",
      "\n",
      "Epoch 09416: loss did not improve\n",
      "Epoch 9417/10000\n",
      "2700/2700 [==============================] - 0s 30us/step - loss: 1.1977e-04 - mean_absolute_error: 0.0062\n",
      "\n",
      "Epoch 09417: loss did not improve\n",
      "Epoch 9418/10000\n",
      "2700/2700 [==============================] - 0s 32us/step - loss: 1.2313e-04 - mean_absolute_error: 0.0065\n",
      "\n",
      "Epoch 09418: loss did not improve\n",
      "Epoch 9419/10000\n",
      "2700/2700 [==============================] - 0s 29us/step - loss: 1.2046e-04 - mean_absolute_error: 0.0063\n",
      "\n",
      "Epoch 09419: loss did not improve\n",
      "Epoch 9420/10000\n",
      "2700/2700 [==============================] - 0s 39us/step - loss: 1.2032e-04 - mean_absolute_error: 0.0063\n",
      "\n",
      "Epoch 09420: loss did not improve\n",
      "Epoch 9421/10000\n",
      "2700/2700 [==============================] - 0s 28us/step - loss: 1.2057e-04 - mean_absolute_error: 0.0064\n",
      "\n",
      "Epoch 09421: loss did not improve\n",
      "Epoch 9422/10000\n",
      "2700/2700 [==============================] - 0s 34us/step - loss: 1.2197e-04 - mean_absolute_error: 0.0064\n",
      "\n",
      "Epoch 09422: loss did not improve\n",
      "Epoch 9423/10000\n",
      "2700/2700 [==============================] - 0s 28us/step - loss: 1.1929e-04 - mean_absolute_error: 0.0063\n",
      "\n",
      "Epoch 09423: loss did not improve\n",
      "Epoch 9424/10000\n",
      "2700/2700 [==============================] - 0s 34us/step - loss: 1.2248e-04 - mean_absolute_error: 0.0065\n",
      "\n",
      "Epoch 09424: loss did not improve\n",
      "Epoch 9425/10000\n",
      "2700/2700 [==============================] - 0s 34us/step - loss: 1.2104e-04 - mean_absolute_error: 0.0065\n",
      "\n",
      "Epoch 09425: loss did not improve\n",
      "Epoch 9426/10000\n",
      "2700/2700 [==============================] - 0s 29us/step - loss: 1.2052e-04 - mean_absolute_error: 0.0063\n",
      "\n",
      "Epoch 09426: loss did not improve\n",
      "Epoch 9427/10000\n",
      "2700/2700 [==============================] - 0s 32us/step - loss: 1.2109e-04 - mean_absolute_error: 0.0064\n",
      "\n",
      "Epoch 09427: loss did not improve\n",
      "Epoch 9428/10000\n",
      "2700/2700 [==============================] - 0s 30us/step - loss: 1.2589e-04 - mean_absolute_error: 0.0068\n",
      "\n",
      "Epoch 09428: loss did not improve\n",
      "Epoch 9429/10000\n",
      "2700/2700 [==============================] - 0s 25us/step - loss: 1.2658e-04 - mean_absolute_error: 0.0068\n",
      "\n",
      "Epoch 09429: loss did not improve\n",
      "Epoch 9430/10000\n",
      "2700/2700 [==============================] - 0s 32us/step - loss: 1.2079e-04 - mean_absolute_error: 0.0064\n",
      "\n",
      "Epoch 09430: loss did not improve\n",
      "Epoch 9431/10000\n",
      "2700/2700 [==============================] - 0s 31us/step - loss: 1.2377e-04 - mean_absolute_error: 0.0067\n",
      "\n",
      "Epoch 09431: loss did not improve\n",
      "Epoch 9432/10000\n",
      "2700/2700 [==============================] - 0s 29us/step - loss: 1.2143e-04 - mean_absolute_error: 0.0064\n",
      "\n",
      "Epoch 09432: loss did not improve\n",
      "Epoch 9433/10000\n",
      "2700/2700 [==============================] - 0s 29us/step - loss: 1.2007e-04 - mean_absolute_error: 0.0064\n",
      "\n",
      "Epoch 09433: loss did not improve\n",
      "Epoch 9434/10000\n",
      "2700/2700 [==============================] - 0s 28us/step - loss: 1.2053e-04 - mean_absolute_error: 0.0064\n",
      "\n",
      "Epoch 09434: loss did not improve\n",
      "Epoch 9435/10000\n",
      "2700/2700 [==============================] - 0s 28us/step - loss: 1.2051e-04 - mean_absolute_error: 0.0065\n",
      "\n",
      "Epoch 09435: loss did not improve\n",
      "Epoch 9436/10000\n",
      "2700/2700 [==============================] - 0s 28us/step - loss: 1.2341e-04 - mean_absolute_error: 0.0068\n",
      "\n",
      "Epoch 09436: loss did not improve\n",
      "Epoch 9437/10000\n",
      "2700/2700 [==============================] - 0s 30us/step - loss: 1.2150e-04 - mean_absolute_error: 0.0065\n",
      "\n",
      "Epoch 09437: loss did not improve\n",
      "Epoch 9438/10000\n",
      "2700/2700 [==============================] - 0s 30us/step - loss: 1.2031e-04 - mean_absolute_error: 0.0064\n",
      "\n",
      "Epoch 09438: loss did not improve\n",
      "Epoch 9439/10000\n",
      "2700/2700 [==============================] - 0s 30us/step - loss: 1.2317e-04 - mean_absolute_error: 0.0065\n",
      "\n",
      "Epoch 09439: loss did not improve\n",
      "Epoch 9440/10000\n",
      "2700/2700 [==============================] - 0s 32us/step - loss: 1.2306e-04 - mean_absolute_error: 0.0066\n",
      "\n",
      "Epoch 09440: loss did not improve\n",
      "Epoch 9441/10000\n",
      "2700/2700 [==============================] - 0s 27us/step - loss: 1.2303e-04 - mean_absolute_error: 0.0066\n",
      "\n",
      "Epoch 09441: loss did not improve\n",
      "Epoch 9442/10000\n",
      "2700/2700 [==============================] - 0s 30us/step - loss: 1.2101e-04 - mean_absolute_error: 0.0063\n",
      "\n",
      "Epoch 09442: loss did not improve\n",
      "Epoch 9443/10000\n",
      "2700/2700 [==============================] - 0s 30us/step - loss: 1.2002e-04 - mean_absolute_error: 0.0062\n",
      "\n",
      "Epoch 09443: loss did not improve\n",
      "Epoch 9444/10000\n",
      "2700/2700 [==============================] - 0s 25us/step - loss: 1.2090e-04 - mean_absolute_error: 0.0065\n",
      "\n",
      "Epoch 09444: loss did not improve\n",
      "Epoch 9445/10000\n",
      "2700/2700 [==============================] - 0s 30us/step - loss: 1.2118e-04 - mean_absolute_error: 0.0064\n",
      "\n",
      "Epoch 09445: loss did not improve\n",
      "Epoch 9446/10000\n",
      "2700/2700 [==============================] - 0s 30us/step - loss: 1.1933e-04 - mean_absolute_error: 0.0062\n",
      "\n",
      "Epoch 09446: loss did not improve\n",
      "Epoch 9447/10000\n",
      "2700/2700 [==============================] - 0s 31us/step - loss: 1.2123e-04 - mean_absolute_error: 0.0063\n",
      "\n",
      "Epoch 09447: loss did not improve\n",
      "Epoch 9448/10000\n",
      "2700/2700 [==============================] - 0s 29us/step - loss: 1.2047e-04 - mean_absolute_error: 0.0064\n",
      "\n",
      "Epoch 09448: loss did not improve\n",
      "Epoch 9449/10000\n",
      "2700/2700 [==============================] - 0s 30us/step - loss: 1.2194e-04 - mean_absolute_error: 0.0065\n",
      "\n",
      "Epoch 09449: loss did not improve\n",
      "Epoch 9450/10000\n",
      "2700/2700 [==============================] - 0s 31us/step - loss: 1.2039e-04 - mean_absolute_error: 0.0064\n",
      "\n",
      "Epoch 09450: loss did not improve\n",
      "Epoch 9451/10000\n",
      "2700/2700 [==============================] - 0s 34us/step - loss: 1.1967e-04 - mean_absolute_error: 0.0062\n",
      "\n",
      "Epoch 09451: loss did not improve\n",
      "Epoch 9452/10000\n",
      "2700/2700 [==============================] - 0s 28us/step - loss: 1.1982e-04 - mean_absolute_error: 0.0064\n",
      "\n",
      "Epoch 09452: loss did not improve\n",
      "Epoch 9453/10000\n",
      "2700/2700 [==============================] - 0s 29us/step - loss: 1.2542e-04 - mean_absolute_error: 0.0067\n",
      "\n",
      "Epoch 09453: loss did not improve\n",
      "Epoch 9454/10000\n",
      "2700/2700 [==============================] - 0s 30us/step - loss: 1.2200e-04 - mean_absolute_error: 0.0065\n",
      "\n",
      "Epoch 09454: loss did not improve\n",
      "Epoch 9455/10000\n",
      "2700/2700 [==============================] - 0s 33us/step - loss: 1.2170e-04 - mean_absolute_error: 0.0065\n",
      "\n",
      "Epoch 09455: loss did not improve\n",
      "Epoch 9456/10000\n",
      "2700/2700 [==============================] - 0s 30us/step - loss: 1.2205e-04 - mean_absolute_error: 0.0064\n",
      "\n",
      "Epoch 09456: loss did not improve\n",
      "Epoch 9457/10000\n",
      "2700/2700 [==============================] - 0s 31us/step - loss: 1.2007e-04 - mean_absolute_error: 0.0064\n",
      "\n",
      "Epoch 09457: loss did not improve\n",
      "Epoch 9458/10000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2700/2700 [==============================] - 0s 29us/step - loss: 1.2202e-04 - mean_absolute_error: 0.0064\n",
      "\n",
      "Epoch 09458: loss did not improve\n",
      "Epoch 9459/10000\n",
      "2700/2700 [==============================] - 0s 35us/step - loss: 1.2060e-04 - mean_absolute_error: 0.0063\n",
      "\n",
      "Epoch 09459: loss did not improve\n",
      "Epoch 9460/10000\n",
      "2700/2700 [==============================] - 0s 28us/step - loss: 1.2141e-04 - mean_absolute_error: 0.0064\n",
      "\n",
      "Epoch 09460: loss did not improve\n",
      "Epoch 9461/10000\n",
      "2700/2700 [==============================] - 0s 29us/step - loss: 1.2068e-04 - mean_absolute_error: 0.0063\n",
      "\n",
      "Epoch 09461: loss did not improve\n",
      "Epoch 9462/10000\n",
      "2700/2700 [==============================] - 0s 31us/step - loss: 1.2101e-04 - mean_absolute_error: 0.0065\n",
      "\n",
      "Epoch 09462: loss did not improve\n",
      "Epoch 9463/10000\n",
      "2700/2700 [==============================] - 0s 31us/step - loss: 1.2284e-04 - mean_absolute_error: 0.0067\n",
      "\n",
      "Epoch 09463: loss did not improve\n",
      "Epoch 9464/10000\n",
      "2700/2700 [==============================] - 0s 30us/step - loss: 1.2069e-04 - mean_absolute_error: 0.0065\n",
      "\n",
      "Epoch 09464: loss did not improve\n",
      "Epoch 9465/10000\n",
      "2700/2700 [==============================] - 0s 30us/step - loss: 1.2192e-04 - mean_absolute_error: 0.0064\n",
      "\n",
      "Epoch 09465: loss did not improve\n",
      "Epoch 9466/10000\n",
      "2700/2700 [==============================] - 0s 31us/step - loss: 1.1979e-04 - mean_absolute_error: 0.0063\n",
      "\n",
      "Epoch 09466: loss did not improve\n",
      "Epoch 9467/10000\n",
      "2700/2700 [==============================] - 0s 31us/step - loss: 1.2028e-04 - mean_absolute_error: 0.0064\n",
      "\n",
      "Epoch 09467: loss did not improve\n",
      "Epoch 9468/10000\n",
      "2700/2700 [==============================] - 0s 37us/step - loss: 1.1916e-04 - mean_absolute_error: 0.0064\n",
      "\n",
      "Epoch 09468: loss did not improve\n",
      "Epoch 9469/10000\n",
      "2700/2700 [==============================] - 0s 38us/step - loss: 1.2040e-04 - mean_absolute_error: 0.0063\n",
      "\n",
      "Epoch 09469: loss did not improve\n",
      "Epoch 9470/10000\n",
      "2700/2700 [==============================] - 0s 31us/step - loss: 1.2033e-04 - mean_absolute_error: 0.0063\n",
      "\n",
      "Epoch 09470: loss did not improve\n",
      "Epoch 9471/10000\n",
      "2700/2700 [==============================] - 0s 31us/step - loss: 1.2193e-04 - mean_absolute_error: 0.0065\n",
      "\n",
      "Epoch 09471: loss did not improve\n",
      "Epoch 9472/10000\n",
      "2700/2700 [==============================] - 0s 30us/step - loss: 1.2000e-04 - mean_absolute_error: 0.0064\n",
      "\n",
      "Epoch 09472: loss did not improve\n",
      "Epoch 9473/10000\n",
      "2700/2700 [==============================] - 0s 34us/step - loss: 1.2088e-04 - mean_absolute_error: 0.0064\n",
      "\n",
      "Epoch 09473: loss did not improve\n",
      "Epoch 9474/10000\n",
      "2700/2700 [==============================] - 0s 30us/step - loss: 1.2218e-04 - mean_absolute_error: 0.0065\n",
      "\n",
      "Epoch 09474: loss did not improve\n",
      "Epoch 9475/10000\n",
      "2700/2700 [==============================] - 0s 27us/step - loss: 1.2348e-04 - mean_absolute_error: 0.0067\n",
      "\n",
      "Epoch 09475: loss did not improve\n",
      "Epoch 9476/10000\n",
      "2700/2700 [==============================] - 0s 29us/step - loss: 1.2557e-04 - mean_absolute_error: 0.0067\n",
      "\n",
      "Epoch 09476: loss did not improve\n",
      "Epoch 9477/10000\n",
      "2700/2700 [==============================] - 0s 29us/step - loss: 1.2096e-04 - mean_absolute_error: 0.0065\n",
      "\n",
      "Epoch 09477: loss did not improve\n",
      "Epoch 9478/10000\n",
      "2700/2700 [==============================] - 0s 29us/step - loss: 1.1987e-04 - mean_absolute_error: 0.0062\n",
      "\n",
      "Epoch 09478: loss did not improve\n",
      "Epoch 9479/10000\n",
      "2700/2700 [==============================] - 0s 30us/step - loss: 1.2146e-04 - mean_absolute_error: 0.0064\n",
      "\n",
      "Epoch 09479: loss did not improve\n",
      "Epoch 9480/10000\n",
      "2700/2700 [==============================] - 0s 27us/step - loss: 1.2140e-04 - mean_absolute_error: 0.0064\n",
      "\n",
      "Epoch 09480: loss did not improve\n",
      "Epoch 9481/10000\n",
      "2700/2700 [==============================] - 0s 30us/step - loss: 1.2064e-04 - mean_absolute_error: 0.0063\n",
      "\n",
      "Epoch 09481: loss did not improve\n",
      "Epoch 9482/10000\n",
      "2700/2700 [==============================] - 0s 29us/step - loss: 1.2342e-04 - mean_absolute_error: 0.0066\n",
      "\n",
      "Epoch 09482: loss did not improve\n",
      "Epoch 9483/10000\n",
      "2700/2700 [==============================] - 0s 33us/step - loss: 1.2152e-04 - mean_absolute_error: 0.0065\n",
      "\n",
      "Epoch 09483: loss did not improve\n",
      "Epoch 9484/10000\n",
      "2700/2700 [==============================] - 0s 30us/step - loss: 1.2352e-04 - mean_absolute_error: 0.0067\n",
      "\n",
      "Epoch 09484: loss did not improve\n",
      "Epoch 9485/10000\n",
      "2700/2700 [==============================] - 0s 31us/step - loss: 1.2567e-04 - mean_absolute_error: 0.0067\n",
      "\n",
      "Epoch 09485: loss did not improve\n",
      "Epoch 9486/10000\n",
      "2700/2700 [==============================] - 0s 28us/step - loss: 1.2045e-04 - mean_absolute_error: 0.0064\n",
      "\n",
      "Epoch 09486: loss did not improve\n",
      "Epoch 9487/10000\n",
      "2700/2700 [==============================] - 0s 31us/step - loss: 1.2061e-04 - mean_absolute_error: 0.0064\n",
      "\n",
      "Epoch 09487: loss did not improve\n",
      "Epoch 9488/10000\n",
      "2700/2700 [==============================] - 0s 30us/step - loss: 1.2160e-04 - mean_absolute_error: 0.0065\n",
      "\n",
      "Epoch 09488: loss did not improve\n",
      "Epoch 9489/10000\n",
      "2700/2700 [==============================] - 0s 27us/step - loss: 1.2065e-04 - mean_absolute_error: 0.0065\n",
      "\n",
      "Epoch 09489: loss did not improve\n",
      "Epoch 9490/10000\n",
      "2700/2700 [==============================] - 0s 26us/step - loss: 1.2252e-04 - mean_absolute_error: 0.0065\n",
      "\n",
      "Epoch 09490: loss did not improve\n",
      "Epoch 9491/10000\n",
      "2700/2700 [==============================] - 0s 29us/step - loss: 1.2295e-04 - mean_absolute_error: 0.0066\n",
      "\n",
      "Epoch 09491: loss did not improve\n",
      "Epoch 9492/10000\n",
      "2700/2700 [==============================] - 0s 30us/step - loss: 1.2042e-04 - mean_absolute_error: 0.0063\n",
      "\n",
      "Epoch 09492: loss did not improve\n",
      "Epoch 9493/10000\n",
      "2700/2700 [==============================] - 0s 29us/step - loss: 1.2178e-04 - mean_absolute_error: 0.0064\n",
      "\n",
      "Epoch 09493: loss did not improve\n",
      "Epoch 9494/10000\n",
      "2700/2700 [==============================] - 0s 27us/step - loss: 1.2005e-04 - mean_absolute_error: 0.0063\n",
      "\n",
      "Epoch 09494: loss did not improve\n",
      "Epoch 9495/10000\n",
      "2700/2700 [==============================] - 0s 31us/step - loss: 1.2090e-04 - mean_absolute_error: 0.0064\n",
      "\n",
      "Epoch 09495: loss did not improve\n",
      "Epoch 9496/10000\n",
      "2700/2700 [==============================] - 0s 32us/step - loss: 1.2307e-04 - mean_absolute_error: 0.0067\n",
      "\n",
      "Epoch 09496: loss did not improve\n",
      "Epoch 9497/10000\n",
      "2700/2700 [==============================] - 0s 30us/step - loss: 1.2208e-04 - mean_absolute_error: 0.0065\n",
      "\n",
      "Epoch 09497: loss did not improve\n",
      "Epoch 9498/10000\n",
      "2700/2700 [==============================] - 0s 32us/step - loss: 1.2032e-04 - mean_absolute_error: 0.0064\n",
      "\n",
      "Epoch 09498: loss did not improve\n",
      "Epoch 9499/10000\n",
      "2700/2700 [==============================] - 0s 29us/step - loss: 1.2174e-04 - mean_absolute_error: 0.0065\n",
      "\n",
      "Epoch 09499: loss did not improve\n",
      "Epoch 9500/10000\n",
      "2700/2700 [==============================] - 0s 31us/step - loss: 1.2242e-04 - mean_absolute_error: 0.0065\n",
      "\n",
      "Epoch 09500: loss did not improve\n",
      "Epoch 9501/10000\n",
      "2700/2700 [==============================] - 0s 31us/step - loss: 1.2407e-04 - mean_absolute_error: 0.0067\n",
      "\n",
      "Epoch 09501: loss did not improve\n",
      "Epoch 9502/10000\n",
      "2700/2700 [==============================] - 0s 30us/step - loss: 1.2018e-04 - mean_absolute_error: 0.0063\n",
      "\n",
      "Epoch 09502: loss did not improve\n",
      "Epoch 9503/10000\n",
      "2700/2700 [==============================] - 0s 31us/step - loss: 1.2289e-04 - mean_absolute_error: 0.0065\n",
      "\n",
      "Epoch 09503: loss did not improve\n",
      "Epoch 9504/10000\n",
      "2700/2700 [==============================] - 0s 28us/step - loss: 1.2044e-04 - mean_absolute_error: 0.0064\n",
      "\n",
      "Epoch 09504: loss did not improve\n",
      "Epoch 9505/10000\n",
      "2700/2700 [==============================] - 0s 26us/step - loss: 1.2315e-04 - mean_absolute_error: 0.0066\n",
      "\n",
      "Epoch 09505: loss did not improve\n",
      "Epoch 9506/10000\n",
      "2700/2700 [==============================] - 0s 30us/step - loss: 1.2072e-04 - mean_absolute_error: 0.0063\n",
      "\n",
      "Epoch 09506: loss did not improve\n",
      "Epoch 9507/10000\n",
      "2700/2700 [==============================] - 0s 30us/step - loss: 1.2306e-04 - mean_absolute_error: 0.0067\n",
      "\n",
      "Epoch 09507: loss did not improve\n",
      "Epoch 9508/10000\n",
      "2700/2700 [==============================] - 0s 30us/step - loss: 1.2290e-04 - mean_absolute_error: 0.0066\n",
      "\n",
      "Epoch 09508: loss did not improve\n",
      "Epoch 9509/10000\n",
      "2700/2700 [==============================] - 0s 33us/step - loss: 1.1998e-04 - mean_absolute_error: 0.0063\n",
      "\n",
      "Epoch 09509: loss did not improve\n",
      "Epoch 9510/10000\n",
      "2700/2700 [==============================] - 0s 30us/step - loss: 1.2306e-04 - mean_absolute_error: 0.0065\n",
      "\n",
      "Epoch 09510: loss did not improve\n",
      "Epoch 9511/10000\n",
      "2700/2700 [==============================] - 0s 28us/step - loss: 1.1970e-04 - mean_absolute_error: 0.0062\n",
      "\n",
      "Epoch 09511: loss did not improve\n",
      "Epoch 9512/10000\n",
      "2700/2700 [==============================] - 0s 30us/step - loss: 1.1881e-04 - mean_absolute_error: 0.0063\n",
      "\n",
      "Epoch 09512: loss did not improve\n",
      "Epoch 9513/10000\n",
      "2700/2700 [==============================] - 0s 30us/step - loss: 1.2132e-04 - mean_absolute_error: 0.0065\n",
      "\n",
      "Epoch 09513: loss did not improve\n",
      "Epoch 9514/10000\n",
      "2700/2700 [==============================] - 0s 29us/step - loss: 1.2070e-04 - mean_absolute_error: 0.0064\n",
      "\n",
      "Epoch 09514: loss did not improve\n",
      "Epoch 9515/10000\n",
      "2700/2700 [==============================] - 0s 31us/step - loss: 1.2004e-04 - mean_absolute_error: 0.0064\n",
      "\n",
      "Epoch 09515: loss did not improve\n",
      "Epoch 9516/10000\n",
      "2700/2700 [==============================] - 0s 29us/step - loss: 1.2141e-04 - mean_absolute_error: 0.0064\n",
      "\n",
      "Epoch 09516: loss did not improve\n",
      "Epoch 9517/10000\n",
      "2700/2700 [==============================] - 0s 29us/step - loss: 1.2406e-04 - mean_absolute_error: 0.0066\n",
      "\n",
      "Epoch 09517: loss did not improve\n",
      "Epoch 9518/10000\n",
      "2700/2700 [==============================] - 0s 30us/step - loss: 1.2049e-04 - mean_absolute_error: 0.0063\n",
      "\n",
      "Epoch 09518: loss did not improve\n",
      "Epoch 9519/10000\n",
      "2700/2700 [==============================] - 0s 31us/step - loss: 1.2053e-04 - mean_absolute_error: 0.0063\n",
      "\n",
      "Epoch 09519: loss did not improve\n",
      "Epoch 9520/10000\n",
      "2700/2700 [==============================] - 0s 29us/step - loss: 1.2002e-04 - mean_absolute_error: 0.0063\n",
      "\n",
      "Epoch 09520: loss did not improve\n",
      "Epoch 9521/10000\n",
      "2700/2700 [==============================] - 0s 27us/step - loss: 1.1954e-04 - mean_absolute_error: 0.0063\n",
      "\n",
      "Epoch 09521: loss did not improve\n",
      "Epoch 9522/10000\n",
      "2700/2700 [==============================] - 0s 30us/step - loss: 1.2112e-04 - mean_absolute_error: 0.0064\n",
      "\n",
      "Epoch 09522: loss did not improve\n",
      "Epoch 9523/10000\n",
      "2700/2700 [==============================] - 0s 29us/step - loss: 1.2266e-04 - mean_absolute_error: 0.0066\n",
      "\n",
      "Epoch 09523: loss did not improve\n",
      "Epoch 9524/10000\n",
      "2700/2700 [==============================] - 0s 29us/step - loss: 1.2009e-04 - mean_absolute_error: 0.0064\n",
      "\n",
      "Epoch 09524: loss did not improve\n",
      "Epoch 9525/10000\n",
      "2700/2700 [==============================] - 0s 29us/step - loss: 1.1832e-04 - mean_absolute_error: 0.0063\n",
      "\n",
      "Epoch 09525: loss improved from 0.00012 to 0.00012, saving model to keras/best_baseline_weights.hdf5\n",
      "Epoch 9526/10000\n",
      "2700/2700 [==============================] - 0s 27us/step - loss: 1.2639e-04 - mean_absolute_error: 0.0069\n",
      "\n",
      "Epoch 09526: loss did not improve\n",
      "Epoch 9527/10000\n",
      "2700/2700 [==============================] - 0s 30us/step - loss: 1.1986e-04 - mean_absolute_error: 0.0063\n",
      "\n",
      "Epoch 09527: loss did not improve\n",
      "Epoch 9528/10000\n",
      "2700/2700 [==============================] - 0s 30us/step - loss: 1.1989e-04 - mean_absolute_error: 0.0063\n",
      "\n",
      "Epoch 09528: loss did not improve\n",
      "Epoch 9529/10000\n",
      "2700/2700 [==============================] - 0s 30us/step - loss: 1.2179e-04 - mean_absolute_error: 0.0065\n",
      "\n",
      "Epoch 09529: loss did not improve\n",
      "Epoch 9530/10000\n",
      "2700/2700 [==============================] - 0s 28us/step - loss: 1.2039e-04 - mean_absolute_error: 0.0063\n",
      "\n",
      "Epoch 09530: loss did not improve\n",
      "Epoch 9531/10000\n",
      "2700/2700 [==============================] - 0s 27us/step - loss: 1.2001e-04 - mean_absolute_error: 0.0063\n",
      "\n",
      "Epoch 09531: loss did not improve\n",
      "Epoch 9532/10000\n",
      "2700/2700 [==============================] - 0s 30us/step - loss: 1.2094e-04 - mean_absolute_error: 0.0065\n",
      "\n",
      "Epoch 09532: loss did not improve\n",
      "Epoch 9533/10000\n",
      "2700/2700 [==============================] - 0s 29us/step - loss: 1.1950e-04 - mean_absolute_error: 0.0063\n",
      "\n",
      "Epoch 09533: loss did not improve\n",
      "Epoch 9534/10000\n",
      "2700/2700 [==============================] - 0s 30us/step - loss: 1.2757e-04 - mean_absolute_error: 0.0069\n",
      "\n",
      "Epoch 09534: loss did not improve\n",
      "Epoch 9535/10000\n",
      "2700/2700 [==============================] - 0s 29us/step - loss: 1.2250e-04 - mean_absolute_error: 0.0065\n",
      "\n",
      "Epoch 09535: loss did not improve\n",
      "Epoch 9536/10000\n",
      "2700/2700 [==============================] - 0s 27us/step - loss: 1.2024e-04 - mean_absolute_error: 0.0063\n",
      "\n",
      "Epoch 09536: loss did not improve\n",
      "Epoch 9537/10000\n",
      "2700/2700 [==============================] - 0s 29us/step - loss: 1.1910e-04 - mean_absolute_error: 0.0062\n",
      "\n",
      "Epoch 09537: loss did not improve\n",
      "Epoch 9538/10000\n",
      "2700/2700 [==============================] - 0s 30us/step - loss: 1.2049e-04 - mean_absolute_error: 0.0064\n",
      "\n",
      "Epoch 09538: loss did not improve\n",
      "Epoch 9539/10000\n",
      "2700/2700 [==============================] - 0s 29us/step - loss: 1.2190e-04 - mean_absolute_error: 0.0066\n",
      "\n",
      "Epoch 09539: loss did not improve\n",
      "Epoch 9540/10000\n",
      "2700/2700 [==============================] - 0s 27us/step - loss: 1.2078e-04 - mean_absolute_error: 0.0064\n",
      "\n",
      "Epoch 09540: loss did not improve\n",
      "Epoch 9541/10000\n",
      "2700/2700 [==============================] - 0s 28us/step - loss: 1.2524e-04 - mean_absolute_error: 0.0067\n",
      "\n",
      "Epoch 09541: loss did not improve\n",
      "Epoch 9542/10000\n",
      "2700/2700 [==============================] - 0s 29us/step - loss: 1.2319e-04 - mean_absolute_error: 0.0067\n",
      "\n",
      "Epoch 09542: loss did not improve\n",
      "Epoch 9543/10000\n",
      "2700/2700 [==============================] - 0s 30us/step - loss: 1.2028e-04 - mean_absolute_error: 0.0064\n",
      "\n",
      "Epoch 09543: loss did not improve\n",
      "Epoch 9544/10000\n",
      "2700/2700 [==============================] - 0s 31us/step - loss: 1.2015e-04 - mean_absolute_error: 0.0064\n",
      "\n",
      "Epoch 09544: loss did not improve\n",
      "Epoch 9545/10000\n",
      "2700/2700 [==============================] - 0s 27us/step - loss: 1.1988e-04 - mean_absolute_error: 0.0062\n",
      "\n",
      "Epoch 09545: loss did not improve\n",
      "Epoch 9546/10000\n",
      "2700/2700 [==============================] - 0s 29us/step - loss: 1.1963e-04 - mean_absolute_error: 0.0063\n",
      "\n",
      "Epoch 09546: loss did not improve\n",
      "Epoch 9547/10000\n",
      "2700/2700 [==============================] - 0s 28us/step - loss: 1.2043e-04 - mean_absolute_error: 0.0064\n",
      "\n",
      "Epoch 09547: loss did not improve\n",
      "Epoch 9548/10000\n",
      "2700/2700 [==============================] - 0s 30us/step - loss: 1.2384e-04 - mean_absolute_error: 0.0068\n",
      "\n",
      "Epoch 09548: loss did not improve\n",
      "Epoch 9549/10000\n",
      "2700/2700 [==============================] - 0s 31us/step - loss: 1.2664e-04 - mean_absolute_error: 0.0069\n",
      "\n",
      "Epoch 09549: loss did not improve\n",
      "Epoch 9550/10000\n",
      "2700/2700 [==============================] - 0s 30us/step - loss: 1.2260e-04 - mean_absolute_error: 0.0066\n",
      "\n",
      "Epoch 09550: loss did not improve\n",
      "Epoch 9551/10000\n",
      "2700/2700 [==============================] - 0s 31us/step - loss: 1.1984e-04 - mean_absolute_error: 0.0063\n",
      "\n",
      "Epoch 09551: loss did not improve\n",
      "Epoch 9552/10000\n",
      "2700/2700 [==============================] - 0s 32us/step - loss: 1.2396e-04 - mean_absolute_error: 0.0067\n",
      "\n",
      "Epoch 09552: loss did not improve\n",
      "Epoch 9553/10000\n",
      "2700/2700 [==============================] - 0s 33us/step - loss: 1.2339e-04 - mean_absolute_error: 0.0066\n",
      "\n",
      "Epoch 09553: loss did not improve\n",
      "Epoch 9554/10000\n",
      "2700/2700 [==============================] - 0s 32us/step - loss: 1.2098e-04 - mean_absolute_error: 0.0064\n",
      "\n",
      "Epoch 09554: loss did not improve\n",
      "Epoch 9555/10000\n",
      "2700/2700 [==============================] - 0s 32us/step - loss: 1.2014e-04 - mean_absolute_error: 0.0064\n",
      "\n",
      "Epoch 09555: loss did not improve\n",
      "Epoch 9556/10000\n",
      "2700/2700 [==============================] - 0s 29us/step - loss: 1.1932e-04 - mean_absolute_error: 0.0063\n",
      "\n",
      "Epoch 09556: loss did not improve\n",
      "Epoch 9557/10000\n",
      "2700/2700 [==============================] - 0s 30us/step - loss: 1.2136e-04 - mean_absolute_error: 0.0065\n",
      "\n",
      "Epoch 09557: loss did not improve\n",
      "Epoch 9558/10000\n",
      "2700/2700 [==============================] - 0s 31us/step - loss: 1.2021e-04 - mean_absolute_error: 0.0064\n",
      "\n",
      "Epoch 09558: loss did not improve\n",
      "Epoch 9559/10000\n",
      "2700/2700 [==============================] - 0s 29us/step - loss: 1.2088e-04 - mean_absolute_error: 0.0064\n",
      "\n",
      "Epoch 09559: loss did not improve\n",
      "Epoch 9560/10000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2700/2700 [==============================] - 0s 30us/step - loss: 1.2050e-04 - mean_absolute_error: 0.0064\n",
      "\n",
      "Epoch 09560: loss did not improve\n",
      "Epoch 9561/10000\n",
      "2700/2700 [==============================] - 0s 30us/step - loss: 1.1928e-04 - mean_absolute_error: 0.0062\n",
      "\n",
      "Epoch 09561: loss did not improve\n",
      "Epoch 9562/10000\n",
      "2700/2700 [==============================] - 0s 30us/step - loss: 1.2762e-04 - mean_absolute_error: 0.0070\n",
      "\n",
      "Epoch 09562: loss did not improve\n",
      "Epoch 9563/10000\n",
      "2700/2700 [==============================] - 0s 30us/step - loss: 1.2087e-04 - mean_absolute_error: 0.0064\n",
      "\n",
      "Epoch 09563: loss did not improve\n",
      "Epoch 9564/10000\n",
      "2700/2700 [==============================] - 0s 33us/step - loss: 1.2004e-04 - mean_absolute_error: 0.0064\n",
      "\n",
      "Epoch 09564: loss did not improve\n",
      "Epoch 9565/10000\n",
      "2700/2700 [==============================] - 0s 29us/step - loss: 1.2035e-04 - mean_absolute_error: 0.0063\n",
      "\n",
      "Epoch 09565: loss did not improve\n",
      "Epoch 9566/10000\n",
      "2700/2700 [==============================] - 0s 29us/step - loss: 1.1997e-04 - mean_absolute_error: 0.0063\n",
      "\n",
      "Epoch 09566: loss did not improve\n",
      "Epoch 9567/10000\n",
      "2700/2700 [==============================] - 0s 27us/step - loss: 1.2124e-04 - mean_absolute_error: 0.0065\n",
      "\n",
      "Epoch 09567: loss did not improve\n",
      "Epoch 9568/10000\n",
      "2700/2700 [==============================] - 0s 29us/step - loss: 1.2020e-04 - mean_absolute_error: 0.0063\n",
      "\n",
      "Epoch 09568: loss did not improve\n",
      "Epoch 9569/10000\n",
      "2700/2700 [==============================] - 0s 28us/step - loss: 1.2133e-04 - mean_absolute_error: 0.0064\n",
      "\n",
      "Epoch 09569: loss did not improve\n",
      "Epoch 9570/10000\n",
      "2700/2700 [==============================] - 0s 30us/step - loss: 1.2298e-04 - mean_absolute_error: 0.0067\n",
      "\n",
      "Epoch 09570: loss did not improve\n",
      "Epoch 9571/10000\n",
      "2700/2700 [==============================] - 0s 31us/step - loss: 1.2415e-04 - mean_absolute_error: 0.0067\n",
      "\n",
      "Epoch 09571: loss did not improve\n",
      "Epoch 9572/10000\n",
      "2700/2700 [==============================] - 0s 27us/step - loss: 1.2232e-04 - mean_absolute_error: 0.0067\n",
      "\n",
      "Epoch 09572: loss did not improve\n",
      "Epoch 9573/10000\n",
      "2700/2700 [==============================] - 0s 30us/step - loss: 1.2487e-04 - mean_absolute_error: 0.0067\n",
      "\n",
      "Epoch 09573: loss did not improve\n",
      "Epoch 9574/10000\n",
      "2700/2700 [==============================] - 0s 31us/step - loss: 1.2149e-04 - mean_absolute_error: 0.0064\n",
      "\n",
      "Epoch 09574: loss did not improve\n",
      "Epoch 9575/10000\n",
      "2700/2700 [==============================] - 0s 34us/step - loss: 1.2496e-04 - mean_absolute_error: 0.0067\n",
      "\n",
      "Epoch 09575: loss did not improve\n",
      "Epoch 9576/10000\n",
      "2700/2700 [==============================] - 0s 36us/step - loss: 1.2036e-04 - mean_absolute_error: 0.0065\n",
      "\n",
      "Epoch 09576: loss did not improve\n",
      "Epoch 9577/10000\n",
      "2700/2700 [==============================] - 0s 32us/step - loss: 1.2029e-04 - mean_absolute_error: 0.0063\n",
      "\n",
      "Epoch 09577: loss did not improve\n",
      "Epoch 9578/10000\n",
      "2700/2700 [==============================] - 0s 26us/step - loss: 1.1949e-04 - mean_absolute_error: 0.0063\n",
      "\n",
      "Epoch 09578: loss did not improve\n",
      "Epoch 9579/10000\n",
      "2700/2700 [==============================] - 0s 29us/step - loss: 1.1926e-04 - mean_absolute_error: 0.0062\n",
      "\n",
      "Epoch 09579: loss did not improve\n",
      "Epoch 9580/10000\n",
      "2700/2700 [==============================] - 0s 37us/step - loss: 1.1927e-04 - mean_absolute_error: 0.0063\n",
      "\n",
      "Epoch 09580: loss did not improve\n",
      "Epoch 9581/10000\n",
      "2700/2700 [==============================] - 0s 31us/step - loss: 1.2165e-04 - mean_absolute_error: 0.0065\n",
      "\n",
      "Epoch 09581: loss did not improve\n",
      "Epoch 9582/10000\n",
      "2700/2700 [==============================] - 0s 34us/step - loss: 1.2657e-04 - mean_absolute_error: 0.0069\n",
      "\n",
      "Epoch 09582: loss did not improve\n",
      "Epoch 9583/10000\n",
      "2700/2700 [==============================] - 0s 30us/step - loss: 1.2267e-04 - mean_absolute_error: 0.0065\n",
      "\n",
      "Epoch 09583: loss did not improve\n",
      "Epoch 9584/10000\n",
      "2700/2700 [==============================] - 0s 29us/step - loss: 1.2060e-04 - mean_absolute_error: 0.0064\n",
      "\n",
      "Epoch 09584: loss did not improve\n",
      "Epoch 9585/10000\n",
      "2700/2700 [==============================] - 0s 28us/step - loss: 1.2291e-04 - mean_absolute_error: 0.0066\n",
      "\n",
      "Epoch 09585: loss did not improve\n",
      "Epoch 9586/10000\n",
      "2700/2700 [==============================] - 0s 30us/step - loss: 1.2176e-04 - mean_absolute_error: 0.0065\n",
      "\n",
      "Epoch 09586: loss did not improve\n",
      "Epoch 9587/10000\n",
      "2700/2700 [==============================] - 0s 30us/step - loss: 1.2255e-04 - mean_absolute_error: 0.0065\n",
      "\n",
      "Epoch 09587: loss did not improve\n",
      "Epoch 9588/10000\n",
      "2700/2700 [==============================] - 0s 29us/step - loss: 1.2080e-04 - mean_absolute_error: 0.0064\n",
      "\n",
      "Epoch 09588: loss did not improve\n",
      "Epoch 9589/10000\n",
      "2700/2700 [==============================] - 0s 33us/step - loss: 1.2328e-04 - mean_absolute_error: 0.0065\n",
      "\n",
      "Epoch 09589: loss did not improve\n",
      "Epoch 9590/10000\n",
      "2700/2700 [==============================] - 0s 35us/step - loss: 1.1961e-04 - mean_absolute_error: 0.0063\n",
      "\n",
      "Epoch 09590: loss did not improve\n",
      "Epoch 9591/10000\n",
      "2700/2700 [==============================] - 0s 30us/step - loss: 1.2045e-04 - mean_absolute_error: 0.0063\n",
      "\n",
      "Epoch 09591: loss did not improve\n",
      "Epoch 9592/10000\n",
      "2700/2700 [==============================] - 0s 28us/step - loss: 1.2256e-04 - mean_absolute_error: 0.0065\n",
      "\n",
      "Epoch 09592: loss did not improve\n",
      "Epoch 9593/10000\n",
      "2700/2700 [==============================] - 0s 31us/step - loss: 1.2270e-04 - mean_absolute_error: 0.0065\n",
      "\n",
      "Epoch 09593: loss did not improve\n",
      "Epoch 9594/10000\n",
      "2700/2700 [==============================] - 0s 28us/step - loss: 1.2061e-04 - mean_absolute_error: 0.0064\n",
      "\n",
      "Epoch 09594: loss did not improve\n",
      "Epoch 9595/10000\n",
      "2700/2700 [==============================] - 0s 29us/step - loss: 1.2127e-04 - mean_absolute_error: 0.0064\n",
      "\n",
      "Epoch 09595: loss did not improve\n",
      "Epoch 9596/10000\n",
      "2700/2700 [==============================] - 0s 31us/step - loss: 1.1984e-04 - mean_absolute_error: 0.0063\n",
      "\n",
      "Epoch 09596: loss did not improve\n",
      "Epoch 9597/10000\n",
      "2700/2700 [==============================] - 0s 28us/step - loss: 1.2286e-04 - mean_absolute_error: 0.0066\n",
      "\n",
      "Epoch 09597: loss did not improve\n",
      "Epoch 9598/10000\n",
      "2700/2700 [==============================] - 0s 30us/step - loss: 1.2128e-04 - mean_absolute_error: 0.0064\n",
      "\n",
      "Epoch 09598: loss did not improve\n",
      "Epoch 9599/10000\n",
      "2700/2700 [==============================] - 0s 30us/step - loss: 1.2034e-04 - mean_absolute_error: 0.0064\n",
      "\n",
      "Epoch 09599: loss did not improve\n",
      "Epoch 9600/10000\n",
      "2700/2700 [==============================] - 0s 28us/step - loss: 1.2160e-04 - mean_absolute_error: 0.0065\n",
      "\n",
      "Epoch 09600: loss did not improve\n",
      "Epoch 9601/10000\n",
      "2700/2700 [==============================] - 0s 30us/step - loss: 1.1952e-04 - mean_absolute_error: 0.0063\n",
      "\n",
      "Epoch 09601: loss did not improve\n",
      "Epoch 9602/10000\n",
      "2700/2700 [==============================] - 0s 30us/step - loss: 1.2356e-04 - mean_absolute_error: 0.0066\n",
      "\n",
      "Epoch 09602: loss did not improve\n",
      "Epoch 9603/10000\n",
      "2700/2700 [==============================] - 0s 29us/step - loss: 1.2220e-04 - mean_absolute_error: 0.0066\n",
      "\n",
      "Epoch 09603: loss did not improve\n",
      "Epoch 9604/10000\n",
      "2700/2700 [==============================] - 0s 31us/step - loss: 1.2699e-04 - mean_absolute_error: 0.0069\n",
      "\n",
      "Epoch 09604: loss did not improve\n",
      "Epoch 9605/10000\n",
      "2700/2700 [==============================] - 0s 52us/step - loss: 1.2045e-04 - mean_absolute_error: 0.0064\n",
      "\n",
      "Epoch 09605: loss did not improve\n",
      "Epoch 9606/10000\n",
      "2700/2700 [==============================] - 0s 28us/step - loss: 1.2294e-04 - mean_absolute_error: 0.0067\n",
      "\n",
      "Epoch 09606: loss did not improve\n",
      "Epoch 9607/10000\n",
      "2700/2700 [==============================] - 0s 30us/step - loss: 1.1966e-04 - mean_absolute_error: 0.0063\n",
      "\n",
      "Epoch 09607: loss did not improve\n",
      "Epoch 9608/10000\n",
      "2700/2700 [==============================] - 0s 30us/step - loss: 1.1948e-04 - mean_absolute_error: 0.0063\n",
      "\n",
      "Epoch 09608: loss did not improve\n",
      "Epoch 9609/10000\n",
      "2700/2700 [==============================] - 0s 43us/step - loss: 1.2160e-04 - mean_absolute_error: 0.0065\n",
      "\n",
      "Epoch 09609: loss did not improve\n",
      "Epoch 9610/10000\n",
      "2700/2700 [==============================] - 0s 36us/step - loss: 1.2462e-04 - mean_absolute_error: 0.0067\n",
      "\n",
      "Epoch 09610: loss did not improve\n",
      "Epoch 9611/10000\n",
      "2700/2700 [==============================] - 0s 30us/step - loss: 1.1991e-04 - mean_absolute_error: 0.0063\n",
      "\n",
      "Epoch 09611: loss did not improve\n",
      "Epoch 9612/10000\n",
      "2700/2700 [==============================] - 0s 29us/step - loss: 1.2111e-04 - mean_absolute_error: 0.0065\n",
      "\n",
      "Epoch 09612: loss did not improve\n",
      "Epoch 9613/10000\n",
      "2700/2700 [==============================] - 0s 30us/step - loss: 1.2199e-04 - mean_absolute_error: 0.0064\n",
      "\n",
      "Epoch 09613: loss did not improve\n",
      "Epoch 9614/10000\n",
      "2700/2700 [==============================] - 0s 31us/step - loss: 1.2477e-04 - mean_absolute_error: 0.0068\n",
      "\n",
      "Epoch 09614: loss did not improve\n",
      "Epoch 9615/10000\n",
      "2700/2700 [==============================] - 0s 39us/step - loss: 1.2558e-04 - mean_absolute_error: 0.0067\n",
      "\n",
      "Epoch 09615: loss did not improve\n",
      "Epoch 9616/10000\n",
      "2700/2700 [==============================] - 0s 29us/step - loss: 1.2069e-04 - mean_absolute_error: 0.0064\n",
      "\n",
      "Epoch 09616: loss did not improve\n",
      "Epoch 9617/10000\n",
      "2700/2700 [==============================] - 0s 29us/step - loss: 1.2264e-04 - mean_absolute_error: 0.0067\n",
      "\n",
      "Epoch 09617: loss did not improve\n",
      "Epoch 9618/10000\n",
      "2700/2700 [==============================] - 0s 28us/step - loss: 1.1994e-04 - mean_absolute_error: 0.0064\n",
      "\n",
      "Epoch 09618: loss did not improve\n",
      "Epoch 9619/10000\n",
      "2700/2700 [==============================] - 0s 30us/step - loss: 1.2143e-04 - mean_absolute_error: 0.0065\n",
      "\n",
      "Epoch 09619: loss did not improve\n",
      "Epoch 9620/10000\n",
      "2700/2700 [==============================] - 0s 30us/step - loss: 1.1952e-04 - mean_absolute_error: 0.0063\n",
      "\n",
      "Epoch 09620: loss did not improve\n",
      "Epoch 9621/10000\n",
      "2700/2700 [==============================] - 0s 31us/step - loss: 1.2012e-04 - mean_absolute_error: 0.0063\n",
      "\n",
      "Epoch 09621: loss did not improve\n",
      "Epoch 9622/10000\n",
      "2700/2700 [==============================] - 0s 28us/step - loss: 1.2183e-04 - mean_absolute_error: 0.0064\n",
      "\n",
      "Epoch 09622: loss did not improve\n",
      "Epoch 9623/10000\n",
      "2700/2700 [==============================] - 0s 28us/step - loss: 1.2059e-04 - mean_absolute_error: 0.0063\n",
      "\n",
      "Epoch 09623: loss did not improve\n",
      "Epoch 9624/10000\n",
      "2700/2700 [==============================] - 0s 29us/step - loss: 1.2566e-04 - mean_absolute_error: 0.0067\n",
      "\n",
      "Epoch 09624: loss did not improve\n",
      "Epoch 9625/10000\n",
      "2700/2700 [==============================] - 0s 32us/step - loss: 1.2064e-04 - mean_absolute_error: 0.0064\n",
      "\n",
      "Epoch 09625: loss did not improve\n",
      "Epoch 9626/10000\n",
      "2700/2700 [==============================] - 0s 34us/step - loss: 1.2176e-04 - mean_absolute_error: 0.0065\n",
      "\n",
      "Epoch 09626: loss did not improve\n",
      "Epoch 9627/10000\n",
      "2700/2700 [==============================] - 0s 33us/step - loss: 1.1986e-04 - mean_absolute_error: 0.0064\n",
      "\n",
      "Epoch 09627: loss did not improve\n",
      "Epoch 9628/10000\n",
      "2700/2700 [==============================] - 0s 29us/step - loss: 1.2131e-04 - mean_absolute_error: 0.0064\n",
      "\n",
      "Epoch 09628: loss did not improve\n",
      "Epoch 9629/10000\n",
      "2700/2700 [==============================] - 0s 34us/step - loss: 1.1999e-04 - mean_absolute_error: 0.0063\n",
      "\n",
      "Epoch 09629: loss did not improve\n",
      "Epoch 9630/10000\n",
      "2700/2700 [==============================] - 0s 28us/step - loss: 1.2077e-04 - mean_absolute_error: 0.0064\n",
      "\n",
      "Epoch 09630: loss did not improve\n",
      "Epoch 9631/10000\n",
      "2700/2700 [==============================] - 0s 31us/step - loss: 1.2066e-04 - mean_absolute_error: 0.0064\n",
      "\n",
      "Epoch 09631: loss did not improve\n",
      "Epoch 9632/10000\n",
      "2700/2700 [==============================] - 0s 32us/step - loss: 1.2187e-04 - mean_absolute_error: 0.0065\n",
      "\n",
      "Epoch 09632: loss did not improve\n",
      "Epoch 9633/10000\n",
      "2700/2700 [==============================] - 0s 36us/step - loss: 1.2217e-04 - mean_absolute_error: 0.0066\n",
      "\n",
      "Epoch 09633: loss did not improve\n",
      "Epoch 9634/10000\n",
      "2700/2700 [==============================] - 0s 29us/step - loss: 1.2196e-04 - mean_absolute_error: 0.0066\n",
      "\n",
      "Epoch 09634: loss did not improve\n",
      "Epoch 9635/10000\n",
      "2700/2700 [==============================] - 0s 32us/step - loss: 1.2153e-04 - mean_absolute_error: 0.0066\n",
      "\n",
      "Epoch 09635: loss did not improve\n",
      "Epoch 9636/10000\n",
      "2700/2700 [==============================] - 0s 33us/step - loss: 1.2279e-04 - mean_absolute_error: 0.0067\n",
      "\n",
      "Epoch 09636: loss did not improve\n",
      "Epoch 9637/10000\n",
      "2700/2700 [==============================] - 0s 27us/step - loss: 1.2077e-04 - mean_absolute_error: 0.0064\n",
      "\n",
      "Epoch 09637: loss did not improve\n",
      "Epoch 9638/10000\n",
      "2700/2700 [==============================] - 0s 31us/step - loss: 1.2194e-04 - mean_absolute_error: 0.0066\n",
      "\n",
      "Epoch 09638: loss did not improve\n",
      "Epoch 9639/10000\n",
      "2700/2700 [==============================] - 0s 30us/step - loss: 1.2172e-04 - mean_absolute_error: 0.0065\n",
      "\n",
      "Epoch 09639: loss did not improve\n",
      "Epoch 9640/10000\n",
      "2700/2700 [==============================] - 0s 32us/step - loss: 1.2188e-04 - mean_absolute_error: 0.0064\n",
      "\n",
      "Epoch 09640: loss did not improve\n",
      "Epoch 9641/10000\n",
      "2700/2700 [==============================] - 0s 28us/step - loss: 1.1923e-04 - mean_absolute_error: 0.0064\n",
      "\n",
      "Epoch 09641: loss did not improve\n",
      "Epoch 9642/10000\n",
      "2700/2700 [==============================] - 0s 27us/step - loss: 1.2035e-04 - mean_absolute_error: 0.0064\n",
      "\n",
      "Epoch 09642: loss did not improve\n",
      "Epoch 9643/10000\n",
      "2700/2700 [==============================] - 0s 33us/step - loss: 1.2335e-04 - mean_absolute_error: 0.0067\n",
      "\n",
      "Epoch 09643: loss did not improve\n",
      "Epoch 9644/10000\n",
      "2700/2700 [==============================] - 0s 30us/step - loss: 1.2155e-04 - mean_absolute_error: 0.0064\n",
      "\n",
      "Epoch 09644: loss did not improve\n",
      "Epoch 9645/10000\n",
      "2700/2700 [==============================] - 0s 30us/step - loss: 1.2110e-04 - mean_absolute_error: 0.0065\n",
      "\n",
      "Epoch 09645: loss did not improve\n",
      "Epoch 9646/10000\n",
      "2700/2700 [==============================] - 0s 34us/step - loss: 1.2025e-04 - mean_absolute_error: 0.0064\n",
      "\n",
      "Epoch 09646: loss did not improve\n",
      "Epoch 9647/10000\n",
      "2700/2700 [==============================] - 0s 27us/step - loss: 1.2289e-04 - mean_absolute_error: 0.0065\n",
      "\n",
      "Epoch 09647: loss did not improve\n",
      "Epoch 9648/10000\n",
      "2700/2700 [==============================] - 0s 31us/step - loss: 1.2247e-04 - mean_absolute_error: 0.0066\n",
      "\n",
      "Epoch 09648: loss did not improve\n",
      "Epoch 9649/10000\n",
      "2700/2700 [==============================] - 0s 30us/step - loss: 1.2037e-04 - mean_absolute_error: 0.0064\n",
      "\n",
      "Epoch 09649: loss did not improve\n",
      "Epoch 9650/10000\n",
      "2700/2700 [==============================] - 0s 30us/step - loss: 1.2084e-04 - mean_absolute_error: 0.0065\n",
      "\n",
      "Epoch 09650: loss did not improve\n",
      "Epoch 9651/10000\n",
      "2700/2700 [==============================] - 0s 28us/step - loss: 1.2359e-04 - mean_absolute_error: 0.0066\n",
      "\n",
      "Epoch 09651: loss did not improve\n",
      "Epoch 9652/10000\n",
      "2700/2700 [==============================] - 0s 27us/step - loss: 1.1975e-04 - mean_absolute_error: 0.0063\n",
      "\n",
      "Epoch 09652: loss did not improve\n",
      "Epoch 9653/10000\n",
      "2700/2700 [==============================] - 0s 33us/step - loss: 1.2146e-04 - mean_absolute_error: 0.0066\n",
      "\n",
      "Epoch 09653: loss did not improve\n",
      "Epoch 9654/10000\n",
      "2700/2700 [==============================] - 0s 32us/step - loss: 1.2324e-04 - mean_absolute_error: 0.0066\n",
      "\n",
      "Epoch 09654: loss did not improve\n",
      "Epoch 9655/10000\n",
      "2700/2700 [==============================] - 0s 34us/step - loss: 1.1998e-04 - mean_absolute_error: 0.0063\n",
      "\n",
      "Epoch 09655: loss did not improve\n",
      "Epoch 9656/10000\n",
      "2700/2700 [==============================] - 0s 32us/step - loss: 1.2145e-04 - mean_absolute_error: 0.0064\n",
      "\n",
      "Epoch 09656: loss did not improve\n",
      "Epoch 9657/10000\n",
      "2700/2700 [==============================] - 0s 30us/step - loss: 1.2000e-04 - mean_absolute_error: 0.0063\n",
      "\n",
      "Epoch 09657: loss did not improve\n",
      "Epoch 9658/10000\n",
      "2700/2700 [==============================] - 0s 34us/step - loss: 1.1937e-04 - mean_absolute_error: 0.0063\n",
      "\n",
      "Epoch 09658: loss did not improve\n",
      "Epoch 9659/10000\n",
      "2700/2700 [==============================] - 0s 32us/step - loss: 1.2073e-04 - mean_absolute_error: 0.0064\n",
      "\n",
      "Epoch 09659: loss did not improve\n",
      "Epoch 9660/10000\n",
      "2700/2700 [==============================] - 0s 30us/step - loss: 1.2026e-04 - mean_absolute_error: 0.0063\n",
      "\n",
      "Epoch 09660: loss did not improve\n",
      "Epoch 9661/10000\n",
      "2700/2700 [==============================] - 0s 31us/step - loss: 1.2501e-04 - mean_absolute_error: 0.0068\n",
      "\n",
      "Epoch 09661: loss did not improve\n",
      "Epoch 9662/10000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2700/2700 [==============================] - 0s 30us/step - loss: 1.2229e-04 - mean_absolute_error: 0.0065\n",
      "\n",
      "Epoch 09662: loss did not improve\n",
      "Epoch 9663/10000\n",
      "2700/2700 [==============================] - 0s 27us/step - loss: 1.2543e-04 - mean_absolute_error: 0.0069\n",
      "\n",
      "Epoch 09663: loss did not improve\n",
      "Epoch 9664/10000\n",
      "2700/2700 [==============================] - 0s 28us/step - loss: 1.2030e-04 - mean_absolute_error: 0.0065\n",
      "\n",
      "Epoch 09664: loss did not improve\n",
      "Epoch 9665/10000\n",
      "2700/2700 [==============================] - 0s 30us/step - loss: 1.2357e-04 - mean_absolute_error: 0.0066\n",
      "\n",
      "Epoch 09665: loss did not improve\n",
      "Epoch 9666/10000\n",
      "2700/2700 [==============================] - 0s 31us/step - loss: 1.2044e-04 - mean_absolute_error: 0.0064\n",
      "\n",
      "Epoch 09666: loss did not improve\n",
      "Epoch 9667/10000\n",
      "2700/2700 [==============================] - 0s 28us/step - loss: 1.2371e-04 - mean_absolute_error: 0.0067\n",
      "\n",
      "Epoch 09667: loss did not improve\n",
      "Epoch 9668/10000\n",
      "2700/2700 [==============================] - 0s 30us/step - loss: 1.2370e-04 - mean_absolute_error: 0.0066\n",
      "\n",
      "Epoch 09668: loss did not improve\n",
      "Epoch 9669/10000\n",
      "2700/2700 [==============================] - 0s 28us/step - loss: 1.1836e-04 - mean_absolute_error: 0.0062\n",
      "\n",
      "Epoch 09669: loss did not improve\n",
      "Epoch 9670/10000\n",
      "2700/2700 [==============================] - 0s 35us/step - loss: 1.2087e-04 - mean_absolute_error: 0.0064\n",
      "\n",
      "Epoch 09670: loss did not improve\n",
      "Epoch 9671/10000\n",
      "2700/2700 [==============================] - 0s 29us/step - loss: 1.2014e-04 - mean_absolute_error: 0.0063\n",
      "\n",
      "Epoch 09671: loss did not improve\n",
      "Epoch 9672/10000\n",
      "2700/2700 [==============================] - 0s 33us/step - loss: 1.2115e-04 - mean_absolute_error: 0.0065\n",
      "\n",
      "Epoch 09672: loss did not improve\n",
      "Epoch 9673/10000\n",
      "2700/2700 [==============================] - 0s 32us/step - loss: 1.1957e-04 - mean_absolute_error: 0.0063\n",
      "\n",
      "Epoch 09673: loss did not improve\n",
      "Epoch 9674/10000\n",
      "2700/2700 [==============================] - 0s 30us/step - loss: 1.2222e-04 - mean_absolute_error: 0.0065\n",
      "\n",
      "Epoch 09674: loss did not improve\n",
      "Epoch 9675/10000\n",
      "2700/2700 [==============================] - 0s 27us/step - loss: 1.2054e-04 - mean_absolute_error: 0.0064\n",
      "\n",
      "Epoch 09675: loss did not improve\n",
      "Epoch 9676/10000\n",
      "2700/2700 [==============================] - 0s 28us/step - loss: 1.2214e-04 - mean_absolute_error: 0.0065\n",
      "\n",
      "Epoch 09676: loss did not improve\n",
      "Epoch 9677/10000\n",
      "2700/2700 [==============================] - 0s 29us/step - loss: 1.2353e-04 - mean_absolute_error: 0.0067\n",
      "\n",
      "Epoch 09677: loss did not improve\n",
      "Epoch 9678/10000\n",
      "2700/2700 [==============================] - 0s 29us/step - loss: 1.2279e-04 - mean_absolute_error: 0.0067\n",
      "\n",
      "Epoch 09678: loss did not improve\n",
      "Epoch 9679/10000\n",
      "2700/2700 [==============================] - 0s 29us/step - loss: 1.1971e-04 - mean_absolute_error: 0.0063\n",
      "\n",
      "Epoch 09679: loss did not improve\n",
      "Epoch 9680/10000\n",
      "2700/2700 [==============================] - 0s 27us/step - loss: 1.2119e-04 - mean_absolute_error: 0.0063\n",
      "\n",
      "Epoch 09680: loss did not improve\n",
      "Epoch 9681/10000\n",
      "2700/2700 [==============================] - 0s 32us/step - loss: 1.2364e-04 - mean_absolute_error: 0.0066\n",
      "\n",
      "Epoch 09681: loss did not improve\n",
      "Epoch 9682/10000\n",
      "2700/2700 [==============================] - 0s 34us/step - loss: 1.2717e-04 - mean_absolute_error: 0.0068\n",
      "\n",
      "Epoch 09682: loss did not improve\n",
      "Epoch 9683/10000\n",
      "2700/2700 [==============================] - 0s 31us/step - loss: 1.2446e-04 - mean_absolute_error: 0.0067\n",
      "\n",
      "Epoch 09683: loss did not improve\n",
      "Epoch 9684/10000\n",
      "2700/2700 [==============================] - 0s 27us/step - loss: 1.2139e-04 - mean_absolute_error: 0.0064\n",
      "\n",
      "Epoch 09684: loss did not improve\n",
      "Epoch 9685/10000\n",
      "2700/2700 [==============================] - 0s 32us/step - loss: 1.2365e-04 - mean_absolute_error: 0.0066\n",
      "\n",
      "Epoch 09685: loss did not improve\n",
      "Epoch 9686/10000\n",
      "2700/2700 [==============================] - 0s 27us/step - loss: 1.2018e-04 - mean_absolute_error: 0.0063\n",
      "\n",
      "Epoch 09686: loss did not improve\n",
      "Epoch 9687/10000\n",
      "2700/2700 [==============================] - 0s 34us/step - loss: 1.2092e-04 - mean_absolute_error: 0.0064\n",
      "\n",
      "Epoch 09687: loss did not improve\n",
      "Epoch 9688/10000\n",
      "2700/2700 [==============================] - 0s 32us/step - loss: 1.2194e-04 - mean_absolute_error: 0.0067\n",
      "\n",
      "Epoch 09688: loss did not improve\n",
      "Epoch 9689/10000\n",
      "2700/2700 [==============================] - 0s 31us/step - loss: 1.2131e-04 - mean_absolute_error: 0.0065\n",
      "\n",
      "Epoch 09689: loss did not improve\n",
      "Epoch 9690/10000\n",
      "2700/2700 [==============================] - 0s 33us/step - loss: 1.2050e-04 - mean_absolute_error: 0.0064\n",
      "\n",
      "Epoch 09690: loss did not improve\n",
      "Epoch 9691/10000\n",
      "2700/2700 [==============================] - 0s 31us/step - loss: 1.1971e-04 - mean_absolute_error: 0.0063\n",
      "\n",
      "Epoch 09691: loss did not improve\n",
      "Epoch 9692/10000\n",
      "2700/2700 [==============================] - 0s 32us/step - loss: 1.2143e-04 - mean_absolute_error: 0.0064\n",
      "\n",
      "Epoch 09692: loss did not improve\n",
      "Epoch 9693/10000\n",
      "2700/2700 [==============================] - 0s 27us/step - loss: 1.2237e-04 - mean_absolute_error: 0.0065\n",
      "\n",
      "Epoch 09693: loss did not improve\n",
      "Epoch 9694/10000\n",
      "2700/2700 [==============================] - 0s 30us/step - loss: 1.2043e-04 - mean_absolute_error: 0.0064\n",
      "\n",
      "Epoch 09694: loss did not improve\n",
      "Epoch 9695/10000\n",
      "2700/2700 [==============================] - 0s 29us/step - loss: 1.2173e-04 - mean_absolute_error: 0.0065\n",
      "\n",
      "Epoch 09695: loss did not improve\n",
      "Epoch 9696/10000\n",
      "2700/2700 [==============================] - 0s 34us/step - loss: 1.2080e-04 - mean_absolute_error: 0.0065\n",
      "\n",
      "Epoch 09696: loss did not improve\n",
      "Epoch 9697/10000\n",
      "2700/2700 [==============================] - 0s 31us/step - loss: 1.2056e-04 - mean_absolute_error: 0.0063\n",
      "\n",
      "Epoch 09697: loss did not improve\n",
      "Epoch 9698/10000\n",
      "2700/2700 [==============================] - 0s 29us/step - loss: 1.2094e-04 - mean_absolute_error: 0.0064\n",
      "\n",
      "Epoch 09698: loss did not improve\n",
      "Epoch 9699/10000\n",
      "2700/2700 [==============================] - 0s 33us/step - loss: 1.2113e-04 - mean_absolute_error: 0.0063\n",
      "\n",
      "Epoch 09699: loss did not improve\n",
      "Epoch 9700/10000\n",
      "2700/2700 [==============================] - 0s 32us/step - loss: 1.2267e-04 - mean_absolute_error: 0.0065\n",
      "\n",
      "Epoch 09700: loss did not improve\n",
      "Epoch 9701/10000\n",
      "2700/2700 [==============================] - 0s 30us/step - loss: 1.2382e-04 - mean_absolute_error: 0.0067\n",
      "\n",
      "Epoch 09701: loss did not improve\n",
      "Epoch 9702/10000\n",
      "2700/2700 [==============================] - 0s 33us/step - loss: 1.2376e-04 - mean_absolute_error: 0.0066\n",
      "\n",
      "Epoch 09702: loss did not improve\n",
      "Epoch 9703/10000\n",
      "2700/2700 [==============================] - 0s 36us/step - loss: 1.2095e-04 - mean_absolute_error: 0.0064\n",
      "\n",
      "Epoch 09703: loss did not improve\n",
      "Epoch 9704/10000\n",
      "2700/2700 [==============================] - 0s 30us/step - loss: 1.2102e-04 - mean_absolute_error: 0.0063\n",
      "\n",
      "Epoch 09704: loss did not improve\n",
      "Epoch 9705/10000\n",
      "2700/2700 [==============================] - 0s 27us/step - loss: 1.2270e-04 - mean_absolute_error: 0.0066\n",
      "\n",
      "Epoch 09705: loss did not improve\n",
      "Epoch 9706/10000\n",
      "2700/2700 [==============================] - 0s 32us/step - loss: 1.2151e-04 - mean_absolute_error: 0.0065\n",
      "\n",
      "Epoch 09706: loss did not improve\n",
      "Epoch 9707/10000\n",
      "2700/2700 [==============================] - 0s 30us/step - loss: 1.2401e-04 - mean_absolute_error: 0.0067\n",
      "\n",
      "Epoch 09707: loss did not improve\n",
      "Epoch 9708/10000\n",
      "2700/2700 [==============================] - 0s 35us/step - loss: 1.2006e-04 - mean_absolute_error: 0.0063\n",
      "\n",
      "Epoch 09708: loss did not improve\n",
      "Epoch 9709/10000\n",
      "2700/2700 [==============================] - 0s 30us/step - loss: 1.2130e-04 - mean_absolute_error: 0.0065\n",
      "\n",
      "Epoch 09709: loss did not improve\n",
      "Epoch 9710/10000\n",
      "2700/2700 [==============================] - 0s 29us/step - loss: 1.1999e-04 - mean_absolute_error: 0.0064\n",
      "\n",
      "Epoch 09710: loss did not improve\n",
      "Epoch 9711/10000\n",
      "2700/2700 [==============================] - 0s 27us/step - loss: 1.2341e-04 - mean_absolute_error: 0.0068\n",
      "\n",
      "Epoch 09711: loss did not improve\n",
      "Epoch 9712/10000\n",
      "2700/2700 [==============================] - 0s 29us/step - loss: 1.1996e-04 - mean_absolute_error: 0.0063\n",
      "\n",
      "Epoch 09712: loss did not improve\n",
      "Epoch 9713/10000\n",
      "2700/2700 [==============================] - 0s 31us/step - loss: 1.2184e-04 - mean_absolute_error: 0.0064\n",
      "\n",
      "Epoch 09713: loss did not improve\n",
      "Epoch 9714/10000\n",
      "2700/2700 [==============================] - 0s 30us/step - loss: 1.2142e-04 - mean_absolute_error: 0.0064\n",
      "\n",
      "Epoch 09714: loss did not improve\n",
      "Epoch 9715/10000\n",
      "2700/2700 [==============================] - 0s 29us/step - loss: 1.2679e-04 - mean_absolute_error: 0.0070\n",
      "\n",
      "Epoch 09715: loss did not improve\n",
      "Epoch 9716/10000\n",
      "2700/2700 [==============================] - 0s 28us/step - loss: 1.2006e-04 - mean_absolute_error: 0.0064\n",
      "\n",
      "Epoch 09716: loss did not improve\n",
      "Epoch 9717/10000\n",
      "2700/2700 [==============================] - 0s 29us/step - loss: 1.2035e-04 - mean_absolute_error: 0.0064\n",
      "\n",
      "Epoch 09717: loss did not improve\n",
      "Epoch 9718/10000\n",
      "2700/2700 [==============================] - 0s 29us/step - loss: 1.2026e-04 - mean_absolute_error: 0.0063\n",
      "\n",
      "Epoch 09718: loss did not improve\n",
      "Epoch 9719/10000\n",
      "2700/2700 [==============================] - 0s 28us/step - loss: 1.1943e-04 - mean_absolute_error: 0.0063\n",
      "\n",
      "Epoch 09719: loss did not improve\n",
      "Epoch 9720/10000\n",
      "2700/2700 [==============================] - 0s 27us/step - loss: 1.2768e-04 - mean_absolute_error: 0.0070\n",
      "\n",
      "Epoch 09720: loss did not improve\n",
      "Epoch 9721/10000\n",
      "2700/2700 [==============================] - 0s 29us/step - loss: 1.2061e-04 - mean_absolute_error: 0.0064\n",
      "\n",
      "Epoch 09721: loss did not improve\n",
      "Epoch 9722/10000\n",
      "2700/2700 [==============================] - 0s 29us/step - loss: 1.2472e-04 - mean_absolute_error: 0.0066\n",
      "\n",
      "Epoch 09722: loss did not improve\n",
      "Epoch 9723/10000\n",
      "2700/2700 [==============================] - 0s 28us/step - loss: 1.2100e-04 - mean_absolute_error: 0.0065\n",
      "\n",
      "Epoch 09723: loss did not improve\n",
      "Epoch 9724/10000\n",
      "2700/2700 [==============================] - 0s 29us/step - loss: 1.2073e-04 - mean_absolute_error: 0.0064\n",
      "\n",
      "Epoch 09724: loss did not improve\n",
      "Epoch 9725/10000\n",
      "2700/2700 [==============================] - 0s 27us/step - loss: 1.2180e-04 - mean_absolute_error: 0.0064\n",
      "\n",
      "Epoch 09725: loss did not improve\n",
      "Epoch 9726/10000\n",
      "2700/2700 [==============================] - 0s 26us/step - loss: 1.2028e-04 - mean_absolute_error: 0.0063\n",
      "\n",
      "Epoch 09726: loss did not improve\n",
      "Epoch 9727/10000\n",
      "2700/2700 [==============================] - 0s 30us/step - loss: 1.2081e-04 - mean_absolute_error: 0.0064\n",
      "\n",
      "Epoch 09727: loss did not improve\n",
      "Epoch 9728/10000\n",
      "2700/2700 [==============================] - 0s 28us/step - loss: 1.2158e-04 - mean_absolute_error: 0.0065\n",
      "\n",
      "Epoch 09728: loss did not improve\n",
      "Epoch 9729/10000\n",
      "2700/2700 [==============================] - 0s 27us/step - loss: 1.2107e-04 - mean_absolute_error: 0.0064\n",
      "\n",
      "Epoch 09729: loss did not improve\n",
      "Epoch 9730/10000\n",
      "2700/2700 [==============================] - 0s 29us/step - loss: 1.1977e-04 - mean_absolute_error: 0.0064\n",
      "\n",
      "Epoch 09730: loss did not improve\n",
      "Epoch 9731/10000\n",
      "2700/2700 [==============================] - 0s 32us/step - loss: 1.2144e-04 - mean_absolute_error: 0.0065\n",
      "\n",
      "Epoch 09731: loss did not improve\n",
      "Epoch 9732/10000\n",
      "2700/2700 [==============================] - 0s 28us/step - loss: 1.2029e-04 - mean_absolute_error: 0.0064\n",
      "\n",
      "Epoch 09732: loss did not improve\n",
      "Epoch 9733/10000\n",
      "2700/2700 [==============================] - 0s 29us/step - loss: 1.1894e-04 - mean_absolute_error: 0.0063\n",
      "\n",
      "Epoch 09733: loss did not improve\n",
      "Epoch 9734/10000\n",
      "2700/2700 [==============================] - 0s 27us/step - loss: 1.2196e-04 - mean_absolute_error: 0.0065\n",
      "\n",
      "Epoch 09734: loss did not improve\n",
      "Epoch 9735/10000\n",
      "2700/2700 [==============================] - 0s 29us/step - loss: 1.2025e-04 - mean_absolute_error: 0.0065\n",
      "\n",
      "Epoch 09735: loss did not improve\n",
      "Epoch 9736/10000\n",
      "2700/2700 [==============================] - 0s 31us/step - loss: 1.2175e-04 - mean_absolute_error: 0.0064\n",
      "\n",
      "Epoch 09736: loss did not improve\n",
      "Epoch 9737/10000\n",
      "2700/2700 [==============================] - 0s 28us/step - loss: 1.1878e-04 - mean_absolute_error: 0.0063\n",
      "\n",
      "Epoch 09737: loss did not improve\n",
      "Epoch 9738/10000\n",
      "2700/2700 [==============================] - 0s 27us/step - loss: 1.2264e-04 - mean_absolute_error: 0.0066\n",
      "\n",
      "Epoch 09738: loss did not improve\n",
      "Epoch 9739/10000\n",
      "2700/2700 [==============================] - 0s 29us/step - loss: 1.2019e-04 - mean_absolute_error: 0.0064\n",
      "\n",
      "Epoch 09739: loss did not improve\n",
      "Epoch 9740/10000\n",
      "2700/2700 [==============================] - 0s 28us/step - loss: 1.1842e-04 - mean_absolute_error: 0.0062\n",
      "\n",
      "Epoch 09740: loss did not improve\n",
      "Epoch 9741/10000\n",
      "2700/2700 [==============================] - 0s 30us/step - loss: 1.1902e-04 - mean_absolute_error: 0.0063\n",
      "\n",
      "Epoch 09741: loss did not improve\n",
      "Epoch 9742/10000\n",
      "2700/2700 [==============================] - 0s 31us/step - loss: 1.2308e-04 - mean_absolute_error: 0.0066\n",
      "\n",
      "Epoch 09742: loss did not improve\n",
      "Epoch 9743/10000\n",
      "2700/2700 [==============================] - 0s 28us/step - loss: 1.2670e-04 - mean_absolute_error: 0.0070\n",
      "\n",
      "Epoch 09743: loss did not improve\n",
      "Epoch 9744/10000\n",
      "2700/2700 [==============================] - 0s 30us/step - loss: 1.2116e-04 - mean_absolute_error: 0.0064\n",
      "\n",
      "Epoch 09744: loss did not improve\n",
      "Epoch 9745/10000\n",
      "2700/2700 [==============================] - 0s 30us/step - loss: 1.2104e-04 - mean_absolute_error: 0.0064\n",
      "\n",
      "Epoch 09745: loss did not improve\n",
      "Epoch 9746/10000\n",
      "2700/2700 [==============================] - 0s 30us/step - loss: 1.2387e-04 - mean_absolute_error: 0.0066\n",
      "\n",
      "Epoch 09746: loss did not improve\n",
      "Epoch 9747/10000\n",
      "2700/2700 [==============================] - 0s 27us/step - loss: 1.2129e-04 - mean_absolute_error: 0.0065\n",
      "\n",
      "Epoch 09747: loss did not improve\n",
      "Epoch 9748/10000\n",
      "2700/2700 [==============================] - 0s 28us/step - loss: 1.2191e-04 - mean_absolute_error: 0.0065\n",
      "\n",
      "Epoch 09748: loss did not improve\n",
      "Epoch 9749/10000\n",
      "2700/2700 [==============================] - 0s 30us/step - loss: 1.2150e-04 - mean_absolute_error: 0.0065\n",
      "\n",
      "Epoch 09749: loss did not improve\n",
      "Epoch 9750/10000\n",
      "2700/2700 [==============================] - 0s 29us/step - loss: 1.2107e-04 - mean_absolute_error: 0.0065\n",
      "\n",
      "Epoch 09750: loss did not improve\n",
      "Epoch 9751/10000\n",
      "2700/2700 [==============================] - 0s 28us/step - loss: 1.2059e-04 - mean_absolute_error: 0.0064\n",
      "\n",
      "Epoch 09751: loss did not improve\n",
      "Epoch 9752/10000\n",
      "2700/2700 [==============================] - 0s 27us/step - loss: 1.2336e-04 - mean_absolute_error: 0.0066\n",
      "\n",
      "Epoch 09752: loss did not improve\n",
      "Epoch 9753/10000\n",
      "2700/2700 [==============================] - 0s 28us/step - loss: 1.2067e-04 - mean_absolute_error: 0.0065\n",
      "\n",
      "Epoch 09753: loss did not improve\n",
      "Epoch 9754/10000\n",
      "2700/2700 [==============================] - 0s 30us/step - loss: 1.2549e-04 - mean_absolute_error: 0.0067\n",
      "\n",
      "Epoch 09754: loss did not improve\n",
      "Epoch 9755/10000\n",
      "2700/2700 [==============================] - 0s 30us/step - loss: 1.2140e-04 - mean_absolute_error: 0.0065\n",
      "\n",
      "Epoch 09755: loss did not improve\n",
      "Epoch 9756/10000\n",
      "2700/2700 [==============================] - 0s 28us/step - loss: 1.2567e-04 - mean_absolute_error: 0.0068\n",
      "\n",
      "Epoch 09756: loss did not improve\n",
      "Epoch 9757/10000\n",
      "2700/2700 [==============================] - 0s 26us/step - loss: 1.2141e-04 - mean_absolute_error: 0.0065\n",
      "\n",
      "Epoch 09757: loss did not improve\n",
      "Epoch 9758/10000\n",
      "2700/2700 [==============================] - 0s 30us/step - loss: 1.2060e-04 - mean_absolute_error: 0.0063\n",
      "\n",
      "Epoch 09758: loss did not improve\n",
      "Epoch 9759/10000\n",
      "2700/2700 [==============================] - 0s 28us/step - loss: 1.2085e-04 - mean_absolute_error: 0.0064\n",
      "\n",
      "Epoch 09759: loss did not improve\n",
      "Epoch 9760/10000\n",
      "2700/2700 [==============================] - 0s 28us/step - loss: 1.2092e-04 - mean_absolute_error: 0.0064\n",
      "\n",
      "Epoch 09760: loss did not improve\n",
      "Epoch 9761/10000\n",
      "2700/2700 [==============================] - 0s 30us/step - loss: 1.2201e-04 - mean_absolute_error: 0.0065\n",
      "\n",
      "Epoch 09761: loss did not improve\n",
      "Epoch 9762/10000\n",
      "2700/2700 [==============================] - 0s 30us/step - loss: 1.2090e-04 - mean_absolute_error: 0.0063\n",
      "\n",
      "Epoch 09762: loss did not improve\n",
      "Epoch 9763/10000\n",
      "2700/2700 [==============================] - 0s 29us/step - loss: 1.2252e-04 - mean_absolute_error: 0.0065\n",
      "\n",
      "Epoch 09763: loss did not improve\n",
      "Epoch 9764/10000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2700/2700 [==============================] - 0s 30us/step - loss: 1.2220e-04 - mean_absolute_error: 0.0065\n",
      "\n",
      "Epoch 09764: loss did not improve\n",
      "Epoch 9765/10000\n",
      "2700/2700 [==============================] - 0s 28us/step - loss: 1.1938e-04 - mean_absolute_error: 0.0064\n",
      "\n",
      "Epoch 09765: loss did not improve\n",
      "Epoch 9766/10000\n",
      "2700/2700 [==============================] - 0s 28us/step - loss: 1.2128e-04 - mean_absolute_error: 0.0064\n",
      "\n",
      "Epoch 09766: loss did not improve\n",
      "Epoch 9767/10000\n",
      "2700/2700 [==============================] - 0s 28us/step - loss: 1.2031e-04 - mean_absolute_error: 0.0064\n",
      "\n",
      "Epoch 09767: loss did not improve\n",
      "Epoch 9768/10000\n",
      "2700/2700 [==============================] - 0s 28us/step - loss: 1.2077e-04 - mean_absolute_error: 0.0063\n",
      "\n",
      "Epoch 09768: loss did not improve\n",
      "Epoch 9769/10000\n",
      "2700/2700 [==============================] - 0s 28us/step - loss: 1.2367e-04 - mean_absolute_error: 0.0067\n",
      "\n",
      "Epoch 09769: loss did not improve\n",
      "Epoch 9770/10000\n",
      "2700/2700 [==============================] - 0s 29us/step - loss: 1.1952e-04 - mean_absolute_error: 0.0063\n",
      "\n",
      "Epoch 09770: loss did not improve\n",
      "Epoch 9771/10000\n",
      "2700/2700 [==============================] - 0s 26us/step - loss: 1.2099e-04 - mean_absolute_error: 0.0064\n",
      "\n",
      "Epoch 09771: loss did not improve\n",
      "Epoch 9772/10000\n",
      "2700/2700 [==============================] - 0s 29us/step - loss: 1.1977e-04 - mean_absolute_error: 0.0064\n",
      "\n",
      "Epoch 09772: loss did not improve\n",
      "Epoch 9773/10000\n",
      "2700/2700 [==============================] - 0s 27us/step - loss: 1.2093e-04 - mean_absolute_error: 0.0064\n",
      "\n",
      "Epoch 09773: loss did not improve\n",
      "Epoch 9774/10000\n",
      "2700/2700 [==============================] - 0s 29us/step - loss: 1.2126e-04 - mean_absolute_error: 0.0063\n",
      "\n",
      "Epoch 09774: loss did not improve\n",
      "Epoch 9775/10000\n",
      "2700/2700 [==============================] - 0s 28us/step - loss: 1.1979e-04 - mean_absolute_error: 0.0065\n",
      "\n",
      "Epoch 09775: loss did not improve\n",
      "Epoch 9776/10000\n",
      "2700/2700 [==============================] - 0s 28us/step - loss: 1.1975e-04 - mean_absolute_error: 0.0064\n",
      "\n",
      "Epoch 09776: loss did not improve\n",
      "Epoch 9777/10000\n",
      "2700/2700 [==============================] - 0s 29us/step - loss: 1.2214e-04 - mean_absolute_error: 0.0065\n",
      "\n",
      "Epoch 09777: loss did not improve\n",
      "Epoch 9778/10000\n",
      "2700/2700 [==============================] - 0s 28us/step - loss: 1.1971e-04 - mean_absolute_error: 0.0063\n",
      "\n",
      "Epoch 09778: loss did not improve\n",
      "Epoch 9779/10000\n",
      "2700/2700 [==============================] - 0s 26us/step - loss: 1.2037e-04 - mean_absolute_error: 0.0064\n",
      "\n",
      "Epoch 09779: loss did not improve\n",
      "Epoch 9780/10000\n",
      "2700/2700 [==============================] - 0s 28us/step - loss: 1.2310e-04 - mean_absolute_error: 0.0067\n",
      "\n",
      "Epoch 09780: loss did not improve\n",
      "Epoch 9781/10000\n",
      "2700/2700 [==============================] - 0s 30us/step - loss: 1.2233e-04 - mean_absolute_error: 0.0067\n",
      "\n",
      "Epoch 09781: loss did not improve\n",
      "Epoch 9782/10000\n",
      "2700/2700 [==============================] - 0s 29us/step - loss: 1.1943e-04 - mean_absolute_error: 0.0063\n",
      "\n",
      "Epoch 09782: loss did not improve\n",
      "Epoch 9783/10000\n",
      "2700/2700 [==============================] - 0s 25us/step - loss: 1.1954e-04 - mean_absolute_error: 0.0063\n",
      "\n",
      "Epoch 09783: loss did not improve\n",
      "Epoch 9784/10000\n",
      "2700/2700 [==============================] - 0s 29us/step - loss: 1.2333e-04 - mean_absolute_error: 0.0067\n",
      "\n",
      "Epoch 09784: loss did not improve\n",
      "Epoch 9785/10000\n",
      "2700/2700 [==============================] - 0s 27us/step - loss: 1.2012e-04 - mean_absolute_error: 0.0063\n",
      "\n",
      "Epoch 09785: loss did not improve\n",
      "Epoch 9786/10000\n",
      "2700/2700 [==============================] - 0s 29us/step - loss: 1.1985e-04 - mean_absolute_error: 0.0063\n",
      "\n",
      "Epoch 09786: loss did not improve\n",
      "Epoch 9787/10000\n",
      "2700/2700 [==============================] - 0s 30us/step - loss: 1.2440e-04 - mean_absolute_error: 0.0067\n",
      "\n",
      "Epoch 09787: loss did not improve\n",
      "Epoch 9788/10000\n",
      "2700/2700 [==============================] - 0s 28us/step - loss: 1.1994e-04 - mean_absolute_error: 0.0063\n",
      "\n",
      "Epoch 09788: loss did not improve\n",
      "Epoch 9789/10000\n",
      "2700/2700 [==============================] - 0s 27us/step - loss: 1.1967e-04 - mean_absolute_error: 0.0063\n",
      "\n",
      "Epoch 09789: loss did not improve\n",
      "Epoch 9790/10000\n",
      "2700/2700 [==============================] - 0s 28us/step - loss: 1.2079e-04 - mean_absolute_error: 0.0064\n",
      "\n",
      "Epoch 09790: loss did not improve\n",
      "Epoch 9791/10000\n",
      "2700/2700 [==============================] - 0s 32us/step - loss: 1.2130e-04 - mean_absolute_error: 0.0065\n",
      "\n",
      "Epoch 09791: loss did not improve\n",
      "Epoch 9792/10000\n",
      "2700/2700 [==============================] - 0s 31us/step - loss: 1.1925e-04 - mean_absolute_error: 0.0063\n",
      "\n",
      "Epoch 09792: loss did not improve\n",
      "Epoch 9793/10000\n",
      "2700/2700 [==============================] - 0s 28us/step - loss: 1.2166e-04 - mean_absolute_error: 0.0064\n",
      "\n",
      "Epoch 09793: loss did not improve\n",
      "Epoch 9794/10000\n",
      "2700/2700 [==============================] - 0s 30us/step - loss: 1.2127e-04 - mean_absolute_error: 0.0065\n",
      "\n",
      "Epoch 09794: loss did not improve\n",
      "Epoch 9795/10000\n",
      "2700/2700 [==============================] - 0s 31us/step - loss: 1.1904e-04 - mean_absolute_error: 0.0064\n",
      "\n",
      "Epoch 09795: loss did not improve\n",
      "Epoch 9796/10000\n",
      "2700/2700 [==============================] - 0s 29us/step - loss: 1.2056e-04 - mean_absolute_error: 0.0065\n",
      "\n",
      "Epoch 09796: loss did not improve\n",
      "Epoch 9797/10000\n",
      "2700/2700 [==============================] - 0s 31us/step - loss: 1.1991e-04 - mean_absolute_error: 0.0064\n",
      "\n",
      "Epoch 09797: loss did not improve\n",
      "Epoch 9798/10000\n",
      "2700/2700 [==============================] - 0s 27us/step - loss: 1.2022e-04 - mean_absolute_error: 0.0064\n",
      "\n",
      "Epoch 09798: loss did not improve\n",
      "Epoch 9799/10000\n",
      "2700/2700 [==============================] - 0s 31us/step - loss: 1.2145e-04 - mean_absolute_error: 0.0065\n",
      "\n",
      "Epoch 09799: loss did not improve\n",
      "Epoch 9800/10000\n",
      "2700/2700 [==============================] - 0s 29us/step - loss: 1.1908e-04 - mean_absolute_error: 0.0063\n",
      "\n",
      "Epoch 09800: loss did not improve\n",
      "Epoch 9801/10000\n",
      "2700/2700 [==============================] - 0s 30us/step - loss: 1.2269e-04 - mean_absolute_error: 0.0066\n",
      "\n",
      "Epoch 09801: loss did not improve\n",
      "Epoch 9802/10000\n",
      "2700/2700 [==============================] - 0s 30us/step - loss: 1.2041e-04 - mean_absolute_error: 0.0064\n",
      "\n",
      "Epoch 09802: loss did not improve\n",
      "Epoch 9803/10000\n",
      "2700/2700 [==============================] - 0s 29us/step - loss: 1.1901e-04 - mean_absolute_error: 0.0063\n",
      "\n",
      "Epoch 09803: loss did not improve\n",
      "Epoch 9804/10000\n",
      "2700/2700 [==============================] - 0s 29us/step - loss: 1.2098e-04 - mean_absolute_error: 0.0063\n",
      "\n",
      "Epoch 09804: loss did not improve\n",
      "Epoch 9805/10000\n",
      "2700/2700 [==============================] - 0s 29us/step - loss: 1.2239e-04 - mean_absolute_error: 0.0065\n",
      "\n",
      "Epoch 09805: loss did not improve\n",
      "Epoch 9806/10000\n",
      "2700/2700 [==============================] - 0s 31us/step - loss: 1.2466e-04 - mean_absolute_error: 0.0067\n",
      "\n",
      "Epoch 09806: loss did not improve\n",
      "Epoch 9807/10000\n",
      "2700/2700 [==============================] - 0s 30us/step - loss: 1.2177e-04 - mean_absolute_error: 0.0065\n",
      "\n",
      "Epoch 09807: loss did not improve\n",
      "Epoch 9808/10000\n",
      "2700/2700 [==============================] - 0s 28us/step - loss: 1.2276e-04 - mean_absolute_error: 0.0066\n",
      "\n",
      "Epoch 09808: loss did not improve\n",
      "Epoch 9809/10000\n",
      "2700/2700 [==============================] - 0s 30us/step - loss: 1.2162e-04 - mean_absolute_error: 0.0065\n",
      "\n",
      "Epoch 09809: loss did not improve\n",
      "Epoch 9810/10000\n",
      "2700/2700 [==============================] - 0s 28us/step - loss: 1.2076e-04 - mean_absolute_error: 0.0064\n",
      "\n",
      "Epoch 09810: loss did not improve\n",
      "Epoch 9811/10000\n",
      "2700/2700 [==============================] - 0s 40us/step - loss: 1.1981e-04 - mean_absolute_error: 0.0063\n",
      "\n",
      "Epoch 09811: loss did not improve\n",
      "Epoch 9812/10000\n",
      "2700/2700 [==============================] - 0s 48us/step - loss: 1.1921e-04 - mean_absolute_error: 0.0062\n",
      "\n",
      "Epoch 09812: loss did not improve\n",
      "Epoch 9813/10000\n",
      "2700/2700 [==============================] - 0s 41us/step - loss: 1.2224e-04 - mean_absolute_error: 0.0065\n",
      "\n",
      "Epoch 09813: loss did not improve\n",
      "Epoch 9814/10000\n",
      "2700/2700 [==============================] - 0s 31us/step - loss: 1.2119e-04 - mean_absolute_error: 0.0064\n",
      "\n",
      "Epoch 09814: loss did not improve\n",
      "Epoch 9815/10000\n",
      "2700/2700 [==============================] - 0s 34us/step - loss: 1.1931e-04 - mean_absolute_error: 0.0063\n",
      "\n",
      "Epoch 09815: loss did not improve\n",
      "Epoch 9816/10000\n",
      "2700/2700 [==============================] - 0s 30us/step - loss: 1.2207e-04 - mean_absolute_error: 0.0065\n",
      "\n",
      "Epoch 09816: loss did not improve\n",
      "Epoch 9817/10000\n",
      "2700/2700 [==============================] - 0s 31us/step - loss: 1.2078e-04 - mean_absolute_error: 0.0064\n",
      "\n",
      "Epoch 09817: loss did not improve\n",
      "Epoch 9818/10000\n",
      "2700/2700 [==============================] - 0s 28us/step - loss: 1.2093e-04 - mean_absolute_error: 0.0064\n",
      "\n",
      "Epoch 09818: loss did not improve\n",
      "Epoch 9819/10000\n",
      "2700/2700 [==============================] - 0s 28us/step - loss: 1.1978e-04 - mean_absolute_error: 0.0063\n",
      "\n",
      "Epoch 09819: loss did not improve\n",
      "Epoch 9820/10000\n",
      "2700/2700 [==============================] - 0s 31us/step - loss: 1.2106e-04 - mean_absolute_error: 0.0064\n",
      "\n",
      "Epoch 09820: loss did not improve\n",
      "Epoch 9821/10000\n",
      "2700/2700 [==============================] - 0s 29us/step - loss: 1.2406e-04 - mean_absolute_error: 0.0067\n",
      "\n",
      "Epoch 09821: loss did not improve\n",
      "Epoch 9822/10000\n",
      "2700/2700 [==============================] - 0s 28us/step - loss: 1.2203e-04 - mean_absolute_error: 0.0064\n",
      "\n",
      "Epoch 09822: loss did not improve\n",
      "Epoch 9823/10000\n",
      "2700/2700 [==============================] - 0s 29us/step - loss: 1.1996e-04 - mean_absolute_error: 0.0064\n",
      "\n",
      "Epoch 09823: loss did not improve\n",
      "Epoch 9824/10000\n",
      "2700/2700 [==============================] - 0s 28us/step - loss: 1.1905e-04 - mean_absolute_error: 0.0062\n",
      "\n",
      "Epoch 09824: loss did not improve\n",
      "Epoch 9825/10000\n",
      "2700/2700 [==============================] - 0s 29us/step - loss: 1.1944e-04 - mean_absolute_error: 0.0063\n",
      "\n",
      "Epoch 09825: loss did not improve\n",
      "Epoch 9826/10000\n",
      "2700/2700 [==============================] - 0s 28us/step - loss: 1.2370e-04 - mean_absolute_error: 0.0066\n",
      "\n",
      "Epoch 09826: loss did not improve\n",
      "Epoch 9827/10000\n",
      "2700/2700 [==============================] - 0s 27us/step - loss: 1.2267e-04 - mean_absolute_error: 0.0066\n",
      "\n",
      "Epoch 09827: loss did not improve\n",
      "Epoch 9828/10000\n",
      "2700/2700 [==============================] - 0s 28us/step - loss: 1.2039e-04 - mean_absolute_error: 0.0065\n",
      "\n",
      "Epoch 09828: loss did not improve\n",
      "Epoch 9829/10000\n",
      "2700/2700 [==============================] - 0s 29us/step - loss: 1.2599e-04 - mean_absolute_error: 0.0068\n",
      "\n",
      "Epoch 09829: loss did not improve\n",
      "Epoch 9830/10000\n",
      "2700/2700 [==============================] - 0s 30us/step - loss: 1.2227e-04 - mean_absolute_error: 0.0065\n",
      "\n",
      "Epoch 09830: loss did not improve\n",
      "Epoch 9831/10000\n",
      "2700/2700 [==============================] - 0s 28us/step - loss: 1.2047e-04 - mean_absolute_error: 0.0064\n",
      "\n",
      "Epoch 09831: loss did not improve\n",
      "Epoch 9832/10000\n",
      "2700/2700 [==============================] - 0s 28us/step - loss: 1.2067e-04 - mean_absolute_error: 0.0064\n",
      "\n",
      "Epoch 09832: loss did not improve\n",
      "Epoch 9833/10000\n",
      "2700/2700 [==============================] - 0s 28us/step - loss: 1.2110e-04 - mean_absolute_error: 0.0064\n",
      "\n",
      "Epoch 09833: loss did not improve\n",
      "Epoch 9834/10000\n",
      "2700/2700 [==============================] - 0s 30us/step - loss: 1.2041e-04 - mean_absolute_error: 0.0064\n",
      "\n",
      "Epoch 09834: loss did not improve\n",
      "Epoch 9835/10000\n",
      "2700/2700 [==============================] - 0s 29us/step - loss: 1.2156e-04 - mean_absolute_error: 0.0065\n",
      "\n",
      "Epoch 09835: loss did not improve\n",
      "Epoch 9836/10000\n",
      "2700/2700 [==============================] - 0s 28us/step - loss: 1.1962e-04 - mean_absolute_error: 0.0063\n",
      "\n",
      "Epoch 09836: loss did not improve\n",
      "Epoch 9837/10000\n",
      "2700/2700 [==============================] - 0s 28us/step - loss: 1.2335e-04 - mean_absolute_error: 0.0067\n",
      "\n",
      "Epoch 09837: loss did not improve\n",
      "Epoch 9838/10000\n",
      "2700/2700 [==============================] - 0s 28us/step - loss: 1.2163e-04 - mean_absolute_error: 0.0065\n",
      "\n",
      "Epoch 09838: loss did not improve\n",
      "Epoch 9839/10000\n",
      "2700/2700 [==============================] - 0s 28us/step - loss: 1.2248e-04 - mean_absolute_error: 0.0066\n",
      "\n",
      "Epoch 09839: loss did not improve\n",
      "Epoch 9840/10000\n",
      "2700/2700 [==============================] - 0s 32us/step - loss: 1.2011e-04 - mean_absolute_error: 0.0063\n",
      "\n",
      "Epoch 09840: loss did not improve\n",
      "Epoch 9841/10000\n",
      "2700/2700 [==============================] - 0s 30us/step - loss: 1.2475e-04 - mean_absolute_error: 0.0067\n",
      "\n",
      "Epoch 09841: loss did not improve\n",
      "Epoch 9842/10000\n",
      "2700/2700 [==============================] - 0s 27us/step - loss: 1.2123e-04 - mean_absolute_error: 0.0065\n",
      "\n",
      "Epoch 09842: loss did not improve\n",
      "Epoch 9843/10000\n",
      "2700/2700 [==============================] - 0s 29us/step - loss: 1.2053e-04 - mean_absolute_error: 0.0064\n",
      "\n",
      "Epoch 09843: loss did not improve\n",
      "Epoch 9844/10000\n",
      "2700/2700 [==============================] - 0s 30us/step - loss: 1.2095e-04 - mean_absolute_error: 0.0064\n",
      "\n",
      "Epoch 09844: loss did not improve\n",
      "Epoch 9845/10000\n",
      "2700/2700 [==============================] - 0s 30us/step - loss: 1.2117e-04 - mean_absolute_error: 0.0066\n",
      "\n",
      "Epoch 09845: loss did not improve\n",
      "Epoch 9846/10000\n",
      "2700/2700 [==============================] - 0s 28us/step - loss: 1.2050e-04 - mean_absolute_error: 0.0064\n",
      "\n",
      "Epoch 09846: loss did not improve\n",
      "Epoch 9847/10000\n",
      "2700/2700 [==============================] - 0s 29us/step - loss: 1.2074e-04 - mean_absolute_error: 0.0065\n",
      "\n",
      "Epoch 09847: loss did not improve\n",
      "Epoch 9848/10000\n",
      "2700/2700 [==============================] - 0s 28us/step - loss: 1.2025e-04 - mean_absolute_error: 0.0064\n",
      "\n",
      "Epoch 09848: loss did not improve\n",
      "Epoch 9849/10000\n",
      "2700/2700 [==============================] - 0s 30us/step - loss: 1.1979e-04 - mean_absolute_error: 0.0063\n",
      "\n",
      "Epoch 09849: loss did not improve\n",
      "Epoch 9850/10000\n",
      "2700/2700 [==============================] - 0s 30us/step - loss: 1.1975e-04 - mean_absolute_error: 0.0063\n",
      "\n",
      "Epoch 09850: loss did not improve\n",
      "Epoch 9851/10000\n",
      "2700/2700 [==============================] - 0s 26us/step - loss: 1.2031e-04 - mean_absolute_error: 0.0065\n",
      "\n",
      "Epoch 09851: loss did not improve\n",
      "Epoch 9852/10000\n",
      "2700/2700 [==============================] - 0s 30us/step - loss: 1.2043e-04 - mean_absolute_error: 0.0064\n",
      "\n",
      "Epoch 09852: loss did not improve\n",
      "Epoch 9853/10000\n",
      "2700/2700 [==============================] - 0s 31us/step - loss: 1.2120e-04 - mean_absolute_error: 0.0064\n",
      "\n",
      "Epoch 09853: loss did not improve\n",
      "Epoch 9854/10000\n",
      "2700/2700 [==============================] - 0s 29us/step - loss: 1.2120e-04 - mean_absolute_error: 0.0065\n",
      "\n",
      "Epoch 09854: loss did not improve\n",
      "Epoch 9855/10000\n",
      "2700/2700 [==============================] - 0s 31us/step - loss: 1.2228e-04 - mean_absolute_error: 0.0066\n",
      "\n",
      "Epoch 09855: loss did not improve\n",
      "Epoch 9856/10000\n",
      "2700/2700 [==============================] - 0s 27us/step - loss: 1.2194e-04 - mean_absolute_error: 0.0066\n",
      "\n",
      "Epoch 09856: loss did not improve\n",
      "Epoch 9857/10000\n",
      "2700/2700 [==============================] - 0s 29us/step - loss: 1.2084e-04 - mean_absolute_error: 0.0065\n",
      "\n",
      "Epoch 09857: loss did not improve\n",
      "Epoch 9858/10000\n",
      "2700/2700 [==============================] - 0s 31us/step - loss: 1.2248e-04 - mean_absolute_error: 0.0066\n",
      "\n",
      "Epoch 09858: loss did not improve\n",
      "Epoch 9859/10000\n",
      "2700/2700 [==============================] - 0s 30us/step - loss: 1.1950e-04 - mean_absolute_error: 0.0063\n",
      "\n",
      "Epoch 09859: loss did not improve\n",
      "Epoch 9860/10000\n",
      "2700/2700 [==============================] - 0s 28us/step - loss: 1.2110e-04 - mean_absolute_error: 0.0064\n",
      "\n",
      "Epoch 09860: loss did not improve\n",
      "Epoch 9861/10000\n",
      "2700/2700 [==============================] - 0s 30us/step - loss: 1.2085e-04 - mean_absolute_error: 0.0065\n",
      "\n",
      "Epoch 09861: loss did not improve\n",
      "Epoch 9862/10000\n",
      "2700/2700 [==============================] - 0s 28us/step - loss: 1.2236e-04 - mean_absolute_error: 0.0066\n",
      "\n",
      "Epoch 09862: loss did not improve\n",
      "Epoch 9863/10000\n",
      "2700/2700 [==============================] - 0s 28us/step - loss: 1.2196e-04 - mean_absolute_error: 0.0066\n",
      "\n",
      "Epoch 09863: loss did not improve\n",
      "Epoch 9864/10000\n",
      "2700/2700 [==============================] - 0s 40us/step - loss: 1.1967e-04 - mean_absolute_error: 0.0064\n",
      "\n",
      "Epoch 09864: loss did not improve\n",
      "Epoch 9865/10000\n",
      "2700/2700 [==============================] - 0s 30us/step - loss: 1.2133e-04 - mean_absolute_error: 0.0065\n",
      "\n",
      "Epoch 09865: loss did not improve\n",
      "Epoch 9866/10000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2700/2700 [==============================] - 0s 29us/step - loss: 1.2064e-04 - mean_absolute_error: 0.0064\n",
      "\n",
      "Epoch 09866: loss did not improve\n",
      "Epoch 9867/10000\n",
      "2700/2700 [==============================] - 0s 28us/step - loss: 1.2221e-04 - mean_absolute_error: 0.0065\n",
      "\n",
      "Epoch 09867: loss did not improve\n",
      "Epoch 9868/10000\n",
      "2700/2700 [==============================] - 0s 27us/step - loss: 1.2051e-04 - mean_absolute_error: 0.0064\n",
      "\n",
      "Epoch 09868: loss did not improve\n",
      "Epoch 9869/10000\n",
      "2700/2700 [==============================] - 0s 30us/step - loss: 1.2030e-04 - mean_absolute_error: 0.0064\n",
      "\n",
      "Epoch 09869: loss did not improve\n",
      "Epoch 9870/10000\n",
      "2700/2700 [==============================] - 0s 29us/step - loss: 1.1950e-04 - mean_absolute_error: 0.0063\n",
      "\n",
      "Epoch 09870: loss did not improve\n",
      "Epoch 9871/10000\n",
      "2700/2700 [==============================] - 0s 30us/step - loss: 1.2277e-04 - mean_absolute_error: 0.0066\n",
      "\n",
      "Epoch 09871: loss did not improve\n",
      "Epoch 9872/10000\n",
      "2700/2700 [==============================] - 0s 28us/step - loss: 1.2224e-04 - mean_absolute_error: 0.0066\n",
      "\n",
      "Epoch 09872: loss did not improve\n",
      "Epoch 9873/10000\n",
      "2700/2700 [==============================] - 0s 28us/step - loss: 1.2266e-04 - mean_absolute_error: 0.0066\n",
      "\n",
      "Epoch 09873: loss did not improve\n",
      "Epoch 9874/10000\n",
      "2700/2700 [==============================] - 0s 29us/step - loss: 1.2000e-04 - mean_absolute_error: 0.0063\n",
      "\n",
      "Epoch 09874: loss did not improve\n",
      "Epoch 9875/10000\n",
      "2700/2700 [==============================] - 0s 32us/step - loss: 1.2020e-04 - mean_absolute_error: 0.0064\n",
      "\n",
      "Epoch 09875: loss did not improve\n",
      "Epoch 9876/10000\n",
      "2700/2700 [==============================] - 0s 29us/step - loss: 1.2086e-04 - mean_absolute_error: 0.0064\n",
      "\n",
      "Epoch 09876: loss did not improve\n",
      "Epoch 9877/10000\n",
      "2700/2700 [==============================] - 0s 29us/step - loss: 1.1953e-04 - mean_absolute_error: 0.0063\n",
      "\n",
      "Epoch 09877: loss did not improve\n",
      "Epoch 9878/10000\n",
      "2700/2700 [==============================] - 0s 31us/step - loss: 1.2425e-04 - mean_absolute_error: 0.0067\n",
      "\n",
      "Epoch 09878: loss did not improve\n",
      "Epoch 9879/10000\n",
      "2700/2700 [==============================] - 0s 28us/step - loss: 1.2619e-04 - mean_absolute_error: 0.0069\n",
      "\n",
      "Epoch 09879: loss did not improve\n",
      "Epoch 9880/10000\n",
      "2700/2700 [==============================] - 0s 28us/step - loss: 1.1893e-04 - mean_absolute_error: 0.0062\n",
      "\n",
      "Epoch 09880: loss did not improve\n",
      "Epoch 9881/10000\n",
      "2700/2700 [==============================] - 0s 31us/step - loss: 1.2090e-04 - mean_absolute_error: 0.0063\n",
      "\n",
      "Epoch 09881: loss did not improve\n",
      "Epoch 9882/10000\n",
      "2700/2700 [==============================] - 0s 26us/step - loss: 1.2173e-04 - mean_absolute_error: 0.0064\n",
      "\n",
      "Epoch 09882: loss did not improve\n",
      "Epoch 9883/10000\n",
      "2700/2700 [==============================] - 0s 27us/step - loss: 1.1857e-04 - mean_absolute_error: 0.0062\n",
      "\n",
      "Epoch 09883: loss did not improve\n",
      "Epoch 9884/10000\n",
      "2700/2700 [==============================] - 0s 30us/step - loss: 1.2042e-04 - mean_absolute_error: 0.0063\n",
      "\n",
      "Epoch 09884: loss did not improve\n",
      "Epoch 9885/10000\n",
      "2700/2700 [==============================] - 0s 27us/step - loss: 1.2103e-04 - mean_absolute_error: 0.0065\n",
      "\n",
      "Epoch 09885: loss did not improve\n",
      "Epoch 9886/10000\n",
      "2700/2700 [==============================] - 0s 28us/step - loss: 1.2021e-04 - mean_absolute_error: 0.0064\n",
      "\n",
      "Epoch 09886: loss did not improve\n",
      "Epoch 9887/10000\n",
      "2700/2700 [==============================] - 0s 31us/step - loss: 1.2329e-04 - mean_absolute_error: 0.0066\n",
      "\n",
      "Epoch 09887: loss did not improve\n",
      "Epoch 9888/10000\n",
      "2700/2700 [==============================] - 0s 30us/step - loss: 1.2091e-04 - mean_absolute_error: 0.0064\n",
      "\n",
      "Epoch 09888: loss did not improve\n",
      "Epoch 9889/10000\n",
      "2700/2700 [==============================] - 0s 29us/step - loss: 1.2355e-04 - mean_absolute_error: 0.0067\n",
      "\n",
      "Epoch 09889: loss did not improve\n",
      "Epoch 9890/10000\n",
      "2700/2700 [==============================] - 0s 28us/step - loss: 1.2215e-04 - mean_absolute_error: 0.0066\n",
      "\n",
      "Epoch 09890: loss did not improve\n",
      "Epoch 9891/10000\n",
      "2700/2700 [==============================] - 0s 28us/step - loss: 1.2178e-04 - mean_absolute_error: 0.0065\n",
      "\n",
      "Epoch 09891: loss did not improve\n",
      "Epoch 9892/10000\n",
      "2700/2700 [==============================] - 0s 29us/step - loss: 1.2082e-04 - mean_absolute_error: 0.0065\n",
      "\n",
      "Epoch 09892: loss did not improve\n",
      "Epoch 9893/10000\n",
      "2700/2700 [==============================] - 0s 29us/step - loss: 1.2144e-04 - mean_absolute_error: 0.0064\n",
      "\n",
      "Epoch 09893: loss did not improve\n",
      "Epoch 9894/10000\n",
      "2700/2700 [==============================] - 0s 29us/step - loss: 1.1998e-04 - mean_absolute_error: 0.0063\n",
      "\n",
      "Epoch 09894: loss did not improve\n",
      "Epoch 9895/10000\n",
      "2700/2700 [==============================] - 0s 27us/step - loss: 1.2532e-04 - mean_absolute_error: 0.0067\n",
      "\n",
      "Epoch 09895: loss did not improve\n",
      "Epoch 9896/10000\n",
      "2700/2700 [==============================] - 0s 29us/step - loss: 1.2090e-04 - mean_absolute_error: 0.0064\n",
      "\n",
      "Epoch 09896: loss did not improve\n",
      "Epoch 9897/10000\n",
      "2700/2700 [==============================] - 0s 29us/step - loss: 1.1967e-04 - mean_absolute_error: 0.0063\n",
      "\n",
      "Epoch 09897: loss did not improve\n",
      "Epoch 9898/10000\n",
      "2700/2700 [==============================] - 0s 29us/step - loss: 1.2465e-04 - mean_absolute_error: 0.0067\n",
      "\n",
      "Epoch 09898: loss did not improve\n",
      "Epoch 9899/10000\n",
      "2700/2700 [==============================] - 0s 29us/step - loss: 1.2162e-04 - mean_absolute_error: 0.0065\n",
      "\n",
      "Epoch 09899: loss did not improve\n",
      "Epoch 9900/10000\n",
      "2700/2700 [==============================] - 0s 28us/step - loss: 1.2109e-04 - mean_absolute_error: 0.0064\n",
      "\n",
      "Epoch 09900: loss did not improve\n",
      "Epoch 9901/10000\n",
      "2700/2700 [==============================] - 0s 29us/step - loss: 1.1959e-04 - mean_absolute_error: 0.0064\n",
      "\n",
      "Epoch 09901: loss did not improve\n",
      "Epoch 9902/10000\n",
      "2700/2700 [==============================] - 0s 31us/step - loss: 1.1914e-04 - mean_absolute_error: 0.0063\n",
      "\n",
      "Epoch 09902: loss did not improve\n",
      "Epoch 9903/10000\n",
      "2700/2700 [==============================] - 0s 37us/step - loss: 1.1959e-04 - mean_absolute_error: 0.0063\n",
      "\n",
      "Epoch 09903: loss did not improve\n",
      "Epoch 9904/10000\n",
      "2700/2700 [==============================] - 0s 42us/step - loss: 1.2540e-04 - mean_absolute_error: 0.0068\n",
      "\n",
      "Epoch 09904: loss did not improve\n",
      "Epoch 9905/10000\n",
      "2700/2700 [==============================] - 0s 39us/step - loss: 1.2164e-04 - mean_absolute_error: 0.0064\n",
      "\n",
      "Epoch 09905: loss did not improve\n",
      "Epoch 9906/10000\n",
      "2700/2700 [==============================] - 0s 40us/step - loss: 1.2188e-04 - mean_absolute_error: 0.0065\n",
      "\n",
      "Epoch 09906: loss did not improve\n",
      "Epoch 9907/10000\n",
      "2700/2700 [==============================] - 0s 36us/step - loss: 1.2095e-04 - mean_absolute_error: 0.0064\n",
      "\n",
      "Epoch 09907: loss did not improve\n",
      "Epoch 9908/10000\n",
      "2700/2700 [==============================] - 0s 28us/step - loss: 1.2027e-04 - mean_absolute_error: 0.0063\n",
      "\n",
      "Epoch 09908: loss did not improve\n",
      "Epoch 9909/10000\n",
      "2700/2700 [==============================] - 0s 30us/step - loss: 1.1951e-04 - mean_absolute_error: 0.0063\n",
      "\n",
      "Epoch 09909: loss did not improve\n",
      "Epoch 9910/10000\n",
      "2700/2700 [==============================] - 0s 28us/step - loss: 1.1966e-04 - mean_absolute_error: 0.0063\n",
      "\n",
      "Epoch 09910: loss did not improve\n",
      "Epoch 9911/10000\n",
      "2700/2700 [==============================] - 0s 29us/step - loss: 1.2467e-04 - mean_absolute_error: 0.0069\n",
      "\n",
      "Epoch 09911: loss did not improve\n",
      "Epoch 9912/10000\n",
      "2700/2700 [==============================] - 0s 32us/step - loss: 1.2109e-04 - mean_absolute_error: 0.0064\n",
      "\n",
      "Epoch 09912: loss did not improve\n",
      "Epoch 9913/10000\n",
      "2700/2700 [==============================] - 0s 29us/step - loss: 1.2321e-04 - mean_absolute_error: 0.0066\n",
      "\n",
      "Epoch 09913: loss did not improve\n",
      "Epoch 9914/10000\n",
      "2700/2700 [==============================] - 0s 31us/step - loss: 1.2201e-04 - mean_absolute_error: 0.0066\n",
      "\n",
      "Epoch 09914: loss did not improve\n",
      "Epoch 9915/10000\n",
      "2700/2700 [==============================] - 0s 28us/step - loss: 1.2411e-04 - mean_absolute_error: 0.0068\n",
      "\n",
      "Epoch 09915: loss did not improve\n",
      "Epoch 9916/10000\n",
      "2700/2700 [==============================] - 0s 27us/step - loss: 1.2287e-04 - mean_absolute_error: 0.0066\n",
      "\n",
      "Epoch 09916: loss did not improve\n",
      "Epoch 9917/10000\n",
      "2700/2700 [==============================] - 0s 28us/step - loss: 1.1945e-04 - mean_absolute_error: 0.0063\n",
      "\n",
      "Epoch 09917: loss did not improve\n",
      "Epoch 9918/10000\n",
      "2700/2700 [==============================] - 0s 30us/step - loss: 1.2075e-04 - mean_absolute_error: 0.0063\n",
      "\n",
      "Epoch 09918: loss did not improve\n",
      "Epoch 9919/10000\n",
      "2700/2700 [==============================] - 0s 29us/step - loss: 1.2164e-04 - mean_absolute_error: 0.0065\n",
      "\n",
      "Epoch 09919: loss did not improve\n",
      "Epoch 9920/10000\n",
      "2700/2700 [==============================] - 0s 29us/step - loss: 1.1966e-04 - mean_absolute_error: 0.0064\n",
      "\n",
      "Epoch 09920: loss did not improve\n",
      "Epoch 9921/10000\n",
      "2700/2700 [==============================] - 0s 28us/step - loss: 1.1944e-04 - mean_absolute_error: 0.0063\n",
      "\n",
      "Epoch 09921: loss did not improve\n",
      "Epoch 9922/10000\n",
      "2700/2700 [==============================] - 0s 31us/step - loss: 1.2115e-04 - mean_absolute_error: 0.0064\n",
      "\n",
      "Epoch 09922: loss did not improve\n",
      "Epoch 9923/10000\n",
      "2700/2700 [==============================] - 0s 31us/step - loss: 1.2137e-04 - mean_absolute_error: 0.0065\n",
      "\n",
      "Epoch 09923: loss did not improve\n",
      "Epoch 9924/10000\n",
      "2700/2700 [==============================] - 0s 29us/step - loss: 1.1988e-04 - mean_absolute_error: 0.0063\n",
      "\n",
      "Epoch 09924: loss did not improve\n",
      "Epoch 9925/10000\n",
      "2700/2700 [==============================] - 0s 28us/step - loss: 1.2090e-04 - mean_absolute_error: 0.0066\n",
      "\n",
      "Epoch 09925: loss did not improve\n",
      "Epoch 9926/10000\n",
      "2700/2700 [==============================] - 0s 28us/step - loss: 1.1973e-04 - mean_absolute_error: 0.0064\n",
      "\n",
      "Epoch 09926: loss did not improve\n",
      "Epoch 9927/10000\n",
      "2700/2700 [==============================] - 0s 31us/step - loss: 1.2038e-04 - mean_absolute_error: 0.0064\n",
      "\n",
      "Epoch 09927: loss did not improve\n",
      "Epoch 9928/10000\n",
      "2700/2700 [==============================] - 0s 31us/step - loss: 1.1989e-04 - mean_absolute_error: 0.0064\n",
      "\n",
      "Epoch 09928: loss did not improve\n",
      "Epoch 9929/10000\n",
      "2700/2700 [==============================] - 0s 34us/step - loss: 1.2135e-04 - mean_absolute_error: 0.0065\n",
      "\n",
      "Epoch 09929: loss did not improve\n",
      "Epoch 9930/10000\n",
      "2700/2700 [==============================] - 0s 29us/step - loss: 1.2083e-04 - mean_absolute_error: 0.0063\n",
      "\n",
      "Epoch 09930: loss did not improve\n",
      "Epoch 9931/10000\n",
      "2700/2700 [==============================] - 0s 28us/step - loss: 1.1880e-04 - mean_absolute_error: 0.0062\n",
      "\n",
      "Epoch 09931: loss did not improve\n",
      "Epoch 9932/10000\n",
      "2700/2700 [==============================] - 0s 28us/step - loss: 1.2225e-04 - mean_absolute_error: 0.0065\n",
      "\n",
      "Epoch 09932: loss did not improve\n",
      "Epoch 9933/10000\n",
      "2700/2700 [==============================] - 0s 28us/step - loss: 1.2080e-04 - mean_absolute_error: 0.0064\n",
      "\n",
      "Epoch 09933: loss did not improve\n",
      "Epoch 9934/10000\n",
      "2700/2700 [==============================] - 0s 29us/step - loss: 1.1945e-04 - mean_absolute_error: 0.0062\n",
      "\n",
      "Epoch 09934: loss did not improve\n",
      "Epoch 9935/10000\n",
      "2700/2700 [==============================] - 0s 29us/step - loss: 1.2160e-04 - mean_absolute_error: 0.0065\n",
      "\n",
      "Epoch 09935: loss did not improve\n",
      "Epoch 9936/10000\n",
      "2700/2700 [==============================] - 0s 45us/step - loss: 1.2258e-04 - mean_absolute_error: 0.0066\n",
      "\n",
      "Epoch 09936: loss did not improve\n",
      "Epoch 9937/10000\n",
      "2700/2700 [==============================] - 0s 41us/step - loss: 1.2179e-04 - mean_absolute_error: 0.0065\n",
      "\n",
      "Epoch 09937: loss did not improve\n",
      "Epoch 9938/10000\n",
      "2700/2700 [==============================] - 0s 29us/step - loss: 1.2049e-04 - mean_absolute_error: 0.0065\n",
      "\n",
      "Epoch 09938: loss did not improve\n",
      "Epoch 9939/10000\n",
      "2700/2700 [==============================] - 0s 27us/step - loss: 1.2001e-04 - mean_absolute_error: 0.0064\n",
      "\n",
      "Epoch 09939: loss did not improve\n",
      "Epoch 9940/10000\n",
      "2700/2700 [==============================] - 0s 27us/step - loss: 1.2179e-04 - mean_absolute_error: 0.0064\n",
      "\n",
      "Epoch 09940: loss did not improve\n",
      "Epoch 9941/10000\n",
      "2700/2700 [==============================] - 0s 29us/step - loss: 1.2128e-04 - mean_absolute_error: 0.0065\n",
      "\n",
      "Epoch 09941: loss did not improve\n",
      "Epoch 9942/10000\n",
      "2700/2700 [==============================] - 0s 42us/step - loss: 1.2034e-04 - mean_absolute_error: 0.0064\n",
      "\n",
      "Epoch 09942: loss did not improve\n",
      "Epoch 9943/10000\n",
      "2700/2700 [==============================] - 0s 36us/step - loss: 1.2043e-04 - mean_absolute_error: 0.0063\n",
      "\n",
      "Epoch 09943: loss did not improve\n",
      "Epoch 9944/10000\n",
      "2700/2700 [==============================] - 0s 33us/step - loss: 1.2096e-04 - mean_absolute_error: 0.0064\n",
      "\n",
      "Epoch 09944: loss did not improve\n",
      "Epoch 9945/10000\n",
      "2700/2700 [==============================] - 0s 32us/step - loss: 1.1817e-04 - mean_absolute_error: 0.0062\n",
      "\n",
      "Epoch 09945: loss improved from 0.00012 to 0.00012, saving model to keras/best_baseline_weights.hdf5\n",
      "Epoch 9946/10000\n",
      "2700/2700 [==============================] - 0s 34us/step - loss: 1.1916e-04 - mean_absolute_error: 0.0063\n",
      "\n",
      "Epoch 09946: loss did not improve\n",
      "Epoch 9947/10000\n",
      "2700/2700 [==============================] - 0s 30us/step - loss: 1.2178e-04 - mean_absolute_error: 0.0065\n",
      "\n",
      "Epoch 09947: loss did not improve\n",
      "Epoch 9948/10000\n",
      "2700/2700 [==============================] - 0s 30us/step - loss: 1.2045e-04 - mean_absolute_error: 0.0064\n",
      "\n",
      "Epoch 09948: loss did not improve\n",
      "Epoch 9949/10000\n",
      "2700/2700 [==============================] - 0s 35us/step - loss: 1.2016e-04 - mean_absolute_error: 0.0063\n",
      "\n",
      "Epoch 09949: loss did not improve\n",
      "Epoch 9950/10000\n",
      "2700/2700 [==============================] - 0s 41us/step - loss: 1.2127e-04 - mean_absolute_error: 0.0064\n",
      "\n",
      "Epoch 09950: loss did not improve\n",
      "Epoch 9951/10000\n",
      "2700/2700 [==============================] - 0s 30us/step - loss: 1.2008e-04 - mean_absolute_error: 0.0064\n",
      "\n",
      "Epoch 09951: loss did not improve\n",
      "Epoch 9952/10000\n",
      "2700/2700 [==============================] - 0s 31us/step - loss: 1.1958e-04 - mean_absolute_error: 0.0063\n",
      "\n",
      "Epoch 09952: loss did not improve\n",
      "Epoch 9953/10000\n",
      "2700/2700 [==============================] - 0s 32us/step - loss: 1.2092e-04 - mean_absolute_error: 0.0065\n",
      "\n",
      "Epoch 09953: loss did not improve\n",
      "Epoch 9954/10000\n",
      "2700/2700 [==============================] - 0s 31us/step - loss: 1.2047e-04 - mean_absolute_error: 0.0064\n",
      "\n",
      "Epoch 09954: loss did not improve\n",
      "Epoch 9955/10000\n",
      "2700/2700 [==============================] - 0s 31us/step - loss: 1.2162e-04 - mean_absolute_error: 0.0066\n",
      "\n",
      "Epoch 09955: loss did not improve\n",
      "Epoch 9956/10000\n",
      "2700/2700 [==============================] - 0s 35us/step - loss: 1.2076e-04 - mean_absolute_error: 0.0064\n",
      "\n",
      "Epoch 09956: loss did not improve\n",
      "Epoch 9957/10000\n",
      "2700/2700 [==============================] - 0s 34us/step - loss: 1.2386e-04 - mean_absolute_error: 0.0067\n",
      "\n",
      "Epoch 09957: loss did not improve\n",
      "Epoch 9958/10000\n",
      "2700/2700 [==============================] - 0s 35us/step - loss: 1.2093e-04 - mean_absolute_error: 0.0066\n",
      "\n",
      "Epoch 09958: loss did not improve\n",
      "Epoch 9959/10000\n",
      "2700/2700 [==============================] - 0s 33us/step - loss: 1.2047e-04 - mean_absolute_error: 0.0065\n",
      "\n",
      "Epoch 09959: loss did not improve\n",
      "Epoch 9960/10000\n",
      "2700/2700 [==============================] - 0s 28us/step - loss: 1.2204e-04 - mean_absolute_error: 0.0065\n",
      "\n",
      "Epoch 09960: loss did not improve\n",
      "Epoch 9961/10000\n",
      "2700/2700 [==============================] - 0s 33us/step - loss: 1.1962e-04 - mean_absolute_error: 0.0064\n",
      "\n",
      "Epoch 09961: loss did not improve\n",
      "Epoch 9962/10000\n",
      "2700/2700 [==============================] - 0s 31us/step - loss: 1.2133e-04 - mean_absolute_error: 0.0065\n",
      "\n",
      "Epoch 09962: loss did not improve\n",
      "Epoch 9963/10000\n",
      "2700/2700 [==============================] - 0s 33us/step - loss: 1.2050e-04 - mean_absolute_error: 0.0065\n",
      "\n",
      "Epoch 09963: loss did not improve\n",
      "Epoch 9964/10000\n",
      "2700/2700 [==============================] - 0s 33us/step - loss: 1.2601e-04 - mean_absolute_error: 0.0069\n",
      "\n",
      "Epoch 09964: loss did not improve\n",
      "Epoch 9965/10000\n",
      "2700/2700 [==============================] - 0s 34us/step - loss: 1.2128e-04 - mean_absolute_error: 0.0065\n",
      "\n",
      "Epoch 09965: loss did not improve\n",
      "Epoch 9966/10000\n",
      "2700/2700 [==============================] - 0s 35us/step - loss: 1.2089e-04 - mean_absolute_error: 0.0063\n",
      "\n",
      "Epoch 09966: loss did not improve\n",
      "Epoch 9967/10000\n",
      "2700/2700 [==============================] - 0s 31us/step - loss: 1.2056e-04 - mean_absolute_error: 0.0064\n",
      "\n",
      "Epoch 09967: loss did not improve\n",
      "Epoch 9968/10000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2700/2700 [==============================] - 0s 30us/step - loss: 1.2365e-04 - mean_absolute_error: 0.0067\n",
      "\n",
      "Epoch 09968: loss did not improve\n",
      "Epoch 9969/10000\n",
      "2700/2700 [==============================] - 0s 30us/step - loss: 1.2042e-04 - mean_absolute_error: 0.0065\n",
      "\n",
      "Epoch 09969: loss did not improve\n",
      "Epoch 9970/10000\n",
      "2700/2700 [==============================] - 0s 30us/step - loss: 1.2019e-04 - mean_absolute_error: 0.0064\n",
      "\n",
      "Epoch 09970: loss did not improve\n",
      "Epoch 9971/10000\n",
      "2700/2700 [==============================] - 0s 30us/step - loss: 1.1967e-04 - mean_absolute_error: 0.0063\n",
      "\n",
      "Epoch 09971: loss did not improve\n",
      "Epoch 9972/10000\n",
      "2700/2700 [==============================] - 0s 35us/step - loss: 1.2145e-04 - mean_absolute_error: 0.0066\n",
      "\n",
      "Epoch 09972: loss did not improve\n",
      "Epoch 9973/10000\n",
      "2700/2700 [==============================] - 0s 33us/step - loss: 1.2263e-04 - mean_absolute_error: 0.0066\n",
      "\n",
      "Epoch 09973: loss did not improve\n",
      "Epoch 9974/10000\n",
      "2700/2700 [==============================] - 0s 28us/step - loss: 1.2143e-04 - mean_absolute_error: 0.0065\n",
      "\n",
      "Epoch 09974: loss did not improve\n",
      "Epoch 9975/10000\n",
      "2700/2700 [==============================] - 0s 31us/step - loss: 1.2101e-04 - mean_absolute_error: 0.0065\n",
      "\n",
      "Epoch 09975: loss did not improve\n",
      "Epoch 9976/10000\n",
      "2700/2700 [==============================] - 0s 31us/step - loss: 1.2291e-04 - mean_absolute_error: 0.0067\n",
      "\n",
      "Epoch 09976: loss did not improve\n",
      "Epoch 9977/10000\n",
      "2700/2700 [==============================] - 0s 29us/step - loss: 1.2680e-04 - mean_absolute_error: 0.0069\n",
      "\n",
      "Epoch 09977: loss did not improve\n",
      "Epoch 9978/10000\n",
      "2700/2700 [==============================] - 0s 29us/step - loss: 1.2371e-04 - mean_absolute_error: 0.0066\n",
      "\n",
      "Epoch 09978: loss did not improve\n",
      "Epoch 9979/10000\n",
      "2700/2700 [==============================] - 0s 34us/step - loss: 1.2560e-04 - mean_absolute_error: 0.0068\n",
      "\n",
      "Epoch 09979: loss did not improve\n",
      "Epoch 9980/10000\n",
      "2700/2700 [==============================] - 0s 29us/step - loss: 1.2060e-04 - mean_absolute_error: 0.0064\n",
      "\n",
      "Epoch 09980: loss did not improve\n",
      "Epoch 9981/10000\n",
      "2700/2700 [==============================] - 0s 31us/step - loss: 1.2327e-04 - mean_absolute_error: 0.0066\n",
      "\n",
      "Epoch 09981: loss did not improve\n",
      "Epoch 9982/10000\n",
      "2700/2700 [==============================] - 0s 32us/step - loss: 1.2497e-04 - mean_absolute_error: 0.0067\n",
      "\n",
      "Epoch 09982: loss did not improve\n",
      "Epoch 9983/10000\n",
      "2700/2700 [==============================] - 0s 30us/step - loss: 1.1981e-04 - mean_absolute_error: 0.0064\n",
      "\n",
      "Epoch 09983: loss did not improve\n",
      "Epoch 9984/10000\n",
      "2700/2700 [==============================] - 0s 33us/step - loss: 1.2256e-04 - mean_absolute_error: 0.0065\n",
      "\n",
      "Epoch 09984: loss did not improve\n",
      "Epoch 9985/10000\n",
      "2700/2700 [==============================] - 0s 29us/step - loss: 1.2034e-04 - mean_absolute_error: 0.0064\n",
      "\n",
      "Epoch 09985: loss did not improve\n",
      "Epoch 9986/10000\n",
      "2700/2700 [==============================] - 0s 33us/step - loss: 1.2173e-04 - mean_absolute_error: 0.0065\n",
      "\n",
      "Epoch 09986: loss did not improve\n",
      "Epoch 9987/10000\n",
      "2700/2700 [==============================] - 0s 34us/step - loss: 1.1888e-04 - mean_absolute_error: 0.0062\n",
      "\n",
      "Epoch 09987: loss did not improve\n",
      "Epoch 9988/10000\n",
      "2700/2700 [==============================] - 0s 36us/step - loss: 1.1966e-04 - mean_absolute_error: 0.0063\n",
      "\n",
      "Epoch 09988: loss did not improve\n",
      "Epoch 9989/10000\n",
      "2700/2700 [==============================] - 0s 31us/step - loss: 1.2167e-04 - mean_absolute_error: 0.0066\n",
      "\n",
      "Epoch 09989: loss did not improve\n",
      "Epoch 9990/10000\n",
      "2700/2700 [==============================] - 0s 31us/step - loss: 1.1944e-04 - mean_absolute_error: 0.0063\n",
      "\n",
      "Epoch 09990: loss did not improve\n",
      "Epoch 9991/10000\n",
      "2700/2700 [==============================] - 0s 31us/step - loss: 1.1974e-04 - mean_absolute_error: 0.0064\n",
      "\n",
      "Epoch 09991: loss did not improve\n",
      "Epoch 9992/10000\n",
      "2700/2700 [==============================] - 0s 34us/step - loss: 1.2194e-04 - mean_absolute_error: 0.0065\n",
      "\n",
      "Epoch 09992: loss did not improve\n",
      "Epoch 9993/10000\n",
      "2700/2700 [==============================] - 0s 31us/step - loss: 1.2190e-04 - mean_absolute_error: 0.0065\n",
      "\n",
      "Epoch 09993: loss did not improve\n",
      "Epoch 9994/10000\n",
      "2700/2700 [==============================] - 0s 36us/step - loss: 1.2433e-04 - mean_absolute_error: 0.0067\n",
      "\n",
      "Epoch 09994: loss did not improve\n",
      "Epoch 9995/10000\n",
      "2700/2700 [==============================] - 0s 36us/step - loss: 1.2007e-04 - mean_absolute_error: 0.0063\n",
      "\n",
      "Epoch 09995: loss did not improve\n",
      "Epoch 9996/10000\n",
      "2700/2700 [==============================] - 0s 33us/step - loss: 1.2555e-04 - mean_absolute_error: 0.0068\n",
      "\n",
      "Epoch 09996: loss did not improve\n",
      "Epoch 9997/10000\n",
      "2700/2700 [==============================] - 0s 31us/step - loss: 1.2199e-04 - mean_absolute_error: 0.0065\n",
      "\n",
      "Epoch 09997: loss did not improve\n",
      "Epoch 9998/10000\n",
      "2700/2700 [==============================] - 0s 35us/step - loss: 1.2350e-04 - mean_absolute_error: 0.0066\n",
      "\n",
      "Epoch 09998: loss did not improve\n",
      "Epoch 9999/10000\n",
      "2700/2700 [==============================] - 0s 33us/step - loss: 1.2058e-04 - mean_absolute_error: 0.0064\n",
      "\n",
      "Epoch 09999: loss did not improve\n",
      "Epoch 10000/10000\n",
      "2700/2700 [==============================] - 0s 33us/step - loss: 1.2016e-04 - mean_absolute_error: 0.0064\n",
      "\n",
      "Epoch 10000: loss did not improve\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x7f0f9892bc88>"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "filepath = \"keras/best_baseline_weights.hdf5\"\n",
    "checkpoint = ModelCheckpoint(filepath, monitor='loss', verbose=10, save_best_only=True, mode='min')\n",
    "callbacks_list = [checkpoint]\n",
    "model.fit(x_train, field50.A, epochs=10000, callbacks=callbacks_list, batch_size=32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAuoAAAD3CAYAAABLsHHKAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz\nAAALEgAACxIB0t1+/AAAADl0RVh0U29mdHdhcmUAbWF0cGxvdGxpYiB2ZXJzaW9uIDIuMS4wLCBo\ndHRwOi8vbWF0cGxvdGxpYi5vcmcvpW3flQAAIABJREFUeJzsvXmMJOd55vnEmZFHVWZVdVUf1V1X\nH1Q3myLZPETSHo2OlneNubBAy5K9gIEZmNIahhdr2EsOvTastXdWIHfG48FK9pLaMSStvbDE9hoz\nliHbbFsjjWxT7EMSSYkixa7Kuu+qzMrKM679I/KL/OKOPOro4vcDClWVERkRecQXT7zf874vZ5om\nGAwGg8FgMBgMxuGCP+gDYDAYDAaDwWAwGF6YUGcwGAwGg8FgMA4hTKgzGAwGg8FgMBiHECbUGQwG\ng8FgMBiMQwgT6gzGPsJxXI7juBcP+jgYDAaDwWAcfphQZ+w5HMdd4zjuGY7jchHrXWmud2W/ju0A\nuApg6qAPgsFgMA4D75Fxv2P24v1h7/m9BRPq+wzHcVMcx73IcZzJcdzzHMdNUcte5DjuLsdxzxzk\nMfYa0zSvA/g0gMGI9e4AOAvg0U731XxPP0XeQ/r97TUcx73i89hUcwC86r454TjuKoAbEdu83VzP\nsc2eHbR3f1Ou/z37ZzCOEs1z9PnmGPwMGS+aj13rwfbbOof2+xx0j5EHAf2aezHuH2a6/Tz34v3p\ndJvsenEwiAd9AO81TNOc5jjuWQCfMk3zWfJ4U9DdNk3z0wd3dHtKIeZ6dzvdAcdxnwKwCeCrAB5t\nRgumAEx3us2A/ZCouN8A9bJpmo8017sF4AsAPt4c4KZN0yxwHBe2+Y+bpmkf7169hpBtO/bPYBw1\nmmPwZwE8Y5rmC/QyEjxxP94msc+h/T4H3WPkXuwjxjH4vea414d7kV58nh1fF3u1TXa9ODhYRP0Q\n0BTpV03TfOmgj+UeJwegYJpmAcAtAM/txU5M07zh91k1B7Itar0CWmL+CoCpZsRuMGjK0SXSc9ij\n1xC0bTboMt7LNIMnz3Uzi9WGSD+Ic9AeI03TDJ3d2wv2ckw7rByFMZVdLw4WFlE/YJoXhE/T0XXX\nchK93QLwmGmazzYfexHA883HP2Ga5sepKagpADkSFWqeZD8D6044B8A3YuTaLjkBP978f8pnu1dg\nRWWmm8tukBO3uewqgDvNfTr86X6vK+b7FfQarwD4GAASsSav9WMcxw2SY2vn/YxzPC6m4I0MbXEc\nd6Vp/0FTqPt69Zuv4QsAXmzeCDza7WsIer/8tt38n95/4Gfs813JAfgYmRGK+51jMA4hNwBcA/BC\nwLl2DdZ5cqN5juUA3AZwHcBX4D2HDuQcdBMwRvqNGXH3B4RcHwLwHdOay3LNfXheR5zrRdj7QS8z\nTfMl6jP8qGmadzq49sUagwF8Fv6fJ31tvNGcaQ36nkQSNN6GXaOD3ju/9wf7/F1luDBNk/3s8w+s\nL6kJK8p6DcA2gCs+600BeIX6/1OwpmsB60v/YvPvK83fd2Gd4GT51ebfz9PbB3At5NieJ9tt/n+b\nXh/Att+xkXWp13fbtewurMEj6nU9A8sWFHR8vq/R77nN5fSxt/V+xvgcTdf/n4JlfXEfb6ztUe9/\nz15DxPvl2LZ7/2Gfcch3ZYpaFus7x37Yz37/NMcoM2DZiwBejjjXPgVL1JBl9DnqPocP5BwMeG1+\nY6Q9ZnS4P8/1IeK9D3rNQWNJ4Ofgs+2oMYl+7a+4xqiOrn2IHoPpz9NxbWx+HuS5sa9tAa/bMd7G\n+Czd34Wo92dfv6vsp/XDIuoHjGma15t3qC/DSu6guQYr+kHutLeoZQU0PWamdccLAI8AuNqMlgyi\nVV3kRQCvcBw3DetEirpTv039vQV/f/Q1WBEBGjrae8u1jI40h72uKIJeYxzafT/bZQveaHloAq0P\nmxHLe/WdiLP/wM/YbE2bu78r5PW3+51jMA4LgwBuIuRcM62o410AzzZnRenzxH0OH9Q5GAfHmNFM\nMG13f72yPwS9jnavF928H3Gvfe2MwfTn+TOgro2ucbGba5tnvI35WXbLfn5X37MwoX6AkBO5Oeh/\njOO4F03vVNBWyEllDxDNqa+/QTO5g+O4x1zbONuclvo0x3Evm51ZO6KgLS5RAjXsdfkS8Rqjnks8\n4bHezw6Zhs/r7kL4O2j3NbTzfjXtOXGO02NjCjqOffrOMRi95iosy8JVhJ9rN5rJmVtm09rm5oDP\nwbhEjXu93p9NG6+57evFHtPT60g31zbqeBzjLaybTTc9+SwP8Lv6noQlkx4engbwM9RdOmB5Hh2Z\n+a7lNOSiYvv9qPWfa1YyuONzI9Ap12FNldIQ79pX4Y0G0CdrO6+LJuw1+lFASzgPdrHfWLgHrmak\nrduLSzevIer9cm/bTdhnHMVefOcYjD2lGYV8qXkuR51rzwN4FuEVSw7yHOyE/dhf1Gv2O6ZejNub\ncF6HOq16083xfBWuamHN57Z7bXPjN962+1lGvT+H7bv63uGgvTfvtR9YAvZ5WB7159Hyz5GEJBOU\n/w7WCfxM8/e15mNXmuu+jJYPLtf8/1pzXZK8QZ7/qeaya6C8b65jI9t9Ba3yg9vN7eSa27CPj9r2\ntebvqaDjhjUd+DJaHryw1/UKfLxrEa+Rfu5V6r1+sbn+VDvvZ8RneKW5DfIZXvVZRvaTa+O74Xn9\n3byGsPfLb9sB+/f9jGN8V2J959gP+9nvHzjHYPI9fQYuj25zXc+55lr+out/93lxYOegz7E6xki/\nMaOb/cF1fYh4/wNfs9/riPocYh5fDq3rxbXm6365+Xin176oMdjv87yC5nWjeRzk2OJc23w93QgY\nb+N+ls3HAt+f/f6ush/nD9d8ExkMBoPBYDAYDMYhgllfGAwGg8FgMBiMQwgT6gwGg8FgMBgMxiGk\n3aovzCfDYDAYBwvXg22wsZzBYDAOllhjOYuoMxgMBoPBYDAYhxAm1BkMBoPBYDAYjEMIE+oMBoPB\nYDAYDMYhhAl1BoPBYDAYDAbjEMKEOoPBYDAYDAaDcQhhQp3BYDAYDAaDwTiEMKHOYDAYDAaDwWAc\nQphQZzAYDAaDwWAwDiFMqDMYDAaDwWAwGIcQJtQZDAaDwWAwGIxDCBPqDAaDwWAwGAzGIYQJdQaD\nwWAwGAwG4xAiHvQBMN5b6LoOXdchCAJ4ngfHcQd9SAwGg8FoE9M0oWkaAIDneTaeMxh7BBPqjD3H\nNE0YhoFGowFd19FoNMDz1mTO+vo6Tpw4AUEQ7B8y4LNBn8FgMA4Xpmmi0WjAMAzU63WYpgmO41Cp\nVGCaJnK5nGMsZwKewegOJtQZe4ZpmnYEvVgsYnZ2FpcuXQLHcRAEAaZpYm5uDiMjI9B13fN8nuc9\nAp4N+gwGg7H/kAi6pml44403MDk5CVmWwXEceJ5HuVxGrVZDJpOBqqq2gAdgr8MCMgxG+zChzug5\nRKBrmmYP1jzPOwZuAPYgTaLr7m2QC4Oqqo7H6QGf/M0GfQaDwegt9DhMgilBYzZ5zL3MNE0AgGEY\nLCDDYHQAE+qMnkFHXADngE6EelyCRDfZBrkRcMMGfQaDwegOejbUMAwAzjGZDryQMZn+m4aOqvvt\nhwVkGIxwmFBndEVQxMU9kAYN4u0SZ9BXVRWNRsMTvWeDPoPBYATjNxsaNJ4TAU8/1u4YzwIyDEY0\nTKgzOiIq4uKG53kYhrGnA2kng/7KygpGR0c9Ap4N+gwG470CHWyh7YpB0KI8KqLeCZ0EZCqVCnRd\nx8DAAAvIMI4UTKgz2iJuxMUNGcQLhQJWV1eRTqeRSqV6NrBH7Zv+TbO4uIiTJ086LlAEIthFUbT/\nZoM+g8E4KhDfuJ9dMQw/Ud5LoR61b7/xt1KpoFqtor+/3zcK7zejygIyjHsBJtQZsWg34uJ+7sbG\nBgqFAmZnZzE4OIhGo4Ht7W1Uq1W89tprkCQJqVTKFvDpdNquKLCXhCWzAsFTr37TrmzQZzAYhx06\nIh1nNtQPMkNKs19CPYyw8ZzclLCADONegwl1RiidRlwAS+QuLi5iYWEB2WwWqVQKDz74IGq1GkRR\nxJ3H/xHSWlMQAygB6PvaV7G5uYn5+XnU63XwPI9kMmkLePIT9xg6JWrqNWzQDxLwbNBnMBgHhXs2\nFGhfoBMOMqLeCZ164VlAhnEYYEKd4YEI0a2tLQiCgGQy2daA3mg0MDc3h5WVFZw8eRKPPvooBEHA\nzZs3AQCvP/XhwOcu/dOfgZDgYeomRAAP/sN/QbVaRaVSQblcxvr6ut1YI5FIOCLwqVQKkiT14i0I\nJWrQd1cwIM8JE/EMBoOxF5DZ0OXlZQwPD8e2K4Zxrwn1IFhAhnEvwIQ6w8YdcVlbW0M6nUY6nY71\n/HK5jHw+j2KxiLGxMTz11FN25NswDN9BnBc5GJoJIeEfIf/+kx8CJzgHvsf+4VswTRP1et0W8Csr\nK6hUKlBVFYIgOCLw6XQaiqLsi42G/k0TNOhXq1UYhuHo5kdEPRv0GQxGp7hnQ+/evYvjx4/3ZNsc\nx0FVVczNzaFeryOTyQCAb530e5VOAjIbGxsYHh6GJEksIMPoGUyoM3z953T30KjnFgoFzMzMQNd1\nTExM2N1HaUi05c2PfRhSWgJPiW9REVDZrNn/8wIHCBz0huF8DADHc/jukx90PP7gt7/p2JemabaA\nLxaLWF5eRq1mbT+ZTDoE/H5FgIIG/XK5jGq1yrr5MRiMrumF/zyKarWKjY0NVKtVjI2NYWBgALVa\nDYVCAcViEa+99hpEUfTkHCUSiSMxboUFZObn5zE0NIRGoxEahWcBGUY7MKH+HibKf+6XMEQwTROr\nq6uYnZ2Foig4e/Ysstls4L78BiJREaxlPI9En+xYplZUCHK4D11URBiqju//5D/2LHvw299Ef3+/\n4zHDMFCr1VAul1GpVLC1tYVKpYLXXnsNsiw7LiypVGpfklkBf98/6+bHYDDi0m653E4oFovI5/Oo\n1WpIpVIYHR3FyMgIdF2HIAgYGBjA3NwcLl26BFVVUalUUKlUsL29jcXFRdRqNXAc5wiU7FfO0X5h\nmqYtwt2PA60oPAvIMNqBCfX3GO1EXPyEuqZpdoLo4OAg3v/+9yOZTLZ1DLzA2SI9CCklQUpJ0OvW\nTUS91ICSS6C+04Ag8dBV67h4SYChtoQsL1nbff2DH7If4wTrIvDAN/7WvjAQyuUyHn30Uaiqagv4\njY0NlMtlNBoN8DzvubAkk8meXVjcURf7mGPUEQ7r5uf2ULJBn8E4enRaLred7W9sbCCfz0MQBExO\nTiKXy2Fubs6zLu1RlyQJ2WzWE7wxDAPVatUea9fX123730HlHPWadsZzFpBhxIEJ9fcInURcOI6z\nB496vY65uTmsrq7i1KlTePzxx9seRDO/+RwAOEQ61xS8oiJCq1miXEq1tiskROh1zY64J/pl+3n1\nUsNejwh0gFhnBOgN58D3xoc/4vj/gW/8rf06ZVmGLMsYGBhwrKPruh0ZKpVKWF1dRbVahWmaUBTF\n44UXxfZOqSChHkanFQzYoM9gHA3IjXon1bjiYBgGlpeXMTc3h76+Ply8eNH2oZP9dZJMyvO8b96T\nX85RuVyGpmkeG00qldqXnKP9gAVkGHFgQv2I003Ehed51Go1vPnmmyiVSp4E0XZJ9CvgeGu/9VLN\ndx1apAchJSVodd0W71JSwu5aOXB9XuBg6M4LiCDx+OFPXUXKMPED6vH7b/yNcz1BQF9fH/r6+hyP\nm6aJWq1mX1iWl5dRqVTsC4tbwAf5MzsR6kHEGfRVVfUkQOm6DlVVkc1mWTc/BuOQQgs3EkBp9xyN\nGm9UVcXCwgKWlpYwMjKCK1euIJFIeNbjed4+hl50JuU4DoqiQFEUDA4Oeo4pykaTSqVQq9UCrZr3\nIp0GZMrlMvr6+iDLMgvIHBGYUD+idFP/3DRNbG9vI5/PQ9M0XLp0Cffff39XJ/rb/91/4/g/M2IJ\nX0OzBvtqoRYq0jmBg5T0LiePKdnWxYQXONSKdQiyM8pu6CYEKfw9+MHVjzqsNQS3gOc4DslkEslk\nEkNDQ45l5MJSLpdDLyzpdBq6ru+LPzNs0C+VSlhZWYGiKJ7lrJsfg3Gw9Mp/zvN8oFCv1WrI5/PY\n3NzE6dOn8YEPfCB0dpCIcnpbe1WeMa6NZnNzE7VaDevr60gkEp5k1nvRRuNHVEBmdnYWk5OTvg2p\n/MZzFpA5/DChfoToNuJiGIadIJpKpXDq1Cmoqopjx4719DiJuOZ4DoIsQm9oSOYU2wZDqBaqnucA\ngFbXfUU7WU9vaA7hTqhseaP4HM/BNFoXFyLk3WL9B1c/CsCy7dDVaC799Q3vMbRxYSkUCrYP1O2F\nlyRpX0pKkgQoQXDmDbBufgzGwdFr/znJOaIDA6VSCTMzM6hUKhgfH8eFCxdiBQ4OQx11t40mmUyi\nVqthbGzMttFUKhWsrq76lu4lv4+KjQawPgPDMOzykIS4jZ1YQOZwwoT6EYAM6Nvb2zAMA/39/W0N\n6JqmYWFhAYuLizh27BgeeughKIqCjY0NbG5u9uw4Od4/Kg4AomI9TvvKkzkrSVVOy9hdK9mP+21D\nTIjQmomnRPzTSEkJuTMy1KraPBYe1e2qfVy8EP5eBSW//vCnrgY+xy3i/fyZ8/PzEAQBQ0NDtoBf\nW1tDuVy2Lyx+/sxe+1H9ttfp1Cvr5sdgdA4JtszNzeHUqVM9858ToW6aJra2tjAzMwOO4zAxMYHB\nwcG2zk8SnafZb6EeRJiNhi7dWygUHLOddAfsdDqNZDLpCV7cC/iN51FReBaQOdwwoX4P4464lEol\nNBoN5HK5WM+v1WqYm5vD+vo6Tp065ZnuDCvP2A5rv/JzkJKS7U8H4Pg7DCLglWyrsky9VG9r/0E3\nB8mB1jY5nkONiuADrai6W6QLMu+IqpMykuQxQeah1XRHFF6rBTcCGfqjP0YikUAikQi8sFQqFezs\n7DhqwiuK4hHxcZJZ/37iCehV5/GYqomzW7cinwuwbn4Mxl7gtivOzc3h9OnTPd3H8vIylpaWkMlk\ncN9993lyb+JCIrf7YX3pJaIoor+/37d0L90Be3NzE5VKxa5Gcy/ZaIICL0GwgMzhhwn1e5CgjH9B\nEGIJ61KphHw+j93dXYyPj+PcuXO+J3avhLpWU6Fkk7Y4rxWrEc9oCXQ3HM9Byba81LViy87iFuT0\n/2E3BnLairQrOW+ZSV7gHBYcN+5a7+7/w8pQChIPtaZj/Wd/DusATN2gnidCrbSSPulkWKH5twqg\nCGCr2nqeobXWM1XnRZNeRkPW+9bgo4HH6uaDAaI+atD36+ZHRwxFUXREb9igzzjKkBtbTdM8/nMi\nhruNppMZ00KhgGQyiYcfftg3H6UdDoP1pZfQs53Dw8P246ZpotFo2LOdQTaa/W6gF0YvvjNA9wEZ\n0zQhyzILyPQAJtTvIfwSROkvfphQJ9Od+XweAGJNd/ZKqANOoaxkk+BFS8BWt73VWmiRHmZJkdMJ\nh6+dFzhUtioxjiV4ECPinlhkREVC0jVBUS1UfZsx0Y+5Bbo7qk688JIiQHVF20Ul/LSU0hLUsmrv\nU69Gf0ZBIh0AkqMJVBfbm6X41uCj4CTnZ/OPVm8Grh9n0J+enkZ/f78jOZd182McRdyzoYD/eN5N\nsnmtVsPs7Cw2NjZw6tQpDA0NYXx8vGuRTo6VPm73Y0cFjuMiZzvpDtiVSgU3b960O2DTQn4/bTT7\nkdsUFZC5desWHnnkEcdzwmZVGcEwoX7ICYu4uKFLZhEMw8DKygpmZ2eRyWRw4cKF2NOdvRDqa7/y\nc57HxKSV6GmoGpIDznq6taK/0KY96DRSUrJFNQCkBq1mRlIqgdJywXd9ra47/vfbThjEO0/ffNA3\nCFpNj4ykB0GLdCklOaLqQchZEY2i9d7IORGNgvd9cqw/KMJUBdTWGqHrAUDiuIz6avR6APBfjz9m\n/x0m2t3Q0UNSUgxg3fwYRw862T8qQTTuDKmb3d1dzMzM2DOm58+fB8/zeOONN3oWeOF5HtVqFd//\n/vdRqVRse0i9XkehUDj09pBe4GejuXnzJh555JFIGw0t4mVZDtnLvQX5HtO13glxbZEsIOOFCfVD\nSicZ//TArmka5ufnsbS0hOHh4Y6mO3sZUSeISvig1HdqEHrdEqfuaDstjKWkczt+kffkQEu0azXV\nsT268kwQcjr4WKWUDK3mvUFwQwt4URFg6t6IE4mqB0XSadsLWYeOqtvLMv43B/KgiNoa1TBD9R6D\nX1Q9cbzzC8ir5560/37i3X+I9RzSipwQFIV3d/OLGvRZ1IZx0HRSLtcv8BIEKak7MzMD0zQxMTGB\noaEhz3nR7XhumibW19fx4x//GLqu4/Lly0gkEjAMA7u7u1hfX8fq6qqjWZG7ykpQT4mjQlwbTVjR\ngHQ6fU9Xo3GP5UB8WyQLyHhhQv2Q4RdxiTv1yfM8Go0G3n77bWxsbGB0dDSyHm7U9noh1IkYjhLp\nbvpODUJvWAKztr0LoCXQwwS2lPKWZgRgR+/Jc6vb0TYZ/+3Lrv+t/akVr30kSMADsMtCVjaDj0NK\nSXYH1jBLjJwVbUEv50TU1633TR50PseKplvrKSNyYFQ9SqQnR63XHCcqf/OBnwAAPPbG34Wu5ze4\n+xFlo6G7+ZFziER42KDP2C/oJmOd1D+PE1E3TROrq6vI5/NIpVI4f/68J1GS0M14Tncq7e/vx9TU\nFLa3t9Hf349Go2HbQ2RZxn333Wc/j+4psbm5ifn5edTrdfA87xGmyWTySJ+H7dpoSNGAg7bRdELc\nsRxgAZk4MKF+SOimQREA7Ozs4N1338X29jYuXbpkT3d2Q7dCvfiZp5HIpgGeR6MY3DmUQCwxfigD\nGdtbXg/ZVlASKoEW+MmBlO2Vr2zuBlaHCdsXHVXvlNRQCnJGQWPXGpjpv8lyAl3zfXetDEONF3ED\nvKIdsMR6FG77CxHpQfSfT6G64hXwUYK9ncE9iKioDTnP6MeDojZHddBn7C29bFAUFFHXdR0LCwtY\nWFjA0NAQHnzwQSST3mR49/baHc/p0r3Dw8N2p9JCoYCtra3I5wf1lNB13RampOGanzC9l8okduPP\nD6tGU6vVHL03iI1GlmVPMuthsRv1aiynf9O81wIyTKgfIN1GXEzTxObmJmZmZsDzPEZHR2GaJk6e\nPNmT4+tVRF1IyJCb47SUSqK8YtVm5yURhmrdmIjJRGSUnIjiRDZtr1svlm3bi59ID4quA4CUVmyb\nTWoo41jG8Rwqm7v2dsm+3dH0MKRUwjfKDjgFd6dkRpz+fpP+rFxV3cwz/vvbeqdo/03sL91YXqJI\nDEh4/YMfAgCc/fpfIJlM2jeUvRjcg4gz6KuqClVVsbu7i0KhgNOnT4PjOFy/fh1Xr17F+Pj4nhwb\n42jQ6wZFJJmUpl6vY25uDqurqzh16hQef/zx2OKsnfG80WhgdnYWq6urvjOzYV1O4yAIAvr6+jz5\nUu4yiRsbG6hWqzAMw1OONp1OdzxbvBfsRSItmXlIpZwzs8RGQ96n9fV15PN5qKqKarWKt956yyHi\n93u2Yi/HcqC9gMzMzAxOnTqFRCKBRqOBL37xi3jmmWf27Nj2gsPzLX8PQQb0jY0NO9LQzoBOT0P2\n9fXh4sWLyGQyaDQamJ+f79lxdirUSQJrGoCQ9PrilQFrcOZ4zjc6LiQkO3ouyJJtfyGCmRb0iWwa\nUjoJvd5Abbvk2A5ttXFHv0n0XkhItlh3Q4t3v+o0QPCNgFukh4l2OROcO2D4eNr9MF2fk5JNxiqD\nOXjBGeni3me9t+tvbvuuHxVNT487X4uQDJ7VufvT/wT1f/d/wDRNKIqCSqWC9fV1+wKzXxdh97lH\nSp+SG4i/+qu/wk/8xE/sy7Ew7j0Mw0C5XEaj0UAqlWp7NjQIevwtl8vI5/MoFosYGxvDU0891fY+\n4ozn1WoVMzMzKBQKofshyd+9JszfXa/XUS6XUS6Xsby8jHK5DF3XIUkS0uk0dF0Hx3FoNBr70tXZ\nj/3aJ22jGRgYsB83DAO3bt3C6OgoKpVKoI2GvuHZC0Gt6/qB3ET5BWRKpRIkSQLP89ja2sI3v/lN\nJtQZwbj952+99RaefPLJ2Ce3qqqYn5/H8vIyRkZG7GlIgl8EphvaLbdFT8defuWLsZ6TyLaiwo1S\nZ55xgjLQBzGZgF63bBfE106wvfIhFhs/pHTSWdKR41DZ2Ale3yXe/UQ7bW2JSy+i8HEZvjzgKWO5\n8UPvVDftc6dFevKE7Gt/UXIKaoXWa0/86v+MRL+MsZf/E773ve9BVVUsLS3ZF2E6GY1cXPY6GU1V\nVceFvlgsOi6GDAY99a7rOra2trC7u4tz5871bB+CIKBYLGJ+fh6apmFiYgKXLl3q+LsfJtRLpRKm\np6dRq9UwMTGBixcvhu5nv0sxclyr2yhdvhWAnaC5srKCcrmMH/zgB54ETTKG7GWCZjczDL08BkEQ\nYtlotra2Im00nb6evY6otwNJauY4DqVS6Z4cy5lQ3we69Z9Xq1Xk83lsbW3hzJkzgQmiva7SEvck\nJZH85eVlezq28soXISTcSZfBHkoxnQQntk5s2tMebl8J3mbm9DD0WlO0b1nCmoj0MJtNaNfU5nuS\nOuafsOWOmvuJdhp3NJ387/apu1GyiqPZk3NZuFc1iPRwn2338ePYpVYClJ9oj9z+Se9xJfqt78jc\nx/8FxN/6NxgbG3Msp5PRtre3HS2/3ZGhXnlZycBOKBQKsbv9Mo42Qf5zURR7FiQxTRNra2tYXFyE\nLMu4dOmSx+PdCe7rA10pBgAmJycxMDAQa9w/TDXTZVmGLMv2jAaxqNEJmn5jhzuRtdtZkMMg1MOa\nHbVjo5mdnUWj0bBvdtzjbNTr1DTtwIT6Gx/+CLa/v+NoykeO914dy5lQ3yPcERfAO8VOBs6gL3Sx\nWEQ+n0etVsP4+Dje9773RUY59pNarYZ8Po/NzU2MjY3hySeftF+LW6S7CRXKHI9EruVdVHf9bSft\nRMaVwZawDk1GpSLy7e4jO3bMJdT7UNlo2XGISKeTRmnCLDCJvoQtzN0i3VpWbS7rTKS3gyDxOP7g\nMfv/jXdaot2vEVRc+v/X/wUSSYBdAAAgAElEQVT42l87HgtKRjMMA5VKxf5ZX1+3vazd1irWNM1x\nMVNV1TFzxXjvEeU/78Vspq7rWFxcxMLCAgYGBnDq1Ckkk8meiHSgdb0hNwL5fB6KorTVW4Pe1mER\n6kGEJWgSUVoul7G2toZqtQrTND2JrO1YQw6LUG9XIAfZaABrLKxWqyiXy9jZ2XHYaBRF8ZTeJPs+\nyIj69veDZ7u3t7dZRJ3RXsY/GdzdTQE2NjaQz+chiiImJiYO3ReLbqgxOTmJ++67z/H66n/wHKRc\nv/1Yo1hyCHchIcOg2sj7+dhp5FxroNXXLFHoJ6DjiupENm1Xe6F97e7nWwmuvP23Vm2veycApI4R\nP377F1viT0/0Bb+usGUEJZuMLEWZGkqHLg/jxPuP23+v/2jdd530yWRkx1UAmP74P8HUy38RuR7P\n88hkMshknEnAUS2/3WXOFEXxRKDcEfVecOfOHVy5csV32fXr15HL5XDnzp17zjt51CHBlqjZUFEU\n7XXapdFoYG5uDisrKzh58iQeffRRyLKMhYWFnloZSXfqhYUF5HI5PPDAA57oalz2yqO+H4SNHXQi\nK20NITf/tDB1J/GSiiMHSTedbP0QRTEw6bdWqzlmLCqVCnRdhyzL0HXdcdNzUDkD7pvJQqFw6PRU\nHJhQ7xGdZPyTwV2WZRiGgaWlJczNzSGbzeLSpUtIpzsXT3tBoVDA9PQ0dF3H5OSkp6FGEHK2z7a1\nqAVnwqeQVOzoupCQHdFsAA47DABkJkZhNNdpFFp3zu4bAbIdQZFt+wsNn5Bh1BuWrz2dtEV4vSnc\no0S/23ceFg2XMklo1LrZiRHH/4TyWtHzGBHidDTdTSfR9ORAquM68mGMXBoBAKz9cA3JEzIaRc3X\n8hJGXLHuR5xaxX5JVnR0qFKp2D7YXkQNb9y4gU9/+tO4e/euZ9mdO3cAAFevXsX09HSooGfsH6Qb\ndNBsqJtOIuqVSgX5fB6FQgFnzpxxzEiSbapq9+VfSfO72dlZpNNp+0agGw6T9aVX0Ha6Y8daM4b0\nzX+5XA5s6ESu4wcZWQ+zvvQS2kbjfq9UVcW7774LnucjbTR+gZJeQGwv7kBosVh0HO+9AhPqXRI3\n4uKHIAio1WpYXl7GysoKjh8/3pNBtJeQTnQzMzOQJAlnz57taipWIpYWQYBWLIWuKyQTdvlGPxKD\nreNouG4AxBDvetgyZYiyyLiqyBB/epTv3HefqYSvOKdJj2RD/vdWYYkTTSeECfPUUNqTOAoA6WNp\nlDei69/7QQT7dt573O6EUj+6EetBRNUqJtGh3d1dvPvuu/jzP/9zfPvb30apVMLv//7v433vex8u\nX76MkZGRtvZ79epVTE1N+S77yle+go997GMAgKmpKdy4cYMJ9QOCtDjXNK3tcrnteNSLxSJmZmZQ\nr9cxOTkZmLjZTmdSP+r1OmZnZ7G+vo7R0VGcP38etVqtJ9cXItRVVcXGxgYymUzH0fnDTtjNP51D\ns7W1hXK5jJs3bzoaOrXj7e6W/RLqQXAcZ+cMDA4OOt4vunZ+mI2G/PRiVpMUBiAUi0WcP3++6+3u\nN0yod0i7ERc3lUoFOzs7ePPNNzExMYEnnnji0GRJA60SiySCcP/998eK8Nf/4DkATr88rygwtVZk\niE8mYTQaELOWaNdLrQRGjmuWZfSJaAshyagyJdrVQrBHTXTZbMJEe3p0BDpld+F2WsdJoulRIt29\nXIwh6v3InBhw/U//115iZ3Kg8wtq34l+R8Jp9swQyuvB7/eJB05i5Y3lwOWJPgX1kjVYuxtOrf3S\nJzHy+T/p+Fjj4o4ObW1t4fLly3jkkUfw+uuv49d//dchSRL+4i/+Am+99RZ+6Zd+qWf7LhQKjovZ\n5uZmz7bNiEcv6p8LghBqfSEBj3w+D0mSMDk5GZnU1qnvnS7lOD4+jnPnzoHneaytraFS6XwWrfJv\n/ycAQOrXfg+NRgO7u7u4ffs2crmcbRMpl8t4/fXXbYG6lyUADwN0Dk0ul4Ou67h8+bJDlJLZu2q1\nCo7jkEwmPd7uXonrgxbqBD+PelDtfNpyVKlUHDYaUnrTnW8UdG4Wn/tXjv/9CgMw68sRp5uIC6FQ\nKCCfz6NeryOVSuHMmTOOerHdQryDnZ6sJKFpfn4ex44dQyqVwv333995STCXSPceLw+RinDqZf8I\nrqDEF7hSrh9cc5BQiy0R6Rbp7SJTCa6JwSzqW16rCm2JiRNpt4/NFXGX0taxquXoMo59p1pCj9iI\ndhY2fWupE5FOoutub3pqKBNa+aUTTjxgNeDa+LG/f92PzIj1Xu+XWKchFQt4nocsy5iYmMDTTz+9\nr8fA2Hvc5XK7qX8eVHHLbWmMG/AI22YQJFLfaDR8Szl2UxWMiHQAqP7er+KND1yDIAh44oknUK/X\nbTH02muv4fz583aOyPb2Nsrlsu3zdgv4w9SwqFtoy0u3DZ06fX8Os1APgrYc0ZAZG/Jd2tjYsPsV\n0DMW5LeiKJh7Ne/YBhPq7yG6jbgERVPefffdnifkCILQ0cmqqqojoYl0vNva2mrLc8c3BbVZb4BX\nvMLY7zEaMUfZWdat6KKfSKerxvCKAqPR8qFz1AAhZfshDzZF+3YRiFmWUXAfp+t5gpKwrTd2l1RK\nuNsivYNBs9OoOwDIfSk0ShX0n7Y81tkxev/tC2UadzQ9c9xpIXFX8slNjKC62bpRGrl4Amtvrdj/\n94/6l7h0s99inU4K2+sa6iQaCVgXEXeNaEbv6bZcrh/u8dHd86ITS2OciDrdnVoQBLvEoh/dCHUx\nk3LYEB/5/p/j9oP/zPO6ScQ4mXTOVNINiyqViqNXAqnhTQvUuN1WDxNxrpNhDZ1o+93i4qInquxO\nZPXb170o1IOgbTTu7zQ9Y1EqlbC6uopqtQpy1b579y7S6TTq9brj/ehkPI9K9vdbTh6bnp7Gpz71\nqfZeuA9MqIdA+89ff/11PPjgg22dBLquY2lpCfPz88jlcrh8+bLjrrHXDYqAlq8x7p04XWLxzJkz\nHgsOGdxj++77+237ir7jtETwimL7vHlZdohrAOCbkSazYUWW5eEhW+jqVGScT8gwm0lWUcKfRhpo\n3gTwHNTtIngqAZUuy0iLdCGZcNhfrMf890mEe6IZ4HZH3IMEeBz/upRWYkXXw8hNtLzVA1McCvm1\nrrZHSA/3h9pfaEYuWp6duX/IB67jd5NwUOxV3V2y3U984hO4dctKfJqensbVq1d7vi9Gq1wuEdCy\nLGNkZKTnnmF3z4tuLI0k6OKHaZpYWVmxE0RJd+owOhHqW1tbyHz133q3pSh45Pt/DnzgA56kUj/B\nGtSwiI6a0k2LNE3z2B7S6fShyt9y000SKX2DE/T+kDK0+XzermLltoX0uupLp+xF5SyaoBmLN5q/\ns9msHYWv1+u4ffs2/uAP/gD1eh0vv/wyHnroIVy8eBEnT54M3U9Usr/fcsDKN7py5Qpu3LjRkwIB\nTKj74BdxqVQqbTUA8iu35aabkl5BxBX/dInFiYkJT4lFQjuDu/mfP+c8FmJpEQQYpXAhx4f4zwFA\naEba9Z1WgicR6bTgF5r+d/v/5nYNVzUZaSBr31Bo1A0FEenhdd6Dl/GKAqOZIJMYzAJca9DkeA61\nzQKAljh3i/duoul0l9c40MK9OLcRum7fCWf02x1N96x/ypl0xbuq94w9OYHCnNdbHyTSdz/zaWQ+\n82LoPnuB+2Lbi7q7169fx61bt3D9+nVcu3YNAPDRj34Ut2/fxpUrV3Dr1i3cuHEDuVyOJZL2GL9y\nuaSKRy9F+s7ODiqVCl5//fVYPS/i4JdMSlsTh4aG8NBDD0GJGayIO5aTEsHT09NQFAUXyfMlEYaq\n2WMqryjQv/Tb4D7RijKSJNO4rz0saupuwkMEKl1p5TAJ+L2o9hL2/vg1dNrd3YVpmiiXy55E1v0U\n8AfdmZSu7CLLMp566in85E/+JD75yU8ik8nga1/7Gr70pS/hS1/6Uuh2opL9/ZZfvXoVzz77LF55\n5ZWeBV+YUG9CR1w69Z/TSTzuBkB+CIKAer392txhRA3GhUIBMzMz0DQtVonFrrudJpNAowG+zxJ2\nxm5LaBOhHCXSaYT+PvCpNEy1Ab3gjFi7RTof09dOPPKSKFj2mLD9u6LpYYJeSCUd0XheUaAMUdHZ\nZuCEFu9AdOMqP/+63NddxYXcZKsWOi/E+84LcnfT08cfOIPVN+bt/2mR7i5/CeyPWHdfYAqFAo4f\nPx7yjGiuXbtmC3TC7du37b97MTXKcOLnPyc/oij2ZNyle14IggBZlvHYY4/1TBDRQRdiTVxeXnZY\nE9shaiwnUfp8Po++vj5cvnwZ0v/zbxzrSIM5GLU6TF0HJwjgOB7SV16A+d//OoDelm0kAtU9o0VX\nWtnc3MT8/Dzq9brd7IfjOFukJhKJfSuVuN9lGf2qWC0tLUHTNAwODtoVrIgthDR0CmpS1EsOqqa8\nu9kR3bzu1KlT4DiurfE2Ktnfb/mVK1cwNTWFgYEBfOELX+jkZXh4zwv1dhoUBbG9vW3f7fsl8QSx\nF9YXv22SCwopsTg1NRW7xGK7Qp3jQk5OQQCfbQ26+sZGqEjnFAVmw1sD3d5cM8oucryPzcZfpPOp\npB3xdj9uNhotewzHQSM3AqTOe0QyKonw01F1P+uMG2UoZ4t2wGuZIfTC/gJ4by4Sg1k0ii3vee7c\nKACg8O4igPaj6QAgyOFDC4n+E7FOKr8cNO4p252dHVy8eDHkGYzDRBz/uSiKKAckrcfdx/LyMubm\n5tDX12fbTr7zne/01CNMKsn86Ec/sq2JUcGfMMISXkmUfnBwEA8//LAdpadHX96OpCdg1FpjmpjN\nAV9/EepPf3pf6qsHdSteWlpCqVSCJEl2hLlWq3lKJZLEw16L6sPSmVQUxa4aOt2reQJvfPgjnsf2\n2oLjB7E1Pvfcc3j66adt4d4N71mhHhZx8cNdTcU0TayurmJ2dhaJRKIt8UvYK+sLGYwNw8Dq6qod\nIbl06VKkj9FN3A50btsLACARLmwFKlJp7oZUGqErFyT9I8fEZiMIAvSiFaH2qw3e2mTIxbS5P5LY\nKja3o7kj+ErCjuBHeeXDSku6SRwboP62PoPaenAJxqBoupzNQC21yrFJfSnH/1Hkzo1i8D5LFBTu\nLsZ+ntv2EsXxB86gtBivLOFeR9X9qgTshUed0TvIbGjccrmdjrukcdDS0hKGh4dx5coVJBKtgAAJ\nkvRCGBBrYqlUwvj4OC5cuND1DYBbqNOvJ6yHBxHodJI+ryRgNpNLTdMApwPS118Ed+LxA2uEJAgC\nFEXx+I7DSiX6CfhO3+fDJNT9iNvQic4TIDYjOgofVh7xsEGP58Q21Q5Ryf5+y1966SU899xzyOVy\nmJqawvXr17vuOP2eE+qdZvyTwV0QBEf0oZs2zHuVTEqmSUmJRTpC0sn2Ohp4E0qrUoosA+7IuNIU\nrqr1ONe0xpghXvYgke5GIFF7joOxE25lAQAuwiLDJxQY9Zol3KkBilhv2klojYJcFOmIFQAow4PN\n39b/RLgTkR5kwYkS51J/vBu33Fkrys6LwTXR45I+ZV0ktHKrdGRu6iQK0/G23fj3vwr5V/5d18fh\nh3swv1fLeb0X6HQ2VJKktoR6rVbD7OwsNjY2MDo6ig984AO+F/xejOfu7s87OzuRCW9xIUKd5FCt\nrq6Gvh7z6y9G2hL5vtb4YdZreHzlNVQuX+7J8faKsFKJpHY3bREBYFtEyE8cj/dhEert3mhENXQi\niay0zeigGjq1C0lKBqyKL+0GXYKS/cOKAdy4ccN+/rVr1/DSSy91/TreE0K93YiLHzzP491338X2\n9nbHHkE3vRbqqqraEYMzZ8705BhjW1/6iTjmgd2ICiAhkXaur1UD3dwpgJOtCi+2SG9+Zpwkw2yK\nfCQUoN60TrhuCvj+bOt1NG8CeEWB2Uwu5ZSEHXnnZNljtQm7OZBOnoTZ3K8n2h4zgk7bZNpBGR5E\n8mQrwlUPibgDgOyTaCr1ZzxJnlHkLowDAArvzDoeJzcKcaPpQkJ2CHWgPbG+V7gj6ntdnpHRPt2W\ny41qTkQolUqYmZlBpVLB+Pg4zp8/HyqC2ulOSkNbE2VZdnR/fuedd9reXhCkSdGtW7dsG02UqBNy\nA9AL3u7CACAMj8CstYIAnGhda1I3/hDqT3+6Z8e9V/A872sRcdc6X19fR6VSifR436tCPQxJkpDL\n5TwCN2yWQlEU1Ot1rK2t2RH8g6pEQwdeOikMEJTsH1YM4MqVK3jhhRcwNTWFra0tVp4xil74z3d3\nd5HP51EoFJDNZmMNbnHpdGB3Q5dYTKVSOH/+PEZHR3twhB0mk2aaHuZKK3EUZNrUT6QH2FC4/hyQ\nSIJTG9HiPwYkoRUA9PpGZCSdayNSTtd/B7ylKX2PxyXS20qqzaQdz00MuwRyr76j/RloO15bUu7C\nOEr5pVjboG8GSDQ9iNzUSZRXrJsOklDad2YYes15A7VXUXU6AgOwiPphgi6XC3Re/zwsom6aJra2\ntjAzMwOO4zAxMYHBwcHYOUftROpJ9+fZ2Vn09fW11QypHcrlsm2j4XkeTz75ZHtdV3MDQMCsKqek\nHGIdsgyIEqRvfBnqh3++20M/EMJqnQd5vInn3TRN7OzsIJ1OH0jVk/2qox42S7G7u4sf/vCHdjlJ\nuqGT+yZnr/3j9PvRadDFT2hHFQPo1uri5kgK9V40KCIJorquY2JiAjzPY3BwsKcnQbsDuxtyE1Eq\nlTAxMYELFy5gfn6+px7BOELd/C9/ZP2WFUtUE/qou/DCZqRnHYBliVF9EkiJ+C+XHA9zSkDEO+yz\nTqYhHG8Nog67DYnY+4l02isf5UmnkmaFAc5hkzFqNc/zg0S6kPRPfg0jMTxoR7dqq84mRyTyHdfy\nEgQnCug/dwYAsPOuVbmlXW96ELkLYyi8M9eTbbWLO6JeqVQ6trYxegPpBt3pbKgbv3GXFs3pdBr3\n3XefR4TE2W6cwIumaVhYWMDi4iKOHTvm8br3ip2dHUxPT6Ner2NqagqXLl3Cq6++2vsSgkqqNaMJ\nAJp6z4t1P8I83vV6HUtLS9jZ2XE0K3InaabT6T0Vp6TyzUHB87z9micmJuzH223o1G65Tb9EUgL5\nvt/L+UZHSqiTLwOh3YiLYRhYW1tDPp9HKpXCuXPn7NJHhULhwGqeuykWi5ienrZLLN5///2O1sW9\ntNO0G1E3Jdkp1snjA62BjasGVFxwi3TZ3R2UB/pakWuuErNyQ8R3gOtvnbzm5npkJN1q3BSWjOpd\nRirUkIuatt2aTm4nkg5Y0XRyHH4iXujvh1GxbCXK8WEoJ5udU5uiPUykk6ZN6m78pNP+c2cw8EAS\nO+/MhK4XFU2niSPW9yKqrmmaJ5/joKez34uQUrmaptnvf7cCnUDn3bhFczf5PFFJqo1GA7Ozs5He\n8G7Z3t7G9PQ0AGByctLjO46D+XVXwnYqDdDjreiyVNI5SUBLrH/zj/DG8UcdQvVeSkaMA7F7ZDIZ\n8DxvC1R3kuby8rLdjZUWp+SnF1VWgiLq5n/8LUcvEVNraQQh2SwL/PTvdL1/AHYuH02chk7uevmk\noRMt4Lspt3kvz44eCaFOPmxd1/Gd73wHjz/+eFt3lZqmYXFxEQsLCxgaGsKDDz7oaYG8FxVa2mom\nRLWKFkURk5OTvneHJJl0v4/RdItqAIYog9caMF0nlpluitZyEZBkQNNayaUEn+15oEQ76jWrZrub\nhOIU/+79uEQ1f6zVBAjlkEo05OkJxfapt4NIBozmxY0W7m4c5R4z4VPjvHvqnLp4Jo4PQ5Ek1JdX\nPc+TsxlHiUY/pKEc9BAB339hEjvvzESWZnQjUB1i6ZuI3IUxrH/vx9Y6iuyxvwC994UeRDkvhhNd\n16GqKra2trC6uoqLFy/2XNgZhoF33nkH6+vrOHXqVE9Ec1CQhHQr3d7extjYGJ566qm2AkhxvuNu\nn/v58+cd9bW7guTpuMU6IZ1xWmOIFaYp1h8o/ACz/U86khFpEXZUBLz7cwpK0nSL07W1NZTLZUcz\nJ7eAj/u+uIW6+R9/K/I5erUOIZmA+YXftI67S8HeTrOjqIZOJJHVXW6TTvYNmvE0DMPxvjGhfsAQ\nqwvHcbb/MM4XpVarYW5uzh6sw5Iv260UEIc4J5+7xGJUq+j9jKjv7u5CnrkJUeqsK5yZzkJX0uDV\nOvgKZT8hIp28P5LsscOYiiVIObVZHWWEqoywuWb9dlttlGRLmMsK0HAJbPfnQXna+WPHga21yNfE\nKUmYdDOVkIZINKJ7kNre9lR+6RWJk62ymOp6eEfSdum/MAlOErH7Yyu6nhzqR73Yec3qvjPDocvV\n3/s1vPHBn/NUIOi0TrKqqvYYQP/N2D8Mw4BpmpBl2RFR7wXELlipVJBOp3Hu3Lk9aVAEOJNRJycn\nO+pWSsbfoOuZu0nRXvncbYhYT7tqdCeS4OrN5PCmHdFMNMs6airGlm5Be/Sf2+uTrpq7u7vY3NzE\n3NwcGo3GPS3g4wYNwsQpLeA3NjYwOzvb1vtChDr3x/97x9cP8wu/2ZVY71VXUlEUfevl67qOarWK\ncrmMUqmE1VVv4GllZQWyLDuOo1gs4syZM20fx/Xr15HL5XDnzh1f77nf8jt37tizWu5md51wJIQ6\nPTUqSRJUVQ31/JVKJeTzeezu7mJ8fDzWYC2KIiqV+HaAbnG3io47Jdt1J9EY2yPeR1VV8ZDYgJ6y\nyhYKlRgJn7z/V85I9cOQFYjFaOFILgBBGCfHwWkNcKVC60F3JN1NQgEaEQMb7bn3SW7lXPvgEolW\nZRpY0fe4iKdOO3yfXIwyk3w67ah17IbUmjeqrWorcrP0W2PZW21FHojXF0AcyMF0XRQy5ydtsQ44\nbS90BJ1GGT0BveSM6mfOjmP37qzv+gCQGLFmwIj3sVAohEZeosqs0RH1e9nTeBQgY3m3kJyjmZkZ\nGIbhKHvYy5wjURRRrVbtZFTTNG3rSadCk4h/t/AxDANLS0uYm5vDwMBAV5YdN8bffRXoHwC345zl\nMzPWucAFBGZosU6P0aYogdNUiLf+sy3W/bpqAq0oKuk6ei8J+F7M7gVVWaHfl62tLUeZRPo9UVUV\nqf/v33d1DADAffl3YP78b3b03F4J9SAEQfBU63nDtQ6pOlMqlfDqq6/it3/7tyGKInZ3dzE6OoqL\nFy/GGtvv3LkDALh69Sqmp6dx584du/JL2PLPfvazePnll/HCCy94ntMJR0Ko0wQN7iSbP5/PdzSA\niqLYU0tJEKQG+srKCk6cONF2icW9jKjT3kfSIlf97l/ZkWg91ayFzgu+ot1MpBz+dUP23kwZaSqK\nTVeNIY2m/ER6wMXW7MvZCa5c2UfohvnMoy7gmdZxctkBYMe6KSDRdM51o8glKF87XU6S/jvkWPhc\nK/Ji+JRL81he2iRx+jQSp/0FuxsuZklHt1jvFFqsy9nW4JwYGQInCIEXfVIn2d1KG4BvBQKSZEgL\n9W6mSuNGYqanp3tSwuuo0a1QJ03p8vk8ksmkww4yMzMDTdPaTloL21epVLK7Y/bKeuLXpIh46kdG\nRgKbFPUCs38A3OaaLdAJet8AhFJrDDKUNDjDuuY4Iuv0tkQJpqxAeP2vob//pwL3GRRFjSPgG83S\nugdVJnEv9xsWXSZj3PFv/784tlvyf35fBo26fwnfxOmT0Da9yzoV63st1N3QiaQf/ZP/AX/zyf8L\n4+Pj2NnZwdLSEi5cuIDPf/7z+MxnPgOO4/DlL38Zb731Fv70T//U08DIzVe+8hV87GMfA2Bpnhs3\nbjhEt9/y6elpPPbYYwB6V/3lSAl10zQ9gzudzZ/JZHDhwoW2s/mB3pVS9INkjZOmGmfOnMETTzzR\n0Ze910Kd4ziUSiXcvHkToig6LkCNN74Bv2HJEGSgKdq5ktVx0kw4fWSGnIAZEF0HLLGvUxcIobjR\nEukhAtuQ/KNKxBdvchz4susmwh3pdvvaIyL4AOw68hzHA0XnoNdOJB2APXUcBD/Uik4biwu+Ip1P\nOavFCJl433kSYVd/fDfW+uJAeFQic34S/ek0KhHbU0ZPeLedC47oJ0ZaAyz/J8/D+OSznnWC6iST\nMmvE/0iXWatWq5iZmcHbb7+N9fX1jm0EcSIxU1NTuHLlCm7cuNGTqMtRgUS56S7L7UDPRg4ODobm\nHHUrcunItqIoGBoawgMPPNDVNmnIeK6qKmZnZ7GysrJniaiFQgF3797Fw9Rj5vAp33XdYt2xLHsM\nfM1pdzOoMZT7wTdg3v/hto4tjoDf3d21G/McRAT+IG4QSJnEgb/9YuS68rFBNDbC+20AAE/NeHYi\n1vdbqAdBgi48z2NychKmaeIXf/EXMTU1FXsbhULBkV+wubkZuZysc+fOHdy4caNjsc5xnAiAB8Ad\nCaFOVwMgA3Avs/nJdvcqov7mm29id3cXExMTkU01ouiV9cU0Tayvr9vJq+9///tDvfEEXXJeEOuD\nZ8DrVpRZrFm2Br9IehRazkryFKr+EQOgKdJDBkqS1GpH7XlrMOHLxZbtxSPaw0W62bS7kJkCU5LB\nZVsnLgcAtAWHxi/5VUmFl5ZM97WStQCIZ6wGRMZWsGUorkh37Oby/VAXF0LXoUU635eBUdoFJ/kP\nKanzZyPFutv2QpM5O47iD6zEUlqkdwJdZo3GNE185zvfweDgIFZWVvCXf/mX+NGPfoSHH34YQ0ND\n+I3f+A186EMfirWPqEgMADz77LN45ZVXHB3vGJ1Dqqqsra1FNqXrtjiApmmYn5/H0tKSHdmuVquY\nn5/veJtB3L17F6VSCWNjY3jyySd7KoLITPP09DREUcS5c+eAN63z1Ej2wZAUiLstQa4raXCG9b7p\nfQO+Fb4AK8ruFusEztCAH3wDpigD9/1EV8dPC3hBEFCr1TA+Pn4gFhrTNA+ksY/4n/7PttanK74k\nToTnAAGA9PUX22pgpfO7WtQAACAASURBVOv6gSTkP/TL/wyAFVVX4d+8br+sjENDQ3YQ5vr16235\n1DmO40yrLNWjAD4O4O+PhFB3s7i4iJmZmZ5GHnpd9YWUWKxWq5iYmMDly5d7Mlh0G1Gnk5P6+/vt\nQS9QpFPHHBTNJqjJLJAExJozom1ICXs7pFKM45ioCLqe7IMpWJ+nSNlZ/PbtV4nGXj+RAt9MRDXS\nWaDP2ge/S4lqt0h3JbWaLk964P76cjASydbrKgZUeImIpDvWI2Ld0AFeAD94zG4qZYaIdhq+rx9m\n1TtNTZBGTwMAtJ0fQRrqbIDjc4NA831OnT+L+pxXyPhF0+WTJ2FUnTkh2fvPw2j4C4OgqHo7kHKu\nQ0ND+IVf+AVks1lsbGzgmWeewcbGRlvjSFQk5sqVK7Z97Atf+EJXx33UcI+DUZHKcrmMfD6PYrFo\ni9k4OUedjOdk5nN9fd1zfWk0Gj2bzaxUKpiZmcHW1hbGxsZw+fLlnohAjuPsahgbGxuYnp5GMpm0\nixQYf/dVAJZIJ2iZAYdYp9HTOQjlAkxesO0vBCLWDZ9ghyEnwRk69HdvQjj3WNevy003FppOBfxB\nRNSlb/0x6K4p4uAQtC3nWCP29wU2rIpCHLYCZHfv3nXk+oTdLPqVuD0I3MUAisWi5/sQRS6Xw9aW\nNQtRKBQ8Vpmg5SRqn8vlcPPmzbaEutlqhFMB8HkAZ4+MUK/Vavjxj3+MYrGITCaDRx55pOeJQt0K\ndb8SixzHIZvN9uwE7zSiTk/hDg4O2jMQq6urKJW8UezyOzdBx6r8hLIu+QtPTemHLingdc0j2gHL\n9kJvl3MLd0ECp6vQ0lnbBuP2xJtia/ouqLa7ISVssW4/lsnZkXfBbZGhtx+VnBpGdsDehzthywP9\nHY65T27QssYIdAJqzOoz5PhQawn45OX7AQDa8iKETApGre6IpnMBUUs+563fnBg74xDrQYmlvts7\nfhLGfHBiabe4L7S0R51ucNILSKLqc889h6efftoW7gwnYVVPSFM6VVUxMTGBS5cutZVz1M54ToRz\nsVgMLEDQC9thqVTC9PQ0arUaJicnwfM8BgYGenYt4zgOKysrmJubQyaTwQMPPOCYWaIFup5oec/j\niHXAEuf08/VEGpzuPxNt8gIEvbFnYt2PvRTw+y3UpW/9sUOAc3uUqwAA73vnBt699N86bIKJRMLx\nvpBOo4fJ+kLfMJim2fZxfeITn8CtW7cAwDHzScZvv+W5XA7Xr1+31yN+9Q7IASiapvnKkRHqgiBg\ndHQUo6OjWFtb6/kUVDeDMB2lzmQyjhKLvfaUt7s9XdexsLCAhYUFDA8Pe5KTwoS/msjA4KwvvtwI\nr8Ftcv4niKY0K5HwAuSq0yLiFv9W5N3/c9VT/bb4Ff0SWakB1BP5Dviu6Ol+GJJiR8KFZrS9K5Hu\nPq7+Afs1eUQ77ZUP2mfKaYWxEURgoHn3v73pXU7hqBTTF5wAJ54chba8GLqtSKQEEmfPoX733cBV\niE/e9xjOjEPbI7HuN1U6Pj7e0baiIjEvvfQSnnvuOeRyOUxNTeH69es9bzt9FCA5R+QCa5qm3ZQu\nkUhgamqq7SgZEF+oF4tFzMzMoF6vY3JyMvRmoJs8JpKob5qmPdPCcRyKxWJPrIwkV6tUKmFra8vX\ntw8A9fQQEmX/8UJvinCTF237i70snYPYzEfSE5RYF2RAkCE2WjYYU2jd2JOgzH6KdT96IeDddbv3\nEulbfxxrPbE/2PYYx/ZCc+6Hf2lbYEheHXlvlpaW7GZOqqqiVquhVqvZAn6vy9yOPTHhecw9nnfS\nsf3KlSu4desWbty4gVwuZ9sXP/rRj+L27duBy4lY39zcbHtcp6wvFwD8a47jCkdGqCuKAlEU7cYB\nvaaTE5AkNS0sLDii1DQHJdSJx3JxcTHUzxkm1DXBei28oaIhZ6CJCgRDhVzbsRJKA9BFp0ddExPg\nDQ2NpBWllahBnUTTDcnH1x4g2rVmIis4HmLVFWmPaKSkSwp4zb9Mo57J2TYcoRJdLpHgN/VrSgo4\ntZnsaRoAx7tEO5X0Q0S60OHpOjBk2XY2VuI/R0k6ouoEceIsAMBYXQp9ul80nSZx9hzE3Hrsw+GP\nB4t3e50u7S+99DRGRWJorl27hpdeeqnDoz560GMtEeqSJGFpaQnz8/PI5XKeSHC7hAl12rctCAIm\nJydjVf8hVYPi4m5SdO7cOY9Q5Hm+q+sDmSmdnZ3FsWPHkMvlcO7cOV9rQvnt1wAAajIHqerNq6n2\nHUey5K1ZTagMTzrGbhpNTkNslKEmvMJRFxPgDB1G/vuQJh6M+9L2hXYE/O7uLkRRxPb2ti3eM5lM\nWw2L4uAW6VxuACZVCUwcHEJjKXx8bq3ctJEODXoqv4ijp4EAqyHpxkoSqAmmaeLNN9/E4OCgfXNY\nLpehaZqjGyupttVtMjdd8cUNPZ4HdWyNg19Frtu3b4cuJ491UkOdsr78vWma/zfHcZ1e+Q8vvaq9\n2w2qqmJ+fh7Ly8s4ceJEaAmtXnvfOY4LvXMk1QNIG+snnngi1HvrJ9TL79wMPYaG0g8TvB1lN4RE\nKwFJTCA0WRJWpJ4ga1v+It2FLsjgDepzbwpeLdmM2HMCpKbNxgyKyrtEujui7/DKN2vHE8RCqxkS\nbbWhRbqf/z4Is98SuqYggt8Jj4gDsP3pAGAm0+D8asIfa3rB52fANyPnXDLZGoz9oukBU4X88VOR\nYh0AIMu2R92z6Ylz0PPOyDqJprv96TQkqk7PBAjDIzBzw+gm9ugW6t2UZ4yKxDzzzDN44YUXMDU1\nha2tLVae0QeSnEf85ydPnuxZOUJJkjx9Mehyjul0OrK5nBue52NF7ch+ZmZmkMlkcOnSpcD9dBrI\noWdKR0ZG8Nhjj0GWZXz3u9+NdYwkKu72ntNi3ZCtG6WGPSsqoq5kkai1ghj08zU5uIKSyQswDKA+\n+yYS45fbfLX7j5+An5mZQTKZRDKZ9ETgScdRIlA7FfD8G690ddzysUHUV9bbjqYTpL//KtSnfiZw\nOXk9AwMDnhmbRqNhV9paX1+3bWt0N1ZawHd7c0OP5zs7O73r1Lt/vJ/jONk0ze8dOaHea+HrJsyH\nVqvV7BKLp0+fjlVicS/KKfpRr9eRz+exsbERO+EKiO9510RvhKYhWxcfgxd8rTGGIILXteY6InhD\n84j4Sv8pW4DLdX/PuFuk66ICQafqtTdtN6pCRdpd3njdY7MJj7yTmwcivNWBVkKkSPyacco6ollf\n3t0lld5XvxWxMAUBwk50eS3P89NZ8I1WdJwbPwcTAEd3Wu1zVsKJA3/cKt1mblmRcVLxJSqaTkPE\neljFl7BoutBMduoF7uSjbuuoR0VimNXFH47jUK1W7aTNkZGRnlc7oaPf7pnPhx56aE+S4dxNih56\n6CFf+wlNuzlHdDWakydPeoopkGRSNySaTnALbppq33Gkiou2QHdDnqtJ3temSilIausGiZ5dJaK+\nPvsmysogBo/7l4Y8rJim2baFxi1SwwS88N2vw6SDalLC212bIsz2EoXYLCSA7EBw8YMAgjzqpBtr\n8qXfwoDq1WiFf/UbgfYiIuATiYTvezP42APgm9XN1J/6Bes3NZ53O5YfEBcBfJDjuKOTTErX3t2r\neudBiU3lchkzMzMolUoYHx9vq8TiXh4vALsudKFQ6Kj8o9+FQg+xtQRBRDsAyGprelQTg6PlxFpD\nqCsty0CivmNVDXAdi+5zw+C77eZFRsskkKg4xa/HG99GJBwA6rmTEDRrAPVr/GTIARdnV6RfT2bA\nuwZivRlpF2oVy5/uwkw6o1ZGMmPvkxbrAGAOWiKXC0madazfl/VE6s1EEjg5BizPxdqG/bzcILjm\n916YOAf9je+19XzxzLiz1j15/PbXoD3yT9vaFsHP+nIPDu5Hgnq9jsHBQSSTSUiS1PPkNEmS0Gg0\ncPfuXbu53GOPPbYnXlpd122bYbtNigRBiDVD7DdT6veeBQl/t4AGnGK9mrKSqTnTOmcr2VHwpvO6\npfMShGbApK5kHcESAKglsuBN3XdfhIaUBm/qSGgV7Czn0X9yIvK1HxbCgnjdCPh0Oo1j+VdD983l\nBoCKdV01jp+GNOwKblDXlvaLIzuJiqoHCfXa8/8jOJ4Hx/Pgm0EdoynY5f4MBr78WQwASP5rq9yk\npml2M6ft7W0sLCzY3ViJcAf8/enk+d02r4vbtM5v+QsvvNBtMOaLABZM01SPjFAn7GUyB4nWky8h\nSTRqNBqYnJzE/fff3/b+92oGgL55mJycxMWLFzt6b9wD+/L8LEQxBVWw/OiKWobBOy9uJlqDgibI\njgFdFRJ2hFtpBAvEqJuBcmoYQtNOQyLtRKTTFwhNVMDrXksMjda02hBri1T3VrmhbS9uK06QlQaw\nkqxo+OJawJpOoqLx2okJAIC4s2mVa1Trtkj3tb2EoA8eh7AV7D0F/EW6kc62KvKcHAO/Ha8spB/y\nAw8BG95jiONN7xVuoX6PTpceCchFdWlpye442StqtRrm5+exurqKCxcudNxcLgrSZXp5ebnjUsGC\nIKBeDz6fG40G8vk81tfXMTY2hsdTu8D229CL70IHID/orM/vJ9QL0286j1tuef/rShZKxTuLp4rW\n+JRQg2fCqoksknX/qLwqpTxCn2BQhQd2lvNY3qnbdpF0Or3niYmd0knVlyABr6qqLVIHVq3Px4z4\n7hjHT0cfI8cB5y61Hsi/4z2m0ejthB6Hjx+ciHQ3vCRCTFrXbWUoB9No2bKCOk7T3Vj9INY1OgDa\nSb5RnKZ1Qctv3LiBV155pVuhvg2A5zhu4MgI9f3ItiZNj0qlEmZmZiAIAqamproqoi8IQk8vRKVS\nCZVKBW+++SampqY6unmgiZp6rUlpmByPhGZFSHReAk88iS6R7nmu3KrUkmjs2rYXP5EeJobrijXI\nue01bpFuiLJttbGW+8cW7IQnjrN97fY2IvzyRsgMAQA0BlrCUyqseDq2ApZIJ9UQDFmxo+qGpIBX\nnRF2rWmL8UtuJdF0GtNlbdEyA+DVGvTB49Z2CvHEtpH2VtowB46BCxPrzffODLDGGGPnwM8FV4PZ\na9ydKg+qgQmjlWvj5yXvlN3dXeTzeZRKJZw6dQr1eh1jY2M92TZNtVrF3Nyc3WW6G9tOUDJprVbD\nzMwMtre3MT4+jsnyDLDxw9YKHAeYJuo/+BYAIHH/B+3tBVbxckW6a1IzgJEKGXulDBLqri3cCY3m\n/9VEFqmqv12vLqbs6wYAqIICjQr66LwE3tQxnJNR1axqP7u7u3ZiIhHu5Oeg6WV5RkmSLAG//DqE\nauu6Jmw3Az3UdUgfPB7YeCoK88L7wb3zuv0/H7Ncblyvuvoffg16rQGpL203WtLrDXA8D7P5PeRl\nEUajdV02XvwN8J/+3wK3Tbqx5v/5v/BdnkwmUSqVUK/Xcfv2bfzu7/4utre3oSgKrl+/jkuXLuH8\n+fORN3xRTeviNLXrFI7jBgD8LoANAP9wZIQ6DRncet3BrdFo4Hvf+x6y2WzbiUZB9Mr6UiwWcffu\nXRiGAVmW8fjjj/dk0KAH9uVmWTxV8NpL6mIKGi/BNDmkNG9E2jkAixAMzVEysS63Sj0qIVEaIDja\nTuw1RNTTwt0Qnc8Js9wAsG8aiK/dLkMZ4JO39pEIvaFwi+RGzhLtUrUIU1bAqY1YvnaTd562WiLT\nagLVLJFmi/QQz7me8kaLtcETELea1WFISby++OXvjGbLcX59yUok7YKoaLp+fAzCqtdy06n9RVXV\nriqJMHoHLdS7LQ5QKBQwMzMDTdPsmU9d17Gy0kYVpBhUKhXU63XcuXMHk5OTXXeZBrzXB1LPfWdn\nB5OTk3jf+94H7Xt/HbyBZkWpyo9etXJ5+JRDqK/OTQNiK7qtCt4xbCcxhP56cEJ7IXUC6UZwFaxK\nctAO3hBUwRp/abGu8V7hZHCCZYWRREgDA7hw4QKAVmLi7u4ulpeXUS6XUavVbKsQLeD3q6Z3z+uo\n/+i/trbtE03XM1kA3rGZzOKSa0EUbrHeK7TPPQtOEBwiHbB6Z+h168ZCzjo1FMdzMA0zUqwT/Gwv\nx49bQafNzU089thj+OIXv4jPfe5zWFxcxNtvv40/+7M/wy//8i/jiSeeCN12VNO6oOV37tzB1atX\n8fzzz0cefxCmaW4D+Jccx50E8C+PpFB3197tBsMwsLi4aLeGPnv2LE6G1Hlul26sL6Zp2vV3eZ7H\n2bNnkc1m8eqrrwY2CWmXqIg6GdTpQbYutsROUi3Zy8JErMpbA7dgaqhJGZiwBjxireHMlh+dLPPd\njpCA2PRJNuTWdhLqrh1N9xPpjoouYsJhn1HFpP1/I9ESt3J9B4Yog9M1TyRdFxXbpw4AmpT0eDbt\n7SezUOU0BL0BudJe4o4brW8IhiBDKrVvQyE3ANqglRgrrs75inS/aDrgbDJlDJ8CX/QeAx1N92sz\nHhRVN1N94CqtG0B90NvNtFtIpA7orpwXo3u6Fep06UNJkjz11nuZG0RmWKvVKhKJBB5++OHIJNG4\nkPF3d3fX7mRN13NvvP43nrKtZOzRpSR4Q4UmKuBMAybH44SsAcVlrJVJ+UUOJsdD53gIhoqGkIRk\neK02fmK9KrVEVlnO+or1mtiKdKcCrI47yjEYpoCE4T9zQoIkhilgZW0LJ0YG7cRE2ndMSgHmcjmU\ny2UsLi6iXC7DMAwoiuJpztNrAd9Loa7mvw/feK+UgDpgCVH37Go3mBfeDwHwDXzYZJ0ebzWsnOYf\nfgYAfO0uACDn+lDfcn5f9GodQrJ1HeW+/Dswf/43fZ8vvfKHgd50wLLHkPE7kUhAFEV85CMfwc/+\n7M8GPqdXkP4Z3cBx3DCAfwzgFoDPHxmh7ld7t5vMfb8Si/Pz84eikRK5CE1PT0NRFNx3333o62sl\nFpJt9lqoNzgFJjjIsAZyMjXpFwkBABUyDNm6ONJTnG40PjjyupsYAG/qUDSvH03nRYe1RmsKeY2X\nIBqqQ9DXpQxUJQHB1CybDb0dQbYqzsBb451exy20G4l+O0ou+3RYtUtE+lQ/CLrZaKSswdBtudEV\n/6ndoMRUtc9K/pLKAd0EU/2eCL/nWM7cB6ngjDoGiXQ3WjoHTklDWp0F0n1AuRRoeQGsqjlCvZkM\nNXYO/P/P3pvFWHacZ4JfxDnn7rlV1saqYrEqq4osVpGyxEWk3K0B1C4b/eI3yXwYww8NN5/acI8x\ngACjMbBe7LEwMx40GnCbFmY8sB/sFoE2MOi2BJV7YFuyRJdYalMWNzFvVmXtW653O1vEPMSJOBFx\n4twtLykqxQ8oVObZ771543zni+///q1ixrpN1oFcVeeL08WNGdesedS3t7c/8ad/DDApUZfZzdev\nX8fc3BwuXrzotEXMglBtbW2h3W6DMYbTp0/jwIED+Md//MeZNCiSGAwGePDgAbrdLlZWVnDgwAF1\n7dFbf126X1ydA01jMBqI7zkDQAgoS8CoD8IZUhqA8PxaI0+MJTGtOsn6bu0A5vsPsFN3f9e6lQXM\nD8TDuU7QJXqVebTCfDzSBR0ACGlDkfWQls9s3b6/hWOHi3ZTQgh838fy8nIh23swGKiiTb27Zr1e\nNzLP6/X61Pf4WRD1wbqwL3kQs7nBYMdQ0yVJ15FWm/Az64trljSeP4hgt0geR/UUGYbwkLsRHOcc\nL/7wvyAdhIqk8yQ1CLiErqbr9hepqqf9EMHX/zekX/qfJ76+WfTEGNW0zrVequkzQBPAYwBeBXB+\n3xB1CRmRNK1KrccY2hGL41bgT4JJiLqevzs3N4dnnnnGeROaNNJrGOSxrt+8hxQeKBiirG68gtAg\n6ZxnHnN48GC+ptBvgPHM2sK6yvbiIumSxOrr5MAfkypacZF8JhMk0YSa0l6LNJU2I+mSkNupM8Mg\ns+OB4fYYQHjnPS2vPamYNyVOqGr+JK7RPUXPKnWj8Mp5XXPiplrZfZB3AXQM5mWIF4VybRN2AOB+\nAO4HBVXc2P/IE4Ks2/uOiL+MD59EcH+yJBkd6eqb8M48P9E+s0gJ+ASzgSQ84xJ1PTf84MGDeO65\n51Ct7jXfogjOOR49eoS1tTX4vq9mMSVmFQ6wtbWF1dVVJEmCer3ubEOe+jVDqCBpbKResawDqBcP\nkAZizEn9qjFm6DOJIWqoQqi0Ia2jyvoIqSkElJF0tb52cOj6TrX4nUp4AJp1QJAEPeIVVIgWsQuT\nPJeRdRcIISrf/ODB/Po45+j3+4rAP3jwAP2+SMaSBF764Ov1+kgSvleiLkm6jt35E1h49AGi5rIa\nv4PBzkzVdImwuQysLKPR/sHUxyD/91dAazWj2zetVZ3xu5WjRxDddYcYePUq/KOPgW0XG28F3/q/\njN8PvPhsYZtZ9MQY1bTOtb7dbqPdbmNjYwMbGxuFAtQJsAngLznnNwBg3xB1l6I+CbrdLq5du4ad\nnZ3SiMUPo5nSOAM7Ywx37tzB9evXx8rfneW0rnxfU1hFiNwHIxTgQIWUpxKk3IdHEsRcEGkPKQa0\nmVtSeB8D0kCVm9GBNoFPSP5AMNCmXCtpvt8oS4wLg8qccbOS/nhJ0vmQgVd2YrUh7TGMeqhmpF2R\nfys+Mqk0hl43APSWRBV+tb+p7CmlEY8liOYOIfGrqHXHs8TI+EqZTx8vHlX58CP3tZJu4iNPjO2X\nnAbpkZOgYbGL6qSwFfVPiPpPDnLcGdVEKIoirK+v4+7duzh27Fhph+UyjEuuOBfFjGtra0ObIe1l\n7JVWxtXVVfi+j7Nnz6JSqeCdd94pbNt/+zsAcg83IFR0Tj14FoljXgDCUtFUiHjwWYTIr6t95Uyp\nh7RA1vu8gTrJZ0N7TAgmDVp8OB8wMSYxUOf6kNXU+jotn2GNsnuFTdYTLr6bKfdACMft+1tIuI+T\nR6arFSOEoNFooNFo4NCh/AGEMaYI/O7uLu7evYt+v6+214tYa7Wa+vvZC1HfuHcbrjkEj8WImsuO\nNQJptQm/I4SrSQSYYeitfGYoWdfV9PjaP6IdClvRyb/9M5C5FriWkU5r7vuuvzAPHkXwFxeQbAkL\nDK2Iz7dy9pyK3qUHlkG/8Rrifyl6UtgkvQyzIOqjmta51sttXnvtNWxtjXe/LMEAACeE1AAc3zdE\nHZhchQGKEYvS++eC7/vqiXtWGDawM8Zw8+ZN3LhxAwcPHsTzzz8/lko0S0UdAI6eFAU8FEwNljrk\n4E3BUCVikI9RrnDrxLSLOXGDIHVUeR8p8Q3Fh/LUIOk6YlTA/Jxkc05QS7vGOVwEfRgxHgQt8CD3\nx5fBJukJrai4SACI/Bp8FiOs5sWoNSuqzFbSXWBa4WhYFwONq3mU7olPgxq8eFAoOvWTEIPmwQJZ\nZ0ENRE/DkQW0nm80khosHEVte3gBniTpqVeBryXuhEvHUd28NXRfG8NU9bTaVFaZMqyurk7kSdXr\nOj5R1D/e6Pf7uHbtGjY3N/H444/j53/+5ye2LJT1xdChiySLi4v4uZ/7uZmLJLaV8fz588rKGEVR\n4Xj9t79jFMczcRD1u/z+yw7PjHgg2Xo/DTGozKEWdzAIRAG/nAGV42KIGgJE6PPy8anPGopsS4Ku\no8eaiqxLgm7vD2AoYQeA3VS8hirNrB3ZrCznBCGvghCO9Xudqcm6C5RSNW4cPpw3VGOModfrodPp\nYHt7G7dv38ZgMFDby+W+75c25nFhI+vwHAZNVLV7zqAy7xSChmJYmIEe3rD4mFEP1V0QYpCfCH7T\nW/kMGneK0Y0uHDlyBMvf+hoQ+AZJ1+HNtZyqOq3V4C9CkfVZYVZdpkc1rSvrKP3qq69O1W2aEEK4\nUCa+AOBfAVgHcHNfEXWJUUSdc46NjQ2sra2BUorTp0+P9SF+GJnnroE9SRLcvHkTt27dwpEjRyZW\niT6sJko6SafEJO1y6lKSdkI4qpnSLtX0smNJdDGnlB2p6ugkXVpLEgTwIT5fqdhL283AayKFD+qx\n0uIk4zpoBR7PP9OYVuEzcVOQ0ZOAmURTUMUtkh5nFhrplQfEA8egKqbIOSGG5UbC9sHHQaPgiw8r\nLVVUW3fEnsUl3QIBYFBbgJ8MMGgeBKMB6t3xMt2NYywIK0y1ayrk3B9tPZqWrEvFaFIsLCwUPKmy\nqEwqYzqB15VbOb05LUY1yrh69Sra7TYA4Itf/OLU59mvsEmOVCtl4Wav18OpU6dw/vz5qVVMuy+G\nDt1Kc+jQobFFkknuEbpK32q1nFZGl+gSVedBwNTYIMcDaadLvCoSr2rYYjgRRaOM1uGxBIlXAeEM\njAao8gFC1Ay7Yoe14BFxXltVl+izBgjM2Y6IB/BJ1hSJNQsWyIT7oCR/PTvpPBq0DwYKCoYBr6l7\niY6QVdRxjeVpBR5N8cHdEIwfQBW3C9vMCpRStFqtwkxKmqbodrt47733sLOzg4cPHyIMQ6OzprTR\n2F1HJUmX6NSWUbPiLqNKC0FSFAjTav63Mis1Xcf2yU9jYd1sROfypi9/62uFZbRSAaj5vfSyv23i\nF79v/qK4N3qupK+M+wxT02VXUgm7y/Q0HvWfBHh+E3ofwK9CeNU/t6+I+qjsXenxloH458+fnyhi\n8cMg6vrUrmyQIadxp2mQIY85K6L+T9e78EhFqS0VIoini2hLtYOBwANHyMWNLSDmQ5O+r5cNvvbx\nQtQU+R5mrSlcQ+bmSuEpvyMHQY0JpSIleQGqTdKl3SahFfgsUmq8z2JDaQegiltti048KvYxw6Ai\nBhabsA9NxrGyivv1A0hpgEZvtJ1lUCsWgfabhwtkPRlC9HX0Fo6hsW3eZGzLiwvh0nFUOsNtMOkI\nW0/02JnCsv7y46g/umEsW9q+joOaT10WlclmGTdv3lSpENVqFWEY4s6dO7h37x4ePHgwtaI+qlEG\nAPze7/0evv71r+OrX/3qXnyMPxOglOLhw4dYX18H51wVbu61cE+O5zoB15sUTWOlGUckkQWv165d\nG6nS28fbXv2hAyQUUQAAIABJREFU+tl+sE/9qiDrGQ9Ks/ohj8VgRNhepKoqZy19FmNAGmLsJoIQ\n5+o1Nch6mIixrurl59S3d6GbNtD0hgsmPVZHg+ZEVJJ2GyHLzp+dL2Li9aUsqyGjKcLKsaHn+jDg\neR7m5+dRq9Vw6tQp9VnaXUevX7+OOI5V19Ha4glIeumxWMUey3uAx2JVe2XUEgQtdQ8jnKPOJrvX\nh4vu1LrIrylFfRT6jWVU4i4a73xP1CnXauBxDJI99JIgACtpSGSD1mpgA8tzH1SE/SVNAc9D8I3X\njHQjvnQQB158FhtXfmjsFv/SrwMoKup2j4yPI0g2oGVk/UkAZwF8m3P+X/ctUdcVdT1icWlpaeT0\nZRk+rC6ijDG8//77ePDgAR5//PE9d8rzPG/P1hdZVIua2aWsn4rBhBCOChXvMeek4GHXB/koG2AT\n7oGDoE612EKNoJdZUqRK75FUeRZd1hpJ0l3HGdCmOleDFKfghiXPuLATLCtVvx4Lsi1Juuv8kVdT\n6rqO7foRpeA3wtzTFgfmtHPs18FKUlp6WXvvuZ2bzvXxEItNvymmdhs7t4eS9EFtqTCI62R9HJIu\n0Vk+jdajtfz6lsqjFgfNg2CtI5i/+27pNmFreAGbhF5UZqdC9Pt9vPXWW4jjGH/4h3+IK1euII5j\n/NVf/RUuXLiA3/qt31L5vKMwqhHG66+/rooD99i5bt9CjuX37t1Dt9vFjRs3cO7cuZkm8ejjuR4i\nsJcmRaOsjLdv38b6+jqWl5fHUulHPYyEQUuNH4Ag6ykN4LFYRTLK5kOEc2WBoUgFWedMCCEkH4tD\nVkHCfPg0McZxdc60gpoXKqI8DlkHoAg749RQ1QFB1gdJBa3ATRSlCDTsfGEagAJ45xYDA3Dx+Ecb\nr2o3SBvWdfT2RoQKiRCijhQeap543dWkh4QGIODqoYpwruoJXPeWfvOwEaYwt5sLKPH86LFR2l5c\nUKr69mZBTW+88z3wJAax0vVIEACMg9YbYH3zIY3OtcBL7MPeydNAOLpIdlh6mESSJD91PTG4WYxz\nHMCnAfwGIeTRviTqcgBOkgQ3btzA7du3ceTIEbzwwgt7eqqaNVGX3eV6vR6azSbOnj07k/jHvVhf\nwjDE2toaNjY2cOrUKWzEgnhK1QIQJB0QikbKPNS8XPFmsKesswQXfaDNrDAJ89Hwhj/By5uHVN4j\nXkHMfXiEKWtNwssVL5Ugoz0QhCR/SOOEKJvNMO/6sHW9Sk4eXDGSxnGG3Hh71UWhkGd5xFIts5X0\nMmwunkZrINRq6U8fRtJ1bB1YQUtT5pk33tDQWzgGLADVQbnHUE+iCGsLIJwXyPpPEjLWrVar4eTJ\nk/ijP/ojfOUrX8GlS5dw8eJFvP322xMN+qMaZVy5cgWAUN4vX778CVkvQbvdRhzHWFpawtmzZ2ce\nl+n7vsrb3traKg0RmPSYYWjOAOo2msOHD099H3p4/X3AryKQFhdrBhDIm9FJsp5qdSq2ms6zDHXK\nU/g0BgUDB0HEAvg0QcLEvn3mFca/iPmoZcp6mAYI0wAx8zBXycfzblKDRzQ7WTyHlp+vj+WDQRog\nYT48kqIT1xVZl6KQTeqj1Eec3Y8qNOuNwcVnJrekAH50S/z2URH2cYtJb29oD1bwUEMfHovBCUWs\nWZYYyd53AhBwpPDhO3pxSJIuGwnuzolZhWThCSzumDU+Uk2fpF/H9slPo9Y3t198669FQ7skE+vi\nWBB0Cy6yTur1Aln3Tp4eeg380DGAFbmXK/EFMBV1zvnQgvSPAwghVQAvA3iDcz4A8B0A/4lzvksI\nWdpXRF1CVs+/8cYbOHHixNQWEhuzIuq9Xg/tdhu7u7s4ffo0NjY2cPz48T0fV2KaYlK9JfXp06fx\n1FNP4QdrYlBg2SBY9XJFWBJ3hnxKEgACattctCgwB9m1pzMBoZ64CldjRdqzKC9eRcI9eGCo0nJ7\njMumo19PiBoSLm4UkrSXpcQAIiKyDAO/CcY91NOiB92GzC220cuy5+f6D8Ym6RKdmlCK57v3xibp\n3eoignSATqbMt8aw0hSO0TiEZk9kn5d1jg0t+01n+TTqQ841aOZq0M7R80NVdSC3v5RN7Q6DnJKW\nkMVHjz/+OB5//PGJjzcKy8vLeO6553D58mW8/vrrn/jUHThz5gwIIXj33XdnnrbV6XTw6NEjPHjw\nAE8++SSefvrpmWSr6yKJLhQ99thjM7sPxX4VlWx2S5K00G8YyjoAJFlOue4j18dgAi46lwII0lAk\nwpAYvldDL62DEqbGflsFZ5yil9SUgCLRjetoBn2nnxwAOkndIOthWiR3nbiOgBb3j9LiexdlDxPy\n2uT1Jpyq1/1W5oj71OMfLlkbdc99cP8+KE8hzbZEI4+cUPXZDFPPdeXccxBXSdYBYWvamj+JxZ11\nxHMHnLU+w9T0MlTX3xYkPU1BqjXwcAA6twA+yAg5M99nWs/uQ3SIEFapgUTjRU5yvwqSuseDq1ev\notlsYnd3F/V6Hb1eD5xzZ4z1KIyqM3Ktf+211wCIIIMJO5OeAfA/AvgBROILBZAAokvpviLqnHO8\n/fbb2NnZASEEn/vc52baoGivaSp2d7mLFy+CEIK1tbWZdkKcRFEfDAZot9vY3t5WLan1G1bMPKWK\nhGkgGh5Rc4BIOVHbRKm0uVBwTlD3x/OXD1hOfqs0KvjYY4ts6w8ACfeAbP+Y+2qKNYWnBm7zer2C\nN17+rkg7T1Gl5sAhCfowdT3iVfhI0PeE+5B5FHVWtNpEXn1kLON2Q1gtyjr6DYIWgtT9/m61hKrS\njIoRUYzqUZfFAazTOGjYcEZBKns6WR8X/cZBJ1nXSbrEztHzWGhfmej44+ap611Jgb3FM45qlLG8\nvIyVlRW17ZUrV6Yi6oSQBQBzAB5wzscv5PgpACHESPGa1Uzm9vY22u02kiTB4uIiFhYWxrY0jQPZ\na2N1dRV3797F8ePHZ0LQH15/HwmtKDU88uqKmKckq8uxPOkSHEQkv4AbDY7s8YfyFOApGohRp12x\nLQFkhPl6fLKgbsuxNNZmXB/25jBXzRKotHuDxEaY1ef4WWQt841jpdxDmmY9N7wIm2Ez+zlriKMd\nLz9vljFOUzXmcxBF1lNO8IN1Ag6C507OPmhB4t5miAoR47XLZy+95vq1uUi5/EwlPJ4YyxjxDKKe\n0vK/r635kwCAOX/8viC29fLOwnk8tv0uqndFATyol/2jIEEFSBKQWrZPtQpwDr61CaJbuxpN8G3x\nsGCo6idyNT05chJ+WXdU6jtVdR3PPvssut0utre3sb29jT/+4z/GX/7lX6Lf7+M3fuM3cOHCBXzh\nC1/A+fPnhx5nVJ2Ra/3GxgYuXbqElZUVfOlLX8Lly5cnaX50FsD/BKAHAJzztwkhv0QIucw5Z/uK\nqHueh8ceewwXL17Ed7/73Zl3EZ1WcdFvDnZ3OSAn1rMk6qMUqH6/j3a7jZ2dHZw+fbqgKF1th4Yv\nUIdUMXyLsOc2F63wJVNMdMLfCAZqcLYRMw8xq6ttqzQaStLlOXVI0q+y2klxupCDOJV2wyLDBGln\nIGg6YsQSBPCIlvYiLT3w4SNf3qdCQ6mjA48nBSVdn7rWE20kepV5pNxHMxX2ksSrqCnQ2KsWyLp+\nk+5WFkfacVzYqR/GfN+dCtOvLSpFz0a3cagQQwkU1XTjXPPHMb+Tp8G4SLqEq5DUOM8Uajowuzgv\nYHSjjC9+8Yt4/fXX1TJXM5sxcRzASwAoIWQdwN8CeAbATc65u5PITyH22r9Cpny12234vo+VlRUs\nLCzg1q1bM1XqoyjCrVu3cP/+fZw7d25mQtHK48fgoif6uAHk5E/aX2S0n251gabcAjCSYXQQzhSR\nlOT+lC+sarpCr1Rh/WVatw25P6dU/GwNu9wrvkd6Q7xtCG/0IJX3HZMAc05yO6Zjm5SbswhX18UF\n7pWwr93to5LNBBNwPHb8CQCxSieTVyCvjXBuvPfy87JJuW59kZA/65/XwG+CgJd2/R74TePv41Hr\nJJY7OQkeV01/GByDhxSVrTuikVGtAUSh0dRIQSPmZHEJkLaXRpb4snwYCAfgneFNAQGAHT4Bet9d\nd1WGIAiwuLiIIAhw7tw5/M7v/A6+9KUv4Q/+4A/wK7/yK/jRj36E27dvjyTqo+qMXOsXFxfRbrfx\n6quvYmVlRaV6jYljAFqcc/1mHXEuvjz7iqgDwNLS0kymMGcB2byCEIIzZ86UxgP5vo80TSdKFxiG\nYcq/brtZWVkpzY3vJRU1/ShVEUnAPWtQBIC6HxsE3UWg1bHjWk6iPTFtqSsyOkJWydNfvGh4U6MS\ni4ucxgWAGpW+dpv8awo+ikk0MsEmYgFaXpH4uiIo7W562+SAUO45nHFnxr6O9JeuJ8iuJOw27Bu3\nRCcQf3eteAuJXwPNUgJ0NT32aghScwZhp34YqJcr+i6EfgOh38BC906+TCPpsVdFJcnPIwtwd+aP\nY+nB+4XEl0HQMh4K7i08iSPb5fm+O02hjs53J+OpLqI+bZzXqEYZKysrWFxcxOuvv45Hjx5N7VHP\nVJcUwL8G8ApE9m4PwJ9MdcCPGfRwANv3PQ70+MNGo1FoUuT7PgZ22sQU0G2DR48exYEDB3Dy5Mk9\nH1ciIjVQkoLy4pg+oE0QMPjcJOVAbt9TBaQO5VZub5JvptYJwm5CV4MloZbHzcm9aeuwfyaO16IX\nT8r9gzTE+eqPC+e3j5vQCtYHeeJLwigSJgphKXVbXiRh55zg0yfCkcXDH9wNUSExCLiwSWanFzp9\n8RwcFFSzAKUW3fKQGJ8HAxXbZJ50W7ABxGeiizkcBKHfQIQqGmy03dIm6xLRCLX9iev/H0iUfQd1\nki55hu8D+qwXISLbv97IyboG0poHac2Dz5WPsXQzE4o0dwD3q9g4egHLt/4R5OQZ8PVV5752l+kj\nR47g85//PD7/+c8PfZ0So+qMXOv1cfzq1at45ZVXxjpXhh8A+LeEkG9BRDOmANSHsq+Ieln27qwx\n7LiyxXS73UalUsGTTz45sgjK87yZFqm6rC+9Xg+rq6vodrtYWVlRthsX/vZd8T20ibgO3YYWMw9R\n6BvpABWvXK1IODXsNLYyD+QPB7pKnyQ1RbirXoxBUjF88zrkAJhaDwCdJGvORJgqgnV6AUtIPwAM\ntAYeFWoqWmWIrYLXPm+IDn4kJ/0yTtJGxKtGHnHXW0CVmor2wB/twesEi2jFwtIiSXpKiw+HfW8O\ngeak2KkuYz58hMSvq+SXyK8XVPV+IKa0KU+x3XwMaOaJOOOguzS5F1xPfNlsiht1kIYTE3abqO/1\nwXncRhlTWl4oBCn/Mef8PULIV7KiozkAFwBMXmTwMUYQBOh0ivaxMthNij71qU85i4F939+Tot7r\n9bC2toadnR2V6R6GITY3p8v9t5GmKW7dya1kIWog4KggExsg/j45KBISFIihBCcEhHNljyBWYZ2t\nqtvEWqfUiqBr+7rGPb1oXif28ne7MY+xnpD8AUIj9HYjPHUsiNmD05XrhrVEfx2MUKx389k2nbwT\nwvHWrfy7LgWqgKaoeSEoGAjhqFH99ecPJjyj6vbrSeGBcK+QJQ8IUs5QAQNFAHdiToIACffhE8tq\nCk+JQBWEiCAeyHp0bmyyPu+wPLrwMDiGlfXL5sKMpPNGC0TnLbH7YZofPArS066rWlPpLqxaN7pK\n+1v3h9tfJoDkNx91l2lpk5kkcpdz/gYh5J8B+PcQM6MBgHcAfAPYZ0Rdxzhd56aBJMG259BuMX3h\nwoWxM9pn3aBIj2fsdrtYXV1Fv9/HysoKDh48OGbLbHObXiwGsoqfX2eZCp4waqS/uAqDJBgXDwQV\nz/2goqv0kqSnzEOPeaCEKWtNh9W06c6g4KMX12V+ZoM0t8joyTU6SS/qSTDW2d56F2IelEdPcjFg\nzpcM1jZJB0SUWZw1gmphuNptd3XtBIuIeBUtPlknOEnWC8fP/OmSpCcIUNGud6dyEPORyRsjv2ao\n6hIb1cdwILxTWG7DpapLkl647uYR3L3F8PSI5Ic4jn+a4ryWAFwCsAvRue4QIWSec34LwBs/0Sub\nIQghYIyNbX1J01TF8I7TyXnacABZa9Tr9QqzkrMIHEjTFDdu3MDNmzdx8tRZAOaMnSRmLg8045rN\nJYNHkqFpU7aqbqvdrj2VEGLZNmxlWdlerEJJ41iEGsTf8NdnZFuq+/o1S9+9/Vrs6+cQDypPNG47\n3wfOCZjt14H5HtozBkrVl6+LUxDC1Wci44o5iPGz/pkxrYGfhH58eR+S//skMY5FwNHjTaNPSZ+2\nkBAfNWJF6dI5Q6G/XzmBw9FoW8np23+XXRgFr9ZBkhjwfHDPKzzwQXrUB6MbDQJAenC0TVG3v/CS\neOLBc/8CjffLa5c2NzcnJuqj6oyGrb98+fKkhaQAAM75/0EI+Y8AXgRwi3P+gVy3r4i6TkDl4D5r\noi4HYknUJ2leUYZZK+qUUgwGA7z11lvo9/s4c+YMlpeXxyLoQk3XlGyWKRbS7pJ4SDlB4GlFSUNs\nLkCuzFe8fKrPLjLqx5Xs2jNvuhcXHgR0dVyR9hIfvQ5KWIGkyxQDeY5BdvPj3CTtEgn3tEQBz5lq\nIOPE6p5m7RgSHaljC8sAA+q0r2wvES+SjMiy2HQwD+ZRNCEUR/0mF5I6PMvd2mNN+CRBhyxMRdbj\nagXzqdkRVZL00v0cZN2GtPWMS9bLsJvO4QAmt0nYcV4fc6wA+A+c81uEEI9z3iaE/HNCSMw5n7zd\n7McUOvkdRtSTJMH6+rpKVxm3SdGkpHp3dxerq6uIoggrKyvOMXUvootMibl16xaOHTuGo09cQMy5\nUcApCZpQZMU44SMB134HTMLHrOQtndzq4wMHEQq5ZU8RjZJogazKSEF9f/vcVHtol2OTTrBtYi4f\n/Alnqhmd8thrarl9TmO945oMhd+6X3EUt9d/N6w+lnpOwPPiVU7U2G2n5ChCj3LBIOaBOo9OvuX5\nQ14tqOtyP7mPJPUDXkeN9EvtkMBosj73YBWsUoOXJayQJAYv41P6cknY+13wpttNED92GjQ2xZpJ\nkl9unPw8Hl//O+c6e/ze2trCsWOTNcIaVWdUtv61115TFpgJi0lBCKGc8x6Av7HXfbQdAT5C7LUA\nadhxkyQBYww3btzAd7/7Xezu7uL555/HhQsXpm6kNCtFvdPpYHV1FY8ePcLx48fx2c9+dmwVHQDC\n1APnBAkjBZIufX824lTzClrr9UGxFweIEg9Rkg3YHEgZRZrtI0l6wii6cRVh4iNMsqzdpLwTqg3O\nicr2Fceb7Hl0kFaxGzewE48f6aSny/TTGh5GS06SbjeGKuzL6tiMl8Yi6UA+8He5OXujZ8WrZdz0\nIXbIAnbZcFuWrsjLJlM7Xu7NG0XSJXYqB4dGXgL5DX6jOllBqK2mb8AsRnVZq2zoRL3b7U4V5/UR\n4gCAEwDAuZInYwB7N1x/DFGW+hKGId5//3288cYb8DwPn/vc53DmzJmxLUvjEvWtrS1cvXoV7733\nHp544omhY+o0VsskSdBut/G9730PhBC8/PLLKhUIEA3j9LFEfucZp2CcIuYBYh6ItBRNUZcKrvy5\naM9givCn8JHCR0ICpPCQIiPn8l/B2y4yv1PiIyV+gTzrCrr8ZyMlPmJaNQi8vT6kdeMY8qHBLrSU\nmeP5eSkYoeCEKOsP4x4Y98BBjX/ymsV7wtV7LN9naW+R7y/nRP0TYQPmQ5Ieack4NfbV/0lSzUHU\n5yuvQzaJspFwv6C0S8hjRDxAxAPssHnsjBjf71dOFPzpXhIqkk7jENzPuqNqZFyp6Q7izvyK+Dc3\nvd0kOTy5DVJ2JU3T1BBot7e3J643krYVV51R2frLly/jy1/+Ms6cOTOV1UYWjrqwrxR1CVd30lmB\nUoobN25gY2MDhw8fxosvvrjn1rSzsL5ItSeOYxw7dgxBEBSma0bhmz/0QSlXBF1CJ9+ygj1OZbFO\n/nsMWqjIF/ubhB8AelFgxGxVvdQ4j+h2SuARpkh6lHrK+66TdOYQQKVlJo5rTmUDAJpWdKR8qNCV\n/H6iWVs0P7xuiYlZUMgTBnKFPeUETd/kULLpU+BQSBJO0U2ybn6+mEaUJD0ZMoPQ5a3SYteQ10of\nbHbZPOaosNDIWEkbA143XuOOdwBVbzJeuE2XscBy+4wsJJVqeswDpRgNgqJtbKNyVKXp3Ft40unH\nnRZ28dFH6WmcAv8dwL8hhHwGwLcBbAA4CuCHQ/f6KYMez6iP5f1+H2tra6pJ0bSN4kYRdZkU43ke\nzpw5U+gwuVfEcYz19XXcuXOn0A31gxti1kqROU4QIwDnxPgeujzTadYBmoAbRelKxc6oOXfodBwU\nxLJvSFsIAYeHpFDozgkxLDAeS5zEXILytGCZSbRaGVk0K0UCeT7qsNEwjWxLMFCAm9agVHsN9rb2\neyhtLPYy9Xot24t+LMaocV9LuAfwrDhU+9xcjfhs6D1GbIFHEHb3e2y/xvvsKFq0q4pQO6xlXMsW\nPYhFJmY8/bgHmkRglZrobOsHoikWABIPwKvivsR1vlLzVOMjA34ARFmCTWsRtLMFJDHiw2ahdbj4\nGKpb5bOo7PAJUZgKYOPohdLtJOx6o2k96uPWGUlcunRpZvUpNvYVUbezd2dJ1OX06v3793Ho0KGZ\nNa8A9qao7+zsYHV1FUmS4OzZs1haWkK/38e9e9Ols3GeK+RJmqkyMirRzwZQh6qeMKEwqCistBij\npe/LuSgMlxjmBdf3TRjV7DFZ9rlU2/3ESKdRx9amIROWP0x04ppBvss894AYWAdZRnzMPCxUuohZ\nUbmLmO+0AnUTQdptwm6jl5qqczdpOK04MfedJB8AOqlQgyVht5V0F3bZPEDmUUcxdnHA3bNEO2we\n8zT3yOteS8BsDCUfNGyyLkm6jXELo2YFveHRx52oc87vEUK+DeDfQcQzhgAeICs82i+QYzmlFJxz\ndDodrK2todvt4tSpU3tuUuRKx+Kc4+HDh1hbW0O1WsVTTz2FubnxZo3GRRzHuHbtGu7fv18g6ADw\n9voukH2XPIcP3fXA7ZFUKb9AkazpEYZ6MWLhOCUEXp0bvqokpSTNu51q15QSP7fHaNYa5cnWZunc\nySZUFJty89qlN18e005RodkMgX4+CmaQXNf74yLjjFOD6Eu7UZ7LXrRh6hYYwMx7B3KRpZCZzjy1\nbYXGiKz7it5QUO2jnV+fMUxY/t7ryztJEy0/F3B0+2bIKriHYzjbuSoUdM8HTYRdhhMKmhWJSpLu\nhJ9ds0XYdTuLIusT4t0j/wLn7/712NvPMmr3owQhpAaAcc4LXqV9RdR17LWiXyKKIly/fh3379/H\niRMncPLkSTSbzZmRdGC83HMb29vbWF1dBWOsMNUyjUL/zR/mrydJ3Te/bpRnoQc+F6SYjXejFNta\nBJoRRbqlHabipwjjfBCqB+6Hg5RR9BgF47lSHya+Uexqw0XE88541gCjvPSeoWDJY+xEYtCq+7Fa\nH41hsekmNSTMRzNrk63fHFwDMiC6+QEwOvqVHRvIi3c7aVPFUY6L3bSFOS9P2JAk3Z4x6KdipkJO\nreqE3UafmWr8Nl3GIdwqJekSmziIpSEBJjvpPOa94cW0D/lhAMA7IwpK9cF9mqnSjxqc828C+CYh\n5Iz4lU8U2vvThO3tbfR6Pbzzzjs4ffr02PU2o0AM37IZBnDx4sU92Z9cyWBRFOHatWt48OABTp48\nOTRnnWWdNZOMZPokzWYZ8+0liWcgYNxM3covJI+b1QULZxILitY8DqL21/dhyG0dulKvq80EOblW\narytDENYbTytwFS3nIjXmSDRrD82Qc+P5avrkJACgT7+SBXbFamoQ77Gwnm4Z5DwlFOkWbiBAZ75\n9Il4X2wLpk8TVXelHkZYULgunZTLhlDG9TAfPk0Kx7d/7yTNUhvg2c5V8aDnZTM4+qyJ62/U80Rs\nom198YvilSTrnHoYHHsSniNIwIXk8OPqIeHW8Zdw/Ja7Tt4uJI3j2LC/fdyJOiGEcGGs/yyAHYgZ\nUwP7jqjr2btR5C6iGAeDwQDXrl3Do0eP8MQTT6hBdX19faaFn4Ag1uPm+W5vb+ODD0QxcFk2+6Qd\nVP/zmxV4lINzINC+d0PjBhMCfbwoFOhYynwtYMY6c9u8cDWJfMMisxtWDAWdUq487TYSRsGTLJ/d\nT4YWuSqbS3aNUVqDR5nqfmdsC2KQfMYIAAKPMvSTABzlMZGpdQ2DpAKfMnRjQYDng24pQbchPfON\nTJWXN7QwLd9/N7PQzPl5Jb4e+RWxABVqXvtu2sK8x0qVdGnpMa6NzaNBiw8SfeY+xg1/BXMYP3Jv\nUmzgIA5MkFLIOVek6eM+sOvgnLuDhPcBCCH40Y9+hDiOUalU8MILL8w8bpdzjtu3b+P69etYWFiY\nKgzAhrwHyWsNwxDXrl3Dw4cPjXuJC/90vQtoyi0gCHaYDbaSDHokLZ2F1NVzAE7/NJCTaUlidUVe\nR4rMlw1iFDPapFwSZUnsbdIva1z0FKvcXuOy4WRkP6vX0cm2vr1LSbdfa+pQs+V2VD3wmMd0zVxI\nVVwemxKmyLa+TIddWCohRSJfW6cHJvg0KVxDmX0xYT5i5iGgqfHwFDHfSEDrJTV1/wCAI+F1+Mkg\nfwDzKvAica9gQTWL9SyZaQ7cefgAlCIvwSs1RA0xpqZ+rUDWo4UjqGzfA69k95Ysh/3tw5ec6UaA\nKCg9oeWo93o91Ov1mfTEeP3117G4uIirV686+1y41o/aZww8DeBdwCDvAPYhUZcIggC93ngxQTpk\nNu729jZOnTqFJ5980hhUfd+fqvnGMIyT+rK1taWaJ509e3aoX3Ivnvc4JQg8jkFMQQhQ8WWBZ7Ea\nXmzL0I+osrFUfa0oNJVTgECYyMGTFBQD39NSCLTCVRth6oHF+XVI9VxPkEkZgUc5wsRX3vWanzrV\ndL0IVmIdoqYIAAAgAElEQVSQ+ogSDz5lqAfiMxlmicmvTQy6/djHQtY+O7H8inouvcRmJLzYDT8q\nkHp5XM+Kt9QH22EkXcdWNIe5wPw+9FKrQ6p2E3gYi4JRW8V3kXSJnWQO835uV5Ek3VbjZaSlrt7b\nhbdhts0mDmLJf1jaCVXHdrLgvCGOA50A/jQR9f2Op556CpVKBW+88YYzFndaMMZw+/ZtdLtd7Ozs\n4DOf+QxqtfFbrA+D9L4zxnDt2jVsbGzg1KlTOHfu3FAv/Q/WBtDbevrE9GRLCGW96LlOuabqcihr\ng51aYhwrK3QsJLpkCr6+nTwHkHu75XdbJ7n6d9lVu+MqqFeeeovs6sRU/myq2Z46n/7dl8cxZkM1\nP7ifWYXEMvEg4jpnwdOekW75UCNVa307ObOqf9IpRFqa0w7KKXyN8KvlzFfrbXBOjEhjeY+SZF2/\njjKyfqz3YxDOwLwAVCa78BQsMBtlFZARd0498ao5M4g59wKkXgBkJN6LRo/dnBRf47tLZnOiW8df\nQj017ZB6NOPq6ioGg4FS1B8+fIh2u404jif6bl+9ehWA8Jy3222Viz5svUTZPmMigFDUwa3omn1H\n1CfN3pXQs3FPnz5d2rHT9310u5O3ZB+GYR512d3U8zycO3duZPMkYLLkgf/8ZlbRnf1ZpIwgZUQR\n7ygR6jFjQCWwPHcpkKQ5SQeAMCFK8ZaedkAQdPE/ABCNkJtpBLykixwgVHy92/RAs8jo58rPI8hy\nJ8qLXit+iu1+VZFwoHzmoB/nVflVP4sKczyw2OjGYqBrBuKBzibo0ievE/NeUkHCqNpnFHpJDVHq\noxVkDwWOwbyfVI2pzt04U9eD4Q+w/aSqHg46SV2RdZukxyxAkKnx8qYiyLrZFMq4bq1LLFC02riw\niYMIKu7v8jD7i53+UgbOuRHptbW1hePHj4+1rwvjKitf/epXp+5K+rMAQogi5nKGdK9EXc9aP3z4\nMObm5nD27NmZWhkB4L333sPu7i5Onz6Np556auSYLEi6CV0gIIQbDeCAogJOCTP80coTnSnsRixj\nSYG9/F0vctQVTT1ZRZJ8W5nWUZrMZZ3TI6mhgEvy7SLKuUe+qFyXnd+efXAVYrr21x9g9DQXSphz\ndkIfhxmKsXpSgDIKTpmPMPvcdHVdHouAGwRffsZ67LEubIWJD896IAiTGhjEg8LT5G3QMM6ItqdI\nOgAwLZmLsOzeQX3QJFQEHUAh05z5MlZT+7wIBThDWqkjrs0j8WqohOVWRamqA8B7S/8MKKcCTjz7\n7LMAgOvXr4MxhocPH+KNN97A3bt38cILL6BWq+GXf/mX8du//dtDj/MXf/EX+MVf/EUAwMrKCi5f\nvmyQbtf6R48eDd1nTDQA9zTzviTqwPjFpDs7O2i320OzcXXMoqGFDZcCvrGxgdXVVQRB8KEUNEnE\nmVXELxGNk1QMDB7liGJB2nUFPN9Ou4FkP4YJRcXnBatLGZI0J/QAUHGcR23L8iJXABjE+UDVrJoJ\nMkYyTSp87YPEQ83ys6eMqgFOkmk5UIeJjyilaAblf1PSZy+x2a+DUo5Gto/rRmCjG1fRDEKlsojr\nMj2RANCNKgg8hk4sCHHNH9/mtRs3Sq06OkmX6CT1QgOpYfn198NlzAfFh1lbwZcYlTW/GzdwoDpZ\n5rsL//AP/4BqtYpWq4Vms4lms6maHNlxXs8888xU5xilxkhcvnwZ3/rWtz4h6mOiLKJxXMiM8tu3\nb+Po0aMqa31nZ6cwVT4t+v0+2u02dnZ2cOjQITzzzDNjiSZvtiPA8mW7IO0nDIDUMwyLSza++NS0\n/ekJJnkxZDFy0SfCNqHvyyB88VS7LkO5dlhOXNetwy6+BHLibPvsDRHHOg7jFCmoUXDr8vAnjsJP\nuxiUEG4o2vr7Kh+Y9FcXOZR06dnX35+ISWVfT4Ip3ptc64zlMoRBez/ksij11H1LimTyf48y9fOT\ntTYqSR80jcWDB/XhpRE49ZBSDzTNv1+SpPOsm61dflXw0WcRj5xQUGbeP5jniFb2a4irc6h2HhjL\nVx//hbGtoGVgjKHVauH06dN48cUX8f3vfx8/+MEP0Ov18OhRsWmfja2tLRw4kEcQ2/u41o/aZ0xU\nAThV4H1H1CVGEfXNzU2026L+amVlZeyp7g+DqEtFnXOuCHqlUsH58+c/NIIOAP/pDfH0nKRCSVfX\nM4Qg21GIqaOYVE90iRKi7DODSCyUyvywQtQ4ESq+gLDjiPNBqeqMC7JuPwhEDkuL0+aSiCnPWpAP\nLDbZto8pVXZdkZf7uKwrQN7V1X4w0KE/WHTjKlJG0Qzc5Fum3MQpVY2ndsMa5qrj1TnEzFM3n1yR\n94woSh2dqIYgS9iZC4ZPY/aSLGs989NLwl5G0gHgYbiAgyVEXM4CbIQLBlnvsLzQr0xVv99bwMF6\nPlU6d/wFnF6O0O120el0sLGxgV6vhzRNEccxVldX8Z3vfAd37twZu6uwjVFqzCeYHHuJ25URiHfv\n3sXx48cLaV17fQAAhF2y3W5jd3cXKysr4JxjcXFxbJIuC0eBXLG2iZDwpAvoI5ntRyfICac6Ds+J\n5zBrmK0yGyRU93brJHdC2VMUvkriy9UyiZQXCakNScAlKZf/2wk5NokX1168DziVdFk3ZSnk9vZq\ndoG7lHftWrg70rHsnhGnnhpzhcXGvEZdYWcgQEndVpj4eLJ5DUEagqRMNLTKLCteqt1fOAejHihL\nQViiCDrgIO2Zmk44E8ewbCvyOOoaghaq8eh6pBvz04kjNvQH7zAMVXx2o9H4WHaf1mwuAX5WiPow\nRV0S4Xa7Dd/3R3q9XfgwiDqlFL1eD1euXEG1WsWFCxemJgqTIHHwRqmgS7iU9ighSkGvBqMH6l6Y\nxaxJ8h4LewwhXO3vuhYdsabYl51REv84JKAUqAUMnYGHRkW34BT3kxaahBHUNdLOQQzSL28sUUoR\npRVwLh4adNIuj+NbFh7OiUHy5QAtST51WH66WbdWnbC7Gj8BQOAx7IZC1ThQN7/rtvdRRyeu4UA1\nLiXpg8RUN3bjOhol6n0vKSohO3HTUO9toiBtQi6yLkn6ONhOxvse12o11Go1o8fA7u4u1tbWUK/X\n8e677+Lq1au4cuUK6vU6nnrqKfzpn/7p2Dnd4ygrV69exaVLl6ZqM/2zhL3E7eoJK48//jhefvll\nZ5fqvYzn3W4X7XYb3W4XKysruHjxIggh2NzcHKtG6HsfpABkl06ixkeXqi5n1Qg47CPLkZHqv5Qc\nh3FaIPcqCcbhSdePYc/qcS7UfXu5vl6Hnt3NuDDKyPMor7uDNNvHj7lfKNBknKogSP2hJ8muo8wm\nQwg3yLL93thRiy7VXVe75SyoXKafNzYKRcXyOM2XSWIul8n/Xe9vlHiFe4ZO5n3KkDCK841V+Jl/\n3GcR0iyv3kvznylPASKun1Ef1M5293yltjPL8pJ6FSR+DX5WHEo4Ew2pMrIeV/Ix3E8HiKrzyv4S\nhLmIcmPu4tDZpHc2T+DppfIuqjr01JdpMtQXFxexsSH6GGxtbRX60ZStH7bPmAhQ0rRu3xJ1mb0L\nCIL+4MEDdTN++umnpybCsyTqMrNXFkG89NJLH1lHxD/5G0GQKiOa+MXZS41BUAm4YXHxqLC8SJJt\nC0i6Mi9vQlKBl6p7GJuKRDVgTpVeIkkJUm3MrVfMiEhdXZd2mDAhqPrWVB3Lb4xyfwKu9kkYQb1S\nVJ9can0/9pEyoF7RlflMVdEiKPXtw4SiWXH/HUWJB0/bpxtXECYUrYr0g7uvQQ7+97tzmKtG2bZ5\nrm4/yVIGLP/i7e4i5ipF8j3IfPM2eknFIOsJ8w2SnjIK6uXn2A7rWKgWlXhJ0iUehgs4WnuEkFWd\nJF2q6r6Wzbw1EN+XhWqWVFCSsDAMaZqiVqvh2LFj+N3f/V288sor+NrXvoZDhw5hbW1tqmY6wyAH\n9E8wPsYl6jKta2NjY6xmSNNE48p6pn6/jzNnzhTskuMU8wuSbpLKlEORdiAfMyXK0i+4XkyfjaMU\nlnrLc9KorDBKNS7aOHS7jCLcmjLv9I5rcJEuPR9cRhZKpJqf3nhtgkGqffRzlVkJCx1YiVC5Uy77\ngmgEn3mZtVNLonEIG857kkMNT7IHoXy/7Fptjzyjhdcap56aJTbPndtYxDUT43/7+Kdbt1Fhgu9R\nnoJRDx5LcmLOUnDigfBUKOxDZi/U8T1fNbGiUmEnFKmXNeLTyLpEWJ0D5Sl8FhmqelxtIQg7yv7y\nXvBzaKHvnEmaBnttXvfKK6/g+9//PgCg3W7j0qVL6liLi4ul613LJkTKOXcWqe07oq6Dc447d+7g\n2rVrmJ+fx7PPPrvnqY9ZEHWdoDebTTzzzDN46623Zk7SXVm+OhgDwoxvVSswCLDYP/85TszBKvBN\nFZxz8U/eE207SlpCpsXvWlFlSPMblA9FWPUEGYkkJegM8sF6mLofJkSdtyzJRh03W96PxLHrFYZe\nSNV+rtcgtvey7YffpMOEgnGCbiS+fpKwS3Xds4h9N/TgexydKFBkfRR2QzGASsIuSXpxu0Co8ZHY\nfrEmCLWtpOuIUw/bmZXlYD12Kuk2bLKuk3R9mvfuYNkoqCqDJOgS97rzONIcnqlehrIGGUEQ4Mkn\nn5zoWKPUGKmmf4LxIKMOfd8fGmEru5XKtK5xCjgB8QAwbkKW7P4cRRHOnDmDAwcOlAYOlN0jvvtj\n+bddtEKYKre5n26pcCmsqbbI9j1LKGKr1/vp6rGeDqOK/2mhEFV65fXfXYkrw0hXasVNjlO/45oN\n0NXwssQwvXBffx8lbH+6el3QAxDMZa4MemE7yo9NiPjn6uxtRwxTwrXABVJYJvcRQQ/u9/Xc/E34\nLBYPVOBCKUdGzOXDH8+bVAEe0ox8c0LU9ik8eGlU2l2WUTdtTDKfupdGiINynsVBEVdbeFQ9BgBY\nws7IOqULS+sY1ogr/qVfz69DG883NzcnJurPPfccvv/97+Py5ctYXFxU1sVf+IVfwJtvvlm63rVs\nXBBCKID/VrZ+3xF1mfoio7e2trZmGr2lK/WTQir77XYbrVYLn/rUpz40z5RUdVxFUn/yN1UwhqxZ\nkFgWRjlRr1ZyH7hU1O2XHGoCrK3Ky23jRKjf0iIi4bLTyHPrN6g4IRiwosVGV/V1xbo7oOo89Sor\nbdwkyX2jano7deIuHyw8yhVhHxf9SERxNTIbjf4gIkm6jt0wMCw3OmSspbr2KECSErSqbiJg36R2\nw0yNz7bX1XRpxdG97luD+lAvvY3bu4toVccrZN0O60ak2DTYCBcQpeVWnmmgdyUFhCo77XgxSo1p\nt9tot9vY2NjAxsbGtDFePzPQ+2Ls7hY71dr2k0m7lY7TGE/v/iwJ+jCUKerf/TEzxlF5mTZZBqDU\ndTke2uq4a7+8ULSoKAPFBBIGAJyYnneQguLOORHWCE4MVV3fRhaS6kTaLkwFTCVddLJ2F3Da+xEt\n8aasP0bKifMhwUjPsR5E9PMaD01ZtYDuUZfLZNGoOA6y1++4tky8cj8AWfc6x0xtWqJ06+c4NX8f\nFRKCcgbCxHvr8cymQjxFviXkMsK5RtihtpNkOPaFECNTXCiLi0kvxAMn1PC6x34dg8qcitPVryEM\nWvAyNV6S9A8DaZoqm9s0GeoA8OqrrxaWvfnmm0PXu5aNC845w88SUQdENFYQBJibm8O5c+dmHr01\nKWTXu3a7jfn5+Zk01RgFz/NKmx5Jkq4jSbUGHRrv0km1bESWJGazMn37WiaU2uq8hFDm5W8EQclH\nY9tpwjhXxOU+w56X+iHNFfRAqBNRYqpAA+26y65DR5QQRImwzEhbjOt1yhtlL/O+NzOF3SbdxvVm\n20rCLreVDxtJaqbtdEJxwUuNSBFuF7qRB59ydELfIPfD9tkNxZPXXDUnMLrqLSHtPZ2wUkrWd8OK\neggAgO1BFQu1fHbPlS1vbyOxE4o/rpo/Ptl/2J9TBaXXt+bw9PFirY4r9WPaxjqj1JgvfvGLAIDX\nXnsNW1uTt9P+WYNO1HWVWle3x0nrKsOwvhjDuj+POqbebO/v35eWE9XHxUBZy3nAHKd10k7AC2Sz\nYPmwyGGqncNIdiGCnqlvqU2ure2lN33YemjH00c9O9XEyBrXiHjhveBF0m/DaGZUotC7kmmUHVMn\n1NqDh3581z2HZQ8khHD1QGE8kCH/bPTPU/7sahYImLPH+vFOL9wHFX1TQcAESc+84ZSnSgn3WCws\nVTS3PxLOkRJqNCrihIJxT5Fobn2POCHK4gLAOAfhTK3Lz5sg8usgnCFIw4Iyv1k9UnwTZww5Fkzj\nUf84Yt8RdUIInn76aVBKcfXq1YJa9lGCc4579+5hbW0NCwsL+PSnP/2hE3QJSmlB1UmSBF/7bw0A\nWlJKYGaT20hSgGUjSuDrSnZO1vUklr42Ox0Ew8m0DcZg3AAYB2Cp8YAg+1EsHhqqgbxO92AHAL0B\ngZxq1rPgZXOkKCFqsK5VGLqZ4l6rcONYOtHvR7TgYZcFr7YldquXFZE6PO+2uv6o48P3RMRkGfRi\n1QedKlqObfW4SolO6CNhRD04jMJmr4K5mkmKZYFTP/aMKfZOZrWxC2tdKCPiALDdr8KnrLCNJOkA\n8LBbxcFmcX/d/nK/5y4u/caPmviXF02yniTJzGbcgNFqjNxmLwrMzxqkR10nzysrKyPV7VFw9cWQ\nzeWA8u7PwyAb2Embi/38UPzdHCRty4htgwFMUgm4ya3Te+wYj0UDpfJrGmVEs9frpN8+tq2al41E\nSj0f4p+Wr3kSAq+u2fE+SEKtPzi5iLc6hmVbKtvORdLl9Q+97qyO68TcBnySqBoFRdCRN8Si4CCa\nei4sLoKQG38nhGQedfP+wAlF6rC00OwT0sm2UtKZORPFCAW1OpXGWS67xxJ0giWEvIoAsfHwMi3e\n3jiO/6G2PnSb/dK8bt8RdR1ycJ81OZb2mrIiJc457t69i7W1NSwtLY1tvRnlKZ8E+vRrkiS4fv06\nvrX6dMGKEsccoVKe8/MnieaXVDYYrtTzSiCV3vJr6A943rG0osct5ogTodCLbfLlRcU/e13WWx7G\nQJkyL/fRHySi2L2t3GYQUfUQMogImjVTidex2/fUfrWMhOt/EiLdRuzre8JCkzLTcmO+lvw83VAM\njHYjJxc62batjMtKkm5PgUtrTzfy0KykTo9+L/INBXx3IN4snbD3Y7f1ZJB4GCQeluqjmzZtD6ql\ndh99m3G+DoOSJJxxYcd5VavuBJxP8JNDt9vFo0ePlLo9aVpXGXQ/uWwuRymdKhEMkEWiBwEcLPUS\nu+Dyc7sIetm2457L5bF2bjeCRA7DsG+17W8v65g67JqGWWTKjuMi5qPOOeo9oOAFy4s+Y6LGLs6z\nIlbr9RKurkt+1ocaQmioEEGCvayDqjifIOji2OKYhGd2HE1FN14bKQZocusPyybXksRzQsC0LrJ2\nPrqRGpNBJ+tcU+67wd6/r/PRQ0X8y2Dbkre3t3H69Ok9n/snjX1L1GUB0jTZu6MgB3eZz6mfUxav\nLi0t4bnnnhtbqaOUgjHmjBCbBpRSxHGMdruNO3fu4MSJE06bhj6ARTHAso2CIPfgScsLkBPaKDbz\ny72qUBNS14gIIIzM5dUKgV5vRYg4vzyH4d8DoE+KRNZHqhfFAqNtLKG2f8P63seWILzbo4YKT0lR\nvQcEwQeARo1l11g+yPdCgkbVLBQKS7bvDDy0au5bn020O6FXiIWU29nLu5GHKCaYq49ZSDfw4Q/5\n2xxo+fOb/SqW6qFB+F3ox95Ism5DPpQ86lWx3BiviysAo0OrDTslYBpP4yf4cEAIwZtvvokgCFCp\nVGbu55eK+pUrV6ZuLndlNfMEcwylfq5UizIyXoYygu20i4y4llHH3CvGIt8jFPNxl486l3yf9duT\n63XrHn0TxeMTwvOpA8ILooLeT0RcAwMlgnz7lKlC2krW3VkQca62ldco/6lzloBwVrCZ6EWl6ros\n64uupMvlNnkHgMSrGNuIfQVZl++/zyIwQrOUGR99bw66UapKQqf9qBPX0bJ6dIwzWyLx1ltvodVq\noVarGULqLMbzUZ2mXetfe+01AMDq6upYMbyEkAUAcwAeuJJf9h1Rt7N3Z515DhSJOmMMd+7cwfXr\n13HgwAE8//zzE6tyUgGfBVFP0xTdbhdvvfUWnnjiCbz88sv4P//fKgAGQoBKhRoqM5Ar6HJAi+P8\naV+q5zaJ1dHt5wNVZYTlJUmAVIspqARm5CLnHCkHvOxiopir9Y1a+RdXKug6UZf7yderD9RJAuxo\nanmzZOJFkm47ntJGygSxB0Txqx3ppb/GXkjQDwladV5Yn6RmbUBnIH6pOawzEmFMDB97szqcJMvX\ntNv3nGTdfp2dgUiekQ8Nuu1FknT9ujf7uWIui1UF2deKvLJOsfM1caPa7he/M9t9Hwv1vX2HdZ+6\nC58Q9Y8vCCF49tlnUavV8Pd///czO67sqfH+++8jDEM8//zzYxN00UlUU1Gz5R4pJ4xiqt+NaYjy\nJKTcdZ5ZReG54HoPprE6DFPOXccf5zpGPRgx7cZlbOt4vwk4NMHZSMchhKsxUpJumRVPwY2UHApW\nGnupK/Ae0TqHcvOztMm4vk4n70Sp3Z7zfJLE6+Rd96QDuQ3Gts8AQEJz8VIWtXJQEDDEPEBAYvX/\nLHHu3Dl0Oh1sbW0hDEN873vfw2/+5m+iXq+Dc45ut4tnnnkGp06dmsi1MKrTtGv9xsYGLl26hJWV\nFXzpS1/C5cuXx0n6Og7gJQCUELIO4G8BPAPgJuf83r4j6jqm7WY3CpKoy3SZ9fV1LC8v44UXXiio\n7JMec9r9AUHQb9y4gVu3biEIApw/fx5HjuSFG55HwBhHFDH1uw29Gl0qAlHMwTPFWlfagZxccsZB\nPEms8/WVIFcV5DOTnjYDmGq7PH4Zuv18W/F6TPsNJbm6Lgpkxc/VSn5c/dlNf2DZyayq9YwvSqtO\nrvSLY0hfvP0QoGO3lzVdKvk4JVGW3VqlH96GfuxOX/zQqpskXJJ04/x9MZhKhV9X3yNte9/j2O17\nSFK/cFwJPfVGPjQs1BNDRXdhu++rbSXsolgA2BkEWG4WbzRbPR+UlpN1XVV/1PGz/w+o85jXUu5T\n1Btk2A2LPsFPFoSQPY2JNmQ0brvdRr1ex/nz5/HBBx+MJOk/WBtk+5Oc/ml/YraFw0UsJ833HwfT\nku0Pi6QPO/awc44i8S4f/8h9Rnj4XSiLtpTXQK3fJShhKFO99UZPeha+kVtvvTeeZiCSirRu/dEL\nPgkX1henJYoz50OZsrc4XqedwR/TXEDxeAICjpRoKnym+Bv7kCoGpGG8jsK1lXyGD3otHGq4O5n2\nvTnU06LoUq/XUa/XUavVkCQJnn76aXz729/Gr/3ar+Gpp57ClStX8Od//uf4sz/7s9LrcWFUp2nX\nepnu9eqrr2JlZQXtdnvkeTjnbxNCUgD/GsArAL4AoAfgT4B9qKgDZlJAWUX/XuB5Hu7cuYOHDx/i\n0KFDeyLo+jHHzfO1wRjDjRs3cPPmTTz22GN46aWXsLa2pp4c//e/zFr/ZnKyJKC6n4tzkySXprbE\nHIOMAFcq2Zfdsrvoanp/YCrn+vFFwyST5A7CfPtGTTwklGG3m8eX6deepPnrlBMUYcQh315J2ste\nYz/7kykrst3t5ddnw7blDKIiWRfedXPZ1q4g9s16+euV71mnL2ZEysi9jt6AKrIuz12GTp+iVWeI\n07yleVk05aNOgGZtPOKx3fdHeu3XHzWw0Jj87//WVvFDGDbr4cIs4rw+wUeDaet49Ghc2bui2WyC\nMVY66/rWtX62rxZhaGR4lxDICW0oo6b4h3XKHPX7R4lpzy0efjTS6uiA6lo+LGqxAMfbSzH8vdXP\n4VK87WW05DXo1hWg+MDmamTFQDUCrEdEsuJnnn0fiDWNTTSvuFpDPMO64uxeq6nnNpFOidb4SEU6\nis8vJoLQp/AUQU8yiimb1I3KS99rgakuulSrVXS7Xfzqr/7q1GP6qE7TrvW6Pebq1at45ZVXSo+f\n5ad/AcCPOefvEUK+wjnfJYTMAbgA4CHwM0DUOx33k9k0YIzh5s2buHv3Lg4cOIAXX3xxZmrPNERd\nXs+NGzdw9OhRvPTSS2oKXx7vf/26qMH3fWoQaCYCXpW9BBAkHBC2FEKBSiAjqqCW65DKvFLPAZDs\neHJb3aenk27fL5J2IFPms2N0e/mA4nl5Rb2R7a5ZdVJWzHTX/fUSUsHXr0FHknDjmqoVAs8zSTgl\nwCAj9LVqTu7VelW8mnn/GdCsjx6Eun2xzUKLK7W9DIOIjE3Wwxho1tzb6sRWqva66i1nEqQaLreX\n6TjNGisUFesFuFFMEMWmxUbPDZbH2e55pWR9u+8XGkFNgtsPCI4dEvvbyS86+dsvKQH7EXLWUd6I\nx4EejTs3N1foXWH3xfjhtZ762aWc6yidTxpBVF0pKXKfYY+z+nZl52KkvC2M87wzAkNRcbbXu9aJ\nokh3lKNcD1KMogQ0Yk2HiwBllp8CqScZqbbIu+sBgZYcw2VdKXsIsWdh1HVZ12tuK2wkhfNmBas6\naee65cVIfsnfZf0otoXGtrbYHnj5c5rRSAYKCgYPqSLrKTxwECTwUSEhIi7I/F7tL29vHAcANN6/\nYiy3o3Z3d3cxPz8/9Xn2AmmTGVFXswTgEoBdAOsADhFC5jnntwC8ITfa90R9FtYXnRAfOXIEJ0+e\nRKPRmOmU7CQdTxljuHXrFtbX13HkyBF89rOfLdy89HjGNOVGprpv2UtsIs65GM6iOCPirJzUMmYS\ncXnT0xVuu6gGEGQ4zM4nlXk9acZGt1c8ro4o5vA8osi03VRJLpfkO2VAmhH2Rk0o3GXnD6PcI69b\naOR55bFrVZ2cFo8jyXxds2LbfnRAvKebO+JYdYdqr6vxg4ggjvPtkpQUfJjdvijG7Q5M604ZxPak\nlD7nUNwAACAASURBVNjb2NilmG+4b5aDKL8e2w/fHRRv3ds9D8ut8S0Cu12CuaZ5nVs7HIvz4ysz\nukL7CVH/eEH/bOR4Pg5RnyQad+H4p/Cj652skNA+zhAv94Tin5zqH2YYG1WhNEqtpyXrSUZRZ41x\nXtOocwvbiLm+QHhtYko4QMZX8SdJytGJuGFvGkHK9WXDrr/Mhy5hW1WK63PRjWgRjdkPBWuMvl5X\n3V2JO0zrVGor9Ckpfu9YRsRlrrsk64BQ1gEg4T58kiCG4Es7cRPzQVddwzgq+ly0MXIboEjUOeel\n6XwSsvBTx8rKCi5dujSy0/Sw9ZcvXx6nkHQFwH/gnN8ihHic8zYh5J8TQmLO+X250b4k6hJ7TX1J\n0xQ3b97EzZs3cfToUUWIb9y4MfMi1XEUdemJv379Og4fPuwk6Prx/uy7J03ynX154ygnQvLd8X2q\nttORZARYkli5vlKhhbjFlHFQeY6Yq8IcqcwDJhnmXCjPUZR37JP2GGmnkaSUMQ5KiVL9GePOolj1\nujT13uXF17HT0arSK0VbShxz0OwYYcRRrxJjdkBeu7TtxDFHvZ5fFLOsQVu7HB4FGpnCLpV5PZte\nYnuXo9kYPZDJ/Hqd2OvpNkmSJ+d0++WFs/o+ktiX+ewBYe0hBNjJimjnG0w9eLhmBHb73sjIRZk7\nb2O7S7HQzD+cB5uTaYK6qi5h/73v7OzgzJkzEx33E3z4GFd4sZO3yqJx317Pfa6ElBNyFYM3ROV0\nX0d5UanrHMP2MbaZ0Ms903OXbCOtHEOPMYIoj+uZH0qAJ3ztLthhhuMo5uNen0TZ35O+v9l8qfwh\nRirsxvH17rCumYhsAHYW/WodS3Wy7/psZXQjATcIup7qIq0uA1YF1zrH7sYN1P0QdZo3XnElvwBm\n86p3Nk/gufl3jPXxL/26+nmaOr9h/SxGdZouW//aa68pC8yIYtIDEBNJtzhX0xkxgIG+0b4m6tMq\n6npRpvR8609pvu+j3y/+Qe0Fw4i6fvM5ePDgWJYbz/PQ6yXwPGrZTMSX1VZdk4RllheiGhslcTmp\n6fdTRfwrFarIF+OCrCcJU+Q2ihmkLU5eS1kqTBRzJAkzrDc2JPG1i2LTVDxU6J75OOFI89lseNWi\neq4r/mHE1TWWPQTsdk1i70IY8dJ1Er2+fJApbqc/CEiff91KvJFFs/pn2R+YUZb286T8vZv9+Uo7\njiToru072bJWw7TJSJKuY6dHjRkC+++sH4oHklYDTvQGAEAw1zD/QDr98dTAh5vu5Z0uQ6vpSClI\nEiNp6ROP+scLdopX2Xg+TvLWe+vbKve68Nc05M9L90LryR7j+sptlBE1SthURZX2cUqvf8ixx1GL\n7d3HIaXTYNLjjCLhxSTx0eccdcxRBF1i3HqF0qjIMZa7DEecw0ngOYjhd7FtPvr6YbUTKcxiUiAn\n6An3wSFIudE4i1PDn78VtvCQzeNE84HzHDbe6H565DZxHCtrW5qmI9X0URjVadq1/vLly/jyl7+M\n3//938fGxga+/vWvDzvFfwfwbwghnwHwbQAbAI4C+KG+0b4k6vLDmTSeMUkSRdCPHz9eIOgSk9hU\nxoXrmDpBnzRV5j9+6ygAZCkv5pfdtwiwVN2lNzxOOFjIFAnzPE+RdLmtntwiFPHMmlLRlOQ0V6Il\nkiS3x+Qqfr5OXEd23Mx642fHkKq6OpZWNCohSXtcYmPpdIs58TrimBtefPnQIl9LHJt5ub2+OF69\nRtX+EmHEwVKOajW/fjviMmVAP1Pi61ViFNMCpsK+vSuurdnIFxaJMIeXcZnGGJ74B5scvjfetp2e\nINiDqHybbk886LgUe93HL4+lo6dpCDtdgvlm8TOUqrr+uW13CBZa5TfDW3fzB2BbVbc9z59YXz6+\ncI2R+izjwYMHCwT9/Rtb2tbESdqYRnyoIh2amijJvct+MQWGkjwXPx5D9XYdd9L1zn20c9v+bR1j\nFXROcl7HteoNhuTvYx9vzGsal4APWz+Nsl82SzOMrI86tovAu8l7iSA2ZsdXl8deNHhihhru0RQp\n8wrXv7ZzBIu14cKnTtKv7jxdup0+ns9KdBnVadpef+nSJWxulihGFjjn9wgh3wbw7yDiGUMADwB8\nQ99uXxJ1CbtQqAxJkmB9fR137tzBsWPH8PLLLzsJusSHQdQ9z1MJNdJf2W63p8pl/1/+n/zL6EpJ\nSGIGlhWMSrIsSTqTpF0no71EEWSXV11/jwf9VB1LklzOzEJUST6ThCFORHKLvA4JppFwZhWx6uSf\nMxjGzm43J2VBQNR1y4cAqbpLQl3mvZfohxkRr2oFOBwFJbk/YGjU3QNemJHvKOaoZYTeVuvjmKtr\n0r3uYj/zeJLMm5744t95r8+RpOXZ8/o+vT43yLpuldHxaEvsY9txQou8S8V+rlEstE0Zh0cJOtlM\nhy16yD+nnS7B4tx0N/1HWxzLi+Wf7T/90z+h1WopNV1+T/ZC1Ec1xpi0CcYnMKEr6nqdzqFDh4xZ\nxh/fyG+S8i+grJGNSHRxk0JtK2Mdc5CkfMvh+eTDc9aHq697yU6fOMPcfg1kBGkdtX5MjCLfrvWj\nij8L209psxl3Xdk1jHNeZ5TiiJmSYdvZ2zCrmqDsmmTqjH181zkkGXetO320jn//X4QiI+2gLz+Z\nB3zI2OAbWy0cnguV9cWOaOyG1OgN8ndXhKrzd3gVv6M9fOge9e3t7Z8K0YVz/k0A3ySEnBG/8kKe\n474k6uPGdyVJguvXr+Pu3bs4fvw4Xn755bEaDn1YRD1JEty9exdra2tqGmXczqY6oigF50Jd/v/Z\ne/PoyK7yenTfW7NUmtWSuiW1ZrXc82i3GVbWspvxR0jgYQw45CUEDHEeBNtgDGEweNkYj2CwDYZA\nmGncJrHj9wxBJoxNoNWCdrrtllRVmkvzUPNwp/dH6Zw699atUVVSSV17rVrdqnPvuaduVZ3a5zv7\n2x9LgFnCyzFkWZLi2nKjKSZj0Tq8EIiiQqUnIgCLZS3zWxOVB2JRbTEkgec5mNai+ISUaSPeorim\nb1PiBF8PkhSPeJPoPSHf2v/HouMyTVbVvia2LyAejSdacUFQ6H0KRWLyHQtD2NlzRUmh0XoAsNk4\ner8JIeY4IBKRaR+SrI7Ak/6IZl2SlKTXAwCPV4LBwFHCrvV1J04swTXpDCHsZDxap5blVRkGHjRi\nz37E2YRaAx+LnNvLiW5/7R7o7GIse5QEyY4W4YiSsDghIO+FPxDv2x/gEmRFHj+nupep4F7g8OoT\nHfD7/VhcXITP58Nzzz2He++9FwDw4x//GMePH8eBAwdQX1+fUZ/pCmMQnWKWRTBKAFTmAIFAABMT\nEzSxn+TpOCeXmMieGiklBNlU81yLMKYik+nSoLMlf9oocq7IlUST66c6P3Y/1k/SWeS8W5GD7j2b\nYzM5LumuQwa5Dexx6a6dWVRdZ2cohVxK73iVUw4UaCPpWuxutKv+DgRElJfHqeZzZ2ML6tdes7a7\nbYhF26ZWrKjb6UWLfRlT/uS1LH71x3DSNm3xuq1A1AkURXEma8unQ1PRQhtVFwQBTqcTf/jDH2A0\nGnHy5Em0t7dnXBU030RdURT4fD5MTU1heXkZR44cwVVXXZUTSb/z6+rwqyjKiERERCIihDVmxvFx\nksqSdABrx0oQRZmSZ4D1YI/fS1lSEA7Hj2efJw8CQZAhCDJCISmpLIV0LYhKTH5DE0njz7OIRGRE\nBZlKZKJR9c8k+7y2TYvYfZIQiWjKLWtOi0RkRCIywuG1QkKSQiU4LEIhGT6/pBvpjkRk+P0S/H6J\n9pcMqdoItHIZLcjr9/rllL70BIGgTK0xI1FFVZCKhT+gYNWTfnysl772PSTj0XsNwbCC2SVgdjHt\nJbJGeXk5Ghsb0dDQgJ07d+Iv//Iv8fzzz8NsNqOqqgrPPPMMPvOZz2Tc3+nTp+k2Kyl8wcLlctHn\nMi2CUUIMHMdBkiQsLy9jfHwcgiDg6quvRnd3NybnPHBNxT4gxAqPfSR7PtVDdwxrso9UD2Ct6mQe\nH6TPXPte75gyOX99723ye5lxH1m8j8nOyfQ6+QB75WzPy6Zv9hpaAq43Br3+2fPJQ1Z4yAofk80w\n8pi5iZcTSDoAfPzt+lrJ0YWYPpItxveH8caUJF0Ps7Oz8Pv9tCZCPqtMnzlzBv39/bj//vuzbk92\nTi7YlhF1FiRSTbZNx8fHMTc3h9bWVlx77bU5JRtkq31PBrYIh9lsRm1tLfbu3Ztzf4SkJ1P7yJKC\nqKbUsNlkgKS1b2EgCKyUxJAgQ1HkeHSedZMxmvgEtxNJksFxHERi+6goMJkTF0dU9sJci0SWSeVU\nTdfw+QRV5J6QdHptWaGJtBBiiaqqxYUS13tHIlKCFIcl7GQXQlzzCjeZebrYIX1KUkxKQYi21cqr\nIr5aSVIwKKleJxs9DwSk2IJqbUg2myEhek2i62R7UZTUCxciAQpHFEQFGZX2+H0XpcRouCTH3HDS\nFYcCYosS1uVGFJUESVEoHCP8qRxswhFF9zrBUKKsyOuXUWlP/91l9el6YO3+yKSeygUgGdIVxmD7\nTFcEowQ1otEo/vCHP6C2thY7duzIypUnlSY4mwQ+0lc2CZnrBUuC9IrppNOsa8/LBzazsFI+iHKu\nfWRzXjYFrzKJiqc7nj0n06g52866tJBzU5F2bZJpa2MFJEnC/FTy+TgQEAEYVb8TADCzYsbOmuRJ\nTyOL1eipX03aDgCRSARLS0sIBoMIBAJ4+eWXcfbsWbjdbip/zqVIWrpd0lTt/f39+PnPf64rgcwF\n2zKirvXeDQaDGBkZwR//+EdYLBZce+212L17d84ZwYT85wpC0P/whz9gfn4eBw8eRE9PT04fJoKP\nPBGJab6jEkQh9oiERapHTyTYMVlIKCwkSEDiiaNq1hQOx6Ly5KHuL/5/SZYRCgmIRESGuGqTWGLX\nEKKxKHYgIOiOU5JkSJKMYFCEEJUgROPXTRa5D4fT2FxKCsJhKYGcsuSfROqjQjzird1loNeMyggE\nRN22+Lh0SIHOiioSkSlpF0VFV04SCkl0TFo5TCgkIxTS30Fgdx9WPSKCocT7pCXKkagCX0D/fobC\n8YO9Pokm1hKQ18xG5YkfvhbhiKIbVQ+HE19Hsgi/FkRPT7C8kvid1fruFhoZFsEogYHFYsE111yD\n9vb2BGeszpZ6dLbUo6u1Dl2tdRlHZbOJvqc7L5eIbiZIF21OF+EvxEN77axezwbez3z3kS1ItDmT\nxVTCuTlE3PWi48ke6a4hg09+HvOaWhsr0NpYAUBd3VkPyaLqswsSvKH4/Gs1KxgcykzZQNDW1oZ9\n+/bhxIkTKCsrQ0dHB6qrqzEyMoL/+q//wtGjR/GqV70KTmdSZYku0u2SpmvPJ7Z1RD0SiSAYDOLF\nF19ER0dHzhF0LXIl1IqiYGlpCU6nE2VlZaoqecFgMOvKpCxkUab6cNaeUJQl1fNGU+KXgJBoSQKN\nDMuKAqORp04repVGWbLOa+1HGMQ087Hz2Ug16/FOjgMSfc/JIoBE7gOB2M4Bx3Ewmw0JkXtZVuvo\njSZeReYlSYbBEC+yRF4/ie6z3YmCDJ7nEF57b4xGLoEcR6OxYwh51kaTFUVZizLre9UT8kyqxEqy\nmqBrr0c2QMj1tPr82MIrdgxxt9HuMBAQsq5nhcme4/eL4DgO5eWpJ1E2+p2MUAeCCirtMS96LTkP\nBCWUlyVeI11UfWYu9kNQX5u+GI57Jq5xFEVRZeeVqfxNi3SFMQgyLIJRAgOO46jzVLoASWdLYk6B\nc3JJ50id6ySJQqaKymfaX6bQ9qmNlBYb9O5ZJrKTQo2hmPojkef17D7k8hnL5H3IJPIO6O/aEHLO\nIpO5c8y1iqv2JcpanJMKulo5mI0KrWj9x0s8/s8JoHfHimoMzzy/jOq6uF3YXe9W/67FfqPK8fa3\nvx3T09O46aabcOONN8Ln82UtJU63S5qsfXBwEKdOncrrPL8tiTrHcdQ1xWq1or29PeOksEKBEHSr\n1Yr9+/ejvLxc1b4e3fuHHgnEFg9MIqUWhOyJgnoxwGvdVhSFEn0hKqmSQw0aU3HqEMOryWtidb94\nWyQiQZEVcDyXKC9Z26IihJ2Qf0OKxRVZBJh0FiAkwi0IrOc7SX6VE14PidanWoiFQxJkRaFJtPpj\nkteuxav6IpF+dmFRVqb/FYwKclIfedWYBZkmfWrHFIugxyQ9WttMFrKk0IWI1WqgTjd6xwcCEqKC\nrCLTWt350rIQS3K16t+jqCBjflFKaCcLA62UhoBE7LWf8RRrxLTQahqrqqpy6iddYQwgqyIYJTDI\nxEc9Fbpa61RBEpvNhvKa5tTXTENgtMckOzZbop1KdrNZSCUHYrFRYyz0dTJZpGXUj4bkZiOJyfga\nKT5XmXzmslkA6hF0gkyI+sMfNOO2Ly9jh46OXQ/jq9Voq04te2Ehy7Lq99bj8dC5t6Ii+djzDRKw\nySe2JVEHgLq6OjQ0NGBsbCzvDi3ZYHl5GU6nE2azGXv37oXdrv8hzaQyqR4+9EhA9XcsmhojNPwa\nGWZJNKvFBkD14gRaj3V13zI9nhBeQuTpNTn11KZosjFV0X5Rpn+T62olMrHoePLIPZXQrC1AJElR\nLQC00etIJPZZIMfEdxPUcp+Y3t0AnudU90imkh0ZApDgE8+S8GhUhiBIsFjiXzPSP5lQyO4AABis\nRqqjFwVZZaNJYLUa114v6Yd9bRLKyoy60XOSP0Ci/0BMD8+On026tdpST7qhkASbzaAi6Vp5Tzis\nJuNkXKSQFlm46BH6UEhOWPABgNcrUBcfAAgEEo8hGHV5sXOX+vumlb8IgpAXl4B0hTGyLIJRgg5y\n2cnUEnS9IMnCwgJWVlZgsOnvgmQTUc+G0BZrlFyLzVwkbNa1C3XdpBVrN1Drz0Jv8aA3xpnxlzE/\nZYDdbqeP8vJyOndmsxv50vlx7D3WRv/+n/92YKixGqeub1Ad96vfB3DyeA3VqD/zfGoCrJUxrqys\npJ3PiWUui87OTpw6dSrtLqleO4mm5xvblqiTBLFcozDpwPM8ZFlOKqVZWVmBw+GAyWRCX19f2hUd\n6S8b/NODsTLYsR8w/S+6rm5ap9ymwhBEepiBg5xEdy0yxJjjORhN8YRKQkZJn4TMsxFsRSNXEQWZ\nkmBCoqnTDLvQkOLEXluESfua2cirlrCLooxgUADPcVTyol0khIICHb/ZZKDjY8Em2xJCrr3nkYhI\nSUayHQ8glgOQjoyEgmsLDc2CilzT641rAU1mA3hOP6k0Nq5EMkykTeEQ0cnHziWRf3YREApJiEZl\n1a4Am0xrMHAIh2MuP6zNZOLrlhIWPEBMbmO3p5+i2I/S7FwETY2Z1xxgC2Ss13c3VWGMbIpglKBG\noQg6AdnN7G1NJOqpZDNaMpcy0VRnfk5HBrcKkV8vNnMhUCzQI8fJfP+TteV6zVQ6ehJFb228GqIo\nwu/3w+/3U6cVSZJgtVppoDEQCKCsrCztd/al8+O4NKCVgHIwm9TP/c+AH78MKfCtTsNWnnpeZ4Mu\nAOD1etPO56mMA9Ltkuq1u1wuuFwuLC8vY3l5OSEBNVdsW6JOvHeNRiPC4eS+m7nCaDRCEISEQkSr\nq6twOBwwGAwZEXR2vNngnx70UbKrkAjNmqQEAHhDXF9OCC8hq1pNNyHQ8aJHa57mknpsnCaarfIR\n1xB8FmRc0aikiuaz/bH/F8XEyL36urEEUyqhYWQvLNlWGNuneMRem0CqUMmLHlkkiAoxyU5cx66o\nyL8kKQj4YySZHKO3OBDFGBG2WIy0nXrIayqvaiU/KucZQYbJrHauIa+N+shHY7aZurIgnag7kcew\neQjE1ScYFFFerq//ThUZJxF34h8vJrGHDIckGsWXFQWhtYRalqx7vdkvuGfc/oSoOout7Lt7pSKV\ni0M2BJ0gleywS0Peo9EoJud8usdmIpfJBtlIa/J53Wyht0Ow1cl3pnKfgo4hBYHONDKvPU7r2JIM\nejIXo9GI6upqleWhoigIh8OYnp6Gz+fD6OgoQqEQ1YqTyLvdbk9bVf3Xv55LiKpng3xXmU63S6rX\nTo558sknsbqauWwnHbY9UTeZTPD59CfW9YBM7oSoezweOBwOcByH3t5eVFZW5v2aBO//fOwDYDDw\nqsg0S3altUivzBB3Au0PncgkhWp12+w5CusMw5zPLhCAGEEkpDMuMVlbJCiMvSLTn5HXj96TyL12\nkaCS0KyRaCCuuddG90Uhrrc3mgwJ0fMY+Y9fh+f0r0dIvdGklhSx42GPSYZIRIQsKSmPiQok0dOg\nkr+QJF42mVdvQRONxopNkai/gdck1cqyajEQDsfICtkZEDTOMSsrYfr5sNqMCWSfyHi0WnlyP8i/\nRL4TH2esH5aspwJ7HLmmrUy9iBh1eXXPdQ4toLktPnmzu2L5KjldQn6hdfESBCHhRz8Xgk6QSX6Q\nIAgYGxvD/Pw82tvbsXPnTtVuqjbynmsCajoUIlFTS0r1/i7UtYsZmb7uYkI6Ep4rSU8GjuNiOR/l\n5bBYLGhtbQUQk8IEg0H4/X4sLS3R+gd/+2ozHvphIz1XG8yacGenKmChlb6sJ+eIINUuabJ28nwu\nNr/JcEUQ9UJo1Mnk7vV64XA4oCgKuru71/3BSIeb74ltn8cSONnosQKDjtsmIdFSmiRSAm2yqdEU\nc1tgo/Dky0V+QHmeU5F4tjgSG/EliwXSlczotQWm0JCWlGtJPdusldDIaxaKrD873XlYuxfhNUmL\nyaIveZFFmVYYNJoMqmskI/5ayIpCk2KB5Im4hOxqo/nsIiAqSBAiIswMwSXzG+Ew4VD8M26wGVXX\nZvshMK+NWUvWgbiOP1USbzgkQpJkWCzGhJ2ASERCNCqt6eu5BOIeDosJZD3eb8yuk114+P1iwi5Q\nKszORTI+ll20liLqxQ+yk0mI+noIOttnst8ItvZGW1tbUucwbeR9fn4eHo8HvFXtcpFJlHyjoWeZ\nmOrvKwmp/Mm3G7Ih6FpoNeoGgwEVFRUJioJoNAogMxembKEl6pIkqSLsWxnblqgTFEqjLssyLl++\nDIPBgO7u7rxF4lJt677nrkUq9dASVCBGIrXnk6i7Vl8tCxJ4jqPPJ4uka4k7S6L1NOKKrKii9yyE\nqMgkj64lo2p17cwiwGgyJJB6QtgIMdQmy7L+7Ol04bTwkhyPbGvvayQs0nuqPYYQ/8haJNrMyGIA\n0B0CWVmzeeQAk8VISTq7QGDziC1WY0I7AETDYsrFAUE4JCYQf1GQVe9dKCyoxswWvSLHkuRmi8WQ\nQPzJ2AipJ/1oj1NkBZGIRMk6aY9G49Vs2TyBeN/JyT1v4BAOSfB6wiivSL2dmg08Hg/6+vry1l8J\n+QUbeMkHQSfQI+qiKGJ8fByzs7M5Fccjmt2eDHTvWy259ErClfCerIegE2RKis1mM0RBhNGkTz2H\nX15E71W5OfRpifp66tIUG7YtUV+vpVcy+P1+OBwOeDwetLS0ZFUlLx3I5K5XgOU9d8XrqGvJMxAn\nruR1x/XPastBbbSa7TOZFSNLXtn/80Zet01a826XJBk8x9HovTYyTYgsx3PgDXzCNlg0HE/mTIhI\nr0Vx1YmaOjsKitqTXPvl1Uui1Z5PIAryWt5Dohc8sEY+iVxFR4oCAEJEpOdoZS/k+YAvShN0ASRI\nghRZgUAIsjWR+JN+yNtJSD3dXWEWADHiLCVIfVgE/FEosqKK6LNgC0HRRGBNLoDfG6Hj1YMQlVRk\nPRQSYLPFJv5gMPn3N+CLorzCjFBQoPKX2WkP7JXpPXO1yeDrTSYtoTDgOI5+b41GI5aWlnD58uV1\nE3S2fwJRFDE5OQm3242WlhacPHkyJ299o9GY4OJFfjtEUURPT49q99U5uYRkFSFLKB5kY9W5FZAP\nkg5k5/ryw4db8e6PzaQ9rq7ehqXFUNL2W143i0CgnCavCoJAa2LoFRPcytj2RJ1sla4Xfr8fTqcT\nkUgE3d3d8Hq9ed9WIZO7lqj//WcWAKzJeXQ06YqsUFs/qrHWIeSszpxEprWJpKQ/QRLpNdnjSLSY\nVDZlz2PJHiGDsqIAopw2ci+vJYgSYq/9opHkUSAuoWGj8BzHJWjtk1VYJTAa1Vp1PUkP20aeT1WB\nlCCZ5EetrZchihJMZkbSwkTrYxaNkkrrr17sKAgFhISIvxZslNvCEGU24g/Eib3JYlQtBll9voAY\noU52D9gouR6oV73O59PvjcDGJKyyZB1QR9W9nrDqPHtlckeAUecKOrpq4Bxa0Iw10c6rpFEvTiiK\nguXlZczNzSWtRbHe/sfGxjA1NbUugk7AVq8OBoNwOBwIh8Po7u5WFUkh0EpnAMAxmX8/5hIKg1yt\nOgsFNs9ALwE5XwSdINdicUtTs6hracrpmsFgEPPz8zR5VZIkVFVVYXl5GTzPJ7XCzhRnzpxBdXU1\nBgcHaf2LdO2Dg4NwuVwAgLe97W3ruj6LbUvUCdZbiTQQCMDpdCIUCqG7u5t6aQaDwbxr38nkzjrJ\n/P1nFqCsyRIUABxPCJsMTuF1I8Qcz0HSEKlkkXRKZkl1TJ3IdiwqzWjIdRJJFVmBhMRFhBaEcMci\n0/FILyvNkUWZLgK0CbOxRQnjq66p9knJJ0uUjYm7A4qsIMpUc9VLtmVJPLkv2rwA9rVq34u4Vl+i\nfegtDFhSrLeQYbX+Ce+3JuIvSTKNoGvVUbIkI+CLqIi/9hhJUiCtRbCTEf+gPxrfsWL056yUCFiz\ntTQbaPSfhRCR6LmyotAchVBAUF036I9CTlJRl4DjOPi9Efi9Eawu+mCzp46mz0yuArAnEPVSRL14\nsbKyArfbjebmZpjN5ryRdFmWMTk5iUAgAEVRcPLkSd0dzWxhMBgQjUZx8eJF+P1++tuRzXZ8d2uc\n0Hs8HkxOTsJatWvdYythY5Dr7ki6PIZMqpEm253ZnWGxoWyxnqrOLFbnPfjjvAdHX9mZ9tj2oy38\nGwAAIABJREFU9nbV9S9dugSz2YwXXngB3/3udzExMYHXvva1OHjwIN75znfi2LFjGY9jcHAQAKjl\notZmMVn75z//eTz11FO4//7782bNCGxjor5efVIwGITT6UQwGERXV1fCJGs0GhGJZJ60lgm0RY/+\n9l9mVe08z1HSDsTIOvQWIppgJ4mka2UxxMIx3p8CUVZv1xo0nuZrR6rHpZNJnswhhr1+XKrBkktF\nRaYlyJAlea2MeOJr1ZLABImMokBaO0ZPQkP7icb7IQm0QJzwEzJNI/4ahqvICrXJZO+rzOrQGXmR\n3sJAlmR6vMlsTJA4xfpkpEeaiD+5Jnue3s5KsiRfLbTae4U5j1aRXTuG9KG9L7GkUlklFyKEnpBz\nkybZlLSzhF0UJBhNBkTCIkKBCERBgr3SpjtuAPB7w2nlLyWivnVQV1eH2tpazM3NIZCqylWGkGUZ\nU1NTmJycRFNTEyoqKtDW1rbuwA4ARCIROJ1OeDwetLW1Yd++fev+PSK/Dd2ttZAkCaIoUunW5Pz6\n70cJxYN0BD/XaqQ7ay2QJInZMV7/Z50gW6L+3S/szEj+kikMBgN4nkdTUxPe97734eTJk3jyySfx\nhS98AS+++CJstuS/FXo4ffo0XvOa1wCIFUDq7+9XkW69dpfLhRMnTgCAbgR+Pdi2RF2LVEmaLEKh\nEJxOJ/x+P7q6ulBfX697XiaWXtmCJepakg4k+p8DAK9l5dDSaCYKr5WTCKKqXS9aKwqS6nm9SDk7\nLi0x1JJL9hiWXGqTRanUhpG3kGRZvbESsJF/PQLKSmiSJdoS8q+V+2jbAX1XGNbj3aAhsHouPLyR\nVxF6IK7PBwCDyZBwn2KLmPj7QRc6mog/OcbI9MGOlf1XKwWiYyE+80l2SdjrJuwqEJccnR0B8l6J\nokR3V1gEfRGYLEZaC4CQdYJwKAqrzUz7XVnQt2QEYvIXFqurq4hGo3mz88plm7SEzMFxHGRZXnfO\nkSzLcLvdGB8fR2NjI66++mqYTCasrKzo1sXIBtFoFKOjo1haWkJ7ezu8Xi8aGnL3hWbB8zwkSUI0\nGoUkSeB5HgaDAXNzc5gdH0dDQwPa2tpUn+dRtycv1y5ha8Mk+1TabVJYkXANkgPCcVzO5D1Zbl2u\n8HqjqKzMzihAW2W6uroajY2NlFBng9XVVZVEbWlpKW07OWZwcBD9/f15nee3LVHXRr+1ZvhahEIh\nuFwueL1edHV1pY2CFML2kYzzb+50J5WOaCFJ6gWILlGWkyefAlBJa1joEXw2Uq5H7GVJndCqR5Zl\nWVGRW44pykSgkoMYE8fBEu5ksghZklXEP6Eaqg6pV2nIddq1RJYdJ5szQP6VoF4YaK9BiD2BlvjL\nigJ5zS1Hb0eAPM8uHogUiUVkjfjzHAcD4yOv9n+P98FrFkqx/8f7I7sO7NjZ5GCT2ag6N17JVp1g\nrL02q9cHYosikyX+nG81SN9vUZAQRpysE6zOe1DdoCbc4WCsGBX53M/Pz2N5eRmiKOLSpUv45S9/\nCVmWMTIygp6enqx+dHLdJi0hc7A5R7nMu7IsY2ZmBmNjY2hoaKAEnUBbFyMbiKKIsbExauHY09MD\nnucxNjaWdV/Jxg7EKi1evHgRdrsdiqJgYWEBdXV1OHbsmG4xmY5diYvOEnm/ctBcb4PP54PfL8Ht\ndlN5F1uIyGazwWw2J5B3ILY4zDT6LopiXqQv60ExFK+rq6vD0aNH0d/fjzNnzuRNp75tiToLEoXR\nI+rhcBgulwsejwednZ3Yu3dvRpF3g8GQd9vHz36jHEA07XEstHaJ2sqbgH70OVUUnPYn61fMpO0J\nY1HLOYAY0WPlOrzOl5kcw/GJmntAI+VIQnbZ5/TkOnrEX3sN7fl6Y0hGmBOukYT4a3cMtBCiIr3f\nyV5rOuKvrLnckCh1QnEnQUqa3Kv1iU/VDjDRek1OhBAV6bEGnUWOLMq6bkBCVITJbFTtKAgREQaT\nAaFAXGpmNBnoGMKhKPwrARhS6NhZPHaHHUAvpqenoSgKent7EYlE8Jvf/Aaf+9znMDw8jNe85jW4\n7777Muovl23SElHPDdlG1BVFoQS9rq4OJ06c0CW1uSwAJEnC+Pg4ZmZmcrJwTAdZliGKIpUVvOIV\nr8Ds7CzGxsbA8zx1wPH7/aioqEBlZSXsdnvKEu4l8n5lgLzPdXV1NK8OiH2mgsEgfD4fPB4Ppqam\nEIlEYDabKXkvKytDWVlZVtF3ssuTDb77hZ14w7tmsDQ1i5rGatXzgIRHns3+dZMxZCJjfPLJJxOe\n6+zsxKlTp1BdXY3l5Vgy9+rqquoeAkja3tnZSdvPnTtXIuqZgnjvaif3SCQCl8uFlZUVdHZ24qqr\nrspKR6hnv5UrgsEg3neXemtezyddL8rOaqP1jiHtySL0sS1lJmKeRFqijXjrEXdJlNIeI0tsBDqe\nGEv/5Xn6N6fzxddKRLROOESuk4r4S6Ks0unHrq2o2lXX0ETKATVh1ru3KkKts3BgSSt7n9hjZEmG\nJOon1QJAVCfSrt11IP+yOQAs9KLoyXYE9O4DEF/ksAsHWWcc9HUpCniOo1FudjEgSTIiYSHhs+P3\nBmG2mun1VFF1f8wBRhIkSta9y7FqxESn7lv2oaJW7XQgiiKsVisaGhpwww034LHHHsMPf/jDhPuY\nDrlsk5aQHcgPcKZEXVEUzM7OYnR0FLW1tTh27FjKaHk2RJ0koE5NTaG5uXndDjF6/ROCznEcjEYj\ntXY0Go04fPgwTaZVFAWRSAQ+nw8+nw+zs7MIhUKqgjMVFRUoLy9POsYSed8+0HsvWRA3FK0jSjQa\nXYu++5NG38vKymAymXSj7zQwp7G8TYfnf3AUb3jXoG6b4/ICaurtGL00jvrmHRn3CcTmXFIlNRlS\nVQ698cYbMTAwAABwuVw4deoU7be6ulq3vbq6GmfOnKHHEb16PrBtiTrrvctO7pFIBKOjo1heXkZH\nRwf6+vpySvTJh/QlHA7j7z+1kLSd59Qf+IQqnIqsmnyVNUmJllARKYQe0UpG4hVFLU/RJj9qI/fJ\novYs4WEJGZHksAsDvWRZlrgaNDpmRZahIE5uDdRFRp/4k2i+iswK6vdQG/FnLS3JOBPeB2ZHIBnx\n1y4MVOfLCl2AUMmJrJ9Uyx6jbSf/NxjVxBdIn2BKXge7MNC+DmLJqWhcWKh/vo6jDfsaCYwmA6LM\nAkFPF08WS+S1GAw8hEgiQQt4AkkJiHfZB3ulFQtT+sSY3Sr1+XyqKnrbqVjGdkI6Qq0oCubn5+Fy\nuVBdXY2jR4/Cak3vqZ8JUZdlGdPT05iYmEBTUxOuueaavOpy9Qg6yZkSRRHd3d2orKxUncNxHKxW\nK6xWK3bsiJMZQRDg9/vh8/kwOTkJv98PRVFgt9tVBD6ZHDQZ4SsR+OJEOoKeDmazWTf6HggE4Pf7\nsbq6qht9N5lMmJmZgc1mo8Q9W+378z84ig8+FMTqvGctmp4ed71bU1tE83uzurqKAwcOZNSXHo4e\nPYqBgQH09/fTeQQArr/+epw/fz5pOyHrS0tLJY16tiAT3tDQEE302bNnz7p+jElyTy6IRqNwuVy4\n6+tlque1hFlWEgkMz/Gq5/Ui73qOJOy/elF37Tj0yGiqsWYSgWQJGcdxul7pqmto3h+9iL0qSq3T\nzsp3ZFlMaGehPSaXhYHMjEN7PqCWAiXbMdCTCqm14ur7GB/fGmHOYGFA+lEtwJIQf63HPsC8V2m+\nAtr3CIgnyrLXJgsmA1OxTogKEKJQ7RaQ1ytEBJgsprWxJDoOEGmNeyz5QjhfmsZct0lLyB7J5myi\n2XY6naiqqsKRI0cyIugEqYg6K5/ZsWNHgr49FTKJMMqyDEmS6O+J0WhENBrF8PAwgsEguru7s/5s\nmkwm1NTUqM4jxMvn82FhYQEulwuCIMBms6nIu9VqzVg6E41GMZ2iKE0JhcV6CXoq8DxPPxM7d8YJ\ndDQahdfrxdTUFFZWVmAymWAymeBwOFBeXk6j79lo3798exkANR967A47PvnNzMaq/Z7lQ6OuF3E/\nf/58ynbyXD491IFtTtQ5jkMkEsHKygr8fj96enpook8++s4WgiBgbGwMCwsLePTploT2TOQuWvKu\nR+a1SBeZTzeOZJKb9UDRKtw1b4lW0qNXuVTSSWxVSz+SJ77G2vO/MGCf0+4IaBcG7CJCrx3Qlwqp\n+lCU2HUEJenCgIw1VTsZD29IjJRnshiMjY9L6FNiKr4aTEbdBQe7MCGEnX1v2IRSlqyvzC3DbFXL\nGSSNpaVnPkaQ7TWxH7SYPn2tXyZvZT0Tey7bpCVkh1QEfXFxEU6nE3a7HYcPH87aig3QJ+qKomBu\nbg4ulwu1tbU4fvy4rr49VZ+ptLuyLNMoOhCvozEyMoLV1VV0dnYmdR3LBSzxIlAUBeFwGD6fD16v\nF9PT0wiHwzCZTAnSGa0meWJigibQNjU1qcZZirwXHoUk6alArKvr6upw4MABGAyGjKLvJHk1F+eZ\nyZdH0Xu8DwBweXAUeHebql1rFrLdrHa3NVH3+Xx48cUXUV1djZqaGjQ3N2/KOERRxPj4OGZnZ/Ho\nmRYAzUhMxYwhXbQ7EyQj99roaCqoIvdSnOyT57Tkf71jTBf110b0tdAS/3TnJ5P70PYsiH/cF17d\nrtX+s+0GRuJCx8hIhfSSTckiQFGURBkRkhN/mUbaY1VOJeJLb2RJ+dq5axOnLOgnEKcEw5E5RnJE\nm4V4omzCqZKsek3a4lBCRKTJp+FAGBzHwcSQJr2oOiHpQiQK/4oH/hUPgKtoOxtRX8/Enus2aQm5\ngVQhXFlZgdPpRFlZGQ4ePEgt6HIBWxeDkH+Hw4GqqqqM5TNaELtdbfRdlmM7XkSOyfOxIMTo6CgW\nFhbQ1taG3t7eDZFfcRwHm80Gm82mspIkmmWfz4exsTEEAgFwHAe73Q5ZluHxeNDS0oKrr75al1iV\npDOFw2YR9Gg0ipGREUQikYTKwKmi7+RzND09TWsgEOKeTvsOqL87s+NzumNjrRmBzXN9KRS2NVGv\nqKjANddcg9XV1YIlcKXyZ5ckCZOTk5ienl4j6IlR9IT+MkwizbYPtk1P2pJt5F77N0vck0l2VO1S\n8nYyTr3xJXttetaIeu3J+tciIeKfpl17T9MtHNhkUyBxxyCTdlX/XKLERCs1Yp9TFCUt8acLgAxq\nEGjJN7uzoF0waBcSbNIrGQexm+SNPHgjj0gwojJEihdcitCoeigQhK08Tta0uwg/+doe1RapVvpS\nXR13HsgWuWyTlpA5tAvjc+fOoaysLIEw5AoSUV9aWoLD4UBZWRkOHTq0LvKvLWAHxH4TCEEn8r+p\nqSm43e6UxHejodUsE+2/0+mE1WpFTU0N5ufnMTs7i7KyMlX0PVXSbilxNXdsFkEnn9GpqSl0dnai\noaEh40VkJtr3yclJRKNR3ej7ywOOhD6/90AzzePQm8uB9c/nxYZtTdR5ngfP8+sukpEMZCLWJhSx\nSUc7d+7El55KjKBnEy3JR1Sd7Ucvsp7tNbS6ZUnDvDPR26dq10bwUxH7bIh/uh0B0k6is+kWBuSY\n9S4MJMbuMJP2hOvnsKOQamGQLAdBkRO98bW7BdrdCElSdHdK2Odjf0sQBa27jjpSLq59j41MlJKQ\ndbPVglAgiMCqD3pgk51iY5boNux2i8BsR3g8HjgcDkSjUezbt0+VPLlehEIhuN1uhMPhvJN/AAnV\nRAFgbm6OJqZeffXVm+5DnQzkvlut1oTdBUVRqN3fysoKJiYmEIlEYLFYVOS9ZBm5PmwWSfd4PBga\nGkJNTU3ePqPJou+RSIQmQJPo+8ffA3z+m9WwVapdarTR93A4TCU4pK9cJHDFim1N1AkKRdTJREyI\nujbpKEbQ9ZGN/RuQvSY+Wx06kF6LrpdUmGn/6c7NlvhTbVsyB5McFgbsc9pImLadl1NLgTJdGOgh\nkx0R1rdct1psmvZM3j9yjNbvXFv9NIHUa+xCtc4xQOL9ZfvRniNJUkKykCgIKrIeCcYsGoNeP8RI\nFEZLopbY4XDAbrfDZDLB7XajurqaaoSfffZZHD9+POU9KWFzIcsy9uzZg8nJyYyTOdPB6/ViZGQE\nkiShoqIChw4dyku/QLzWhraa6OLiIkZHR1FXV4fjx4/n7bXkG8FgEA6HA5Ikobe3V6VtJ+A4jkZA\nm5qaAMR+20jCod/vx9zcHLWMZF1n7HZ7yTIyDXbVWbPKi8gXotEoHA4HQqEQ9u3bl5eFazpYLBZY\nLJaE6Du+OaI6bnBwEBaLhX7uQqEQ5ufn0dvbC0mS4HA4MDU1lXE1+q2AbU3UyQ+70WgsKFFnLcFq\nampw/PhxvOsjU8jnDqYesdfTUbNuKqk+pHof4lzIfSqwhCsTcp7rtdeb2Aog6x2BbJN6ZSW17zoz\nEBW0BF9WZLpIoKfoLCpYZCIjkhWZnqclytpiRslkRmwf5G/29ehJAbT9KLIC3shDZGwzZUkCvxYt\nMZpMNKouCgLC/iCs9jJEgiHIsqKbcAsAz3xjL3w+H8bHx7GysgKLxYJ//dd/xblz57C0tIR9+/bh\n9a9/fcIWagnFAY7jqBd9PgIvfr+fEvSenh5YLBZcunQpH0MFEHehmJiYQH19PSoqKqjbV2VlJY4c\nOZJTFdSNABmn1+tFd3e3qgZAJuA4DhaLBTt27FDteoiiSPXKU1NTKstIu92OysrKrCwjye9uUMo+\nf6DYYeWC8Pv9uDTro0mZ2h2KQkikFEXB9PQ0Jicn0dHRgcbGxk0lu3qv8dprr0UkEqELXjK+N7zh\nDaisrMT09DT+8R//EQsLC6q8i62MK+IXKR+e53og0ZGLFy+ioqICR44cwd/c4QYwBSC+PcMilQNA\ntl88PfLOPpdJ1F57jN6XUi/JUq99I4i/9vxUco31QBsFzmff2v6B5FFvsoBg29nkYL1z0zoBJbFU\nJEQ7GfHXkyFp21IlLrPkmzrE6ET+iV866YM3GOJJrszYoqFY8l9g1QeD0QBJEGBY+5EnUXVJlPDT\n7x/F6uoqhoaGsGPHDuzfvx9+vx8AUFtbi5tvvhkrKyt44oknEAgE8LrXvU7/BpVQFFgPUSdR4nA4\nrCKhRJqyXrBe6K2trfB6vTQxVZZluh2/uLhIo8rFoEkH4pVW5+fn82JhrIXRaMybZaTH48HIyAjK\ny8vR1dWVEHXeytH32IJEvShhZSGLi4sIBALgeR7l5eWqHYr17M54PB4MDw+jqqoKJ06cKJqAxX98\nbQ9e985Yvs9Tj3ZClmVMTU1haWkJBw4cQFVVFc6ePYuysjK88pWvxP79+3Hp0iX88z//My1gt9VR\nHO9EgVGIFeHKygoWFxcRCoVw8OBB/O2dswDcac9LRd712th2bT/rneDTEf1Mz0nVrhf112vPBIWM\n+Kdr10vCTfV3KmIvrxUVymbs2e4opNqtSDY2sjBIR/xVshSdnYhUuyj0PEEnsi4pCd8Bej85HpIQ\nc34RIgJIgSlIsYRRSYxF3SWGwImRKP7f7x3Gyy+/jGAwSLXH//mf/4l7770XH/7wh/H4449vm+3R\n7Q6yW5hNFVECUjjI7/eju7sbdXV1qvd9PXUxAP1iRcFgENPT0wBizkAVFRWqqPLExAR1wSCSkMrK\nStjt9g0lSbIsw+12Y3JyEs3NzRua0JrOMtLn88HtdiMUCsFkMsFms9F71tfXl1AAimArSmdS6dD1\nZCGSJMHv91N5kcPhgCiKWfniAzHHFIfDgWAwiKuuuiqhamkx4Gc/PAYAWF5exvDwMJqamnD8+HF4\nPB588IMfhNvtxo9//GN0dnZu8kgLg21N1AvxA0wSa3ieR2NjIz71hAXA7Lr6TEbQ07WnOo+daMlx\nhZ58C0H8tciF+CfbEciWHGfzd6F3E9JhPVKhdFaZ2uNU0X4xTTQ/BcjnlO037qu+RqLW+Jme/eP/\n9524vph4YA8MDKC9vR19fX2Ynp7GP/zDP6Cqqgr9/f15TUYsofAgRN1kMlGylg6RSAQulwurq6vo\n6urCvn37dOeIXH8r9Ah6JBLB0NAQIpEIurq6VO4TelFlQrh8Ph9mZmbg8/kgyzLKysqoHKSioiLv\nWmViQ8n6xBeDXl7PMlIURTidTiwuLqK6uhqKouCll16ilpEsMU22yClWy8hcE0UNBgOqqqpQVRU/\nP9kix2g0qiLvZCfH7XZjYmKCzpHFGrQg1pDRaBSHDh2C1WrFmTNn8PDDD+POO+/EO97xjqIdez6w\nrYm6FutJLiCaRlmW0d3djaqqKrztQ65YvwWQRqwXeiQ+U2Kvd/xmEH89bHbEP9357N/5nDjyocPP\n17WTJYhm1JfOfdZ+LjmegyxJUDQSG1mRISsy/uNre1JeIxQK4fLlyzCbzTh27BgMBgO++tWv4nvf\n+x4eeOCBUsGhLQqWqKeTvkSjUYyOjmJpaQkdHR15JyF61UQFQYDT6YTX60VnZ2dC1D4Z9AiXLMvU\nTYVocVlJCCHwFoslp9dFpCM2m40Sn2KEoiiUTLa0tODaa69NKLxEFjmzs7M0+dVms9HdiXT3aTOj\n7/l2c0nmiy8IAr1PU1NT8Hq9CIVCsFgsaGpqgtlsRjQaLbq8CWLQMT4+Tq0hx8fHcfvtt6OlpQW/\n/OUvrwi3rm1N1NkvJpncs41KsJrGnp4e1NTU4P/6f5wAFlXHpZNQFDtyieqnO4fn+YzkPOmIfyEX\nBvkm/tnmCGiRbuGQybnJrp+v/IMEq8c01wXSf1YyIfv//kRvynZZljExMYHZ2Vns2bMHNTU1+POf\n/4zbb78d1113Hc6ePbutLLuuVKQi6qIoYmxsDHNzc2hvb8+6EnW6BbZeNVFJkuByubC4uIiOjo68\naLt5nqeRT2JhR6KlXq8XHo8HU1NTCIfDWVkhsk4ufX19RSlzICCe9qmi/cmiymSRQ3y6i80ycqPt\nFk0mE2pqamC32xEMBmEymbBv3z7wPA+fz4elpSWMjY1Rsk4i74VMXE2HQCCAy5cvo7y8nDpyffGL\nX8S///u/45FHHsGrX/3qDR/TZmFbE3UWJOKRKVEPh8NwOp3w+XxU0/i2D7oALGd97XQkROuisRWI\nfSZIRc5yIf7ZSn2ygXYiSrabUGjkQtAzPXc97Zm26eVO6L0feknAik4S7JmvdCUfMAM2WfTqq69G\nKBTCJz7xCfzpT3/CN77xDezbty+jfkooXhBSpUfURVHExMQEZmZm0NramhB5zQTJ6mIA+tVEAdCK\n062trQXXdrPR0sbGRvp8JBKBz+eD1+vF3NwcgsEglTqwTipjY2M5O7lsJAKBAIaHh2EwGHDw4MGs\nF9d6lpFA/D75fL6YW0wwqNLIb4Rl5GYWLSKR6ba2NtViUrsYZKuJLiws0PtEiDv5t1B5FLIsY3R0\nFIuLi+jr60NVVRXOnTuHO+64A29605tw9uzZTbGs3ExcEUSdbJdmkoAUjUbhdDqxurqKzs5O7N27\nd42gF24rLJ32ORMkcw3ZLqQ/HXIh55menyvxT7ew0C4GtDsHeouFfCQRFwpkvKlet95nW9ZJ4nvq\n0cySgthEKJIs+tOf/hSf/exnccstt+Dhhx8u2vtVQnbQI+qyLGNychJTU1Nobm7GyZMncy7Koq2L\nQUAcYdhou9vtxtTUFHbt2rXpxYpIomF9fT19ThAE+Hw+eDwejI+PIxgMwmKxoLa2ljqGVFRUFFWR\nJWIL6fP50NPTk/fKknr3SRRFVZEdv98PWZZVbip6+QGhUAgjIyNQFAU9PT0JFWy1BH6zCDoA+Hw+\nDA0NwW63p81DINaa2vuUSmLE5gikS1xNh5WVFQwNDaGpqQknTpyA3+/HbbfdBofDge9+97vo7U29\nq7pdsa2JOsdxKaMwLARBwNjYGBYWFmhiRYyg+zdotOtDMnKfL0nOdov2FxrZ7g6wf6fbbUjmHJRO\nZpTrYibdoiNTZLoA/d4DyQuF0b6Y2gVtbW3o6+vD7OwsPvCBD8BkMuFnP/uZKppWwvaBwWCAKIqY\nnJyklT2vueaadUf4tG4y2mqiPM9jfn4e4+Pj2LFjR1FZ2GlhMBgQDAYxOzuL5uZmtLS0QFEU+P1+\neL1euN1uFSllk1Y3OqGULLbcbndBbCFTwWg0orq6WrUoYC0j2fwAq9UKu92OUCiEQCCAnp4eFZll\nsZnEnIAk4Hq9XuzZsyepQ04mSCYxCoVCdDdnenoa4XAYJpMpobBVumCJXrLos88+i/vuuw+33XYb\nnnjiiW2dLJoOXJZb7ZuX0ZYjwuFYxcLR0VHYbDZVyVog9mEm25dtbW3YtWsXbvjQ6GYMdcsjm+qj\nJVw5SOsysyZ5efTjNhq1YTWlWq0kmyza09MDo9GIb37zm/jmN7+Je++9F2984xs34mVtJvLxRdpy\nc7ksywiHw5ibm8PFixfR0dGB9vb2vBHLS5cuobm5GZWVldTJhed5cByH5eVluFwuVFdXo6Ojo2i3\n3hVFoX7k9fX1aGtrS3l/WFJKCJcoiigrK6NEq7KysiBJhmSxPTo6isbGRuzevbuoIvwsZFnG9PQ0\nxsbGUF5eDo7jKCllI+/l5eWbvoOnKApmZ2cxNjaG3bt3Y9euXRtKcsluDpnLtYWt2F0KvWTR6elp\n3H777aitrcVDDz2UdDG0TZDRG7PtiXokEoGiKLSk7O7duwHEoiWTk5OYnp5GS0sLWltbwfP8WqJo\nCVsdmXie66G0c5A7spVsEYKeTObCakr9fj8CgQAEQYAkSTAYDAgEAqiqqsJdd92FkydP4q677ipI\nqeszZ86guroag4ODuOOOO7JuLwCuWKJ+/vx5mM1mLC4u4lWvelVe+798+TKqqqpQXV1NI+gejwdO\npxM2mw2dnZ1FnYzMOrl0dXXl7ORCIqVer5d+/8jCmY2822y2nAmg1+vF8PAwysrK0NXVVXRuIyx8\nPh+Gh4dhs9nQ3d2tWqQRUsrOU9lYRuYbfr8fQ0NDKCsrQ3d3d1HYbQLqBSEh8OFwmO4DI1UzAAAg\nAElEQVRUzM/Po7m5GWfPnsXp06fx4IMP4rrrrsv7OLbqXF6c+3Z5hNZ7l6yMJyYmsHPnTrplWiLo\n2wvZeqCnO7/QyKVA0UYj3/cknQ6d1Up6PB4MDQ1h586dqK+vx8DAAB599FFKrhwOB5599lm8853v\nzOsYBwcHAQCnTp2Cy+XC4OAgjh49mnF7CfnFvn37YDAY8Pvf/z5v+RrExaWiogKjo6NQFAUWi4UW\n2VmvbKDQCAQCtPJpPpxcOI5DWVkZysrKqHxMURRV0urMzAy9P2zkPZ1DSDgchsPhQDQaxZ49e1SF\njooNJF8tEAigt7dX9zNgMplQW1urSs6VJAmBQIAm95JCROwuxXqsNfUgiiKtF7Bnzx6VRKUYwCbt\nyrKMsbExzM/Po6enBxzH4dlnn8UvfvELLCwsoK2tDadPn0ZfXx927dqVtzFs5bn8iiHqBoMBKysr\nmJubo84QJpOpRNBLKAqsp0DRVsSPv9Se0XHEmzoQCGDfvn0oLy/HCy+8gM985jN4z3veg1tuuQUc\nx2FsbIzK3PKJ06dP4zWveQ0AoLOzE/39/arJO117CfmDXs7ReiKx2mJFDQ0NqKiogMPhQCQSQUND\nAwRBwOXLl1UJhqyTymaCTb7s6uoqqJMLx3GwWq2wWq2qQmGsQ8jo6KgqUZXVKCuKgvHxcSwsLKCr\nqwv19fVFqzkmJeqnp6dz8uA3GAyorKxUEftklpFmszlBOpNVtW4mV6e1tZUS32IFmyx69dVXIxgM\n4p577sGFCxfwox/9CHv37kUgEMDFixfzvtjYynP5tifqpEKhw+EAx3G4+uqr6dZViaSXUMLGIlOC\nrk0W3bNnDxYWFvChD30I4XAYzz33HJqb40mnhSodvbq6qiJAS0tLWbWXkH+wRY9yIep61URJMlsg\nENAlvaQAkdfrxcLCApxOJyRJolFSQt43QrsuSRLGx8cxPz+fN9/2XGE2m1FXV6cqbc86qUxNTWF5\neRnRaBR2ux1NTU0wGo1JbTA3G8vLyxgZGUFdXV1e3XwysYzUWiGSz1Qyy8hAIIChoSFYrVYcO3as\naPMmgFjAZXh4mCaL2mw2PP/887j77rvxT//0T3jkkUfoTkx5eTmuueaavI9hK8/lxfdNyTNkWcbq\n6ir279+PsbEx1Yf56a90lch6CSVsADIl6EBiZVGTyYTvfOc7+OpXv4rPfvaz+Ou//uvCDbSEokU2\nLl560KsmKooiHA4HVlZW0NHRgauuukqX9LIFiAhIlNTr9WJpaYm6g7DkvbKyMm8ESpZluN1uTE5O\norm5ueC+7bmCOKnIsoyZmRk0Njaira2NklISOGMXOuR+bRbZDIVCGB4eBoCcvNtzRS6WkeXl5Vha\nWoLX60Vvb2/ebSzzCTaxlSSLzs7O4n3vex+sVit+9rOfqWoClKCPbU/UicZQkiTdif1pTUGVEnEv\noYT8IlOSzlYW7e3tRW1tLYaGhnDrrbfi0KFD+O1vf7vhmtbq6mosL8eKnK2urqoih5m0l1AYaK0U\nU0GvmijRyc7NzaGtrS0nyQAbJWULxhDLupWVFYyPjyMajcJqtVLinq0+WevkUsy2kEAs0jsyMgKO\n47B//37qMU6SUQnYhc7y8rLqXrG7FOv15k4FSZIwOjqKpaUl9PT0FEUhqFSWkW63G2NjYzCZTOB5\nHqOjoyrpzHoSfPMNUlm0rKwMx48fh8FgwNe//nX827/9G+677z68/vWv39DxbOW5vHi/7XlGphP7\nU492qKy5eJ7HO26d2IARllDC9kI2UXSSLEq2nKPRKO655x688MIL+MpXvkJLSG80brzxRgwMDAAA\nXC4XTp06BSA2kVdXVydtL6GwyCSirldNlOM4qj9ubm7GNddck9eoNJuISSKFiqIgHA7TAkRsSXuW\nvOsR0tXVVTgcDpSVleHw4cM5O7lsBARBgMvlgsfjQU9PD2pqalIen2yhQ+4ViSiHw2GVlpskra6H\nkBJJ7OjoKJqbm3HixImi3J0gIEm4ZrMZr3jFK6i1IXuvSIIvqUqbjY95PkEWwQsLC9izZw+qq6tx\n8eJF3HbbbXjlK1+Js2fPJhSI2ghs5bl82xN18mUmSaXJwOoWeZ6H0WhEKBSC0+nEnf+3iO7ublU0\n4O3/PFbooZdQwpZFpiSdSA/YZNFf//rX+MQnPoGbbroJv/3tbzc1enj06FEMDAygv78f1dXVNLno\n+uuvx/nz55O2l1AYsC5eqYi6tpoox3GYm5vD+Pg4GhsbNzQqzXEcbDYbbDYbGhoaACS6qLCElEhA\nFhcXwfN8XpxcCgk2+bK9vR29vb05k2i9ewXEk1ZJjkAwGITBYMiJkBJryPLy8qLXdrMR/97eXtXi\nJ9m9Yi0jJyYmEAgEAIBaRpJ/C5EMvbKyguHhYfodC4fD+PSnP43/+Z//weOPP46DBw/m/ZqZYivP\n5dveR12WZUQiEfA8j9/97nd45StfmdDOJhYZDAaaTZ8ssSgZSuS9hCsduSaL7ty5E8vLy/jkJz+J\npaUlPPbYY2hrayvsYLcurkgfdSBG2CRJwsLCAnw+H7q7u1Xt2mqiHMdR/XhtbW1eCyQVAj6fjya1\nWq1WiKIIk8lEo+75iCbnC6wkp6GhAW1tbRtasEgURVWhJi0hrayshN1upwuyaDQKh8OBUCiE3t7e\noraGBECTlnft2oWWlpZ1RcWJZSTr+Z5Py0hBEDAyMoJIJIK+vj7YbDb09/fjM5/5DN773vfiAx/4\nQNEWs9pklAoeAWqi/vvf/55udWoTi0hZ6tHRUayurqKjowM7duxY94RYIu8lXAn49Pv8qgk/FRnS\nVhY1mUz40Y9+hEcffRSf/OQn8ba3va0oiEgR44ol6oIgQBRFrK6uYm5uDldddRWAxB1RnuexsrIC\nl8sFu92Ojo6OopaNkArZCwsL6OjoQENDA/0OsNFkn8+niiYT6Uw6//J8w+v1YmRkBFarFd3d3UVT\nsEiSJJqISR6yHCusFolE0NzcjNbW1qIZrx7I/GgymdDT01OwsbL5FOShlRmxFaGT9UGSRTs6OtDY\n2Ij5+Xl8/OMfhyAI+PKXv5xXL/RtiBJRB+KaN57nce7cORw4cEClVyeJRWSSbGtrQ1NTU0GJQom8\nl7Cd8KNHdtMCH2y0xmazqSZ8s9mMqakpzMzM0GRRp9OJ2267Dd3d3bjvvvuKrlBHkeKKJ+qBQACj\no6PYv39/AkH3+XxUz9vV1bUpethMoXVyyTRyysobSDSZ+JcT8l6IcvbhcBhOpxPhcHhLRKWXlpYw\nMjKCqqoqVFZWUhIvCAKdn3JJ8C0EiO3mwsLCpia2RiIR+P1+1cKQuB6x83kkEsHly5dpxVaDwYDv\nfOc7+NrXvoa7774bb37zmzdl/FsMJaIOxIk6AFy4cAGNjY2oqamB0WiEoiiYmpqC2+1GS0sLmpub\nNy2hpETeS9hqSCVz0UZrlpeX4fP5YDabMTIyglAohImJCfz+97/Hl7/8ZVx77bUFGWO6ktBPPvkk\nAMDpdOILX/hCQcZQAFyxRF0URQiCgHA4jBdffBF79+6FxWIBz/MIBoNwOp2QZRnd3d1FTSK1Ti7t\n7e3r1sxrpSB+vz9jT+50kCSJJggWe8EiAAgGgxgeHgbP8+jp6UmwWyS8gA0uhMNhWCyWhGjyRrzO\nxcVFOBwONDU1Yffu3UWX2CpJkuqztbS0BEEQYLfb8dxzz6G1tRWnT5/GyZMncffddxcsp2Ibzucl\nok4wPDyMuro6mhkdCASoGwDRLVZUVBTdxFMi7yUUI7Jxc2GTRfv6+mAymfC9730PP/zhDxEMBsFx\nHMxmMz73uc/hta99bV7HOTg4CJfLhbe97W148skncfz4cVWCUH9/Pzo7O9HZ2YkbbrgB73//+4sq\n0z8Frlii7vV6MT8/j7q6OoyNjcHr9SIajUKWZXAcRwMuxaxDZ51cOjs7CyrJ0RIsv98PACryXlFR\nkZS8K4qCmZkZjI+PZxXx3yyIooixsbGc7RbZBF8STSYuKuR+5XOngvi3cxyH3t7eopZnAfFk0YaG\nBrS2tmJhYQGf/vSncfHiRVitVoTDYXR2duInP/lJ3vnUNp3PM7pJ2971BYitsn7+85/DarWitrYW\nw8PDuOuuu3DttdciGo1ibGwMgUAARqORbhsWQ9LOjx7ZTbPpW1tbsWvXrpJVZAmbimySRUkyFKks\n6vF48PGPfxxTU1P47ne/S6uJBoPBjD2xs0G6ktAulwsulws333wzOjs74XK58j6GEvKLyclJfPCD\nH8TCwgKampogCALMZjPuvfdeVFdXw+fz4cKFC7TwEDufbzZ5DwQCcDgcUBRlw5xcDAaDric3kTa4\n3W5aUEdL3klia1VVFY4fP77p9y8VWK10S0tLznaLegWIWJnR+Pi4aqeCTVrNZqeCyG3n5ubQ09NT\nVJ7deiDJouFwGAcOHEBZWRl+9atf4V/+5V/w7ne/G9/+9repSmF+fr4gvOlKns+vCKL+4IMPwul0\n4qabboLZbMbf/M3f4LnnnsNDDz0Em82GQ4cO4fDhwzhy5AhqamoQCASwsLCwaeSdeLyOjY2hoaFB\nVcpYjyiVIu8lFBrZVhYdGhqC0WiklUWffvppPPjgg/jYxz6Gd73rXarvUKE0xOlKQt988830/4OD\ng7jxxhsLMo4S8od9+/bhF7/4Bb70pS/hySefxHXXXQeTyYSPfvSjWFxcRHt7O44ePYrDhw+jubkZ\nRqMRi4uLcLlcEEUR5eXlKgeVjSCfkUgELpcLfr8f3d3daf3FCw2e5+nvGQEpqOP1ejE1NYWlpSUo\nioLq6mpYLBb4fL6CWfqtF8Ru0W63F2RBYTKZUFtbq5pLSNIqsdZkFzvpkuqJbr6xsbFoq8sS6CWL\nLi0t4dZbb4XH48F//Md/YPfu3fR4juMKVmn0Sp7PrwiiDgBNTU34/ve/j66ueCVSRVGwurqKwcFB\nDAwM4KGHHsLQ0BDKy8tx6NAhHDlyBIcPH6bkfX5+HsFgECaTKSHjPl/kfXl5GQ6HA5WVlTh69GhG\nHq8l8l5CIZFNZdHJyUlVsujY2Bhuv/12NDc345e//GVRVP7TYnBwEEePHi0q39wSUuN1r3sdbrnl\nFhURkmUZDocD586dw29+8xt88YtfxPLyMjo7O3H48GEcPXoUra2t4Hlel7wTAp8voqd1cunr6ys6\neSUBz/OwWq1wu90IhUI4ePAgqquraeVQsjsmSRK19CP3a7N8yFm7xT179mxoToLBYEBVVZUq+Z0s\ndnw+H+bn5+F0OlUWiFarFXNzc+B5vuiLVwGxnc7Lly/DarXi+PHjMBqN+MEPfoCvfOUr+PSnP423\nvvWtRfl53o7z+RVD1MvLy1UkHYit/mpqanD99dfj+uuvBxAn7+fPn8fAwAAeeOABulon5P3gwYO6\n5H09Xrd+vx8jIyMwGAyqssu5Ihvyriiy6m+O43WfZ9uS9ZGsvYStAfZ9zLWy6IkTJ6AoCr74xS/i\n6aefxsMPP4y/+Iu/KNCIkyPTktD9/f1bJfGohDX09fUlPMfzPHp7e9Hb24ubbroJQCzyOTIygoGB\nAfz3f/83HnroIaysrKCrq4sGYgh5J2R0veRdlmVMT09jamoKLS0tRR81ZQsWtbW1qQoW2e12lURH\nURRK3peWljA2NoZoNAqbzab6/Suk/SEJCLjdbnR2dqqsLDcTxHWnoqKCWhIqikIdiiYnJ2GxWKAo\nCl566SWVzMhmsxXFawDispz5+XlaWdThcODWW29FX18ffv3rX2+KO9eVPJ9fEcmk64WiKFhZWcH5\n8+dx7tw5DA4OYnh4GJWVlTh8+DAOHz6MQ4cOYceOHXT7kCXv5KH3ZSR2V6FQCD09PRv+BXj7P4/p\nEvKNRKqFAduud4zewoA9Rtueyfnssdtp4ZHp+/zUo50Z96lNFi0vL8fAwAA++tGP4o1vfCPuvPPO\nTfMsJjtlN998M+6//36cOnUKR48epSWjgVj+Ctky7e/v3wrJR8AVnEyaD0iShOHhYZw7dw7nz5/H\n4OAgPB4Puru7qQSSLASINplEklOR90I4uRQSiqJgcXERTqcTO3bsQHt7e06OMKzDE0nCjEQisFqt\nCeR9vWSUuKNsRoGlXLC8vEyTL9vb28HzfEJVWp/Ph1AopNqpT+dfXiisrq5iaGiI3l9RFPHII4/g\n+eefx6OPPoqTJ09u6HhYbNP5vOT6UkgoioKlpSUaeT9//jwcDgeqqqroZH/w4EHU19frkvfy8nJ4\nPB54vd6is7u64UPbJwljo5FqUaFt1zsmm0VLquNyRTYknWzv7t69G7t27YLP58PnPvc5DA0N4fHH\nH8eePXvyOrZc8OSTT9LEIjKBHzt2DOfPn0d/fz9uuOEG1NbWYnl5GU899dRWmNiBElHPOyRJwuXL\nlzEwMICBgQH86U9/gtfrRU9PD4289/X1QVEUFXkvLy9HRUUFeJ7H7Ows7HY7urq6irqgDhBbgAwP\nD8NisaC7uzvvMgxif8iSUWJ/yJJ3q9Wa0e8ea7e4FdxRwuEwRkZGIEkS9uzZk2APqQdS2Io8WG98\n8sjVXjMd2GTRvr4+lJWV4ezZs7jzzjtxww034LbbbiuK/IRtOJ+XiPpGg0Qo2Mi70+lEdXW1arL/\nzW9+g56eHprMYzabVZNXMW2DkWSSD30+tNlDKaGAyIagh8NhXL58GUajEb29vTCbzXj22Wfx+c9/\nHh/+8Ifxd3/3d0Xz+d2mKBH1DYAoignk3e/3U/J+5MgRGAwGvPTSS9i3bx+NnmsTVospqh6JRKiu\nu7e3V5VQuhHQkvdQKJTy949UC19ZWUFPT8+mJ+KmA5un09XVhR07dqyrP1EUVcWH/H4/FEVJcOjJ\n9TOmlyy6urqKT33qU5iZmcHjjz+Ojo6Odb2GElKiRNSLAWQ79I9//CO+/e1vo7+/n66wDx06hKNH\nj+LAgQOoqamhX0h28iJfxs0g76urqxgZGUFFRQU6OzsTkoZKkfftgS/dac2oDLlesujU1BQ+8pGP\noLq6Gg899NC6f5hKyAglor5JEEURL7/8Mvr7+/GNb3wDS0tL6Orqwq5du6jbTG9vL2RZpoRUluUE\nzftGk3dS9XJ+fh6dnZ3YsWNH0Symo9EoJaLszjPP8/D7/WhubkZHR0dR6/yBuMc4kT0VSpZD7DXZ\n6Dub5EsWPOmSfNlk0Z6eHhiNRpw5cwYPP/ww7rzzTrzjHe8oms/INkaJqBcThoaG8Nhjj+FTn/oU\n6uvrMT8/TyM158+fx9jYGOrq6mik5sCBA6iurk5K3rPZNswWwWAQDocDkiSht7cX5eXlGZ9bIu9b\nBz94qFVVmY9stepVMvR6vbh8+TLq6urQ3t4OAPja176G73//+3jggQcKssWYrgodwf3335+yfRui\nRNQ3Gffccw+uuuoqvOUtb4EoinjppZfofH7hwgUEg0Hs2bOHyiB7e3tp8aGNJO9sxHTXrl00cbaY\n4fF4cPnyZVolNBAIIBgMwmAwFKzw0HoQiUQwMjICQRCwZ8+egtnNpgKb5Evm82g0qsoTIM4ziqIk\nJIuOjY3htttuQ2trK+6///6C7FyU5nNdlIj6VgLxTh8YGKCymbGxMezYsQNHjhzB0aNHsX//flRV\nVdEvYr7JuyAIdJuxu7s7b0UYSuS9uJBK5qKtZEgSw4B40pjdbsddd92F6667Dp/85Ccz0l9mi3RV\n6AhIhv/Pf/7zvI+hiFEi6kUOQRBw6dIlmrB64cIFhEIh9PX1UfLe09MDURTp902v6NB6yPvKygpG\nRkZQWVmpuyNabCCynEgkgt7e3oSCUGzhIa/Xm6DhJoWHNoq8s245ROZSTBFoNk+AvWfRaBQVFRUY\nHx9He3s7fvWrX+GZZ57BI488gle/+tUFGUtpPk+KUmXSrQSO49DU1IQ3velNeNOb3gQgXr6ZRGp+\n8IMfYGJiAo2NjVTzfuDAAVRWVsLn88HtdtOEHdbnPR15Z+3Edu/ejZ6enrxOOHrEsETeNwfptOhs\nJcP5+Xma7FxZWYnf/e53ePTRR3H58mXU1NRgZGQEzzzzDN7xjnfkfZzpqtCVUEIxw2QyUUew973v\nfQBiEo+LFy9iYGAAP/nJT/DnP/8Z0WgUfX19dD7fsWMHBEHA7OwsRkZGVOSdEPh0kgqyIyrLMvbt\n25fVjuhmQJZlTExMYHZ2NqUsR6/wEKvhnpiYgN/vB8dxCeQ93zKU1dVVDA8Po7a2VlWQsJjAcRxs\nNhtsNhudrxVFwf79+yGKIn784x/jgQcewOLiIjo7O3H69Gl0dHSgpaUl72MpzefrQ4moFzE4jsOu\nXbvw5je/GW9+85sBxMi72+2mkffvf//7mJiYwM6dO2mk5tChQygvL08g73rZ9sTuqr6+HidOnNgw\n7WSJvG8sck0WPXbsGMxmM37605/i7rvvxi233IL3vve9UBSFJqUVAumq0AGxKM2pU6e2nWduCdsT\nZrM5oRBLNBrF//7v/+LcuXM4c+YMLly4gGg0ir1799L5vKGhAdFoFDMzMxgeHk5K3smO6OrqKrq7\nu4uyuJgW5PensbERJ06cyJrwGo1GGlgg0FYN9fl8AECrhma64NFDNBrFyMgIIpHIllgEkZ360dFR\ntLe3o6mpCT6fD/feey8cDgeefvpp9Pb2wuv14sKFCwVLLi7N5+vDtiDqpBKVHjLVRW0VcByH5uZm\nNDc346/+6q8AxL6M09PTOHfuHAYGBvCd73wHU1NTaG5uViWslpWVwe/3w+12IxAIQBRFWCwWtLa2\noq6ubtOjAiXynn9kQ9AVRaGFRHp6elBXV4eZmRnccccdMJlM+OlPf4qmpiZ6/GbbL5LiFyVsH1xJ\nczkQI+/Hjh3DsWPH6HORSISS99OnT+PChQsQRRF79+6lkXdC3t1uN7xeL6LRKCRJQn19vcpRrFgR\nCAQwPDwMo9GY9yqdyaqGEvKuXfCw5D1ZoEpRFExNTWFqaqqoiiylgl5l0WeffRb33XcfbrvtNjzx\nxBP0NVRWVhZM9pIpSvN5cmx5ot7f34/3v//9cDqdCW2Dg4MAgFOnTsHlcqX8EdjK4DgOLS0taGlp\nwVve8hYAcYcO4vP+rW99C263G3V1dRAEAVVVVbjnnntQU1MDn8+Hy5cvqyLv5JGPIhXrwVOPdmJp\naQkjIyO0CMM7bh3ftPFsJWRD0kmyaG1tLU6cOAGO4/D1r38d3/rWt3DvvffijW98YwFHmoh0VehI\n9KWE7YPSXB6DxWLB8ePHcfz4cfpcOBzGiy++SCWQL774IkRRRH19PVwuFz760Y/ita99LURRxMzM\nDJXNsBKQXKPI+YQoinC5XFhdXUVvb68qEl5I8DxPf9MIZFlGIBCAz+fD3NwcNVAgha3IfQsGgxga\nGkJNTU3RylxYsJVFe3t7UVNTg8nJSXzkIx9BbW0tXnjhBdTX12/omErz+fqw5Yn6qVOn0NmpT0iu\nZF0Uz/Noa2tDW1sb3vrWtwKIFQv40pe+hDe84Q0wGAz42Mc+BrfbjdbWVhw+fJhG3i0WC9023Ezy\nzha5OHToEE1aLEXeUyPbyqJOpxM+nw979+6F3W7HpUuXcOutt+IVr3gFfve7323K9u6NN96IgYEB\nAIDL5aKTOKlC53K54HK5sLy8jOXl5f+/vXMPavLO3vjzhnu4g4VFQGgQEFHAVKit2K5rrHacdqcO\njrs77W63LTq/ne1O7Voju62dUXdZKdNtO9ZpQ7ct49QONZ292NapxB2mF5ydhHgpWqEhKGId7pAL\n4Rbe3x/x/ZpAMNxyIZzPDCMkjPlC4OHJec95jl8bt8UCafnUhIaGoqioCEVFRQBs1cedO3ciKCgI\nzzzzDP73v//hvffeAwDk5uY6tM0MDQ3hxx9/hNFoBM/zDsbdU+ZdmLe6fv06UlNT530OajbYD6Iu\nXbqUndPevDc2NsJqtSI6OhqBgYHo6+ubVvSht7DfLFpYWAie5/H222/j448/RmVlJX72s5955Vyk\n53NjwRv1uzGdvqjFxIYNG/Cb3/zGYWue8OpbaJt599130dHRgdTUVJYLvHr1agQHB8NgMKC9vZ2t\nh7YfWJ1P8z6bJRdk3m3MdrNoVlYWhoaG8Oqrr6K+vh7Hjh1Dfn6+G096d6RSKTQaDVQqFWJiYpho\nb9q0CQ0NDSgpKQFge/HZ39/vtXMSnoG03BFhb0FeXh67TUhlunjxIruK2tjYCI7jsGrVKoe2meHh\nYWbeATj0vM/38OXAwACam5sRFRWFtWvX+sSGy6ngOA7h4eHo7+/HwMAAsrOzkZCQAIvFAqPRiN7e\nXly/fh0jIyMICwtzqLx7cxvt6OgomxkS2lzPnz+PvXv3QiaTob6+3qvbXEnP54ZfxDNu3rzZaZzP\n7t27sXv3bkilUqhUKtTW1tKgwjQYHx9Ha2srS5vRarXo7OzEsmXLmHlftWoVgoODWeyTYN7tB5xm\nKgz2mb+pqalITk6e96qLv5r3mQ6LNjU1ISAggG0WPXv2LA4cOIBnnnkGv/vd73z+8u4ix2/jGUnL\n5xchX/vChQtsZ8fly5chEomwevVqVnlPS0tjZnS+zLuQLz4yMuI0btEXMRgMaGpqQnR0NCQSyV17\n1oeGhmAwGBxibN1ZwJrqHBOHRc1mMw4fPoxLly7h2LFjWLlypdsen5gzFM/oqi+KcI5IJEJGRgYy\nMjKwc+dOADbzrtfroVarce7cORw9ehTd3d1IT09nS5pSU1MRFBSEgYEB3LhxY5J5v1vVQai6REZG\nurXq4m+V97kOi3Z2dmL//v0YGRnBZ599huTkZDeeliBmB2n57BAqxOvXr8f69esB3GnvEMy7QqHA\n5cuXERAQgLy8PFZ5T0xMhMViQXt7O0wmE4Dpmffpxi36EkJFenBwEDk5OS5fVNhHHyYmJgKwfV+H\nh4eZebdvHZ1JXPJ0sVgs+P7779mwaFBQEE6fPo2DBw/i97//Pd544w2fWAhFzK2gSlYAABPmSURB\nVB2/NOpC39NUfVHEzBGJRFi+fDmWL1+OX/7ylwBsgqzT6aDRaPD111/jzTffRE9PD+69915m3pct\nW4bAwMApzXtoaCja2towNDQ0LYF0B1OZXV838HMZFhWJRKiursY777yDgwcPsgQhgvAlSMvnH47j\nEBERgeLiYhQXFwOwmUyTyYTz58+joaEB77zzDq5cuYLAwEDk5+ezyrsz824/sDo0NITW1lYkJiai\nqKjI542ife98eno6VqxYMWsTzXEcQkNDERoaioSEBHa7YN6NRiNu3brFFhXazwqIxeJpP67wQqij\no4MNi966dQsvvfQSQkNDcebMGfbigfAPFnzri1KpRGlpKaqqqlif03333YeGhgYAtp4niUQCvV6P\nXbt2ueXx7xYZJtzvrsf3NaxWK3Q6HdvIp9Vq0dvbi4yMDCb2ubm5sFqtuHjxIsRiMYKCghwqNd7u\n95sKq9XqE4kzB/9vmG0yDA8PdxD8iVci7IdFV6xYgYiICDQ1NWHPnj0oKCjAoUOHEBkZ6aWvhJgl\nftn6QlruWwjmXavVshbIK1euIDg4GPn5+azynpKSgubmZhiNRgQHB0+6iurJbaEzwWg0oqmpCZGR\nkZBIJB7tnR8ZGWHm3WAwwGKxICAgwKF1NDw8fJJ5F4ZF77nnHqSnp4PnefzjH/9AdXU1ysvLsXXr\nVo99DcS8MC0tX/BG3Zu4WosrRIoJfZVxcXGLcpLZarWiubmZ9byfPXsWt27dwpo1a/Dwww8jPz8f\nubm54DiOCZevDet0dXWhpaUFSUlJSE1NdfjD48nKu30VfXx8HIODgw6CPzY2xsz7+Pg4Ojo6sGzZ\nMiQnJ2N4eBiVlZX473//i6NHjzrEv80nrgyP8HsDgBkyYkb4pVH3JqTl04PneRgMBpw/fx4ajQbn\nzp3DN998g9DQUMhkMhQWFmLNmjVISkqCxWKBwWBwqLz7gnkfHR1FS0sLTCYTsrOzfaZQMTo6ynTc\nYDBgcHAQAQEBrOLe19eHsbEx5OTkQCwW47vvvsOLL76I4uJivPrqqxCLxW45F+m5W6EedXczncgw\nuVyO2traRX25NiAgADk5OcjJyYHFYkFXVxc+//xzmM1mNgleUVEBg8GAzMxMVnm/9957AdiqCG1t\nbQ7mXXhzd0yWxWJhg5dr1qxx+mLBE33vzh5DJBIhIiLCoV2I53n09fXhhx9+wNjYGIKCgrBv3z50\ndHTg5s2b2LJlC5RKpVvWRAPTy7suLy/HyZMnUVFRQTFchE9AWj49OI5DdHQ0fvrTn2LDhg34z3/+\ng7/+9a/Yvn07Lly4ALVajddffx1NTU0ICwtDQUEBe0tMTITZbMaNGzdgNBqZfnnKvNuHFaSlpSE7\nO9uneueDgoIQFxfnkG40OjqKtrY2tLa2IiwsDO3t7di9ezeio6PR1dWFw4cP44knnnBbEYv03Dcg\noz4HXEWGSaVSSCQSxMbGoqqqytPH80mee+45h8vGK1euxK9//WsAtjaNpqYmqNVqfPnllygvL4fJ\nZMLy5ctZ2oxg3vv6+ibFZM2neR8fH8e1a9fQ1dWFzMzMGa/jni/zPtdh0Z6eHkRFRcFqteLJJ59E\ne3s7nn32WfzhD3/Atm3bZnweV7gyPEqlEoWFhQDgN9sliYUPafnMCQgIQF1dHTPXGzduxMaNGwHY\ntKi/v5+1zVRWVqK5uRnh4eEObTOCeW9ra4PJZHLINp9P824ymXD16lVERET4fESkgMViwdWrVxEc\nHIwHHngAwcHBGBgYQFhYGDZs2ICUlBScOXMG1dXVOH36tFvOQHruG5BRdyPCIFRZWRlKS0uZ2C9m\n7ia6gYGByM3NRW5uLp5++mkANvP+/fffQ6PR4IsvvsBf/vIXmM1mZGVlMbGXSCTged4h41YsFjtc\nap2Jee/u7oZOp8NPfvITNng5H8zUvM91WPTEiRN466238Morr6CkpMQj1SNXhketVgOwVWpUKhWJ\nO7EgIC13zlTayHEcYmNjsWnTJmzatAnAnat9Wq0WarUar732GpqbmxERETGp8m4ymSaZd0HLw8PD\np63JwoyOwWBAdna2w2ZSX8U+NSc7OxuxsbHo6OjA/v37MTY2hlOnTrEFTe6G9Nw3IKM+B1xFhikU\nCpSVlSEmJgYSiQRKpZJ+kGdIYGAgVq9ejdWrV+O3v/0tAJv4XrlyBWq1Gp999hkOHz4Ms9mM7Oxs\nZt4zMjJgtVrR29uLa9euYXR0lK2GFvreJ5p3IV+c4zgUFBR4ZEHETMy4M5xtFtXpdHjxxReRlZWF\nr7/+GtHR0fN02vkhPj6e9foqlUrqayS8Dmm5++E4DnFxcZDJZKx1SCiwNDQ0QKPR4MiRI/jhhx8Q\nGRnJWiDz8/ORkJAAs9mM69evT8u82+eLCwvdfKnNZSrsh0WFLbQffPABFAoFDh06hMcff9zLJ5wM\n6bn7IaM+B1ytxbVHGFIi5k5gYCDy8vKQl5eHZ599FoCtl+/y5cvQaDT497//jYMHD8JisTiY96Sk\nJFitVvT09KC1tZWZ98jISDb4lJWVtWAymru6uqDT6ZCamoqsrCyMjo6ioqICX3zxBd5880088MAD\nHj+TK8MTHx/PKpExMTFQq9Uk7ITXIS33DhzHIT4+Ho888ggeeeQRADaT3dPTw8z7559/Dp1Oh5iY\nGIfK+z333OPUvIeEhKCrq8vtOznmE/sc91WrViE8PBxXr17Fnj17IJVK8e2333olupj03Dcgoz4H\nXK3F3bdvHyoqKiCRSNDb2+uVSLHFMpEdFBTEBPy5554DYBO/xsZGaDQa/Otf/8KFCxcwPDyMFStW\nsGqNTqdDf38/8vLyEBgYiObmZofKe1RUlM8JvVD5F4lEkEqlCAkJwblz5yCXy1FSUoJvv/3Wa2d2\nZXhKSkqgVCrZbUJ/42zR6/XQ6/Wora1FYWEhYmJi8O677+LkyZNz+0KIRYUvaDlAeg7YzPuSJUuw\nZcsWbNmyBYDNvHd3d6OhoQFqtRqnTp1CS0sLYmNjmZZnZGTgn//8J9atW4eYmBiWTmOfHDaTthlP\nwPM8Ojs7odfrWY778PAwDh06hLq6Orz99tteHc4kPfcNKJ5xAeMqUgwAduzYwSayZTLZop/IHhkZ\nQWNjI86cOYOqqirwPI+EhARIJBJWec/Ozsbo6CiLPZzYNuMt8+5sWLSvrw8HDhzAzZs3cezYMZ/o\nm3WWdz0xDzsuLg5qtXrOa+BVKhVkMhmUSiVqampw8uRJKBQKf8+5pnhGP4T0fGbwPI+uri6o1Woc\nP34cX375JXJychAcHIyCggJIpVLk5eUhPj4eJpMJBoMBZrMZIpHIQcvFYrFXzLv9sGhmZiaCg4NR\nV1eHl19+GU899RSef/55BAZ6v5ZKeu5WKEfd35HL5di8eTNkMhlUKtWkKoxSqYRer6deSifs3bsX\nMpkMW7duxfDwML777jtoNBo0NDTg4sWLGBsbw8qVK1m1JjMzEyMjI5Pyyu2rNe4070ajEVevXmU9\nsiKRCJ9++ikqKyshl8vxq1/9akH0YLoLuVyOwsJCv60yToCMuh9Cej47dDod/va3v6G8vBxLlixB\nZ2cnNBoN1Go1tFotWltbsWTJErYtOz8/H7GxsQ7mXcgrt+95d5ee2g+LZmVlIS4uDt3d3fjzn/+M\ngYEBHD16FMuWLXPLYy8UFpGeU466v0MT2bOnsrKSvR8SEoK1a9c6LAAaHh7GpUuXoFar8fHHH+Pi\nxYuwWq3Izc1llXdhiVB3dzf0ej2sVuukgdW5mner1YqWlhYMDAwgJycHERERuHbtGv74xz8iOTkZ\ndXV1M46O9EdUKhXKysq8fQyCmDWk57Nj+fLleO+999jHiYmJ2LZtG4ufFfLThYV7NTU1uH79OhIS\nEhwq7zExMTAajWhtbXWbeR8YGEBTUxOWLFmCoqIicByHjz76CEePHsWBAwewffv2RV1wESA9d4SM\nup9DE9mzIyQkBIWFhaznjud5DA0N4dKlS9BoNDh+/DgaGxvB8zxyc3NZ5V0w78Im07mYd+H/SElJ\nQWZmJsbGxvDGG2/g008/xd///nc89NBD7vwW+Dx6vZ71Sur1ejb0Rz/nhL9Cej5zOI5DUlISHnvs\nMTz22GMAbHp+69YtZt5PnDiBtrY2JCYmskJMQUEBoqKiYDQaodfrHTaFztS8C9tQzWYzcnNzER4e\nDp1Ohz179mDFihX46quvfC6dy9OQnk8NGfUFDE1kew6O4xAWFob7778f999/PwCb2FssFlZ5//DD\nD9HY2AiO45ya987OTqfmPSoqyqEX0X5YVNiGqtFo8NJLL2Hbtm2or6932ya6hYTQ0yuVSnHkyBE2\n1EQ/48RChPTcc3Ach6VLl+Lxxx9nkYc8z+PHH39kbTMfffQR2trasHTpUuTn50MqlSI/Px8REREw\nmUwO5t2+EGNv3u2HRYVtqKOjozhy5AhOnz6Nt956C+vWrfPmt8JnID2fGjLqCxhPT2RPhaukAoGK\nigq/ulzLcRzEYjHWrVvHxFYw7xcuXIBGo8H777+PxsZGiEQirFq1ilVrUlJSMDQ0NMm8j4+Pw2Qy\nISMjA0lJSTAYDPjTn/6E5uZmHD9+HFlZWW77elw9j8L99kNF3oQEnPAnSM+9C8dxSE5ORnJyMn7+\n858DsOl5e3s7q7xXV1fj5s2bWLp0KSvEFBQUQCwWw2Qyoauri5l3sVgMg8GA0NBQFBQUICwsDPX1\n9di/fz927Njh9nQu0nP/gYz6AsZVpJhEIkFMTAyUSiV6enrcIqparRYAIJPJoNfrodVqnSYRqFQq\n1NbW+pWwO0Mw7w8++CAefPBBADaxHxwcxPnz59HQ0ICqqipcuXIFIpEIeXl5TMQ1Gg2eeuopxMfH\n49ChQ/jqq68wODiIjRs34sCBA0hKSnLbuV09j1qtFhKJhF12n+p5JghidpCe+x4cxyE1NRWpqal4\n4oknANiGQW/cuMGiIj/44APcvHkTKSkpKCgoQF5eHtRqNTIzM1FUVIT+/n52FXZsbAwvvPACHn30\nUQQEBLjt3KTn/gUZ9QWOs1fCQmyS/f3uerVaU1ODzZs3AwAkEglUKhX9wk+A4ziEh4ejuLgYxcXF\nAGzm3Ww2o76+HuXl5WhubkZ6ejr27NmDzMxMtLS0YP369SgtLYVer4dSqcSNGzfw5JNPuuWM03ke\n5XI5amtrHap9BEHMH6Tnvo9IJEJaWhrS0tKwfft2AHeSXE6cOIG9e/ciJSUFZ8+exalTpyAWixES\nEoIXXngB6enp0Gq1eOWVV1BdXY2wsDC3nJH03L8go07MCVdJBYDt1btMJptzxqo/wXEcIiIiMD4+\njl/84hcoLS0Fx3EwmUz45ptvoNPp8PzzzwMAHnroITz99NNuPY+r51EqlUIikSA2NhZVVVVuPQtB\nEN6B9Hx2COa9r68PdXV1yMrKwvj4OK5du4YTJ07g9ddfR1paGgAwA+1OSM/9CzLqhNsRBqSIyWzd\nutXh48jISDz66KNeOs3UCH2yZWVlKC0tZUJPEMTigvTcORzH4bXXXmMfi0QiSCQSvPzyy148lXNI\nzxcWZNSJOeEqqUCovhC+javnUaFQoKysjC1cUiqVft+fShCLDdJz/4D03L/w/N5cwq/YuXMn9Ho9\ngMlJBcJtSqUSCoUCvb29bMiF8C1cPY/2lJSUsIxbgiD8B9Jz/4D03L8go07MCWFAxVlSAWATAWHw\nyZlIEL6Bq+dx3759UCgU7I+0L8R5EQQxv5Ce+wek5/4Fx/P8TD5/Rp9MEJ7EVW6sQqEAALS0tNAg\nFLGQmY8d46TlhE9Dek4sAqal5VRRJ/wC+9xYQdztUalUkMlk2LVrF/R6PVQqlTeOSRAEQbiA9Jwg\n7kBGnfALampqWJ+dkBtrj72YSyQS1r9HEARB+Bak5wRxBzLqiwSlUgm5XM76CrVaLeRyuZdPNX+4\nyo3dtWsX68PTarVYu3atR8/nbu421KVUKqFSqVBRUeHBExEE4S5Iz/1Xz0nLiYmQUV8EKJVKlJSU\nQKvVssimmpoaZGRkePlknkdYlexP2/ZUKhV27Njh9D5Xl5AJglhYkJ7fwd/0nLSccAYZ9UVASUkJ\n+vv7odfr2VIDocfPX3CVGyugUqn8bvBIJpNNuazC1SVkgiAWFqTnd/A3PSctJ5xBRn2R8Mknn7BY\nLQAOIu8PTCc3VqFQsPSAxSJy01kJThDEwoL0fPHpOWn54oWM+iKhpaUFhYWFAGyXTv2p+gK4zo1V\nqVSQy+XIyMhAbGysW8/iqo+Q+gwJgpgLpOek58TiYaY56sQCheM4CYDdANS3/z3J87zCu6fyPziO\nkwKQ8Dyv5DhuFwANz/Pa6d4/h8et5Xl+s5PbjwCo5XlexXFcye3Hpr8oBLGAIT33DN7Qc9JyYiJU\nUV8k8Dyv53lezvO8EkAcgE+8fSY/ZScA4fqsHsDEUper++cFjuOEndA1AIRr4hIA/n+NmCD8HNJz\nj+F1PSctJ8ioLwI4jpNwHHfy9vsy2F710/5n9xADoNfu44lTUK7unzG3qytrb/8rcBYAhOrO7ee9\nfz6q9wRBeA/Sc4/iUT0nLSecEejtAxAeoRdAjd3lst3ePhAxf9yuqikn3Haf3ft0SZwg/AfScz+F\ntJxwBhn1RcDtaovS5ScS80E/bJeiAVu1ZeJovqv7CYIgpoT03KOQnhNeh1pfCGJ+cdpHSH2GBEEQ\nCw7Sc8LrkFEniHnkLn2E1GdIEASxgCA9J3wBimckCIIgCIIgCB+EKuoEQRAEQRAE4YOQUScIgiAI\ngiAIH4SMOkEQBEEQBEH4IP8POwhdb0rg8akAAAAASUVORK5CYII=\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x7f0f69a9ce10>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "evaluation_points = np.column_stack((field50.x, field50.y))\n",
    "model.load_weights(\"keras/best_baseline_weights.hdf5\")\n",
    "pred = model.predict(evaluation_points)\n",
    "plot_field_and_error(field50.x, field50.y, pred[:,0], field50.A, \"kerasBaselineFinal.pdf\",\n",
    "                     [r\"Keras model after $10^4$ iterations\", \"Deviation from the numerical solution\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Comparison of error norms"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Initialized weights and biases for MLP with 20 parameters.\n",
      "\n",
      "MLP structure:\n",
      "--------------\n",
      "Input features:    2\n",
      "Hidden layers:     1\n",
      "Neurons per layer: 5\n",
      "Number of weights: 20\n",
      "Loaded weights from file ./customFunctionLearningAdaptive/baselineMLPFinal/best_weights.npy\n",
      "Custom function approach: \n",
      "L2 norm:  5.203213057741802e-05\n",
      "Lmax norm:  0.04697858781034808\n",
      "Loaded weights from file ./pureNetworkLearningNoP/baselineMLPFinal/best_weights.npy\n",
      "Pure network approach: \n",
      "L2 norm:  0.0073110046045390155\n",
      "Lmax norm:  0.9996581595886426\n",
      "Keras network: \n",
      "L2 norm:  0.00010534950473501825\n",
      "Lmax norm:  0.11197767443802456\n"
     ]
    }
   ],
   "source": [
    "baselineMLP = SimpleMLP(name='baselineMLPFinal', number_of_inputs=2, neurons_per_layer=5, hidden_layers=1)\n",
    "customModel = CustomFunctionLearning(name=\"customFunctionLearningAdaptive\", mlp=baselineMLP, training_data=field50.A, beta=25.0, d_v=0.025)\n",
    "customModel.mlp.read_weights_from_disk(\"./customFunctionLearningAdaptive/baselineMLPFinal/best_weights.npy\")\n",
    "evaluation_points = np.column_stack((field100.x, field100.y))\n",
    "l2_norm = customModel.l2_norm(evaluation_points, field100.A)\n",
    "lmax_norm = customModel.lmax_norm(evaluation_points, field100.A)\n",
    "print(\"Custom function approach: \")\n",
    "print(\"L2 norm: \", l2_norm)\n",
    "print(\"Lmax norm: \", lmax_norm)\n",
    "\n",
    "pureNetworkModel = CustomFunctionLearning(name=\"customFunctionLearningAdaptive\", mlp=baselineMLP, training_data=field50.A, beta=25.0, d_v=0.025)\n",
    "pureNetworkModel.mlp.read_weights_from_disk(\"./pureNetworkLearningNoP/baselineMLPFinal/best_weights.npy\")\n",
    "l2_norm = pureNetworkModel.l2_norm(evaluation_points, field100.A)\n",
    "lmax_norm = pureNetworkModel.lmax_norm(evaluation_points, field100.A)\n",
    "print(\"Pure network approach: \")\n",
    "print(\"L2 norm: \", l2_norm)\n",
    "print(\"Lmax norm: \", lmax_norm)\n",
    "\n",
    "model.load_weights(\"keras/best_baseline_weights.hdf5\")\n",
    "pred = model.predict(evaluation_points)\n",
    "diff = pred[:, 0] - field100.A\n",
    "l2_norm = np.dot(diff, diff) / len(diff)\n",
    "lmax_norm = np.max(np.abs(diff))\n",
    "print(\"Keras network: \")\n",
    "print(\"L2 norm: \", l2_norm)\n",
    "print(\"Lmax norm: \", lmax_norm)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
